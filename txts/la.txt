David Poole 

A  M  0 'D  E R N  I N T  R  0 D  U, C T I  0  N 
4th edition 

Fourth edition 

David Poole 
Trent University 

� .. � CENGAGE 
•- Learning· 
Australia
• Brazil• Mexico• Singapore
• United l<ingdom 

•United States 

�,.,, # CENGAGE 
• • Learning· 

Algebra 

Linear 
A Modern Introduction, 
4th Edition 
David Poole 

Product Director: 

Liz Covello 

Product Team Manager: Richard Stratton 

Content Developer: 

Laura Wheel 

Product Assistant: 

Danielle 

Hallock 

Media Developer: 

Andrew Coppola 

Content Project Manager: Alison Eigel Zade 

Senior Art Director: 

Linda May 

Manufacturing 

Rights Acquisition 
Shah-Caldwell 

Planner: 

Doug Bertke 

Specialist: 

Service & Compositor: 

Shalice 

Production 

MPS Limited 

Text Designer: 

Cover & Interior 

Cover Designer: 

Leonard Massiglia 

Chris Miller 

Source/Getty 

design Image: Image 
Images 

© 2015, 2011, 2006 Cengage Learning 
WCN: 02-200-201 

taping, web 

including 

digitizing, 

transmitted, 

storage and retrieval 

under Section 107or108 of the 1976 

stored, or used in any form 
but not 

electronic, 
recording, 
networks, 

or mechanical, 
scanning, 
or information 

ALL RIGHTS RESERVED. No part of this work covered by the copyright 
herein may be reproduced, 
or by any means graphic, 
limited to photocopying, 
distribution, 
information 
systems, except as permitted 
United States Copyright Act, without the prior written permission 
the publisher. 

us at 
and technology assistance, contact 
For product 
information 
Customer & Sales Support, 
1-800-354-9706 
Cengage Learning 
this text or product, 
For permission 
to use material from 
submit all requests 
on line at www-cengage.com/permissions­
Further 
permissions 
questions 
can be emailed 
to 
permissionrequest@cengage.com. 

of 

Library of Congress Control Number: 2013944173 

ISBN-13: 978-1-285-46324-7 

ISBN-10: 1-285-46324-2 

Cengage Learning 

200 First Stamford Place, 4th Floor 
Stamford, 
USA 

CT 06902 

Cengage Learning is a leading provider of customized 
with office locations 
Kingdom, Australia, 

international.cengage.com/region. 

around the globe, including 
Mexico, Brazil and japan. Locate your local office at 

Singapore, 

solutions 

the United 

learning 

Cengage Learning products are represented 
Education, 
Ltd. 

in Canada by Nelson 

visit www.cengage.com. 

For your course and learning 

solutions, 

Purchase any of our products at your local college store or at our 
preferred 

on line store www.cengagebrain.com. 
Please visit login.cengage.com 

Instructors: 

and log in to access 

instructor-specific resources. 

Printed in the United States of America 
1 2 3 4 5 6 7 
17 16 15 14 13 

Dedicated 
of 
to the memory 
Hilton, 
Peter 
who was an 
exemplary 
mathematician, 
educator, 
unit 
and citizen-a 
sense. 
vector 
in every 

Contents 

Preface vii 
To the Instructor 
xvii 
To the Student 
xxiii 

Chapter 1 

Vectors 1 

of the Dot Product and Cross 

Product 

49 

1.3 

and Angle: 

and Algebra 

1.0 
1 . 1  
1.2 

Game 
of Vectors 

Introduction: The Racetrack 
3 
The Geometry 
The Dot Product 
Length 
and Geometry 
Exploration: 
Lines 
and Planes 34 
Exploration: 
Writing 
Project: 
Force Vectors 
50 

The Origins 
50 

The Cross 
1.4  Applications 

18 32 
48 

Product 

Vectors 

Chapter 

55 
Review 

Systems of Linear Equations 57 

An Introduction 
Independence 88 

to the 

Equations 58 
Systems 64 

ion 82 

y of Gaussian 
Eliminat

2.0 Introduction: Triviality 
57 
to Systems 
2.1 Introduction 
2.2 Direct Methods 
Project: 

for Solving 
A Histor

Writing 
Explorations: 
Pivotin

Lies My Computer Told Me 83 
g 84 

of Linear 
Linear 

Analysis of Algorithms 85 

Partial 
Counting Operations: 
2.3  Spanning Sets and Linear 
2.4 Applications 
Allocation 
of Resources 
Chemical 
Balancing 
Analysis 102 
Network 
Networks 104 
Electrical 
Linear 
Finite Linear 

Economic 

Models 107 

Equations 101 

Games 109 

99 

99 

The Global Positio
2.5 Iterative 
Methods for Solving 
Review 134 

Vignette: 
Chapter 

ning System 121 

Linear 

Systems 124 

Chapter 2 

iv 

Chapter 4 

Which Came First: The Matrix or the 

Determinant? 

Chapter 3 

Matrices 136 

Contents V 

3.0 
3.1 
3.2 
3.3 
3.4 
3.5 
3.6 

3.7 

in Action 136 
Introduction: Matrices 
138 
Matrix 
Operations 
Algebra 154 
Matrix 
The Inverse of a Matrix 163 
The LU Factorization 180 
Subspaces, 
Introduction 
to Linear 
Vignette: 
Applications 
230 
Chains 230 
Economic 

Robotics 226 

Basis, Dimension, and Rank 

Markov 
Linear 
Population 
Graphs 

Growth 239 
241 

and Digraphs 

Models 235 

Transformations 

Chapter Review 251 

Eigenvalues 

and Eigenvectors 253 

191 
2 1 1  

283 

4.0 
4.1 
4.2 

4.3 

4.4 
4.5 
4.6 

254 

of n X n Matrices 292 

s 286 

Method 284 

System 
on Graphs  253 
and Eigenvectors 

Project: 
Lewis Carroll's 
Condensation 
n: Geometric Applications of Determinant
and Eigenvectors 

Introduction: A Dynamical 
Introduction 
to Eigenvalues 
Determinants 263 
Writing 
Vignette: 
Exploratio
Eigenvalues 
s 301 
Writing 
Project: 
y of Eigenvalue
Similarity 
and Diagonalization 
Iterative 
Methods for Computing 
Applications 
and the Perron-Frobenius Theorem 325 
Chains 325 
Markov 
Population 
The Perron-Frobenius Theorem 332 
Linear 
Relations 
Systems of Linear Differential 
Discrete Linear 

Recurrence 

Eigenvalues 

Dynamical 

Equations 

The Histor

Systems 348 

Growth 330 

335 

340 

301 

311 

Vignette: 
Chapter Review 364 

Ranking Sports Teams and Searching the 

Internet 356 

on a Wall 366 

5.0 Introduction: Shadows 
5.1 Orthogonality 
in IR"  368 
5.2 Orthogonal 
5.3 The Gram-Schmidt Process 

Complements 

The Modified QR Factorization 396 

and Orthogonal 
and the 

s with the QR Algorithm 398 

Projections 378 
QR Factorization 388 

Approximating Eigenvalue

Explorations: 

Diagonalization 

of Symmetric 

Matrices 400 

5.4 Orthogonal 
408 
5.5 Applications 
Quadratic 
Forms 408 
Graphing Quadratic 

Chapter Review 425 

Equations 415 

Chapter 5 

Orthogonality 366 

Chapter 7 

Distance 

and Approximation 529 

Vi  Contents 
Chapter 6 

Chapter 8 

Vector Spaces 427 

6.0 
6.1 

6.2 

6.3 
6.4 
6.5 
6.6 

Basis, 

Space 427 

in (Vector) 

and Subspaces 429 

and Dimension 443 

The Rise of Vector Spaces 443 

Spaces 
Project: 
Independence, 

n: Magic Squares 460 

Introduction: Fibonacci 
Vector 
Writing 
Linear 
Exploratio
Change of Basis 463 
Linear 
The Kernel 
The Matrix 
Exploratio
518 

and Range of a Linear 
of a Linear 
n: Tilings, Lattices, 

481 
Transformation 
and the Crystallogra
Linear Differential 

Transformations 472 

497 

Homogeneous 
Chapter Review 527 

6.7 Applications 

518 
Equations 

Transformation 

phic Restriction 

515 

Geometry 529 

7.0 Introduction: Taxicab 
7.1  Inner 
Product Spaces 531 
Vectors 
Explorations: 
Geometric 
Inequalities 
7.2 Norms and Distance 
Approximation 
7.3 Least 
Squares 
7.4 The Singular 
Value Decomposition 590 

and Matrices 
Functions 

Entries 543 
and Optimization Problems 547 

with Complex 
552 
568 

Image Compression 607 

Vignette: Digital 
7.5 Applications 

610 

Approximation 

of Functions 

610 

Chapter Review 618 

Codes 

Online 
only 620 
The Codabar System 626 

8.1  Code Vectors 620 
Vignette: 
8.2 Error-Correcting 
8.3 Dual Codes 632 
Codes 639 
8.4 Linear 
8.5 The Minimum 

Distance 

Codes 627 

of a Code 644 

APPENDIX A 
APPENDIXB 
APPENDIXC 
APPENDIXD 
APPENDIX£ 

Notation 
Inducti
on B 1 
Numbers Cl 

Mathematical 
Mathematical 
Complex 
Polynomials D 1 
Technology 

Bytes Online only 

and Methods of Proof Al 

Answers 
Index II 

to Selected 

Odd-Numbered 

Exercises ANSI 

Preface 

1he last thing 
book is what to put first. 

one knows when writing 

a 

-Blaise 

Pascal 

Pensees, 1670 

into account the reality 

The fourth edition 
and features 
that users 
streamlined 
the text somewhat, 
exercises. 

of Linear Al
A Modern 
gebra: 
added numerous 

found to be strengths 

Introduction 
clarificati

preser
editions. However, 
I have 
ons, and freshened 
up the 

of the previous 

ves the approach 

I want students 

ous usefulne

and techniques 

tremend
cepts 
mathematics and in other 
of theoretical, 
applied, 

algebra 

to see linear 
ss. At the same time, 
algebra 
of linear 

that they 

as an exciting 
subject and to appreciate its 
I want to help them master 
the basic 
con -
both in 
will need in other courses, 
te the 
interplay 
that pervades the subject. 

to apprecia

mathematics 

disciplines. I also want students 

and numerical 

course 

for use 

in linear 

one-or two-semester 

my best to write the 

This book is designed 

algebra. First and foremost, 

in an introductory 
for students, and I have 
it is intended 
not only will find it readable 
but also 

book so that students 
three 
the first 
linear 

editions, I have taken 
algebra 
majors, 
to mathematics 

sequence 
tried 
will want to read it. As in 
that students taking introductory 
discipline
engineering, 
physics, 
hy, economics, 
geograp
taking 
the course 
balances 
theory 
and combines 

as an elective 
and applications, 
a traditional 

science, 
and education, 

of 
are likely 
there are apt to be majors 
from 
environmental 
science, 
as well as other students 

chemistry
psychology

to come from a variety 

in a conversational 
yet is fully rigorous, 
-centered 

s. In addition 

with concern 

, computer 

biology, 

is written 

, business, 

style 
for student
style. 

learning. 
In any class, there will be 

presentation 
as a universally 

There is no such thing 

ons; some who enjoy 

best learning 
and others 

indepen
learning 

dently 
and others 
algebraic 

who work best in groups; 
in a workshop 
some who are adept at 

manipulations, 

who thrive 

lectur

e-based 

who work well 

some students 
some who prefer 
doing explorati
calculations 
numerical 
intuition. 
geometric 
ways-algebraicall
find a path to follow. 
ers can 
tional, 
and applied 
all students 

y, geometricall
topics 

(with 

This book is compatible 
From a pedagogical 
point 

Study Group. 

and without 
In this edition, 

a compute
I continue 

y, numerically, 

I have also attempted 

r), and some who exhibit 

strong 
in a variety 

to present material 
oflearn­
and verbally-so that all types 
computa­

to present the theoretical, 
way. In doing so, it is my hope that 

yet integrated 
algebra. 
of the Linear 
Algebra 
is no doubt that for most students 

of linear 

of view, 

Curriculum 

there 

of 

with the recommendations 

will be exposed to the many sides 

in a flexible 

setting, 

Vii 

or to fulfill degree requirements. Accordin

gly, the book 

recommendations 
Curriculum 

24 (1993), 

For more on the 
of the Linear 
Study 
Group, 
Mathematics Journal 

41-46. 

Algebra 
see 1he College 

Viii  Preface 

See pages 49, 82, 
283, 301, 443 

abstraction. 
is essentially about vectors 

here. 
I have taken this approach 
I also 
and that students 
need to 
insight. 

to gain some geometric 

Moreover, 

to see how systems 

of linear 

equations 
arise 

from geometric problems. Matrices 
as coefficient 
oflinear 

equally 
transforma

of change 
projections, 

(linear 
both of which are best understood 

tions). This sets the 

naturally 

and as agents 

systems 

then arise 

on the cover of this book symbolizes 
a vector 
and 

understanding 

should 

precede 

computational 

early 

I have tried 

students 

setting) 
in order 

that geometric 

my conviction 

and orthogonal 

The dart that appears 

for eigenvectors 

precede 
algebra 

should 
that linear 

examples 
strongly 

concrete 
believe 
first (in a concrete 
see vectors 
introducing 
vectors 
allows 
naturally 
matrices 
stage 
geometric
ally. 
reflects 
techniques. 
to limit 
labeled 
as theorems 
Interesting 
results 
explorati
rations 
chapter 
esting 
an introductory 
proofs of theorems 
to avoid having 
in order 
the scope of this text:' The result is, 
with the applicati
I have not been stingy 
a single 
can be covered in 
course. 
to which linear 
range of problems 
material 
linear 
algebra 
ductory 
text. 
algebra 
linear 
algebra and 
presented 

ons. For example, 
(in Chapters 1 
on determinants. The essential 
material 

contained 
text. Wherever 

as self-contained 

on finite 
linear 

and 4). Unlike 

and coding 

I hope that instructors 

will be used 

the number of theorems 

in the text. For the most part, resu
either 
preceding 
work. 
that are not central to the book have been included 
as exercises 
only in explo­

in the text or summarize 

the cross product 

later 

lts 

or 

in an exploration. 

are all in Section 

most linear 
results 

of vectors 
algebra 

is discussed 
textbooks, 
this book has no 
4.2, with other 
inter­
comprehensi
ve for 
and accessi
elementary 
ble 
to say, "The proof of this result 
is beyond 
I hope, a work that is self-contained. 

The book is, however, 

possible, I have included 

However, 
algebra can 

ons: There are 
it is important 

many more in the book than 
that students 
I have included 

be applied. 

see the impressive 

some modern 
intro­

theory 

that is not normally 
impressi

found in an 
of 
orld applications 
are 

these applications 

There are also several 
one item of historical, 
"vignettes:' 
will enjoy teaching 

ve real-w
if not practical, 
interest; 

I hope 
from this book. More important, 
power, 

that students 
and tremendous 

book will come away with an appreciation 
of the beauty, 
of linear 
and that they will have fun along the way. 

using the 

utility 

algebra 

What's New in the Fou rth Edition 

Algebra: 

A Modern Introduction remain 
the 

ts, five writing 

and style 

structure 

of Linear 

Here is a summary 

and development 
provide 

The overall 
same in the fourth edition. 

cise sets. 
the history 
of the applications 

of what is new: 
to coding theory 

engage studen
These projects give students 

•  The applications 
•  To further 
•  There are over 200 new or 
•  I have made numerous small changes 
•  All existing 

several 
them their own 
them as 

accuracy of the exposition. Also, 
giving 
labeling 

there is now a full proof 
exercise. 

oflinear 
additional 

ments, 
form of a guided 

ancillaries 

theorem

definition boxes and a few results 
s. 

have been updated. 

online 

have been moved to the new 

Chapter 8. 
projects have been added to the exer­
to research and 

of 
a chance 
algebra. The explorations, vignettes, and many 
material 

for student 

projects. 

write 

about aspects 

revised exercises. In response to reviewers' 

com­

of the Cauchy-Schwarz 

Inequality 

in Chapter 1 in the 

in wording 

to improve  the 
clarity 

definitions 

have been made more explicit 

have been highlighted 

or 
by 
by 

Preface ix 

Features 

English

is a simple, direct, 

conversational 
style. As much 

Clear Writing Stvle 
The text is written 
tical 
used "mathema
all proofs that are given are fully rigorous, and Appendix A contains 
tion. However, 
an introduction 
to mathematical 
writing. 
Concrete examples 
by further 
examples 
again-is consisten

always 
ons. This flow-from 
t throughout 
the book. 

for those who wish to streamline 
precede 

almost 
and applicati

theorems, which are then followed 

on mathematica

as possible, I have 

notation 

to general 

n relying 

specific 

" rather tha

excessively 

and back 

their 
own 

l nota­

equations, manipulating 
linear 

vectors 
independence, 

ty, and projection are first discussed 

ly, spanning 
are given a concrete treatment in Chapter 2 prior to their 

notions 
5 and 7. Similar

of inner 

in linear 

of linear 

algebra 

difficulty 

sets, 
all of the key concepts 
them in full generality. 
2 and IR3 before the more general 

Kev concepts Introduced 
Early 
encounter 
Many students 
computational 
(solving 
systems 
ces) to the theoretical 
(spanning 
on). This book introduces 
dimensi
setting, 
concrete 
before revisiting 
product, 
length, 
orthogonali
of IR
concrete setting 
and orthogonal 
projection appear in Chapters 
linear 
alization 
and dimensi
matrix are introduced; 
treatment. 
for 2 X 2 matrices before their 
ter 4, all of the key concepts 
computational 
examples 
later 
intimidated 

n X n counterparts 

In Chapter 4, eigenvalues 

in Chapter 6. The fundamenta

in the book, students 

independence 

have had time to 

them. When these 

to support 

it is not until 

6 that these 

of linear 

algebra 

to vector 

by them. 

spaces 

when the 

of linear 
Vector 

course moves from the 
and matri­
subspaces, 
basis, 
and 
early, in a 
algebra 
concepts 
such as dot 
in Chapter 1 in the 
norm, 
sets and 
gener­
of subspace, 
basis, 
of a 

product, 

l concepts 

appear. By the beginning 

of Chap­

have been introduced, 

with concrete, 

ideas 

appear in full generality 

get used to them and, hence, 

are not so 

on appear first in Chapter 3 when the row, column, and null spaces 
are given a 
general 

ideas 

Chapter 
and eigenvectors 

are introduced 

and explored 

and linear 

that linear 

a comprehensi

geometric 
intuiti

on. Accordingl

that will appear repeatedly 

ty, projection, 
ve treatment of lines 

with the philosophy 

many concepts 

Emphasis on Vectors and Geometry 
In keeping 
book stresses 
it develops 
such as orthogonali
as is 
insight into the solution 
geometry, and visualiza
as matrix transforma
introduced 
tions 
tions 
before general linear 
transforma
values 
are introduced 
Theorem is given 
argument. The geometry 
material 
thogonal 
in the concrete 

first heuristica

of linear 
on eigenvalues 
and eigenvectors. In Chapter 
of subspaces, 
complements 
of IR3 before being 

of systems 

and the Gra

dynamical 

of linear 

setting 

tion is found throughout 

with "eigenpictures" 
as a visual 

is primarily about vectors, this 

algebra 
y, the first chapter is about vectors, and 

throughout 

the text. Concepts 

combination 

are all found in Chapter 1, 
in IR3 that provides 

and planes 

essential 

on vectors, 

the text. Linear 

equations. This emphasis 
transforma
geometric 

tions 
are 
examples, 
in Chapter 6. In Chapter 4, eigen­
aid. The proof of Perron's 

in Chapter 3, with many 
are covered 

lly and then formally, in both cases 

systems 

a geometric 

using 
reinforces 
5, orthogonal 

and summarizes 

the 
projections, or­
are all presented 
Chapter 7, to inner 

m-Schmidt Process 
to IR" and, in 

generalized 

X  Preface 

product spaces. The 
nature 
formally in Chapter 7 via a geometric 
text, 

value decomposition is also explained 

in­
argument. Of the more than 300 figures 
in the 
a geometric 
algebra. 

over 200 are devoted 

to fostering 

of the singular 

nding of linear 

understa

See pages 1, 136, 427, 529 
32, 286, 460, 515, 543, 547 
See pages 83, 84, 85, 396, 398 

See pages 

is a guided 

and linear 

Game" introduces 

individually 

matrix multiplication 

For example, 

"The Racetrack 

Exploralions 
The introduction 
to each chapter 
discover, 
dents 
are invited to 
chapter. 
introduces 
Space" touches on vector 
ized norms and distance 
book include 
of 3 X 3 
duction 
inequalities. 
erations 
rations 
"ownershi

and the analysis 
is one way of encouraging 
p" over a small part of the course. 

magic squares, a study of symmetry 
to complex 

applications 

and optimiza
There are also explorations 

space concepts, 
functions. Additional 
of vectors and determina

of algorithms. Having 

algebra, 

linear 

exploration 
or in groups, some aspect 

(Section O) in which stu­

of the upcoming 

transforma

vectors, "Matrices 
tions, "Fibonacci 
Geometry" 

in Action" 
in (Vector) 

sets up general­
found throughout 

explorations 

the 

and "Taxicab 

nts to geometry

, an investigation 

via the tilings 

of M. C. Escher, 

an intro­

tion problems 

using geometric 

that introduce important numerical 
consid­
explo­

students 

do some of these 
learners 

and to give them 

them to become active 

See pages 623, 641 
See pages 121, 226, 356, 607, 626 

es, including 

biology
among these 

selecti
mathematics, 
economics, 

APPlicalions 
The book contains an abundant 
of disciplin
neering, 
treatment 
worthy 
Standard 
codes (such as International 
Reed-Muller 
correcting 
codes (such as the 
photos 
ally, 
from space). Addition
very modern 
botics, Internet 

oflinear 
search engines, 

, business, 
is a strong 

applications 

digital 

on of applications 
computer 
psychology

science, 
chemistry
, engi­
, geography, and sociology. 
Note­

chosen from a broad range 
physics, 

of coding theory, from error-detecting 
Book Numbers) to sophisticated 
code that was used to transmit satellite 

error­

there 
are five "vignettes" 
algebra: the Global Positioning 
and the 

image compression, 

that briefly showcase some 
(GPS), 

System 
ro­
Codabar System. 

an introductory 
that students 

Examples and Exercises 
There are over 400 examples 
tomary in 
with the philosophy 
Accordingl
be assigned 
have at least 
the example 

y, it is not intended 
for individual 
one counterpart 
before exploring 

algebra 
should 
that all of these 

exercise 
generalizati
ons. 

in this book, most worked 
detail 
than is cus­
of detail 
linear 
is in 

This level 

in greater 

textbook. 
want (and be able) 

to read a textbook. 

keeping 

examples 

be covered 

in class; 

many can 

or group study, possibly as part of a project. Most examples 

so that students 

can try out the skills 

covered 

in 

at a similar 

There are over 2000 exercises, 

to most of the computational 

more than in most textbooks 
odd-numbered exercises 
of exercises 
are graduated, 

Answers 
back of the book. Instructors 
homework 
the routine 
tion to those requiring 
and numerical 
theoretical 
use actual 
exercises 
problems 
on modeling 

will find an abundance 
in each section 
range from those intended 
algebra 
Many of the examples 

progressing 
for hand computa­
and from 
system, 
and 
For example, 
there 
are 
and seal populations, 

to conceptual 
from real-w
of caribou 

level. 
can be found in the 
from which to select 

assignmen
to the challenging. 

ts. The exercises 
Exercises 

the use of a calculator 

orld situations. 

radiocarbon 

data compiled 

exercises 

or computer 

the growth 

exercises. 

dating 

from 

See pages 248, 359, 526, 588 

See page 34 

Preface Xi 

of the Stonehenge 
Working 
such problems 
eling 

real-lif
Additional 

e problems. 

monument, 

and predicting major league 

salaries

baseball players' 
is a valuable 

. 
tool for mod­

reinforces 

the fact that linear 

algebra 

exercises appear in the form of a review 
designed to 

questions 
and theoretical 

test conceptual 
exercises that summarize 

there 
are 10 true/false 
by 19 computational 
techniques 

of that chapter. 

understanding, 

followed 

the main concepts and 

after each chapter. 

In each set, 

nt that students 

Sketches and Etvmological 
Notes 
learn something 

Biographical 
It is importa
come to see it as a social 
short 
ingly, the text contains 
cians who 
to the development 
help to put a human face on the subject and give students 
the material. 

and 
one. Accord­
sketches about many of the mathemati­
biographical 
algebra. I hope that these 
will 
of linear 
to 

endeavor as well as a scientific 

contributed 

about the 

and cultural 

history 

another 

of mathematics 

way of relating 

I have found that many students 

feel alienated 

to them-it is simply 

a collection 

from mathematics 

because the 
of words to be learned. 
etymological notes that 

I have included short 
give 

of many of the terms used in linear 

algebra. (For example, why do we 

makes no sense 

terminology 
To help overcome 
the origins 
use the word normal 

this problem, 

to refer to a vector 

that is perpendicular 
to a plane?) 

icons 

several 

has had 

the reader 

of the book 

to calculus. 

ways. Calculus is 

whose purpose is to alert 

that requires calcul

contain 
not a prerequisite for this book, but linear 
and important applications 

Margin Icons 
The margins 
various 
interesting 
exercise 
us. Alternat
class 
signed 
an example 
numbers. (For students unfamiliar 
the background 
system 
bilities 
for solving 

that is needed.) The cAs icon indica

Mathematica, 
(such as Maple, 
any graphing 
(such as almost 
or exercise. 

if not everyone 
in the 
can be as­
this material 
complex 
C contains 
algebra 

involving 
all 
tes that a computer 
with matrix 
very useful­

or MATLAB) or a calculator 
capa­
calculator) 
is require

The� icon denotes 

in 
algebra 
or 

as projects.) The�  icon denotes 

ively, 
or exercise 

numbers, Appendix 

us. (This material 

one semester 

can be omitted 

with complex 

the example 

d-or at least 

material 

an example 

of calcul

at least 

has many 

In an effort to help students 

learn how to read 
places 

and use this textbook 
most ef­
is advised to pause. These 

where the reader 

fectively, I have noted various 
may be places where 
claim should 
in the margin 
Think about this:' 

be verified, 

a calculation 

is needed, 
or some extra thought 

part of a proof must be supplied, 
is required. 

The _.... icon appears 

a 

at such  places; 

the message  is "Slow 

down. Get out your pencil. 

fully whether or not students 

have access 

used success

Technology 
This book can be 
ogy. However, 
calcula
are now commonplace 
well as help with tedious calculations. In this text, 
dents 
examples 

that are not too computation

tors with matrix capabili

and, properly used, 

need to master 

all of the basic 

systems 
experience 

to technol­
algebra 
as 
of view that stu­
by solving 
by hand 
may then be used 

I take the point 
algebra 
Technology 

can enrich 

the learning 

ties and computer 

techniques 

of linear 

ally difficult. 

Xii  Preface 

in part) 

that rely on earlier 

(in whole or 
techniques 
are first introduced, 
given, and the reader is 
expected 
form of technology. 
tion impractical, 
depend 

use technology. 
on the use of technology. 

detailed 

to solve subsequent examples 

and applications 
and to apply 
equations 

of linear 

ones. For example, when systems 
later, soluti
solutions are provided; 
is a good place 

to verify them. This 

ons are simply 
to use some 

Likewise, when applications 

use data that make hand calcula­

All of the numerical methods 

that are discussed 

With the aid of technology, students 

can explore 

linear 

algebra 

in some exciting 

on technology, and in places 

cover much for themselves. For example, if one of the coefficients 

a particular 

system is replac
ed by a parameter
entry 

ways and dis
linear 
tions? How does changing a single 
is not a tutorial 
specified 
accompanies 
instructi
ematica, 
tions 
systems 

with the cAs icon. The website 

ons for solving 
and MATLAB. By imitating 
these 

and explorations 

this book offers an online 

a selection 

of a 
, how much variability 
of a matrix 

affect its eigenval

is there in the solu­
ues? This book 
I have not 

where technology can be used, 
The student 

from each chapter 
students 

companion website that 
appendix called Technology Bytes that gives 
Math­
can do further 
examples, 
calcula­
the power of these 
rly those marked 
code in Maple, 

CAS they have and exploit 
the book, particula

using 
to help with 
the exercises throughout 

whichever 

also contains data sets and computer 

type of technology. 

of examples 

using Maple, 

and MATLAB formats keyed 
and instructors 

to many exercises 
directly into their 

and examples 
in the 
CAS to save typing 

can import 

these 

Mathematica, 
text. Students 
and elimina
te errors. 

linear 

algebra 

finite fields;' 

early, I have been 

two aspects 
algebra 

linear Algebra 
of linear 
and numerical linear 
able to make finite 
although 
the book. This approach 

Finite and Numerical 
The text covers 
gether: 
finite 
arithmetic 
algebra over 
throughout 
in Chapter 8 (online
that students 
really 
linear 
fields, because they are likely 
abstract 

algebra, mathematics 

algebra, and number theory. 

that are scarcely 
algebra. 
algebra 

linear 

By intr

ever mentioned to­
oducing 

(more properly, "linear 

modular 

I do not use that phrase) 
provides 
to the material 

a recurring 
theme 
on coding theory 

access 

In addition 

). There is also an application 
enjoy. 

linear games 
of finite 
l on finite 
it in such courses as discrete mathematics, 

majors will benefit from seeing the materia
to encounter 

to being exposed to the 

applications 

in Section 2.4 

to finite 

All students 

should 

be aware that in practice, 

it is impossible to arrive at exact 

linear 

pivoting, 

in linear 

provide an indica

of numerical 

algebra will 

ons oflarge-scale problems 

algebra. Exposure to some of the tech­

and partial 
computing eigen

tion of how to obtain 
l topics included 
methods  for 
solving 

ons. Some of the numerica
iterative 

soluti
niques 
highly accurate approximate soluti
the book are roundoff error 
linear 
systems and 
norms and condition 
value decomposition. The inclusion 
some interesting 
algebra, such as 
of linear 
the convergence 
of iterati
but also shows how one might approach them. Gerschgorin disks, matrix norms, 
and the singular 
4 and 7, are useful in 
this regard. 

values, the LU and QR factorizations, matrix 
of numerical 
up 
that are complet

pivoting 
ve methods. This book not only raises these 

also brings 
ely absent from the theory 

squares approximation, and the singular 

and important issues 

the condition 

discussed 

numbers, least 

of a matrix, 

strategies, 

of a linear 

in Chapters 

algebra 

linear 

values 

system, and 

questions 

in 

124, 180, 311, 392, 

See pages 83, 84, 
555, 561, 568, 590 
See pages 319, 563, 600 

Preface Xiii 

of mathematical 
mathematical 

notation and methods 
All students 

of proof, 
will benefit from 

induction. 

mathematic

to them. Some of the examples in these 
Example 

major may wish to pay 
appendices 
B.6 in Appendix B) and underscore 

ally oriented 

are uncom­
the power of the 
familiar 
numbers. For students 
for others, 
this sec­
parts of the text that use com­

Appendix C is an introduction to complex 
results, this appendix can serve as a useful reference; 

an overview 

sections, 
attention 

Appendices 
Appendix A contains 
and Appendix B discusses 
these 
but those with  a 
particular 
mon (for instance, 
methods. 
with these 
tion contains 
plex numbers. Appendix D is about 
require 
Rule of Signs; 
Leslie 
book's website. 

a refresher 

matrices. 

about these 

everything 

Exercises 

polynomials. 

I have found that many students 
with Descartes's 

be unfamiliar 

it is used in Chapter 4 to explain 

facts. Most students will 
of 
can be found 

to accompany the four app

the behavior of the eigenvalues 

endices 

on the 

they need to know for those 

Short answers 

to most of the odd-numbered computational 

exercises are given at 

the end of the 
able on the companion 

book. Exerc

website, 

along 

ise sets to accompany Appendixes A, B, C, and 
Dare avail­

with their 

odd-numbered answers. 

Ancillaries 

�ebAssign 

Access 
Access 

5829-6 
5827-2 
from Cengage Learning, 

For 1ns1ruc1ors 
Enhanced Web Assign® 
Card: 978-1-285-8
Printed 
Online 
Code: 
978-1-285-8
Exclusively 
mathematics 
work solution, 
feedback, 
helping 
matter. Flexible assignment 
condition
cengage.com/ewa 

students 
zable 
nding  of 
conceptual understa
their 
give instructors 
assignments 
to release 
ally based on students' 
prerequisite 

the most powerful online 
home­
with immedia
engages 
te 
eBooks (YouBook), 

content that you know and love with 

rich tutorial 
students 

and interacti
a deeper 

us at www. 

scores. Visit 

ve, fully customi

WebAssign. 

the exceptional 

the ability 

Enhanced 

Enhanced 

combines 

to develop 

assignment 

content, 

WebAssign 

WebAssign 

options 

subject 

to learn 

more. 

Learning Testing 
Testing 
and manage test 

Powered 
Powered by Cognero is a flexible, 
online 

by Cognero 

edit, 

bank content from multiple 
in an instant; 

and deliver 
tests 

Cengage 
Cengage Learning 
you to author, 
solutions; 
your classroom, or 

create multiple 

test versions 

wherever 
you want. 

that allows 

system 
Cengage Learning 

from your LMS, 

Manual 

Solutions 

Complete 
The Complete 
Solutions Manual 
text, including 
Exploration 
Manual is 
online. 

available 

provides 

detailed 

solutions 
exercises. 

and Chapter Review 

The Complete 

Solutions 

to all exercises in the 

Instructor's Guide 
This online 
guide enhances 
work projects, teaching 

the text with valuable 
tips, interesting 

teaching 
exam questions, 

resources 
such as group 
examples 
and extra 

xiv  Preface 

es, and other 

and make linear 

items designed 

material for lectur
tion time 
each section of the text, 
sis, points to stress, questions 
nology 
suggested 

class 
algebra 
the Instruct
or's Guide includes 
solutions, 

the instructor'
ve experience. 
For 
time and empha­
e materials and examples, tech­
and 

projects, group work with 

for discussion, 

and interacti

suggested 

an exciting 

tips, student 

test questions. 

to reduce 

lectur

s prepara­

sample assignments, 

provides 

Solution 
Builder 
www.cengage.com/solutionbuilder 
Solution 
Builder 
ing those in the explorations 
Solution Builder 
solutions matched exactly 
*Access 

exercises 
Cognero and additional 
instructor 

instructors 
to the 

full instructor 
solutions 

to create 

assigned 
for class. 
resources 
online 

and chapter reviews, 

allows 

to all exercises in the 
in a convenient 

includ­
format. 
secure PDF printouts 

text, 
online 
of 

customized, 

at login.cengage.com. 

Manual (ISBN-13: 978-1-285-8

For Students 
Solutions 
Student 
The Student 
Solutions 
numbered exercises 
summaries 
exercises 
understanding. 
exercises 

through 
Challenging 

are explored 

of symbols, definitions, 

Manual 
and selected 

are also included. 

and Study Guide includes 

detailed 

solutions to all odd­
section and chapter 

even-numbered exercises; 
and theorems; 
a question-and-answer 

and study 

and entertaining 

problems that 

further 

format designed 
explore 

to deepen 
selected 

tips and hints. Complex 

4195-3) 

Enhanced Web Assign® 
5829-6 
Card: 978-1-285-8
Printed 
Access 
5827-2 
Online 
978-1-285-8
Code: 
Access 
by the 
Enhanced 
(assigned 
on homework assignments. 
This online 
to textbook sections, 
helpful 

Web Assign 

links 

instructor) 
provides 
homework system 

feedback 
you with instant 
is easy to use and 
includes 
tutorials. 

and problem-specific 

video examples, 

� 

WebAssign 

additional 
course 

CengageBrain.com 
To access 
.com. 
cengagebrain
At the CengageBrain.com home page, 
back cover of your book) using the 
title 
This will take you 

materials and companion 

resources, 
please 
search for the 

visit www. 
ISBN of your 
search box at the top of the page. 

product page where free companion resources 

can be found. 

(from the 

to the 

and often 
about the book. I am grateful for the time each of them took to do 

of this text contributed valuable 

edition 

and helpful 

of the previous 

Acknowledgments 
The reviewers 
sightful 
comments 
this. Their judgement 
opment and success 
Jamey Bass, 
City College 
Clark, The College 
Christopher 
University, Stanislaus; 
Henry Krieger

Francisco, 

the devel­
to thank them personally: 

of this book, and I would like 
co; Olga Brezhneva, 

of San Francis
Marek Elzanowski, 
University; 
of New Jersey; 
Oklahoma 
State University; Brian Jue, California 
Alexander  Kheyfits, Bronx Community College/CUNY; 
Michigan 

Pearlstein, 

Miami University; Karen 

Portland 
State 

Rosanna 

, Harvey Mudd College; 

State 

State 

in­

suggestions 

have contributed greatly to 

See page 284 
See page 286 

See pages 325, 330 

See page 348 
See page 366 

See pages 396, 398 
See pages 408, 415 
See page 427 

xix 

To the Instructor 

ential material students 
need, 

including 
Expansion Theorem. The vignette 

Condensation Method" presents a historica

lly interesting, 

nts that students 
of Determina

may find appealing. 

nts" makes a nice project that contains 

alternative 
The explo­

nts may choose 
of eigenvalues 
and eigenve
of diagonaliza
in class. The power method 

instructors 

who wish to give 
to cover some of this exploration 
ctors 

is found in Section 

4.3, and 

tion. Example 

4.29 on powers 

and its variants, discussed 
be aware of the method, 

and an 

ively, 

Laplace 

all the ess

determina

proof of the 

nts contains 

of calculating 

l but elementary 

4.4 deals with the important topic 

is worth covering 
4.5, are optiona

useful results. (Alternat
to determina

"Geometric 
Applications 
interesting and 

course" in determina
an optiona
"Lewis Carroll's 
method 
ration 
several 
more detailed 
coverage 
in class.) The basic theory 
Section 
of matrices 
in Section 
applied 
indepen
ulation growth 
optional, 
least be mentioned 
a correspond
value with 
relations 
on recurrence 
tions 
and calculus, 
respecti
mathematics 
vely. 
crete 
background. 
has a good calculus 
if your class 
many of the ideas 
revisits 
ical systems 
and summarizes 
in a new, geometric 
Students will 
enjoy reading 
light. 
help rank sports teams and websites. This vignette 
or enrichment activity. 

l, but all 
should cover 
of the rest of Section 
reappear in Section 

4.5. Markov 
4.6. Although 

course 
dently 

because it explains 

the theorem itself 

ing positive eigenve

it in detail. 

students 

should 

(like 

Gerschgorin's Disk Theorem can be covered 

chains 

and the Leslie 

model of pop­

the proof of Perron's Theorem is 
-Frobenius Theorem) 
should 
positive eigen­
applications. The applica­

the stronger Perron
why we should expect a unique 
linear 

ctor in these 
equations 
and differential 
The matrix exponential 
The final topic 

algebra 
to dis­
can be covered 
linear 
dynam -
in Chapter 4, looking at them 

connect 

of discrete 

at 

how eigenvectors 

can be used to 

can easily be extended to a project 

Chapter 5: onhouonalilv 

onto another vector) 
and generalizes 
subspace-a plane), while uncovering 

it takes 
is mathematics at its best: 
it in a 
some 
results about or­
from here on. 

5.1 contains 

the basic 

sets of vectors that will 
matrices 

be stressed. In Section 

be used repeatedly 

should 

5.2, two concepts 

of a vector 

projection 

"Shadows on a Wall;' 
of a vector 
onto a 
unobserved properties. Section 
and orthonormal 

The introductory 
exploration, 
a known concept (projection 
useful way (projection 
previously 
thogonal 
In particular, orthogonal 
from Chapter 1 are generalized: 
orthogonal 
Theorem is important 
the quick 
Section 
rations 
it can be used to approximate 
eigenval
of (real) symmetric 
is needed 
the Spectral Theorem, 
the highlights 
of the 
cations 
include 
ready 

5.3, along with the extremely imp
that follow outline 

of a vector 
here and helps 

matrices 
one of 

in Section 
at least 

5.5 are quadratic 

know about conic 

Rank Theorem. 

proof of the 

the second 

how the 

of these 

sections. 

the orthogonal 
onto a subspace. The Orthogonal 

complemen

t of a subspace 
and the 
Decomposition 
note 

to set up the Gram-Schmidt Process. Also 

The Gram-Schmidt Process 

is detailed 

ortant QR factoriza
QR factorization 

tion. 
is computed 
in practice 

and how 

in 
The two explo­

ues. Section 5.4 on orthogonal 
for the applications 
that follow. 
oflinear 

theory 

forms and graphing quadratic 
in my course 

because it extends 

algebra. The 
appli­
equations. I always 
what students 

diagonalization 
It also contains 

al­

Chapter 6: vector Spaces 

The Fibonacci 
students 

sequence reappears in Section 

6.0, although 

it is not important that 

have seen it before (Section 

4.6). 

The purpose of this exploration 

is to show 

Preface XV 

Sullivan

, Portland 

State University; Matthias 

Weber, 

Indiana 

University; William 
University. 

I am indebted 

views about linear 
like to thank collecti
sessions 
at meetings 
Mathematical 
Mathematics Education 
Forum. 

and the teaching 

to a great many people who have, over the 
my 
algebra 
of mathematics 
in general. 
and specia
tion of America 

vely the participants in the education 
of the Mathematical 

years, influenced 
First, I would 
l linear algebra 

Associa
much from 

participation 

and the Canadian 
in the Canadian 

Society. I have also learned 

Study Group and the Canadian Mathematics Education 

I especia

lly want to thank Ed Barbeau, Bill Higginson, Richard 

Peter 

Hoshino, 

John 
Taylor, 
greatly to the 
who devel­

Bill Ral

Orzech, 

Morris 

Eric Muller, 

ph, Pat Rogers, 

tion contributed 
as well to Robert 
as well as the excellent 

Whiteley, whose advice and inspira
of this book. My gratitude 
instructor 
solutions, 
for his 
Jim Stewart 

Grant McLaughlin, 
and Walter 
philosophy and style 
oped the student and 
Special thanks go to 
his lovely 
book A First 
this book, and I relied 
when compiling these 
system 
colleagues 
Marcus 
namical systems. As always, 
and providing 
me with the feed

ongoing 
Course in Abstract Algebra 
heavily on Steven 
notes. I thank Art Benjamin for introducing 
elimina
My 

support and advice. Joe Rotman and 
l notes 
in 
Schwartzman's The Words of Mathematics 
of Gaussian 
tion. 
of the history 
useful information about dy­
good que

for asking 
a better teacher
. 
ely thank all of the people who have been involved 

Rogers, 
study guide 
. 

I am grateful to my students 

and Reem Yassawi provided 

and Joe Grear for clarifying 

back necessary to becoming 

inspired the 
etymologica

aspects 

Pivato 

I sincer

content

me to the Codabar 

stions 

Kumar and the team 

I thank Christine 
to work with 

this book. Jitendra 
ing the fourth edition. 
of all, it has been a delight 
tion teams at Cengage Learning: 
Ashton, Danielle Hallock, 
Andrew 
offered 
it, but let me write 
them, as well as the 

about changes 
the book I wanted 
staffs on the 

for doing a thorough copyedit. 

at MPS Limited did an amazing 
Sabooni 
the entire editorial, 
marketing, and produc­
Stratton, Molly Taylor, Laura Wheel, 
They 
Zade, and Janay Pryor. 
Eigel 
provided assistance 
to have worked 

I am fortunate 
first through third 
editions. 

Alison 
Coppola, 
and additions, 

sound advice 

to write. 

Richard 

when I needed 

Most 

with 

Cynthia 

in the production 
job produc­

of 

As always, 

I thank my 

family for their 

love, 

support, and understanding. 

Without 

them, this 

book would not have been possible. 

David Poole 

dpoole@trentu
.ca 

To the 
Instructor 

, 

"Would you tell me, please
which way I ought 
"That depends a good deal on where 
you want to get 

to," said the Cat. 

to go from here?" 

Alice's 

Carroll 

-Lewis 
Adventures in 

Wonderland, 1865 

with flexibility 
with 36 lectures 

in mind. It is intended 
per semester
of audiences and types of courses. However, 

. The range of topics 

for use in a one-or 

and applica­

makes it suitable for a variety 

in the book than can be  covered 

in cla

ss, even in a two­

course 

This text was written 
two-semester 
tions 
there is more material 
semester 
for ways to use the book. 

course. After the following 

overview of the text are some brief 

suggestions 

An overview of the Text 

1: vec1ors 

game in Section 

a lot of fun to play!) Vectors 

points of view. 
are first developed 

Chanler 
The racetrack 
quite 
geometric 
properties 
ized to !Rn. Modular 
defines 
nality. The very important 
reappear 
tor methods 
basic 
for understanding 
ter 2. Note that 
concludes 
with an 

the dot product 

but thorough 

in Chapters 

the cross 

1.0 serves 

to introduce 

vectors 
in an 
introduced 
are then formally 
and their 
and scalar multiplication 
of!R2 and IR3 before being 
general­

way. (It's 
also 
and 

from both algebraic 

informal 

arithmetic 

The operations 
in the concrete 
and finite 
linear 

of addition 
settings 
algebra 
and the related 
notions 
of (orthogonal) 

of vectors 
concept 

are also introduced. 
oflength, 
angle, 

and orthogo­

Section 1.2 

projection 

5 and 7. The exploration 
"Vectors 

can be used to prove certain 
to lines 
introduction 
significance 
the geometric 
of vectors 

results 
and planes 

is developed 
here; 
shows how vec­
and Geometry" 
Section 1.3 is a 
in Euclidean 
geometry. 
in IR2 and IR3. This 
section is crucial 
systems in 
of linear 
exploration. 
The chapter 

of the solution 
in IR3 is left as an 

product 

it will 

Chap­

application 

to force vectors. 

Chapter 2: svstems of linear Equations 

The introduction 
think 

to this chapter ser
of the solution to a system 

ves to illustrate 

that there is 

oflinear 

equations. Sections 

more than one 
the 
2.1 and 2.2 develop 

way to 

xvii 

See page 1 
See page 32 
See page 48 
See page 57 

xviii  To the Instructor 
See pages 72, 205, 386, 
486 
See page 121 
See pages 83, 84, 85 

See page 136 
See pages 172, 206, 296, 512, 605 

See page 226 
See pages 230, 239 
See page 253 

6. Section 

first time; 

(Gaus­
in 

tool for solving 

3, 5, and 

on this. The 

in more generality, 

systems: row reduction 
of matrices 
all subsequent computational methods 
it shows 

linear 
Gauss-Jordan elimination
). Nearly 

main computational 
sian and 
Rank Theorem appears here for the 
the book depend 
in Chapters 
up again, 
notions 
introduces the fundamental 
this material. 
tors. Do not rush through 
which instructors 
depending 
class. The vignette 
that students 
will enjoy. 
courses 
plorations 
in this cha
to solve 
computers 
issues

can choose 
on the 

but are essential for a course 

provides 
2.5 will be optional 
methods 
for many 
ex­
with an applied/numerical 
focus. The three 
in that they all deal with aspects 
of the 
use of 

of vec­
from 
Section 
on the time available 
of the 

independence 
six applications 
and the interests 

System 
in Section 

of spanning sets and linear 

be made aware of these 

Global Positioning 

The iterative 

pter are related 

2.3 is very important; it 

2.4 contains 

All students 

another 

systems. 

linear 

should 

at least 

. 

application 

Chanler 3: Malrices 

in the book. It is a long 

quickly, with extra time allowed 
that introduces 

fairly 
3.0 is an exploration 
that matrices 

other 

vectors into 

objects 
basic 

are not just static 
All of the 
vectors. 
found in the first two 
properties are 
representations 
in subsequent sections. 
3.3 is very 
of invertibility are 

because it is used repeatedly 
in Section 
tions 

Matrices 
characteriza

and the multiple 

matrices 

important 

of the 

s, it is worth assigning 
3.5 is to present many of the key concepts 

pre­
of a matrix. 
the very important LU factorization 
If this 
as a project or discussing 
in a work­
of linear 
before stu -
of matrices 
are all famil­

and rank) in the concr
ty. Although 

ete setting 
the examples 

in this section 

algebra 

tion: 

Section 

and their 

3.5. Section 

but the early 

l material 

can be covered 

more times as new 

3.4 discusses 

Theorem of Invertible 

the idea 
transforming 
operations, 

some of the most important ideas 
material 
in Section 
transforma

This chapter contains 
chapter, 
for the crucia
of a linear 
the notion 
a type of function, 
but rather 
matrix 
matrices, 
facts about 
sections. The materia
l on partitioned 
matrix product is worth stressing, 
The Fundamental 
and will appear several 
sented. 
topic 
shop. 
(subspace, 
dents 
that students 
iar, it is important 
what the notion 
understand 
transforma
tions 
transformations 
because it will reappear in Chapter 5. The vignette 
demonstration 
applications 
model of population 
Chapter 

growth 
behavior will be explained. 

in Section 
in Chapter 6. The example 

in clas
is not covered 
The point of Section 

from which to choose in Section 

see them in full generali

of composition of linear 

dimension, 

3.6 is intended 

4, where their 

of a basis 

basis, 

should 

Chanler 4: Eigenvalues 

and Eigenveclors 

get used to the new terminology 

means. The geometric 
to smooth the transi
of a projection 

is particu
on robotic 
(and affine) transforma

and, in particular, 
t of linear 
treatmen
tion to gene
ral linear 
important 
arms is a concrete 
tions. 
There are four 
chains 

or the Leslie 

3.7. Either 

Markov 

larly 

be covered so that they can be used again in 

an interesting 
dynamical 

Section 4.0 presents 
introduces 
4.5. In keeping 

The introduction 
graphs. This exploration 
power method 
in Section 
4.1 contains 
Section 
eigenve
ctors 
use in finding 
their 

of 2  X  2 matrices. 

the novel feature 

with the geometric 

of an eigenve

emphasis 

ctor and foreshadows the 
of the book, 
the 
appear in Section 4.2, motivated by 
. This "crash 

of small matrices

the characteristic 

polynomials 

Determinants 

the notion 

system 

of "eigenpictures" 

as a way of visualizing 

involving 

XX  To the Instructor 

See page 515 

See page 529 
See page 543 
See page 547 

See page 607 

space 

how to 

axioms 

vector 

have alread

be on using 

1-3, students 

space conce

use the notation 

how the construc­

results on matrices 

familiar. The emphasis 

to show students 

should 
the vector 

rather than rely­
6.3, it 

the Gauss-Jordan 
transforma
tions 

properties 
of basis in Section 

pts (Section 
of vector 
find Sections 

method 
are important. The examples 

3.5) can be used fruitfully 
in a new 
spaces 
y been introduced in 
all of the main ideas 
6.1and6.2 fairly 
to prove 
change 
computational techniques. When discussing 
to remember 
is the most efficient here. 
Sec­
to 
(and matrix transformations). In particular, it is impor­
and range of a linear 

that familiar 
setting. 
Because 
Chapters 
here should 
ing on 
is helpful 
tion works. Ultimately, 
tions 
6.4 and 6.5 on linear 
previous 
tant to stress 
space 
linear 
information in Section 
should 
the examples  should be 
and similarity 
Crystallo
graphic 
with the artwork 
nection 
cations 
in Section 
permit. 

However, 
of basis 
between change 
"Tilings, Lattices, 
is notewor
and the 
Restriction
" is an impressive application 
of change 
of basis. 
The con­
The appli­
Escher makes it all the more interesting. 
and interest 

are essentially matrix transforma
3.6, so students 
worked 

the null 
all 
that (almost) 
This  builds 
on the 

and column 
a matrix. 
transformations 

6.6 puts forth the 
tions. 

transformation 
notion 

carefully. The connection 

it terribly surprising. 

ones and can be included 

thy. The exploration 

generalize 

6.7 build 

that the kernel 

are related 

of matrices 

space of 

on previous 

as time 

not find 

ofM. C. 

Section 

Chapter 1: Distance 

and Approximation 

concepts 

of dot product, 

lization ca
exploration, 

"Geometric 

n be ex­

typically enjoy. (They 

Geometry" exploration. Its 
norms and distance 

functions 

the axioms. The exploration 

in Section 

7 . 1 ;  the em­
"Vec­

l diagona

7.2 covers 

l matrix, 

and orthogona

shows how the 

and Optimization 

are discussed 

generalized 

is one that students 

entertaining 
"Taxicab 

spaces. The following 

here should 
Matrices 

fun seeing how 
at all!) Section 

Problems:' 
many "calculus" 

that follows. Inner product spaces 
be on the examples 
and using 
with Complex Entries" 
orthogona

Section 7 .0 opens with  the 
purpose is to set up the material on generalized 
(metrics) 
phasis 
tors and 
symmetric matrix, 
tended 
from real to complex vector 
Inequalities 
will have 
calculus 
how the condition 
linear 
explored 
an important application 
proximation 
Theorem and  the 
proofs are intuitively 
Section 
applications 
tie together 
warded. Not only does the SVD 
also affords some new (and quite 
powerful) 
vignette on digital 
algebra 
sive display of the 
The further 
7.5 can be chosen 
able and the interests 
of the 

image compression 
power of linear 
in Section 
class. 

squares 
in Chapter 2. Least 
of linear 
in many other 

problems 
vector 
is related 

to the notion 
approxi

7.4 presents the singular 

clear. Spend 

algebra. If your course 

number of a matrix 

and a fitting 

is worth 

applications 

algebra 

systems 

of linear 

mation 

disciplin

Least Squares Theorem are important, but their 
time here on the examples-a few should 
one of the most impressi
value decomposition, 
ve 
far, you will be amply re­
it 

gets  this 

suffice. 

many notions discussed 
applications. 
presenting; 

previously; 
ble, the 

If a CAS is availa

it is a visually impres­
culmination 
to the course. 
according to the 
time avail­

can be solved without using 
and matrix 
norms and shows 
of ill-conditioned 

(Section 
es. The Best Ap­

7.3) is 

Chapter 8: Codes 

This online 
Section 8.1 begins 

contains 
with a discussion 

applications 
of linear 

chapter 

of how vectors 

to the theory 
of codes. 
can be used to design 

algebra 

See page 626 

XXi 

To the Instructor 

with matrix operations, 

to correct as well as detect 

Section 8.2 de­
errors

. The 

codes such as the familiar 

Book Number (ISBN). This topic 
on the Codabar system 
presentation 

Universal Product Code (UPC) and 
knowl­
used in credit and bank 

only requires 

that can even be used to introduce 

correcting 

Standard 

is an excellent 

8 . 1 .  Once students 
are familiar 
how codes can be designed 

detecting 
tional 

codes introduced 
Dual codes, 

error-
Interna
edge of Chapter 1 .  The vignette 
cards 
classroom 
Section 
scribes 
Hamming 
error-
way of constructing 
ment, introduced 
important, 
defined in 
here. 
examples of linear 
the definition 
mining 
the error-

and most widely used, 
Section 8.4. The notions 

The powerful Reed-Muller 

of the minimum 

correcting 

capability 

codes. 

codes. 

here are perhaps the most famous examples of such 
in Section 8.3, are an important 

discussed 

class of 

of orthogona

new codes from old ones. The notion 
concept here. 
in Chapter 5, is the prerequisite 
class 
of linear 
basis, 

of subspace, 
codes used 

codes is the 

that is 
and dimension 
are key 
by NASA spacecraft are important 
of codes 
8.5 with 
of a code and the role it plays in deter­

concludes in 

Our discussion 

l comple­
The most 

distance 

Section 

codes 

of the code. 

How to use the B o o k  

s o  I usually have them read 

a section before I 

find the 

book easy to read, 
in class. That way, I can 
spend class 
students 

Students 
cover the material 
important concepts, dealing 
and discussing 
applications. 
assigned 
fairly brisk, slowing  down 
challen

reading 

ging. 

the most 
with topics 
examples, 
I do not attempt to cover all of the material 
from the 

time highlighting 

find difficult, 

working 

for those sections  tha

t students 

typically find 

in class. This approach enables 

me to keep the pace of the course 

is possible to 

In a two-semester 

course, it 
applications. 
For extra 
y a brief 

sonable selection of 
(for example, give onl
topics 
freeing 
up time for 
tions, 
or some of the explorations. 
proofs, much of the 
then be covered in conjunction 
tegrated 
6, and 

more in-depth coverage 
In an honors 
in Chapters 1-
with Sections 
into Chapter 5. I would be sure to 

material 

7 for such a class. 

cover 
flexibil

the entire 
ity, you might 

book, including 
omit some of the 
algebra), thereby 

area­

treatment of numerical 
linear 
remaining 
topics, 
mathematics 
course 

of the 

more applica­

that emphasizes 

3 can be covered quickly. Chapter 6 can 

3.5 and 3.6, and Chapter 7 can be in­
in Chapters 
1, 4, 
the explorations 

assign 

For a one-semester 

mine which topics to 
following 
page. 
allowing 
time for extra 
on the basic course 
still 

course, 
include. 
course, 
in-class 
quite 

topics, 

but are 

The basic 

described 
review, 
flexible. 

the nature 
Three possible courses 

of the course 

first, has 

and tests. 

The other 

and the audience 

will deter­

are described 
fewer than 36 hours suggested, 

below and on the 

two courses 

build 

A Basic Course 
A course 
outlined 
in a concrete setting) 
(all concepts 
thorough introduction 
to linear 
algebra. 

designed 
on the next page. 
are treated 

for mathematics 

This course 

is 
majors and students 
does not mention general 
spaces 
at all 
it is a 
on proofs. Still, 

and is very light 

from other disciplines 

vector 

XXii  To the Instructor 

Section Number of Lectures Section Number of Lectures 
1.1 
1.2  1-1.5 
1.3 
1-1.5 
2.1  0.5-1 
2.2 
2.3 
3.1  1-2 
3.2 
3.3  2 
3.5  2 

3.6  1-2 
4.1 
1 
4.2  2 
4.3  1 
4.4  1-2 
5.1  1-1.5 
5.2  1-1.5 
5.3  0.5 
5.4  1 
7.3  2 

1-2 
1-2 

Total: 23-30 

lectures 

Because 

in a course 

the students 

such as this one represent a wide variety 
of dis­
ciplines, I would suggest using much 
of the remaining 
In my course, I do code vectors 
at least one 
signed 
sufficient 
lectur

can be as­
as desired. 
in detail. 

along with as many of the explorations 

2-5. Other applications 

8.1, which students 

e time available 
to cover 

from each of Chapters 

application 

some of the 

as projects, 

lecture time for applications. 

in Section 

theory 

really 

There is also 

seem to like, and 

A Course with a Comoulalional Emphasis 
For a course 
with a computational 
page can be supplemented 
algebra. 
7.4, ending 
and 5 are particul

emphasis, 
with the sections 
I would cover 

part or all of Sections 
value decomposition. The 
arly well suited 
to such a 

the basic 
of the 

with the singular 

In such a course, 

course, 

course 

outlined 

on the previous 

text dealing 

with numerical 

linear 

2.5, 3.4, 4.5, 5.3, 7.2, and 

explorations 

in Chapters 
tions. 

as are almost any of the applica

2 

the basic 

prin­
course 
will 
algebra 
and deter­
matrices, 
material 
contain 
alread
y, much early 

and pla

in other 

already 

who have 

encountered 

aimed at students 

will be 
algebra 
courses. For example, a college 
an introduction 
to systems 
calculus course 
lines, 
nes. For students 
can be omitted and replaced 

of linear 
equations, 
will almost certainly 
who have seen such topics 
with a quick review. 
to skim over the material 

A course tor Sludenls Who Have Already 
SIUdied Some linear Algebra 
Some courses 
ciples 
of linear 
often include 
minants; a multivariable 
vectors, 
material 
ground of the class, it may be possible 
to Section 
ics majors (and especially 
I would be sure to cover 
permits. If the course 
Sections 
material 
ence students 
the material 

6.1 and 7.1 and a broader selection 
of function
and approximation 
on differential 
equations 
or engineers 
represented, 
I would try to 
algebra 
on codes 

are prominently 
and numerical 

significa
algebra 

Sections 
has science 

if this is the only linear 

3.3 in about six lectur

of applications, 

es. If the class 

as I could. 

linear 

has a 

in the basic 

Depending on the back­
up 
nt number of mathemat­
course 

majors), I would cover 
being sure to include 
s. If computer 

the 
sci­
do as much of 

they will take), 
as time 

6.1-6.5, 7.1, and 7.4 and as many applications 
majors (but not mathematics 

course 

on 

There are many other 

that you find it useful for your course 

types of courses tha
t can success
and that you enjoy using 
it. 

fully use this text. I hope 

To the 
Student 

your 

please 

"Where shall 
I begin, 
Majesty?" he asked. 
"Begin 
said, gravely, "and go on  till you 
to the 

at the beginning," the King 

end: then stop." 

come 

Alice's 

Carroll 

-Lewis 
Adventures in 

Wonderland, 1865 

, applications 
The Student Solu­

to 

areas 

results

to other 

is an exciting 

on how best to use this book; 

general 
has several 

are some 
algebra 

subject. It is full of interesting 

of mathematics. 
advice 

Linear 
algebra 
other 
disciplines, and connections 
Manual and Study Guide contains detailed 
tions 
following 
Linear 

suggesti
ons. 
sides: 
There are 
applications. One of the goals 
of this book is to 
the subject and to see the interplay 
you read and understand each 
that section. 
assigned 
tions 
thing 
as you read. 

The �  icon in the margin 

of terms and the meaning 
more than once before you 

Stop to work out examples 
for yourself 
indicates 
a place 

exercises in 
the text before you attempt the 
exercises tha
that are related to 
t have been 
understand 
the defini­
you have to read some­
with you 
or to fill in missing calcula
tions. 
where you should pause and think 

all of these 
among them. Consequently, it is important that 

computational techniques, concepts, and 
facets of 

of theorems. Don't worry if 
understand 

you will miss much. Make sure you 

it. Have a pencil and calculator 

If you read only examples 

section of 

help you master 

as homework, 

over 

what you have read so far. 

Answers 

to most odd-numbered computational 
exercises 

are in 

the back of the 

before you have completed 

temptation to look up an answer 

remember that even if your answer 
be right; there 

book. Resist the 
tion. And 
may still 
For example, a value 
multiples 
of the 

is more than one correct 
of l/v2 can also be expressed 

vector [ 1�2 ] is the same as the set of 

a ques­
differs 
in the back, you 
from the one 
way to express some of the solutions. 
set of all scalar 
all scalar multiples 

of [ � ] . 

As you encounter new 

as v2/2 and the 

try to relate 

concepts, 
exercises 

them to 
in a logical, 

examples 
connected 

sentences. 

Write out proofs and solutions to 
plete 
Better yet, if you can, have a friend in the class read 
make sense 

Read back what you have written 

to another 

person, 

chances 
sense, 
tor with matrix capabilities 
or a computer 

are that it doesn't make 

what you have written. If it doesn't 
period. 

that you know. 
way, using com -
it makes sense. 

You will find that a calcula

to see whether 

algebra 

sys­

tem is useful. 
indispensable for some problems 

can help you to check your 
tedious 

involving 

own hand calculations 
computations. Technology 
also 

These tools 

and are 

xx iii 

XXiV  To the Student 

of linear 

aspects 

algebra 

in this vector? 

one of the entries 

on your own. You can play "what if?" 
What if this matrix 
like it to be by changing 

enables 
you to explore 
games: What if I change 
is of a 
different 
size? 
Can I force the solution to be what I would 
something? To 
signal places 
recommended, 
I have placed 
accompanies 
the book using Maple, 
appendix providing 
algebra. 

cAs in the margin. The companion 

Mathematica, 
and MATLAB, 
advice 
much additional 

that 
exercises 
from 
out selected 
an 

as well as Technology Bytes, 
about the use of technology 

in the text or exercises 
the icon 

this book contains computer 

where the 

code working 

in linear 

website 

You are about to embark on a journey through linear 

algebra. Think of this book 

as your travel 

guide. 

Are you ready? 
go! 

Let's 

use of technology is 

Vectors 

pouring out of the blue. 

Here they come 
for me and for you. 
Little 
-Albert 

arrows 

and 
Hammond 
Mike Hazelwo
od 
Arrows 

Music/BM!, 1968 

Little 

Dutchess 

1 . 0  Intro d u ction :  The Racetrack 

G a m e  

ble quantities, 

such as length, 
area, 
Many measura
described 
by specifying 
their 
can be completely 
both a magnitude and a direction 
require 
as velocity, force, and accelera
tion, 
description. These 
their 
quantities 
For example, wind velocity is a vector 
are vectors. 
consisting 
of wind speed and direction, 
vectors 

volume, mass, and temperature, 
ies, such 

Other quantit
for 

such as 10 km/h southwest. Geometrically, 
or directed line 
was introduced in the 19th centu
in the physical sciences, 

was not realized 

represented 
as arrows 

the idea of a vector 

magnitude. 

are often 

ry, its usefulness 

segments. 

have found applications 

until 
the 
in computer 
science, 

those 
vectors 

and the life and social sciences

. We will consider 

some of these 

t this book. 
vectors and begins 

properties. 

We begin, though, 

[You may even 

wish to play it with a friend during 

those 

with a simple 

of their 
to consider some 
some 
e!) 

game that introduces 

(very rar

geometric 

Although 
in applications, 
20th century. 
statistics, 
many applications 

particu
larly 
More recently, 
economics, 
throughou
introduces 

This chapter 
and algebraic 
of the 
dull moments 

key ideas. 

in linear 
The game is played 

algebra class.] 
on graph paper. A track, 

paper. The track can 

be of any length 

is drawn on the 
enough 
(let's 
or bicycles 
and Bert as cyclist

call them Ann and 

s.) 

to accommodate all of the players. For this example, we will have two players 

or whatever they 

are going to race around 

Bert) who use different 

colored 

pens to represent their 
(Let's think of Ann 

the track. 

cars 

with a starting 
and shape, so 

long as it is wide 

line and a finish line, 

a dot on the starting 
to a new grid point, subject to the 

line at a grid point on 

following 

Ann and 

Bert each begin 

by drawing 

moving 

the graph paper. They take turns 
rules: 
1. Each new grid poi
must lie entirely 

nt and the line segment connecting 
within the track. 
2. No two players 
may occupy the same grid 
rule.) 

"no collisions" 

3. Each new move is related 
and b units 

horizontally 

to the 

he or she must move between 

point on the same turn. (This 

a - 1 and a + 1 units 

vertically 

move as follows: If a player 
moves 
the next move 
on one move, then on 
horizonta
lly and between 

previous 

is the 

a units 

it to the previous 

grid point 

2  Chapter 1 Vectors 

move is c units 
then la -cl :::::: 1 and lb - di :::::: 1. (This 

words, if the second 

this rule forces the first move to be 

In other 

is the 

ally. 
ally, 

rule.) Note that 

vertic
vertic
leration" 

units 
lly and d units 
tion/dece

b - 1 and b +  1 
horizonta
"accelera
1 unit vertic
A player 

ally and/or 1 unit horizontally. 
player 
who collides with 
finish line. 
cross the 
who goes farthest 

same turn, the one 

another 

or leaves 

is the first player to 

winner 
the finish line on the 
winner. 

the track 

is eliminated. 

The 

If more than one player 

crosses 

past the finish line is the 

Ann was the winner. Bert accelera
ted too 

quickly 

To understand 

In the sample game shown in Figure 
negotiating 

and had difficulty 

1.1, 
rule 3, consider 
Ann's third 
vertic
and 2 to 4 units 
her outside 

would have placed 

she went 1 unit horizonta
were to move 0 to 2 units 
of these 
2 units 

lly and 3 units 
horizon
tally 
combinations 
in each direction. 

the turn at 

the top of the track. 
and fourth moves. On 
ally. 

ally. 

vertic
that some 
the track.) She chose to move 

(Notice 

her third 

move, 

On her fourth move, her options 

:r: r§ 
Rowan Hamilton (1805-1865) 

used vector 
of complex 
generalization, 

concepts 
numbers and their 

the quaternions. 

mathematician 

The Irish 

William 

in his study 

I I  r  r 

A B 

l  l 

Figure 1 . 1  
A sample 

game of racetrack 

Problem 1 Play a few games of racetrack. 

Problem 2 Is it possible for Bert to win this race by choosing a different 

sequence 

of moves? 

Problem 3 Use the notation 

lly 
(Either a or b or both may be negative.) If move [3, 4] has just 

vertic
draw on graph paper all the grid points that 

a move that is a units 

[a, b] to denote 

could possibly be reached 

horizonta

ally. 

and b units 
been made, 
on the 

next move. 

Problem 4 What is the net effect of two successi
move [a, b] and then [c, d], how far horizontally 
altogether? 

ve moves? In other 

and vertically 

words, if you 
will you have moved 

Section 1.1 The Geometry and Algebra 

of Vectors 3 

of the grid poi

the [a, b] notation. 

(O, 0) on the coordinate 

Problem 5 Write out Ann's sequence of moves using 

she begins at the origin 
coordinates 
the graph 
not the origin 
Although 

Suppose 
find the 
nt corresponding to each of her moves without lookin
g at 
ly, so that Ann's starting 
point was 
of her final point be? 
that will be useful in our 
from geometric 
and alge­

but the point (2, 3), what would the coordinates 
simple, this game introduces several 

If the axes were drawn different

ideas 
vectors 

axes. Explain 

consider 

paper. 

study of vectors. The next three 
sections 
braic 

viewpoints, beginning, 

as in the racetra

ck game, in the plane. 

how you can 

The G e o m etrv a n d  Algebra of vectors 

is named 
and 

The Cartesian 
plane 
after 
the French 
mathematician 

philosopher 
Rene Descartes 

(1596-1

650), whose introduction 

allowed 

of coordinates 
problems 
algebraic techniq
ues. 

to be handled 

geometric 
using 

direction. 

"to carrY:' 

The word vector comes from 
the 
Latin 
A 
root meaning 
vector 
is formed when a point 
is 
displaced-or 
"carried 
distance 
another 
pieces 
and their 

off" -a given 
Viewed 
in a given 
way, vectors 
"carry" 
two 
of information: 
their 
length 

direction. 
vectors 

by v, 
v for 

by hand, 

When writing 
it is difficult 
to indica
Some people prefer 
the vector 
denoted 
but in 
ordinary 
ally be clear 
the letter denotes 

te boldface. 
to write 
in print 
it is fine to use an 
v. It will usu­

from the context 
when 
a vector. 

lowercase 

most cases 

Vectors in the Plane 
We begin by considering 
A vector is a directed 
A to another point B; see Figure 
1.2. 

the Cartesian 

line segment that corresponds 

plane with the familiar 
to a displace

ment from one point 

x-and y-axes. 

The vector 
and the 
by a single boldface, 

by AB ; the point A is called 
from A to B is denoted 
point B is called its terminal point, or head. Often, 
letter such as v. 
lowercase 

or tail, 
denoted 

its initial point, 
a vector 

is simply 

The set of all points in 

0. To each 
a with tail at 0, there 

the plane 
point A, there 
corresponds 
s.) 
It is natural 

are at the origin 
vector 
times 

position 
vector

called 

to represent such vectors 

corresponds 
to the 

corresponds 
its head A. (Vectors 

set of all vectors 
the vector 

whose tails 

a  = OA; to each 

of this form are some­

----> 

using coordinates. For example, in 
a = OA = [3, 2] using 
s. 

square bracket

Figure 1.3, A = (3, 2) and we write the 
Similarly, 

vector 
1.3 are 
in Figure 

vectors 

the other 

----> 

coordinates 
is sometimes 
for example, 

b  = [ - 1,3 ]  and c  = [2, - 1 ]  

(3 and 2 in the case of a) are called 

[3, 2] � [2, 3] .  In general, 

said to be an ordered 
components are 

two vectors 
equal. Thus, 
[x, y] 

corresponding 

The individual 
vector. A vector 
important 
since, 
only if their 
x = 1 andy = 5. 

convenient 

to use column vectors 

the components of the 

pair of real numbers. The order is 

are equal 
if and 

= [l, 5] implies that 
of (or in 
to) 

addition 

instead 

is [�].(The important point is that the 

It is frequently 
Another 

row vectors. 

representation of [3, 2] 

y 

y A� B 

The word component is derived 
from the Latin 
co, meaning 
"together 
with;' 
ing "to put:' 
Thus, 
together" 

words 
and ponere, mean­
is "put 

a vector 

out of its components. 

Figure 1 . 2  

Figure 1 . 3  

4  Chapter 1 Vectors 

IR2 is pronounced 
"r two:' 

components are 
what better from a computational point of view; 
representa

ordered.

) In later 

tions. 

chapters, you will see that column 

are some­
for now, try to get used to both 

vectors 

It may occur to 
itself. 

you that we cannot really draw the 
vector 
and has a specia

good vector 

Neverthe

l name: the 

[O, OJ = 00 from the 

with two components is denoted 

by IR2 (where 

IR denotes 

the components of vectors 

in IR2 are chosen). 

it is a perfectly 
is denoted 

by 0. 

origin to 
zero vector. 
The set 

less, 
The zero vector 
of all vectors 
real numbers from which 

- 1, 3.5], [ \/2 , 7f ], and rn, 4] are all in IR2• 

the set of 
Thus, [ 
tors whose tails 
the verb "to carry" provides 
Starting 
same displacem
equivalent 

0, travel 

displacements, 

back to the racetrack 

are not at 

at the origin 

Thinking 

game, let's 

try to connect 
the origin. The 
l origin 

etymologica

to the right, 
ent may be applied with 

a clue. The vector 
3 units 

[3, 2] may be interpreted 
then 2 units 
by the vectors 
AB and CD. 

other init� oin� igure 1.4 shows two 

represented 

vec­

all of these 

ideas to 
of the word vector 
as follows: 
up, finishing 
at P. The 

in 

y 

c 

Figure 1 . 4  

are referred 
to by 
coordinates, 

When vectors 
their 
considered 

analytically. 

they are being 

We define two vectors 

as equal 

if they have the same length 

and the same direc­

AB = CD in Figure 

1.4. (Even though 
they represent the same displacem

tion. Thus, 
minal points, 
equal 
the two vectors 
and B = (6, 3). Notice 
difference 

if one can be obtained 
coincide. 

of the respecti

by sliding 

(or translatin
In terms 
of components, 
[3, 2] that records 
that the 
vector 
ve componen
ts: 

-----> AB = [ 3, 2 ]  = [ 6 - 3, 3 
- l ] 

they have different 

initial 
and ter­
lly, two vectors 
are 
until 

ent.) Geometrica
parallel 

g) the other 
in Figure 1.4 we have A = (3, 1) 

to itself 

the displacem

ent is just the 

CD =  r - 1  - (-4), 1  - (

- 1)J [ 3, 2 ]  

Similarly, 

----->  ----> 
such as oP with its 

and thus AB = CD, as expected. 
discussion 
The foregoing 
dard position. 
Conversely, a vector 
tion) 

so that its tail is at any 

A vector 

point in the plane. 

tail at the origin is 

said to be in standard 
n. 
can be drawn as a vector 

positio
in stan­

shows that every vector 
in standard 

position can be redrawn 

(by transla­

If A = (- 1, 2) and B = (3, 4), find AB and redraw it (a) in standard 
(b) with its tail at 

-----> 
Solulion We compute AB = [3  -(- 1), 4 - 2] 

the point C = (2, - 1). 

-----> 

to CD, where C = (2, - 1), then we must have D = (2 + 4, - 1  + 2) = (6, 1). (See 
Figure 1.

position and 

= [4, 2]. If AB is then translated 

5.) 

Example 1 . 1  

Section 1.1 The Geometry and Algebra 

of Vectors 5 

y 

D(6, 1 )  

Figure 1 . 5  

from Old 

New Veclors 
As in 
to the notion 

the racetrack 

If we follow u by v, we can visualize 

game, we often 

want to "follow" 
of vector addition, 
vector 
the first basic 
the total 
1.6, u  = [ 1, 2]  and v 

one vector 
operation. 
displacement 
as a third 
= [ 2, 2], so the net 

vector, 
effect of follow­

by another

by u + v. In Figure 

. This leads 

denoted 
ing u by v is 

[1 + 2, 2 + 2

] = [3, 4] 

which gives 
is the vector 

u + v. In general, if u  = [u1, u2] and v = [ v1, v2] ,  then their 

sum u + v 

It is helpful 
version 

to visualize 

u + v geometric
ally. 
discussion. 

of the foregoing 

The following 

rule is the 

geometric 

y 

/ 

Figure 1 . 6  
Vector 

addition 

/l �( __ j2 
I 2 U  I :2 
_n 
/__) 

_________ 

3 

4 

6  Chapter 1 Vectors 

The  Head-to-

Tail Rule 

of u. The sum u + v of u and v is the vector 

u and v in IR2, translate 

Given vectors 

(See Figure 

1.7.) 

v s o  that its tail coincides 

with the head 

from the tail of u to the head of v. 

Figure 1 . 1  
The head-to

-tail 
rule 

Figure 1 . 8  
The parallelogram 
determined 

by u and v 

By translating 

u and v parallel to themselves, we obtain 

shown in Figure 1.8. This parallelogram 
is called 
and v. It leads 
position. 

to an equivalent 

version 

a parallelogram, 

as 
determined by u 
the parallelogram 
vectors 

in standard 

of the head-to-tail rule for 

The Parallelogram 

Rule 

Given vectors 
in standard 
v. (See Figure 

u and v in IR2 (in standard 
the diagonal 
1.9.) 

position 
along 

position), their 

of the parallelogram determined 

by u and 

y 

sum u + v is the vector 

Figure 1 . 9  
The parallelogram 

rule 

Example 1 . 2  

Ifu = [3, - 1] and v = [l, 4], compute and draw u + v. 
Solulion We compute u + v = [3 + 1, - 1  + 4] = [4, 3]. This vector 
using the head-to-
Figure l . lO(b). 

Figure l . lO(a) and using  the 

tail rule in 

is drawn 

parallelogram 
rule in 

Section 1.1 The Geometry and 

Algebra 

of Vectors 1 

y 

y 

u 

(a) 

Figure 1 . 1 0  

(b) 

The second basic 

vector 
the scalar 

operation 
multiple 

is scalar 
cv is the vector 

number c, 

a real 
component ofv by c. For example, 

multiplication. Given a 

obtained 

by multiplying 

vector 
v and 
each 

3 [ - 2, 4] = [ - 6, 12]. In general, 

Example 1 . 3  

Geometrica

lly, cv is a "scaled" version 

of v. 

draw 2v, tv, and - 2v. 

Ifv = [ - 2, 4], compute and 
as follows: 
Solution We calculate 
2v = [2( -2), 2(4)] = [ -4, 8 J 
tv = [t(- 2), t(4)] = [ - 1,2 ]  

-2v = [ -2(- 2), - 2(4)] = [4, - 8 ]  

These vectors 

are shown in Figure 1 . 1 1 .  

y 

2v 

-2v 

Figure 1 . 1 1  

8  Chapter 1 Vectors 

/. / 2v 

-2v 

u 
u + ( -v) 

Figure 1 . 1 2  

Figure 1 . 1 3  
Vector 

subtraction 

word scala, meaning 
"lad­
rungs 
and in vec­

The term scalar comes from the 
Latin 
spaced 
on 
dd' The equally 
a scale, 
a ladder 
suggest 
tor arithmetic, 
multiplication 
by a 
constant 
length) 
became known as scalars. 

only the scale 
changes 
(or 
of a vector. 
Thus, 

constants 

Observe that cv has the same direction 

c < 0. We also see that cv is le I times 

vectors, consta
nts (i.e., 
when translation 
each other 

is taken into 
if and only if they are parallel. 

real numbers) are referred 

of vectors 

A special 
the negative 
v is the vector 

case of a scalar multiple 
of v. We can use it to define vector 
u - v defined 

by 

as v if c > 0 and the opposite direction 

if 

as long as v. For this reason, in the context of 

to as 
two vectors 
account, 

As Figure 1.12 shows, 
of 
are scalar multiples 

scalars. 

is ( -l)v, which is written 

as -v and is called 

subtraction: The difference 

of u and 

u - v =  u  + (- v) 

1 . 1 3  shows that u - v corresponds 

to the "other" 

diagonal 

of the 

parallelo­

Figure 
gram determined 

by u and v. 

Example 1 . 4  

If u  = [1, 2] and v = [ - 3, 1 ] ,  then u - v = [ 1 -( - 3), 2 -1 ]  = [ 4, l ] .  

y 

A 

Figure 1 . 1 4  

The definition 

-----> 
�  in standard 
then AB = b - a, as shown in Figure 1.14. [Observe that 

such as AB . If the points A and B correspond 
applied to this diagram gives 
drawn b  - a with its head at A instead 

the equation a +  (b -a )  = b. Ifwe 

with the way we cal­
to the vectors 
the 

head-to-
had accidentally 

of at B, the diagram 

of subtraction 

tail rule 

1.4 also agrees 

in Example 

a vector 

position, 

culate 

would 

a and b 

wrong! More will be said about algebraic 

expressions 

have read b  + (b -a) = a, which is clearly 
vec1ors in �3 

involving vectors 
in this section

later 

.] 

Everything 
triples 
dered 
three 
mutually 
as A = (1, 2, 3) 
parallel 
move 2 units 
corresponding 
vector 

we have just done extends eas
of real numbers 
icular 

is denoted 
coordinate 

ily to three 
dimensions. 
by IR3. Points and vectors 

axes that meet at the origin 0. A point such 

The set of 
are located 

all or­
using 

perpend
can be located 

to the 

travel 

as follows: First 

y-axis, and finally 

a =  [l, 2, 3] is then OA, as shown in Figure 
( 1, 2, 3) parallel 

a box whose six sides 
are de­
(the xy-, xz-, and yz-planes) 
planes 
and by three 
[ 1, 2, 3] 
coordinate 
to the opposite corner 
(see Figure 

planes 
to the 
from the origin 

then corre­
1.16). 

Another 
termined 
through 
sponds to the 
diagonal 

by the three 
the point 

way to visualize 
vector 

1 unit along the x-axis, then 

a in IR3 is to 

planes. The vector 

construct 

move 3 units 

of the box 

to the z-axis. The 

coordinate 

parallel 

1.15. 

Section 1.1 The Geometry and Algebra 

of Vectors 9 

z 

z 

A( l, 2, 3)  I 
• 
3 I I I :3 I I 
I 
---1� 

y  x 

,. 

2

-

x 

Figure 1 . 1 5  

Figure 1 . 1 6  

The "componentwise" definitions 

of vector 

addition 

and scalar multiplication 

are 

extended to 

IR3 in an obvious 
way. 

Vectors in � n 

In general, we define !Rn as the set of 
row or 

all ordered 
v in !Rn is of the 
form 

Thus, a vector 

vectors. 

column 

n-tuples 

of real 

numbers 

written 

as 

arithmetic will be 

the ith component. 

of vector 

addition 

of v are its components; 

The individual 
We extend 
the obvious 

V; is called 
u + v is U; + V; and the ith component of cv is just 
CV;. 

to !Rn in 
way: If u = [u1, u2, • • •  , unl and v = [ v1, v2, . . .  , vn], the ith component of 
in !Rn we can no longer draw 
it is important to be able to 
pictures 
to the arithmetic 

entries 
the definitions 

calculate with vectors. We must be careful 
similar 
we would do 
to those 
do with vectors 
we will encounter 
where vector 
algebra 
ence with real numbers. So it is important 
attempting 

of real numbers. Often it is, and the algebraic 
calcula
we 
with scalars. But, in later 
is quite unlike our previous 

to verify any algebraic 

not to assume 

are similar 

of vectors, 

that vector 

situations 

Since 

and scalar multiplication 

properties 

tions 
sections, 
experi­
before 

is commutativit

to use them. 
One such property 
v. This is certainly 
u + v and v + u are the main 
(The parallelogram rule also reflects 
Note that Figure 1.17 is simply 

true in IR2• Geometrica
diagonals 

this symmetry; 
an illustra

y of addition: 

u and 
u + v = v + u for vectors 
tail rule shows that both 
determined 
by u and v. 

of the parallelogram 

lly, the head-to-

tion of the property u + v = v + u. It 
where u  = v, u  = - v, and u  = 0. (What would diagrams 

it does not cover every possible case. For 

example, we must also 

for these 

is not a proof, since 

see Figure 
1 . 1 7.) 

the cases 

look like?) For this reason, 

cases 
easy to give 

a proof 

that is valid in 
theorem summarizes 

an algebraic 
!Rn as to give 

proof is needed. 
IR2. 
one that is valid in 
the algebraic 
of vector 

properties 

However, 

The following 

addition 

it is just as 

multiplication 

in !Rn. The proofs follow from the corresponding properties 

and scalar 
of real numbers. 

�  include 

Figure 1 . 1 1  
u+ v=v +u 

10  Chapter 1 Vectors 

Theorem 1 . 1  

Algebraic 

Properties 

in !Rn 
of Vectors 
in !Rn and let c and d be scalars. 

Then 
Commutativity 
Associativity 

Distributivity 
Distributivity 

w = u + (v + w) 

Let u, v, and w be vectors 
a. u + v = v + u 
b. (u + v) + 
c. u + 0 = u 
d. u + (-u) = 0 
e. c(u + v) 
= cu +  cv 
f. (c + d)u = cu+ du 
g. c(du) = (cd)u 
h. lu = u 

Remarks 

•  Properties (c) and (d) together 
•  Ifwe read the distributivity 

which 

= 0 as well. 

properties 

a theorem 
we have 

that 0 + u = u and -u + u 
that we can factor a common scalar or a common vector 
Proof We  prove 
ing properties 
[W1, Wz, . . .  , Wn] .  
ly, when we understand 
(a) 

The word theorem is derived 
from 
the Greek word theorema, 
in turn 
comes from 
a word mean­
ing "to look af' Thus, 
is based 
on the insights 
when we look at examples and 
extract 
we try to prove 
Similar
something 
proof 
we often say, "I see:' 

U + V = [U1, Uz, . . .  , Un] + [V1, Vz, . . .  , Vn] 
= [ v1 + u1, v2 + u2, •.. , vn + un] 

properties 
Let u =  [u

= [U1 + V1, Uz + Vz, . . .  , Un + Vn] 

from them properties 

as exercises. 

hold in general. 

in mathematics-the 

of a theorem, 

for example­

that 

from a sum. 

with the commutativity 

property (a) imply 

(e) and (f) from right 

to left, 
they say 

(a) and (b) and  leave  the 

proofs of the remain­

1, u2, . . .  , un], v = [ v1, v2, . . .  , vn], and w = 

= [v1, Vz, . . .  , Vn] + [u1, Uz, . . .  , Un] 
=v + u  

The second 
third 
(b) Fi

and fourth equali
is by the commutativity 
tivity 

equality 
gure 1.18 illustrates 

associa

of addition 
in IR2. Algebraic
ally, 

of real numbers. 
we have 

ties are by the definition 

of vector 

addition, 

and the 

[(u1 + v1) + w1, (u2 + vz) + w2, . . .  , (un + vn) + wn] 
[ u1 + (vl + W1),U2 + (Vz + Wz), . . .  ,Un + (vn + Wn)J 

[u1, u2, . . .  , un]  + ([v1, v2, . . .  , vn]  + [ w1, w2, . . .  , wn]) 

The fourth equality 
ful use of parenthes

= u + (v + w) 
is by the associativity 
es. 

of addition 

of real numbers. Note the care­

u 

Figure 1 . 1 8  

Section 1.1 The Geometry and 

Algebra 

of Vectors 1 1  

By property (b) of Theorem 

1 . 1 ,  

we may unambiguously write 

we may group the summands in whichever way 

parentheses, 
since 
we may also rearrange 
Likewise, sums of four or more vectors 
grouping. 
out parentheses: 

ifv1, v2, ••. , vk are vectors 

In general, 

the summands-for example, as 

can be calculated 

regard 
or 
in !Rn, we will write 
such sums with­

u + v + w without 
we please. By (a), 
w + u + v-if we choose. 
to order 
without 

The next example 

illustrates 

the use of Theorem 

1 . 1  in performing 

algebraic 

calcula

tions 

with vectors. 

Example 1 . 5  

Let a, b, and x denote 
(a) Simplif
y 3a + (
(b) If Sx - a = 2(a + 2x), solve for 

vectors 
Sb -2a) + 2(b -a). 

x in terms of a. 

in !Rn. 

both solutions 

Solution We will give 
in detail, 
in Theorem 1 . 1  that we use. It is good practice 
you do this type of calcula
though, 
space. 

out some of the 

it is acceptable 

to leave 

tion. Once you are comfortable 

with reference to 
to justify all steps the 

all of the properties 
first few times 

with the vector 

properties, 

intermediate steps 

to save time and 

(a) We begin 

by inserting 

parentheses. 

3a + (Sb -2a) + 2(b - a) =  (3a + (Sb -2a)) + 2(b - a

) 

(3a + (-2a + Sb)) + (2b -2a) 
((3a + (-2a)) + Sb) + (2b -2a) 
((3 + (-2))a + Sb) + (2b -2a) 
(la + Sb) + (2b -2a) 
((a + Sb) + 2b) -2a 
(a + (Sb + 2b)) -2a 
(a + (S + 2)b
) -2a 
(7b + a) -2a 

(a), (e) 
(b) 

(f) 

(b ), (h) 
(b) 

(f) 
(b) (f), (h) 

(a) 

= 7b + (a -2a) 
= 7b + (1 - 2)a 
= 7b + ( - l)a 
= 7b - a 

You can see why we will agree to 
simplify 

this sequence 
as 

of steps 

omit some of these 

steps! In practice, 

it is acceptable 

to 

3a + (Sb -2a) + 2(b -a) = 3a + Sb -2a + 2b -2a 

(3a -2a -2a) + (Sb + 2b) 

=-a+ 7b 

or even to do 

most of the calculation 

mentally. 

12  Chapter 1 Vectors 

(b) In detail, 
we have 

Sx - a =  2(a + 2x) 
(2x) 
Sx - a =  2a  +  2
(e) 
Sx - a = 2a + (2 · 2)x  (g) 
Sx -a =  2a + 4x 

(Sx - a) -4x = (2a + 4x) -4x 
(- a  + Sx) -4x = 2a + (4x -4x) 
-a + (Sx -4x) =  2a  +  0 
- a  + (5  - 4)x = 2a 
- a+ (l)x = 2a 

a + (-a +  x) =  a + 2a 
(a + (- a)) +  x = (1  +  2)a 

0 +  x = 

3a 
x = 3a 
omit most of these 

steps. 

(a), (b) 
(b), (d) 

(f), (c) 
(b), (f) 

(h) 

(d) 
(c) 

Again, 

we will 

usually 

Combinalions and Coordina1es 

Linear 
A vector that is a sum of scalar multiples 
of other 
nation of those 

vectors. The formal definition 
follows. 

vectors 

is said to be a linear 

combi­

are scalars c1, c2, . . .  , ck such that v = c1v1 + c2v2 + · · · + ckvk. The scalars 

combination of vectors 

v is a linear 

v1, v2, . . .  , vk if 
n. 

DefiniliOD 
there 
c1, c2, . . .  , ck are called 

the coefficients of the linear 

combinatio

A vector 

Example 1 . 6  

Remark Determining 

whether 
is a problem we will address 

vectors 

In IR2, it is possible to depict 
conveniently. 

quite 

a given vector 
in Chapter 2. 
linear 

is a linear 

combination 
of other 

combinations 

of two (nonparallel) 
vectors 

Example 1 . 1  

Let u = [ �] and v = [ �]. We can 

e1 = [ �] and e2 = [ �] locate the 

way that 

use u and v to locate a new set of 

axes (in the same 

standard 

coordinate 

axes). We can use 

of Vectors 13 

Section 1.1 The Geometry and Algebra 
y I 
I 

u 
I-
I I 

Figure 1 . 1 9  

new axes to determine 

these 
combinations 

of u and v. 

a coordinate 

grid that will let us easily locate linear 

As Figure 1.19 shows, w can be located by starting 

at the origin 

and traveling 

-u followed 

by 2v. That is, 

w = -u + 2v 

We say that the coordinates 
this is 
that 
It follows 

just another 

way of thinking 

of w with respect to u and v are - 1  and 2. (Note that 
combination.) 

of the coefficients 

of the linear 

(Observe 

that - 1and3 are the 

coordinates 

ofw with respect to e1 and e2.) 

standard 
in chemistry 

Switching 

from the 
has applications 
often 
in this book. 

do not fall onto a rectangular 

coordinate 

and geology, since 

grid. 

axes to alternative 

ones is a useful idea. 
molecular and crystalline 

It 

It is an idea that we will encounter 

structures 
repeatedly 

and Modular 

Arilhmelic 

a type of vector 

that has no geometric 

interpreta

tion-at least 

Binarv vec1ors 
We will also encounter 
not using 
can be interpreted 
vectors 
vectors 

Euclidean 

naturally 

geometry. 
as off/on, closed/open, 

represent data in 
false/true, 

Computers 

terms of Os and ls (which 
or no/yes). Binary vectors 
are 
8, such 

Chapter 

each of whose components is a 0 or a 1. As we will see in 
arise 

in the study of many types of codes. 

In this setting, 

the usual rules 

of arithmetic 

since 
scalars must be a 0 or a 1. The modified rules 

must be modified, 

involving 

the result 
for addition 

of 

each calculation 
and multiplication 
are given 

below. 

The only curiosity here is the rule 
if we replace 

0 with the word 

that 1 +  1 

"even" and 1 with the word "odd," these 

= 0. This is not as strange 

as it appears; 
tables 
simply 

14  Chapter 1 Vectors 

of even and 

the familiar 

rules, our set of sca

again: The sum of three 

and multiplication 

lars {O, l} is denoted 

the fact that the sum of two odd inte­

the parity rules 
odds is even.) 

parity rules fo r  the addition 
With these 
modulo 2. 

summarize 
odd integers. For example, 1 + 1 = 0 expresses 
gers is an even integer. 
the set of integers 
is called 

by 22 and 
In 22, 1 + 1 + 0 + 1 = 1 and 1 + 1 + 1 + 1 = 0. (These 
.+ 
With 22 as our set 
by 2�. 
in 2� are called 
in 2� are [O, OJ, [O, l], [l, OJ, and [l, lJ .  (How many vectors 
does 2� 

of scalars, 
ls (with all arithmetic 
performed 
of length 

all n-tuples 
The vectors 

an even is odd; the 

modulo 2) is denoted 

extend the above 
rules 

binary vectors 

odds and 

of Os and 

we now 

sum of four 

n. 

The vectors 
contain, 
in general?) 

calculations 

illustrate 

to vectors. The set of 

Example 1 . 8  

!FR". This should 

We are using 
ferently 

the term length dif­
from the way we used it in 

since 
there 
oflength 

is no geometric notion 
vectors. 

for binary 

not be confusing, 

Example 1 . 9  

Example 1 . 1 0  

Let u = [l, 1, 0, 1, OJ and v = [O, 1, 1, 1, 
Solulion The calculation 
place 

of u + v takes 

OJ be two binary 

vectors 

over 22, so we have 

oflength 

5. Find u + v. 

u + v = [ 1 ,  i, o, i,o] + [o, I, I, 

i,o] 

= [ 1  + o, I  + I, o + I, I  + I, o + o] 
= [ 1 ,0,1,0, 0] 

possible to generalize 

It is 
components are taken from a finite 
first extend the idea 

of binary arithmetic. 

what we have just done for binary vectors 

to vectors 
whose 

set {O, 1, 2, . . .  , k} fork 2: 2. To do so, we must 
23 = {O, 1, 2} with addition 

and multiplication 

given 

modulo 3 is the set 

The integers 
by the following 
tables: 

Example 1 . 1 1  

+ 0  2 
0  0  1  2 

1  2  0 

0  1  2 

0  0  0  0 
1  0 2 
2  0  2  1 
and multiplica
to the 
operations 

2  2  0 
of each addition 

Observe that  the 

result 

{O, 1, 2}; we say that 23 is closed 
to think 
plication
. It is perhaps easiest 
of this set in terms of a 3-hour clock 
and 2 on its face, as shown in Figure 1.
20. 

with respect 

tion belongs 
to the set 
and multi­
of addition 
with 0, 1, 

The calculation 

1 + 2 = 0 translates 

as follows:  2 hours after 

1 o'clock, it is 

Just as 24:00 
on this 3-hour clock. 

0 o'clock. 
equivalent 
are equivalent 
to 0 here; 
of 3 (such as - 2, 4, and 7); and 2 is equivalent 

and 12:00 are the same on a 12-hour clock, so 3 and 0 are 
and negative­

to any number that is 2 more than a 

Likewise, all multiples 

1 is equivalent 

to any number that is 1 more than a multiple 

of3-positive 

Section 1.1 The Geometry and Algebra 

of Vectors 15 

multiple 
around 

of 3 (such as - 1, 5, and 8). We can visualize 
a circle, 

as shown in Figure 
1 .2 1 .  

the number line as wrapping 

0 

. . .  , -3 ,  0, 3, . . .  

2 

Figure 1 . 2 0  
Arithmetic 

modulo 3 

Figure 1 . 2 1  

. . .  , 1 ,  2, 5 ,  . . .  

. . .  , -2, 1 ,4, . . .  

Example 1 . 12 

To what is 3548 equivalent 

in Z/ 

Example 1 . 13 

Solution This is the same as asking 
to calculate 
we need to know the remainder 
3548 = 3 · 1 182 +  2, so the remainder 

how far this number 

where 3548 lies on our 3-hour clock. 
multiple 

nearest  (smaller) 

is from the 

The key is 

of 3; that is, 
by 3. By 
long division, 
we find that 
3548 is equivalent 

to 2 in l'..

is 2. Therefore, 

when 3548 is divided 

3• 4 

detail, 

in abstract 

In courses 
greater 
(mod 3), where= is read "is congruent to." We will not use 
nology here. 

algebra and number theory, which explore 
ence is often written 

in 
as 3548 = 2 (mod 3) or 3548 = 2 

the above equival

this notation 

this concept 

or termi­

In l'..3, calculate 2 + 2 +  1 + 
as in 

1 We use the same ideas 

2. 

Solution 
1 + 2 
2 +  1  + 2 

= 1 in l'..3. 

= 7, which is 1 more than 6, so 

by 3 leaves 

a remainder 

Example 
division 

1.12. The ordinary 

sum is 2 + 2 + 
of 1. Thus, 2 + 

Solution 

2 A better 

way to perform 

this calculation 

is to do 
(2  +  2) +  1  +  2 

2  +  2  +  1  +  2  = 

it step by step entirely in l'..

3• 

= 1 +1 +2 
= (1  +  1) +  2 
=  2  +  2 
=  1 
Here we have used parentheses 
speed things 

to group the terms we have chosen 

to combine. We could 

up by simultaneously combining 

the first two and the last two terms: 

(2  +  2) + (1  +  2) =  1  +  0 

=  1 

16  Chapter 1 Vectors 

Repeated 
multiplication 

tables 

to reduce 

the result of each calculation 

multiplication 

can be handled similarly. 

The idea is 

to use the addition and 

Extending 
these 

ideas 

to vectors 

is straightforward. 

to 0, 1, or 2. 4 

Example 1 . 14  In Z�, let u = [2, 2, 0, 

m -1  0 _ .,---._ 
m -2 

2 

3 

1, 2] and v = [l, 2, 2, 2, 
1,2 ]  + [1,2,2,2, l ]  

u + v = [2, 2, 0, 

l]. Then 

[ 2  + 1,2 + 2,0 + 2, 1  + 2,2 + l ]  
[0,1,2, 0, 0] 

to as 

ternary vectors 

of length 5. 

Vectors 

in Z� are referred 
are in Zm is called 

In general, 
responding 
entries 
length 

n is denoted 

by z::i. 

Zm = {O, 1, 2, . . .  , m -l} of integers 

we have the set 

to an m-hour clock, as shown in Figure 1.22). A vector 

an m-ary vector 

of length 

n. The set of all m-ary vectors 

modulo m (cor­
oflength 
n whose 
of 

modulo m 

F i g u r e  1 . 2 2  
Arithmetic 

..  I Exercises 

1 . 1  

vectors 

in standard position 

1 .  Draw the following 
in IR2: 
(a) a =  [3

= [2
3] 
0] (b)  b 
= [ 3] -2 

(d)  d 

2. Draw the vectors 

in Exercise 1 with their 

tails 
at the 

point (2, -3). 

3. Draw the following 

vectors 
(a) a =  [O, 2, OJ (b)  b 
= [3, 2, l] 
(c)  c = [l, -2, l] (d) d = [-1, - 1, - 2] 

in standard position in IR3: 

in Exercise 

3 are translated 

so that 

their 

4. If the vectors 
heads are at the 
correspond 
to their 

tails. 
5. For each of the following 
pairs 

point (3, 2, 1), find the 
points that 

of points, draw the 

AB. Then compute and redraw AB as a vector 

----> 

----> 

s and also show how the results can be 

6. A hiker 

walks 4 km 

north and then 5 km northeast. 
the hiker's 

representing 

ent vectors 

that represents the hiker's 

net 

Draw displacem
trip and draw a vector 
displacem

point. 

s in Exercise 

1. Compute 

ent from the starting 

the indicated vector
obtained geometrically. 
7. a +  b 

Exercises 7-10 refer to the vector
9. d -c 
Exercises 11 and 12 refer to the vectors in 
3. 

8. b - c 
10. a + d 

ed vector

Compute the indicat
s. 
11. 2a + 3c 
12. 3b -2c + d 
13. Find the components of the vectors 

u, v, u + v, and 
u -v, where u and v are as shown in Figure 1.23. 

14. In Figure 1.24, A, B, C, D, E, and F are the vertices of a 

hexagon 

centered 

each of the following 

at the origin. 
vectors 

in terms of 

regular 
Express 
a =  OA and b =OB: 

----> 

Exercise 

----> 
(a) Ai 
(c) Ai5 
(e) xc 

(b) BC 
(d) a 
(f) BC + ill + PX 

vector 
in standard 
position. 
(a) A =  (1, - 1), B = (4, 2) 
(b) A =  (O, -2), B = (2, - 1) 
(c) A = (2, f), B = (t, 3) 
(d) A = (t, t), B = (i, t) 

Algebra 

of Vectors 11 

Section 1.1 The Geometry and 
21. u = [ -�], v = [ �], w = [ �] 
22. u = [ -�], v = [ �], w = [ �] 

23. Draw diagrams 

to illustrate 

properties (d) and (e) of 

Theorem 1.1. 

properties 

( d) through 

(g) of 

24. Give algebraic 

proofs of 

Theorem 1.1. 

In Exercises 25-28, u and v are binary vector
25.u = [�],v = [�] 

in each case. 

s. Find u + v 

27.u= [1,0, 1, 1] , v= [1,1,1, 1] 
28. u = [1, 1,0, 1,0 ] , v  = [O, 1, 1, 1,0 ]  
29. Write out the addition 
30. Write out the addition and 

In Exercises 31-43, perform the indicat

tables 
and multiplication 
for Z4. 
tables 
for Zs. 

multiplication 

ed calculations. 

y 

Figure 1 . 2 3  

y 

c 

B 

F 

E 

Figure 1 . 2 4  

in Z4 
) in Z4 

in Z3  32. 2 · 2 · 2 in Z3 

31. 2  +  2  +  2 
33. 2(2  + 1 +  2) in Z3 34. 3  + 1 +  2  +  3 
35. 2 · 3 · 2 in Z4  36. 3(3  +  3  +  2
37. 2  + 1 +  2  +  2  + 
1 in Z3, Z4, and 
Zs 
38. (3  +  4)(3  +  2  +  4  +  2
) in Zs 
) in Z9 40. 2100 in Z11 
39. 8(6  +  4  +  3
41. [2, 1,2 ]  + [2,0, 1 ]  inZ� 42. 2 [2,2, 1 ]  inZ� 
44-55, solve the given 
43.2([ 3,l,1,2 ]  + [ 3,3,2,l ])inZ! andZ� 
In Exercises 
there is no solution. 
47. 2x = 1 in Z4 
44. x +  3 = 2 in Zs 
=  1 in Z3 
46. 2x 
51. 6x = 5 in Zs 

48. 2x = 1 in Zs 
50. 3x = 4 in Z6 

45. x +  5 =  1 in Z6 

49. 3x = 4 in Zs 

equation or indicate 

that 

52. Bx = 9 in Z11 
54. 4x +  5 = 2 in Z6 
56. (a) For which values 
(b) For which values 

53. 2x +  3 = 2 in Z5 
55. 6x +  3 =  1 in Zs 
of a does x + a = 0 
of a and b does x + a = b 
have a 

tion in Zs? 

have a solu­

solution 

in Z6? 

(c) For which values of a, b, and m does x + a = b 

have a 

solution 

in Zm? 
of a does ax = 1 have a solution 

57. (a) For which values 

in Zs? 

(b) For which values of a does ax = 1 have a solution 
(c) For which values of a and m does ax = 1 have a 

in Z6? 

solution 

in zm? 

In Exercises 15 and 16, simplify the 
Indicate which properties in Theorem 1.1 you use. 
In Exercises 17 and 18, solve for the 

15. 2(a -3b) + 3(2b + a) 
16. -3(a -c) + 2(a + 2b) + 3(c -b) 

vector 

x in terms 

of the 

given vector 

expression. 

vectors a and b. 
17. x -a =  2(x -2a) 
18. x + 2a - b = 3(x + a) -2(2a -b) 

w. 

u and v and locate 

In Exercises 19 and 20, draw the coordinate axes relative to 
19.u = [ _�], v  = [�lw = 2u + 3v 
20. u = [ -� l v = [ _ � l w = -u -2v 
In Exercises 21 and 22, draw the standard coordinate axes 

same diagram 

on the 
as the axes 
these to find w as a linear combinat

ion of u and v. 

relative to u and v. Use 

18  Chapter 1 Vectors 

Length a n d  A n g l e :  The  Dot 

Pro d u ct 

It is quite  easy 
to reformulate 
and angle in terms of vectors. 
powerful ideas 
these 
simple 
in applica

in settings 
geometric 
tools 

tions-even when there 

the familiar 
Doing so will allow 

us to use these 
more general than IR2 and IR3• In subs
will be used to solve a wide variety 

is no geometry apparent 

at all! 

important and 

equent 

chapters, 

of problems 

arising 

geometric concepts of length, 

distance, 

The Doi Producl 
The vector versions 
notion 

of the dot 

product of two vectors. 

of length, 

distance, 

and angle can all be described 
using the 

Definilion 

If 

then the 

dot product 

u · v 

of u and v is defined 

by 

is the 

sum of the products of the 

components 
of u 
corresponding 
, u · v 
just 
that we have 
First, u and v must have the same number of components. 
the dot 
Second, 
called 
product of u and v.) The dot product of vectors 
and im­
in !Rn is a special 
in 

In words
and v. It is important to note a couple of things 
defined: 
product u · v 
not another 
the scalar 
portant case of the more general notion 
Chapter 7. 

product, which we will explore 

is a number, 

about this "product" 

is sometimes 

is why u · v 

vector. (This 

of inner 

Example 1 . 15 

Solution u · v= l ·(-3) + 2 · 5+( - 3)· 2= 1  

Notice 

that if we had calculated 

in Example 

v · u 

1.15, we would have computed 

v · u  = (-3)· 1+5 · 2  + 2 ·( - 3) =  1 

= v · u 
This commutativity 

That u · v 
commute. 
that we will use repeated
in Theorem 1.2. 

in general is clear, 

since 

components 
property is one of the properties of the dot product 

products of the 

the individual 

ly. The main properties of the dot product are summarized 

Section 1.2 Length and Angle: 

The Dot 

Product 1 9  

Theorem 1 . 2  

Let u, v, and w be vectors 
a. u · v  = v · u  
b. u. ( v + w)  = u. v + u. w 
c. ( cu) . v = c( u . v) 
d. u · u 2: 0 

in �n and let c be a scalar. 

Then 
Commutativity 
Distributivity 

remaining 

properties for the 

and v · u, we obtain 

from the fact that multiplication 

of real numbers 

of scalar multiplication and dot 

product, 

we have 

proof of the 

the definitions 

if u = 0 
if and only 

the definition 

of dot product to u · v 

and u · u = 0 

exercis
es. 
(a) Applying 

= V1U1 + VzUz + . . .  + VnUn 
= v · u  
follows 

where the middle equality 
is commutative. 
(c)  Using 

Proof We prove (a) and (c) and  leave 
u · v  = U1V1 + UzVz + . .  ·+ UnVn 
(cu)· v= [ cu1, cu2, . . .  , cu"] • [ v1, v2, •.. , v"] 
CU1V1 + CUzV2 + · · · + CUnVn 
c(U1V1 + UzVz + · · · + UnVn) 
•  Property (b) can be read from 
•  Property (c) can be extended 

factor out a common vector 
a "right-handed" analogue  tha
t follows 
(v + w) · u  = 
v · u 
version 
extended 
of ( c) essentially 
of vectors, 
the scalar can first be combined 
For example, 

says that in taking 

c(u · v) 

+ w · u. 

right 

Remarlls 

in which case it says that we can 
u from a sum of dot products. This property also has 

to left, 

from properties 

(b) and (a) together: 

58). This 
to give u · (cv) = c(u · v) (Exercise 
of a dot product 

a scalar multiple 

with whichever 

vector 

is more convenient. 

(�[ - 1,-3,2])· [6,-4,0] = [-1,-3, 2] ·(�[6,-4, 0]) =  [ - 1,-3, 2] ·[3,-2,0] = 3  
into the vectors, as the 
original 

we avoid introducing 

fractions 

With this approach 
grouping 

•  The second 
signals a double im plicat

this phrase 

would have. 

cusses 

ion-namely, 

part of ( d) uses 
in more detail, 

the logical 
but for the 

connecti
moment let us 

ve if and only if. Appendix 
just note that the 

wording 

A dis­

and 

if u = 0, then u · u 
= 0 
0, then u = 0 
if u · u = 

Theorem 1.2 shows that aspects 

of the algebra 

of vectors 

shows that we can 

sometimes 

resemble the algebra 
find vector 

analogues 

of 
of 

numbers. The next example 
familiar 

ties. 

identi

2 0   Chapter 1 Vectors 

Example 1 . 16 

y 

b 

v  = [�] 

Length 
To see how the dot product plays 
are computed 

a 

Figure 1 . 2 5  

· (u + v) =  (u + v) 

Prove that (u + v) 
Solulion  (u + v) 

for all vectors 
u and v in !Rn. 
u · v) + v · v 
· (u + v) = u · u + 2(
(u + v) 
· v 
=u · u+ v· u+u · v+v · v  
=u · u+ u· v+u · v+v · v  
= u · u  + 2(u · v) + v · v  
(Identify the parts of Theorem 1.2 that were used at each step.) 

· u + 

a role in the calcula

tion oflengths, recall how lengths 

In IR2, the length 

in the plane. 
of the 

The Theorem 
vector 
(a, b), which, by Pythagoras' 
Observe that a2 + b2 = v · v. This leads 

v = [ �] is the distance 

is all we need. 
from the origin 

to the point 
is given by Va2 + b2, as in Figure 
1.25. 
to the following 

of Pythagoras 

Theorem, 

definition. 

Definition 
tive scalar llvll defined 

by 

The length (or norm) of a vector 
vn 

llvll = VV:V  = V vi + v� + · · · +  v� 

v = [ �:] in !Rn is the nonnega-

· 

In words

, the 

length 

of a vector 

is the square root of the sum 

ts. Note that the 
l.2(d). Note also that the 

componen
Theorem 
which will be useful in proving further 
vectors. 

square root of v · v 

is always 
definition 
can be 

defined, 

of the squares 
of its 
2': 0 by 
since v · v 
to give llvll2 = v · v, 
of 

rewritten 

properties of the dot product and lengths 

11[2, 3J11 = v22 + 32 = vT3 

Theorem 

1.3 lists 

some of the main 

properties 
of vector 

length. 

Then 

Let v be a vector in IR" and let c be a scalar. 
v = 0 
a. llvll = 0 if and only if
b. llcvll = lcl llvll 

Proof Property (a) follows 

immediately 

from Theorem l.2(d). To show (b), we have 

llcvll2 = (cv) · (cv) = c2(v · v) = c2llvll2 
ofboth sides, 

square roots 

using 

the factthatW = I cl 

Theorem 

using 
for any real number 

l .2(c). Taking 
result. 

c, gives the 

Example 1 . 11 

Theorem 1 . 3  

Section 1.2 Length and Angle: 

The Dot 

Product 2 1  

In IR2, the set 

A vector 

of length 
with the unit circle, 
the circle of radius 
1.26). Given any nonzero 
vector 
as v by dividing 
v by its 
show this algebraic

1 is called a unit vector. 
v, we can 
own length 

of all unit vectors 
can 
at the origin 
(see 
find a unit vector 
in the 
by 
property (b) of Theorem 1.3 above: 

always 
(or, equivalentl

be identified 
Figure 
same direction 
1/ llvll ). We can 
Ifu = (1/llvll)v, then 

llull = 11(1/llvll)vll = ll/llvll I llvll = (1/llvll)llvll = 1 

1 centered 

ally by using 

y, multiplying 

and u is in the same direction as 
tor in the same direction 
to as 

is often referred 

v, since 1 / II v II is a positive scalar. 

normalizing 

a vector 

Finding 
vec­
a unit 
(see Figure 
1.27). 

y 

v 

>:;:. / � rr�1r 
In IR2, let e1 = [ �] and e2 = [ �]. Then e1 and e2 are unit ve

in lffi2 

Figure 1 . 2 6  
Unit vectors 

ctors, 

Figure 1 . 2 7  
Normalizing 

a vector 

since 

the sum of the 

of their 

squares 
vectors 

components 

is 1 in each 

case. 

Similar

ly, in IR3, we can construct 

unit 

Example 1 . 18 

Observe in Figure 
in IR2 and IR3. 

1.28 that these 

vectors 

serve to locate the 

positive 

coordinate 
axes 

z 

t 
.--+ 

y 

x 
in lffi2 and !ffi3 

Figure 1 . 2 8  
Standard 

unit vectors 

y 

2 2   Chapter 1 Vectors 

Example 1 . 19 

in !Rn, we define unit vectors 

In general, 
component and zeros 
are called 

elsewhere. 
the standard unit 
vectors. 

e1, e2, •.. , en, where e; has 1 in its 

ith 
algebra and 

These vectors arise rep

in linear 

eatedly 

Nmmalfae 

the vectm v � [ -n 
u � (1/llvlllv � (1/v'14{-:J [ 2/\/14] - 1/\/14 

Solulion llvll = V 22 + ( - 1) 2 + 32 = \/14 , so a unit 
vector 
tion as v is given by 

3/\/14 

in the same direc­

Since 

1.3 describes 
ty suggests 

scalar multiplication, 
natural 
tor addition 
are compatible. 

:s, the resulting 
llu 11 + llvll, but for almost 

how length 
property (b) of Theorem 
curiosi
that we ask whether 
It would be nice if we had an identity 
u and v this turns 
any choice 
all is not lost, for it 
is true. 
on another important 

Exercise 52(a).] However, 
inequality 
Inequality-
relies 

turns 
out that if we replace 
of this famous and important 

inequality-

of vectors 

such as II u + v 
II = 

out to be false. [See 
the = sign by 

result-the 
the Cauchy-Schwarz 

to 
length 

behaves with respect 

The proof 

and vec­

which we will prove and discuss 

in more detail 

in Chapter 7. 

Triangle 
Inequality-

Theorem 1 . 4  

The Cauchy-Schwarz Inequality 

For all vectors 

u and v in !Rn, 

lu· vl :s llnll ll

vll 

- u 

Figure 1 . 2 9  
The Triangle 

Inequality 

71 and 72 for algebraic 

See Exercises 
inequality. 

and geometric 
In IR2 or IR3, where we can use geometry, it is clear 

Figure 1.29 that llu + vii :s llnll + llvll for all vectors 

this is true 

more generally. 

from a diagram 
such as 

u and v. We now show that 

approaches 

to the proof 

of this 

Theorem 1 . 5  

The Triangle 

Inequality 

For all vectors 

u and v in !Rn, 

llu + vii :s llull + llvll 

Proof Since 

�  lent to proving the theorem

both sides 
side is less than or equal 
. (Why?) 

the left-hand 

Section 1.2 Length and Angle: 

The Dot 

Product 2 3  

of the inequality are nonnega

tive, 

showing that the square of 

to the square of the right-hand side is equiva­
We compute 

llu + vll2 = (u + v)- (u + v) 

= u · u  + 2(u · v) + v · v  By Example 
::; llull2 + 2lu · vl + llvll2 
::; llnll2 + 2 llnll ll
= ( llnll + llvll)2 

vll + llvll2 By Cauchy-

Schwarz 

1.9 

as required. 

Distance 
The distance 
points on the 
line (Figure 1 .30), the distance 
ing the absolute 
This distance 
the familiar 

is also 
formula for the 

is the direct 
between two vectors 
real number line or two points in 

analogue 
the Cartesian 

of the distance 
plane. On the 

between two 
number 

between the numbers a and b is given 
that we do not need to know which of a orb is larger.

by la -bl. (Tak­

distance

value ensures 

equal to V (a  - b) 2, and its two-dimensiona

) 
is 
d between points (a1, a2) and (b1, b2)-namely, 

4 I  I  I + -2 
I  I + I  I � 
d =V(a1 - b1)2 + (a2 -bi)2• a 
b 
d = la - bl = l-2 -31 = 5 
In terms of vectors, if a = [ :: ] and b = [ �:], then d is just the length 
of a -b, 

basis for the next definition. 

as shown in Figure 
1.31. 

This is the 

Figure 1 . 3 0  

3 

0 

l generalization 

I I I 
I : a2 -b2 
d 
I I I I 
_f] 
___________ 
a1 -b1 
d = V(a, - b,)2 + (a2 - b
)2 = Ila - bll 
d ( U, v) = II u - v II 

d(u, v) between 

The distance 

Definition 

Figure 1 . 3 1  

2

vectors 

u and v in u;gn is defined 

by 

2 4   Chapter 1 Vectors 

Example 1 . 2 0  

between u � [ 1] •nd v � [ _ � l · 

Find the 

di,tance 

Solution We rnmpute u - v � [ �J '" 

d(u,v) = llu -vii= V(\/2 )2 + (- 1)2 + 12 = V4 = 2 

v 

Figure 1 . 3 3  

te the angle between a pair of vectors. 
vectors 

to the 
angle(} 

u and v will refer 
0 :::::: (} :::::: 180° (see Figure 
1 .3 2). 

Angles 
The dot product can also be used to calcula
In IR2 or IR3, the angle 
between the nonzero 
determined 
by these 
that satisfies 

vectors 

v'D 
v 
� u 
u 
(}  ./7  u 
v4 
u 
0 
u and v 

Figure 1 . 3 2  
The angle 

between 

In Figure 

1 .33, consider 
between u and v. Applying 

the triangle 
with sides 
the law of cosines 

ngle yields 
llu - vll2 = llull2 + llvll2 -2 llull ll
vll cos(} 

to this tria

u, v, and u - v, 

where(} is the angle 

Expanding 

several 

the left-hand side and using llvll2 = v · v 
llull2 - 2(u ·v) + llvll2 = llull2 + llvll2 - 2llull ll
vll cos(} 
simplifica
tion, 
formula for the cosine 
a definition. 

us with u · v 
of the 

= llull ll

leaves 

angle 

after 

which, 
the following 
We state 

it as 

vll cos(}. From this we obtain 

(} between nonzero 

vectors 
u and v. 

times, 

we obtain 

Definition 

vectors 

u and v in !Rn, 

For nonzero 
cos(} = 

u · v  
llull ll
vll 

Example 1 . 2 1  

Compute the angle 

between the vectors 

u = [2, 1, - 2] and v = [1, 1, 1 ] .  

Section 1.2 Length and Angle: 

The Dot 

Product 2 5  

Solution We calculate 

V9 =  3,  and 
e = cos-1(1/3v3) = 1.377 radians, 

u·v = 2 · 1  + l · l  + (- 2)· 1 = 1, JJuJJ = V22 + 12 +(-2)2 = 
cos e = 1/3v3, so 

JJvJJ = Vl2 +  12 +  12 = v3. Therefore, 

or 78.9°. 

...._+ 

Example 1 .2 2  

Compute the angle 

between the diagonals 

on two adjacent 

faces of 
a cube. 

so we will work with a cube 

matter, 
to the coordinate 
to be the vectors 

axes in IR3, as shown 
[1, 0, 1 ]  and [O, 1, 1 ] .  

of the cube do not 

Solution The dimensions 
with sides 
in Figure 1.
Then angle 

1. Orient the cube relative 
oflength 
34, and take the two side 
diagonals 
satisfies 
l ·O+ O· l+ l· l  
2 

e between these 
cos e = -------

vectors 

from which it follows 

that the 

required 

angle is n /3 radians, 

or 60°. 

\/2 v'2 

z 

[O, 1 ,  l] 

y 

Figure 1 . 3 4  

x 

draw 
If we 
at (1, 0, 1) and (O, 1, 1), we get an equila
teral 

at all to get this answer. 

are of equal length. The angle we 

want is one of 

60°. Sometimes, 

a little 
insight can 

tions 

joining 

we don't 

(Actually, 

need to do 

of this triangle 

any calcula
a third side diagonal 
the vertices 
all of the side diagonals 
triangle, 
since 
and therefore 
the angles 
save a lot of calcula
in this case, 

•  As this discussion shows, 

it gives a 

measures 

tion; 

Remarks 

we usually 
to the angle between two vectors. However, 
special angles 
to recognize 
all other 
cases, 
by means of the 

of the 
of these), we should 
(0°, 30°, 45°, 60°, 90°, 
angle exactl
its cosine (Table 
y. In 
to approximate 
the desired angle 

will have to settle for an approximation 
when the angle is one 

we will use a calcula
inverse cosine 

1 . 1 )  and thus give the corresponding 

so-called 
be able 

tor or computer 

or an integer 

function. 

multiple 

nice check on our work!) 

e Table 1 . 1  cosines or Special Angles 
cos e  V4 - = 1 

30°  45° 
v'2  1 
v3 
2 
2  v'2 

2 

60° 

Vi 
2  2 

90° 
Vo - = O 
2 

2 6   Chapter 1 Vectors 

•  The derivation of the formula 
the Cauc�y-Schwarz 
�mplies 
!�equality 

is valid 
IR n, for n > 3, the formula 

only in IR2 or IR3, since 

- 1  to 1, JUSt as the 

cosme funct10n 

for the cosine 

of the angle 
on a geometric 
a definition 
that 
:::::: 1, so 
does. 

I u · v  I u · v  

instead. 

llu II llvll 

llu II llvll 

can be taken as 

it depends 

between two vectors 
fact: the law of cosines. 

In 

This makes sense, 
ranges 

since 
from 

orthos, 

The word orthogonal is derived 
from the Greek words 
ing "upright;' 
"angle:' 
literally 
means "right-angled'.' 
equivalent 

and gonia, meaning 
orthogonal 

The Latin 
is rectangular. 

Hence, 

mean­

ty is fundamental to geometry. Anyone studying 

Veclors 

Orthogonal 
The concept of perpendiculari
geometry 
eralize 

the importance 
ty to vectors 

the idea of perpendiculari

realizes 

quickly 

and usefulness 
of right 

angles. We now gen­

in !Rn, where it is called 
u and v are perpendicular 

orthogonality. 

(J between 

In IR2 or IR3, two nonzero vectors 

them is 
and it 

a right 
follows 

angle-that 
that u · v 

90°. Thus, 
= 0. This motivates the following 

is, if (J =  1T /2 radians, or 

if the angle 
u · v  
llu II llvll 

definition. 

= cos 90° = 0, 

DefiniliOD 

Two vectors u and v in !Rn are orthogonal to each other 

if u ·v = 0. 

Since 0 · v = 0 for every vector 

v in !Rn, the zero vector is 

orthogonal 
to every 

vector. 

Example 1 . 2 3  

In IR3, u = [l, 1, -2] and v 

= [3, 1, 2] are orthogona

l, since 

u · v 3 + 1 - 4 = 0. 

Using the notion 
in !Rn. 
valid 

of orthogon

ality, 

we get an easy proof 

of Pythagoras' 

Theorem, 

Theorem 1 . 6  

Pythagoras'1heoreITI 

u and v in lRn, llu + vll2 =  llull2 + llvll2ifandonlyifu andv are 

Forallvectors
orthogonal. 

Proof From Example 

vectors u and v in !Rn. It follows 
only if u · v = 

0. See Figure 
1.35. 

1.16, we have llu + vll2 = llull2 + 2(u ·v) + llvll2 for all 
that llu + vll2 = llull2 + llvll2 if and 

immediately 

The concept of orthogona

lity is one of the most important 

in surprising 

ways. 

Chapter 5 contains 

but we will encounter 

it many times before then. One 

the distance 

from a point to a line, 

and useful in linear 
a detailed 
treatment 

problem in which 
where "dropping 

a 

arises 

algebra, and it often 
of the topic, 
it clearly 
plays 
perpendicular" 

a role is finding 
is a familiar 

step. 

Figure 1 . 3 5  

_..,.,.--u 
of v onto u 

figure 1 . 3 1  
The projection 

Section 1.2 Length and Angle: 

The Dot 

Product 21 

Proiecli
ons 
We now consider 
context 
projection 

of a vector 

the problem of finding 
of vectors. As you will see, 

onto another 

this technique 
leads 
vector. 

the distance 

from a point to a line in the 
to an important concept: 
the 

As Figure 1.36 shows, 

----" -----> 

(in IR2 or IR3) reduces 

line € 
line segment PB or, equivalen
on€, then, 
the hypotenuse 
AB. AP is called 
at this situation in terms of vectors. 

in the right-

angled 

the problem of finding 
to the problem of finding 

of the 

tly, the length 
triangle 

vector 
fl.APB, the 
other 
the projection 

-----; 

the distance 

the length 

from a point B to a 
of the perpendicular 

PB. If we choose a point A 

two vectors 

are the leg 
AP and 

of AB onto the 

line€. We will now look 

"----> 

B 

B 

e 

A 

between u and v, as 

by dropping 

unit vector 

in 

tution, 

from a point to a line 

Consider 

obtained 

1.37. Then clearly

Figure 1 . 3 6  
The distance 

e be the angle 

elementary 
substi

trigonometry 
we obtain 

a perpendicular 
shown in Figure 
the direction 

two nonzero vectors u and v. Let p be the vector 
from the head of v onto u and let 
p = llP llu, where u = (I/ llu II )u is the 
gives II p II = II v II cos e, and 
of u. Moreover, 
u·v 
we know that cos e = II u II llv 11. Thus, after 
p = llvllC1uil·l�vll)(ilf )u 
(u·v) = �u =(�)u 
u·u 
If u and v are vectors in !Rn and u * 0, then the projection of 
by (u·v) 
proj0(v) = --u 
u·u 

v onto u is the 

the formula we want, and it is the basis of the 
following 

proju(v) defined 

This is 

Definition 

vector 

tors in !Rn. 

An altern
ative 

way to derive this formula is described 

in Exercise 

73. 

definition 

for vec­

2 8   Chapter 1 Vectors 
u 

proju(v) 

Figure 1 . 3 8  

Remarks 

cast, 

projector, for example

v occurs only once on the 

and perpendicular 
or projected, 

(with a slide 
other 
shadow 

•  The term projection 
•  It may be helpful to 
•  Although 
•  If the angle 
•  If u is a unit vector 

as u to be nonzero (why?), it is clear 
zero vector 
Ou= 0. 

the opposite direction 
then pro

variable 
to remember 
vector 

in our derivation 

u (not v). 

Figure 

comes from the idea of projecting 

an image onto a wall 

). Imagine 

a beam oflight 

with rays parallel to each 

to u shining down on v. 
by v onto u. 
think of proj0(v) as a function 

The projection 

right-hand side of the 

definition. 

Also, 

with variable 

v. Then the 
it is helpful 

of v onto u is just the 

1.38, which reminds 

us that proj0(v) is a scalar multiple 

of the 

of the definition 

of proj0(v) we required 
v as well 
projection 
of the 

that the 

geometry 

from the 
onto u is 0. The definition 
is in 

agreemen

t with this, 

between u and v is obtuse, as in Figure 

from u; that is, proj0(v) will be a negative 

1.38, then proj0(v) will be in 
of u. 

scalar multiple 

j0(v) = (u · v)

u. (Why?) 

since (". 0)u = 

u · u  

Example 1 .24 

Find the 

projection 

of v onto u in each case. 

3 

Solution 

1/\/2 

(a)  v = [ -�] and u = [ �] 

(a) We compute u · v = [ �] · [ -�] =  1 and u · u = [ �] · [ �] = 5, so 

(c) v = [�]and u = [ ���] 
(u · v) 1[2] [2/5] 
e3 is a unit 
proj0(v) = (u · v)u = (� +  1  + �)[ ��� ] = 3(1 � \/2 ) [ ��� ] 
� 3 (I : V2J [ � l 

proju(v)= u · u"=s 1= 1/5 
vector, 

llull = V� + � + t = 1 .  Thus, 

(c) We see that 

(b) Since 

1/\/2 

1/\/2 

J Exercises 

1 . 2  

In Exercises 1-6,find u · v. 

.. 

Section 1.2 Length and Angle: 

The Dot 

Product 2 9  

GAs 27. Exercise 21 
GAs 29. Exercise 23 

( - 3, 2), B = (1, 0), and C = 
triangle. 

(4, 6). Prove that 

31. Let A = (1, 1, - 1), B = ( - 3, 2, -2), and C = (2, 2, -4). 
Prove that �ABC is a right-angled 
triangle. 
of a cube and an ad­

between a diagonal 

26. Exercise 20 

30. Let A = 

MBC is a right-angled 

GAs 28. Exercise 22 
GAs 32. Find the angle 
d, � [H andd, � [-�-

33. A cube has four diagona
are perpendicular. 

34. A parallelogram 
has diagona

jacent edge. 

vectors 

ls. Show that no two of them 

Show that the parallelogram 
equal 
and determine 
ABCD has vertices 

35. The rectangle 

is a rhombus 
the side length. 

length) 

(all sides 

of 

B = (3, 6, - 2), and C = (O, 5, -4). Determine 
coordinates 
D. 
36. An airplane 

of vertex 
heading due east 

has a velocity of 
per hour. A wind is blowing 

hour. What is the resultant 

from the north 
velocity of 

200 miles 
at 40 miles per 
the airpla
ne? 

at A = (1, 2, 3), 
the 

37. A boat heads nor

th across 
current 
find the resu

is flowing 
ltant 

at a rate of 
east at a rate of 
velocity 

If the 
per hour, 

a river 

4 miles 

of 

per hour. 
3 miles 
the boat. 

ls determined 

by the 

38. Ann is driving 

a motorboat across 
t has a speed 

a river 
of 20 km/h 

that is 2 km 
in still 

in the river 

wide. 
The boa
the current 
out from one bank of the 
from her on the 
direction 
(a) How far 

opposite bank. She drives 
the current. 
from the dock will 

downstream 

perpendicular to 

is flowing 

river 

the boat in a 

at 5 km/h. Ann heads 

for a dock directly 

across 

water, 

and 

Ann land? 

(b) How long will it take Ann to cross 

the river? 

39. Bert can swim at a rate of 2 miles per hour 

in still 

in a river 

The current 

at a rate of 
water. 
1 mile per hour. If Bert wants to swim across 
to a point directly opposite, at what angle 
to the bank 
of the river must he swim? 

is flowing 

the river 

5.u =  [1,\/2 , \/3,o ] , v=  [4,- \/2, 0, -5 ]  

2.07, 
- 1.83 ] ,  
- 2.29, 1.72, 4.33, 
- 1.54] 

v =  [ 

GAS 6. U =  [ 1 . 1 2, -3.25, 
GAs 10. Exercise 

exercise, 

3 

of u. 

and give 

8. Exercise 2  9. Exercise 

In Exercises 7-12, find II u II for the given 
in the direction 
a unit vector 
7. Exercise 
1 
4 

1 1 .  Exercise 5 GAs 12. Exercise 6 
3 GAs 16. Exercise 4 

In Exercises 13-16,find the distance d(u, v) between 
v in the given 
13. Exercise 
15. Exercise 
17. If u, v, and w are 

exercise. 
1  14. Exercise 2 

in ll�r, n 2: 2, and 
c is a 

vectors 

u and 

explain 

why the following 

expressions 

make 

scalar, 
no sense: 
(a) llu · vll 

(c) u ·  (v · w) 

(b) u · v  + w 
(d) c. ( u + w) 

In Exercises 18-23, determine whether 
u and v is acute, obtuse, or a right angle. 

the angle between 

GAS 2 1 .  U = [0.9, 

2 . 1 ,  1.2], V = [ - 4.5, 2.6, - 0.8] 

20. u = [4, 3, - 1] ,  v = [l, - 1, l ]  

22.u =  [1,2,3,4],v =  [ - 3,1,2,-2] 
23.u =  [1,2,3,4],v =  [5,6,7,8] 

In Exercises 24-29, find the angle between 
given 
24. Exercise 18 

exercise. 

25. Exercise 19 

u and v in the 

3 0   Chapter 1 Vectors 

In Exercises 40-45, find the projection of v onto u. Draw a 
sketch 

in Exercises 40 and 41. 

[ 3.01] [ 1 .34] 
CAS 45. U = -0.33 , V = 4.25 

2.52 - 1.66 
1 .39 suggests two 

Figure 
may be used to compute the area of a triangle. 
The area A of 

ways in which vectors 

(a) 

u 

(b) 

Figure 1 . 3 9  

thetriangleinpart(a)isgivenbytllull  ll
and part (b) suggests 
area of a triangle: A = t II u II  II 
identity 

sine = v 1  -cos2 e to find sine.) 

v -proju(v) II, 
form of the 

v II sine (We can use the 

the trigonometric 

given vertices 

In Exercises 46 and 47, compute the area of the triangle 
with the 
46. A = (1, - 1), B = (2, 2), C = (4, O) 
47. A = (3, - 1, 4), B = (4, -2, 6), C = (5, 0, 2) 

using both methods. 

k for 

s are 

orthogonal. 

the two 
vector

In Exercises 48 and 49, find all values of the scalar 
which 

48. u � [: Jv � [� � : l 49. u � [ -J � [_fl 

v = [;] 

that are orthogona

50. Describe 

all vectors 

l 

51. Describe 

all vectors 

v = [;] 

to u = [�]. 
to u = [�]. 
(a) llu + vii = llull + llvll (b) llu + vii = llull -llvll 

u and v in IR2 or IR3? 

are the following 
true for 

vectors 

52. Under what conditions 

that are orthogonal 

1.2(b). 
53. Prove Theorem 
1.2(d). 
54. Prove Theorem 

the stated property of distance 

In Exercises 55-57, prove 
between 
vector
u and v 
55. d(u, v) = d(v, u) for all vectors 

56. d(u, w) :s d(u, v) + d(v, w) for all vectors 

u, v, and w 

s. 

57. d(u, v) = 0 if and only 
58. Prove that u 
and all scalars c. 

ifu = v 

· c v = c( u · v) for all vectors 
in !Rn 

u and v 

in the Triangle 

u by u - v 

60. Suppose 

u and 
59. Prove that llu - vii 2': llull -llvll for all vectors 

we know that u 

· v = u · w. Does it follow that 

otherwise, 
vectors 

v in !Rn. [Hint: Replace 
Inequality.] 

give a proof that 
give a counterexample 

v = w? If it does, 
= llull2 -llvll2 for all vec­
u, v, and w for which u · v = u · w but v -=F w). 
62. (a) Prove that llu + vll2 + llu - vll2 = 2llull2 + 2llvll2 
(h) Draw a diagram showing 

61. Prove that (u 
+ v) · (u  - v) 

is valid 
(i.e., 

u and v 

tors u and v in !Rn. 

in !Rn. 

in !Rn; 

a specific set of 

(a) to deduce 

a result about 

for all vectors 
in IR2 and use 
parallelograms. 
1 
63. Prove that u 
4 
vectors 
in !Rn. 

u and v 

· v = -llu + vll2 --llu - vll2 for all 

1 

4 

u, v, u + v, and u - v 

Section 1.2 Length and Angle: 

The Dot 

Product 3 1  

64. (a)  Prove 

that llu + vii = llu - vii if and only if u and 
u, v, u + v, 
and u - v 

v are orthogonal. 
(b) Draw a diagram showing 
in IR2 and use 
(a) to deduce 
parallelograms. 

65. (a) Prove that u + v 

and u - v are orthogona
l in !Rn if 

a result 

about 

72. Figure 1.

40 shows that, 

in IR2 or IR3, 

llproju(v) II :::::: llvll· 
(a)  Prove 

that this inequality 

is true 

in general. 

Prove that proju(v) is orthogonal to 
and use Pythagoras' 

[Hint: 
v-proju(v) 

Theorem.] 
(b) Prove that the inequality 

equivalent 
to the 

Cauchy-Schwarz 

llproju ( v) II :::::: llvll is 
Inequality. 

(b) Draw a diagram showing u, v, u + v, and u - v 

(a) to deduce 

a result 

about 

and only ifllull = llvll. 
in IR2 and use 
parallelograms. 

to v + w. 
l to sv 

66. If llull = 2, llvll = v'3 ,  and u · v = 1, find llu + vii. 
67. Showthatthere 
llvll = 2,andu ·v= 3. 

68. (a) Prove that ifu is orthogonal to 

u and vsuch that llull = 1, 

are no vectors 

u is orthogonal 

both v and w, then 
to both v and w, then 

(b) Prove that if u is orthogonal 

u is orthogona

+ tw for all scalars sand t. 
69. Prove that u is orthogonal to 

v -proju(v) for all 

vectors 

(a) and (b) geometric
ally. 

u and v in !Rn, where u * 0. 

71. The Cauchy-Schwarz 

proju(v)· 
proju(v -proju(v)) = 0. 

(b) Prove that 
(c) Explain 

70. (a) Prove that proju(proju(v)) = 
(a) In IR2, with u = [�:]and v = [:J this becomes 

I u · v i  :::::: llull ll

equivalent 
to the 
· v)2:::::: llull2 llvll2. 
sides: (u 

we get by squaring 

Inequality 

inequality 

vll is 
both 

u 

proju(v) 

Figure 1 . 4 0  

73. Use the fact that proju(v) = cu for some scalar c, to­

gether 
the formula 

with Figure 
for proju(v). 

1.41, 

to find c and thereby derive 

v -cu 

u 

cu 
+ vnll ::=::: llv1ll + llvzll + · · · 

+ llvnll 

Figure 1 . 4 1  

ally. [Hint: Subtract 
hand side and show that the 

the left-hand 

Prove this algebraic
side from the right-
must necessarily 
difference 
of (a) in IR3. 
(b) Prove the analogue 

be nonnegativ

e.] 

of the 

generalization 
llv1 + Vz + · · ·
for all n 2: 1. 

74. Using mathematical 
induction, 
prove the following 
Inequality
Triangle 
: 

Exploration 

Vectors  and 

G e ometry 

Many results 
For example, 
this exploration, 
Euclidean 

geometry. 

in plane Euclidean 
in Example 

1 .24, we used vectors 

can be proved using vector 

techniq
ues. 
to prove Pythagoras' 
Theorem. In 

geometry 

we will use vectors 

to develop 

proofs for some other theorems 

from 

As an introduction 

to the 

notation and the basic 

approach, 

consider 

the following 

easy example. 

Example 1 . 2 5  

A 

a = GA, b = GB, m  = oM, and 
If 0 denotes 

the origin 

and 

Give a vector 

description 

notation. 

of the midpoint M of a line segment AB. 

vector 

to vector 

everything 

----->  ------> 

Solution We first convert 
P is a point, 
AB = OB -OA = b -a (Figure 
1.42). 

----> 
let p be the 
M is the midpoint of AB, we have 

GP. In this situation, 
m  - a = AM = fAB = t (b - a) 
m  = a +  t (b - a) = t (a +  b) 

Now, since 

so 

B 

Figure 1 . 4 2  
The midpoint of AB 

1 .  Give a vector 

description of 

the point P that 

is one-third of the way from A to 

B on the line 

segment AB. Generalize. 
joining 

the line segment 

2. Prove that 

the midpoints 

of two sides 

of a triangle 

is 

prove that PQ = t AB in 

c 

parallel 
Figure 

to the third side and half 
1.43.) 

(In vector 

as long. 

notation, 

4. A median 

points of the sides 

3. Prove that the quadrilateral 

PQRS (Figure 1 .44), whose vertices 
gram. 
of an arbitrary 
of a triangle is a line segment from a vertex to 
the mid
triangle are con­

quadrilateral 

ABCD, is a parallelo

1 .45). Prove that 

of any 
they have a common point of intersection) 

the opposite side (Figure 
current (i.e., 
thirds of the distance 
Figure 1.46, show that the point that is two-thirds 

to the midpoint of the opposite side. [Hint: 
from A to P is given 
that t (a + b + c) is two-thirds of the distance 

by t( a + b + c). Then show 

from each vertex 

at a point G that is two­

the three 

distance 

medians 

of the 

are the mid­

point of 

from B 

to Q and two-thirds of the distance 
the centroid 

of the 

triangle. 

from C to R.] The poi

nt G in Figure 

1.46 is called 

In 

.... B 

A ----------

3 2  

Figure 1 . 4 3  

A 

A 

B 

B 

Figure 1 . 4 4  

c 

Figure 1 . 4 5  
A median 

c 

Figure 1 . 4 6  
The centroid 

5. An altitude 

of a triangle 

is a line segment from a vertex 

that is perpendicu­

lar to the 
concurr
Figure 
the orthocenter 

gure 1 .47). Prove that 
the three 

altitudes 
of a triangle 
opposite side (Fi
ent. [Hint: Let H be the point of intersection of the altitudes 
from A and B in 

1.48. Prove that cH is orthogona

l to AB .] The point H in Figure 1 .48 is called 

are 

6. A perpendicular 
of a line segment is a line through 
to the 
perpendicular 
segment 
the segment, 
the three 
bisect
a triangle 
sides of 
tersection 
of t� erpendicular 
bisectors 
to AB.] The point K in Figure 
orthogonal 

are concurr
ent. [Hint: 
of AC and BC in Figure 
1.50 is called 

ors of 

RK is 

Let K be the point of in­

the circumcenter 

of the 

triangle. 

(Figure 1.49). Prove that the perpendicular 

1.50. Prove that 

the midpoint 

of 

of the triangle. 
bisector 

c 

Figure 1 . 41 
An altitude 

A 

B 

A-- � -* ��������
Figure 1 . 4 8  
The orthocenter 

--=e B 

Figure 1 . 4 9  
A perpendicular 

bisector 

7. Let A and B be the endpoints 

circle, prove that LACE is a right 
circle. Express 
everything 
that the  line 
bisect 

quadrilateral 
each other 

8.  Prove 

in terms of 

(Figure 1.
52). 

of a diameter 

a and c and show that AC is orthogonal 
let 0 be the center 
to BC. J 

If C is any point on the 
of the 

angle. [Hint: In Figure 1.51, 

of a circle. 

segments joining the midpoints of opposite sides 

of a 

c 

0 

R 

Figure 1 . 5 0  
The circumcenter 

Figure 1 . 5 1  

D 

Figure 1 . 5 2  

c 

3 3  

3 4   Chapter 1 Vectors 

lines a n d  Planes 

lines in 

We are all familiar 
to consider 
approach 
the linear 
etry oflines 
a problem will serve you well. 

with the equation of a line in the Cartesia
IR2 from a vector 
us to generalize 
we will consider 
and planes; the ability 

The insights 
in IR3 and then to 
has its origins 
and to think 
these 

chapters 
in later 
to visualize 

will allow 
algebra 

We now want 
in IR3. Much of 
in the simple geom­
geometrica

point of view. 
to lines 

we obtain 

n plane. 

planes 

lly about 

from this 

Lines in �2 and �3 

the general form of the equation 

In the xy-plane, 
the equation 
[This is the slope-intercept 
k) is its y-intercept.
nates (O, 

as y = -(a/b)x + c/b, which has the 
form; m is the slope 
J To get vectors 

line, 
into the picture, 

is ax +  by = c. If b * 0, then 
form y = mx + k. 
and the point with coordi­
consider 
let's 

can be rewritten 

of a line 

of the 

an example. 

The lef

the origin. 

passing through 

The vector 
is parallel 

t-hand side of the equation is in the form 

product; in fact, if we let n = [ �] and x  = [;], then the equation becomes 
equation n . x = 0 is the 

The line C with equation 2x + y = 0 is shown in Figure 1 .53. It is a line with slope -2 
of a dot 
n · x = 0. 
is orthogonal to any vector 
of e. 
to the line. 
The 
at time t = 0 and it moves along 
1 unit per second. 
values 
negative 
the past), at t = -2 it is (or was) at ( -2, 4). 

normal 
about this line is to 
imagine 
is initially 
line. 
the line in such a way that its x-coordinate 
the particle 
(i.e., 

Another way to think 
Suppose the particle 
at the origin 

is at ( 1, -2 ), at t = 1 .5 it is at ( 1 .5, - 3 ), and, if we allow 

n is perpendicular 
to the line 

form of the equation 

(Figure 1 .54)-and it is called a normal 

to the line-that is, it 

Then at t = 1 
oft 

where the particle 

we consider 

a particle 

changes 

moving 

was in 

vector 

along the 

x that 

y 

y 

Example 1 . 2 6  

norma refers 
used for draw­
a normal 

The Latin word 
to a 
carpenter's 
ing right 
vector 
to something 

square, 
angles. Thus, 

usually 

else, 

a plane. 

is one that is perpendicular 

The line 2x + y = 0 

Figure 1 . 5 3  

n 

Figure 1 . 5 4  
A normal 

vector 

Section 1.3 Lines 

and Planes 3 5  

This movement 
may write 

is illustrated 
in Figure 
this relationship in vector 

1.55. In general, if x = t, then y = -2t, and we 
form as 

What is the significance 

d = [ _ �]? It is a particular 

vector 
of the vector 
for the line. As shown in 

parallel 

a direction 
vector 

toe, called 
equation of e as x = td. This is the vector 
slightly. 

does not pass through 

If the  line 

form of the 
the origin, 

56, we may write 
the 

Figure 1.
equation of the line. 
then we must modify things 

y 

y 

Example 1 .21 

Figure 1 . 5 5  

Figure 1 . 5 6  
A direction 
vector 

e 

d 

that the vectors 

and a normal vector 

2x + y = 5 (Figure 1.57). This is 

Thus, n is orthogonal to 
every vector 
a general 

just the line 
from 
is the 
5 units. It also has slope -2, but its y-intercept 
1.26 are, respecti
vely, 

the line C with equation 
Consider 
to e. The point P = (1, 3) 
Example 
1.26 shifted upward 
---+ 
is on C. If X = (x, y) represents 
d and n from Example 
point (O, 5). It is clear 
PX = x - pis 
we haven· x = n · p. 
toe and n · (x - p) = 0 (see Figure 1.58). Simplified, 
for this line 
direction 
vector 
point on C, then the vector 
= 2x + y and n · p = [ �] · [ �] = 5 

= [ �] · [;] 

parallel 
As a check, 

that is parallel 

we compute 

n · x 

too. 

a 

Thus, the normal 
form of the 
equation 
n · p = 0 gave the right-

of the line. 
equation.) 

hand side of the 

(Note that in Example 

form n · x 

= n · p is just a different 

representation 
1.26, p was the zero vector, so 

of the general 

3 6   Chapter 1 Vectors 

y 

y 

n 

... x 

-t--j--+--t--t--f--\-t-jl-t-
The line 2x + y = 5 

figure 1 . 5 1  

These results lead to the following 
definition. 

Definition 

The normal 

where pis a specific 

The general 

form of the equation 

normal vector 
for e. 

figure 1 . 5 8  

n • (x -p) = 0 
point one and n =F 0 is a normal 
n · (x -p) = 0 or n · x = n 

form of the equation 

· p 

of a line e in IR2 is 

vector 

1.27, let us now find the 

fore. [ab] i·s a 
d. That is, x -p = td or x = p + td for some scalar t. In 

of e is ax + by = C, where n = 
of X, x -p must be parallel 
[;] [ �] + t[ -�] 
x = 1 + t y = 3 -2t 

form of the equation 

vector 

(2) 

(1) 

Continuing 

with Example 
for each choice 

of e. Note that, 
of-the direction 
vector 
terms of components, 

we have 

or 

to-and thus a multiple 

The word parameter and the cor­
responding 
adjective 
parametric 
come from the Greek words 
para, 
meaning "alongside;' 
and metron, 
meaning "measure:' 
Mathemati­
speaking, 
cally 
in terms 
variable 
variables 
are expressed-a 
"measure" 
placed 
old ones. 

a parameter 

alongside 

of which other 

new 

is a 

Equation 

parametric 

( 1) is the vector form 
equations 

of the equation 
of the line. The 
t is called 
IR3? Observe 
How does all of this generalize to 
over perfectly. 
to three 
leading 

variable 
that the vector 
The notion 
dimensions
to the following 

tions (2) are called 
forms of 
in IR2-which 
convenient 

of a line carry 
to generalize 
of a direction 

of a line 
-is replaced by the more 

of€, and the componentwise 
Equa­

is difficult 

the equations 

of the slope 

definition. 

vector, 

notion 

a parameter. 

and parametric 

Definition 

The vector 

form of the equation of a line e in IR2 or IR3 is 

point one and d =F 0 is a direction 

vector 
fore. 
corresponding to the components 
of the vector 
parametric equations of e. 

x = p + td 

where pis a specific 

The equations 

equation are called 

form of the 

Section 1.3 Lines 

and Planes 31 

often abbreviate 

We will 
normal, vector, and parametric 

slightl
y, referring 
of a line or plane. 

this terminology 

simply 

to the general, 

equations 

and parametric 

to the mtm d � [ -:J 

equations 

x = p + td is 

Solution The vector 

equation 

pornllel 

Example 1 .2 8  

Find vector 

of the line 

in IR3 through 

the point P = ( 1, 2, - 1  ), 

The parametric 
form is 

x = 1 + St 
y =  2 - t 
z = - 1  + 3t 

Remarks 

•  The vector 
p and any direction vector 

forms of the equation of a given line e are not 
many, since 
fore. However, 

we may use any point on e to de­

and parametric 

are infinitely 

unique-in fact, there 
termine 
multiples 

of each other. 

In Example 

1.28, (6, 1, 2) is another 
vector. Therefore, 

another 

direction 

point on the line (take t = 1), and -2 is 

[ 1 o l 

6 

all direction vectors are 

clearly 

(but equivalent) 

gives a different 
the two parameters 
a given point (x, y, z) one, we have 

for the line. 
s and t can be found by comparing 

equation 

vector 

The relationshi
the parametric 

equations: For 

p between 

x = 1 + St= 6 + 10s 
y = 2 -t = 1 - 2s 
z = - 1  + 3t = 2 + 

6s 

implying that 

- 10s + St =  S 

2s - t = -1 
-6s + 3t = 3 
to t = 1 + 

2s. 

Each of these 

equations 

reduces 

3 8   Chapter 1 Vectors 

Example 1 . 2 9  

•  Intuit

ively, 
" will be clarified 

we  know  that  a  line 

is a one-dimensional 

object. The idea of 
moment observe that 

but for the 

in Chapters 3 and 6, 

that the 

with the fact 

vector 

form of the equation of a line 

"dimension
this idea appears to 
agree 
requires 
one parameter

. 

hears 

the expression 

line f in IR3 determined 

One often 
of the 

"two points determine 
by the points P = ( - 1, 5, o) and Q = ( 2, 1, 1). 

Solulion We may choose any point on f for p, so we will use P ( Q would also be 
is d = PQ = -4 (or any scalar multiple 

fine). 
A convenient direction 
vector 
Thus, 

[ 3] 

Find a vector 

we obtain 

a line:' 

of this). 

1 

equation 

Figure 1 . 5 9  

n is orthogonal 

to infinitely 

many 

vectors n 
n · (x -p) = 0 

p 

Figure 1 . 6 0  

Planes i n  lR3 

we should 

The next question 
tion of a line generalize 
general 
line in IR3. In normal form, this equation 
vector 

ask ourselves 
is, How does the general form of the equa­
that if ax + by = c is the 
of a line in IR2, then ax + by + cz = d might represent a 
n is a normal 

n · x = n · p, where 

to IR3? We might reasona

to the line and p corresponds 

form of the equation 

to a point on the line. 

would be 

bly guess 

To see if this is a reasona

ble hypothesis, 
let's 

think about the specia

equation ax + by + cz � 0. In normal furn, it becomes 

l case of the 

n • x � 0, wh"e n � [ � l · 

is the set 

Let's 

or­
have 

of all vectors 

of all vectors 

make this finding 

satisfy this equation 

to n. As shown in Figure 1 .59, vectors 
However, the set 
x that 
thogonal 
a family of parallel 
this property, determining 
appears that ax+ by+ cz = d is the equation 
Every plane 
specifying 
vector 
if x represents 

<!]' in IR3 can be determined 
point on <!I', we have n · (x -p) = 0 or n · x = n · p. If 

n �  [ �] and x �  [ � l then, in teems of  rnmponents, 

in infinitely 
many directions 
planes. So our 
guess 
IR3. 
of a plane-not 
a line-in 
n normal to <!I' (Figure 
1.60). Thus, 

a point p on <!I' and a nonzero 

was incorrect: 

more precise. 

an arbitrary 

the equation 

bernmes 

by 

It 

Definition 

The normal 

ax+ by+ cz = d (where 
d = n · p). 
<!I' in IR3 is 
n · (x -p) = 0 or n · x = n · p 
point on <!I' and n * 0 is a normal vector 
for <!I'. 
for <!I'. 

The general 
is a normal vector 

of <!I' is ax + by + cz = d, where 
n = 

form of the equation 

where p is a specific 

form of the equation 

of a plane 

Example 1 .3 0  

Section 1.3 Lines 

and Planes 3 9  

Note that any scalar multiple 

of a normal vector 

for a plane is another 
normal 

vector. 

-

the 

the general 

the normal equation 

n · x = n · p becomes 

Find the normal and general 

forms of the equation of the plane that contains 

point P � (6, O, 1) and harnormal vodorn � [H 
Withp �[fl andx � [�]. wehavrn ·p � 1 · 6  + 2 ' 0�3 · 1  � 9, 'o 
x + 2y + 3z = 9 . .+ 

planes 
of each other. 
So, for 
sides 
have left-hand 
that is parallel 
to the 
the equation as x + 2y + 3z = 5-from 
t-hand sides of 
their 

Thus, their general equations 
example, 
plane 
which we see that the two planes 
do not coincide, 
the righ

2x + 4y + 6z = 10 is the general equation of a plane 
l vector 
equations 
in vector 
so, we observe that a plane can also be determined 
P (by the vector 
parallel 

u and v parallel to 
to each other). As Figure 1.61 shows, given any 

point X in the plane (located 

or parametric 
one of its points 
the plane (but not 

n. (Note that the planes 
are distinct

We may also express the equation of a plane 

p) and two direction 

have the same normal vector(s). 

that are multiples 

have the same norma

.) 
form. To do 

we may rewrite 

by specifying 

Geometrica

lly, it is clear 

equation 

in Example 

that parallel 

1.30, since 

since 

vectors 

tv x -p = su + tv 
,<= I -x 
SU 
X -p =SU +  tv 

figure 1 . 6 1  

by x), we can always 

that x -p = su + tv or x = p + s u + tv. If we write 

te multiples 

find appropria

s u  and tv of the direction 

vectors 
such 
componentwise, 

this equation 

we obtain 

parametric 
equations 

for the plane. 

Definition 

The vector 

form of the equation of a plane 

where p is a point on <lP and u and v are direction 

X = p +SU+ tv 
. 

zero and parallel 
The equations 

corresponding to 

to each other)
the components 

of the vector 

to <lP, but not parallel 
parametric equations of <lP. 

equation are called 

vectors 

<lP in IR3 is 
for <lP ( u and v are non -

form of the 

40  Chapter 1 Vectors 

Example 1 . 3 1  Find vector 

and parametric 

equations for 

the plane 

in Example 
1.30. 

two direction 

Solulion We need to find 
the plane; if we can find two other 
can serve as direction 
vectors 
trial and error, we observe that Q = (9, 0, O) 
equation 

x + 2y + 3z = 9 and so lie in the pla

one point P = (6, 0, 1) in 
luck they happen to be parallel!). 
By 
ral 

and R = (3, 3, O) both satisfy the gene
ne. Then we compute 

points Q and R in <!J', 

vectors. We have 

then the 

(unless 

vectors 

by bad 

PQ and PR 

Figure 1 . 6 2  
Two normals 

determine 

a line 

which, since 
Therefore, 

we have the vector 

they are not scalar multiples 

of each other, will serve as direction 
of <!J', 

vectors. 

equation 

\ 

<;JP I 

Figure 1 . 6 3  
The intersection 
of 
two planes 
is a line 

and the corresponding 

parametric 

�  [What would have happened had we 

equations, 
x = 6 + 3s -3t 
y = 3t 
z =l - s- t 
chosen 

R = (O, 0, 3)?] 

Remarks 

form, requires 

object, and its equation, 

is a two-dimensional 
two parameters. 

•  A plane 
•  As Figure 1 .59 shows, given a point Panda nonzero 
n1 and n2 do serve to locate a line e uniquel
through P with n as a normal 
a1X + b2y + C2Z = d1 

infinitely 
parallel 
then be the line 
sn1 + tn2 (Figure 1.

many lines 
normal vectors 
through 

to the plane 
in IR3 can also be specified 

P that is perpendicular 

a1x + b1y + c1z = d1 

in vector or 
n in IR3, there 
are 
e must 
P and two non­
x = p + 
y, since 

vector. However, 

62). Thus, a line 

with equation 
by a pair of equations 

vector 

parametric 

each normal vector. But since 

these 
to a 
l?), this is 
just the description 
(Figure 1 .63). Algebraic
ally, 
of all points (x, y, z) that simultaneously satisfy both equations. 

one corresponding to 
pair of nonparallel 
planes 
the intersection 
sists 
this concept further 
equations. 

(why nonparalle
planes 

in Chapter 2 when we discuss 

of two nonparallel 

the solution of systems 

of a line as 
con­

equations 

We will explore 
of linear 

correspond 

the line 

Tables 1.2 and 1.3 summarize 

the information pres

ented so far about the 

oflines 
Observe once again that 
a single 

tions 
and plane
a plane in IR3. [In higher 
single equa

tion of this type is usually 

(general) 
an object (line, 
called 

describes 
by a 
a hyperplane.] The relationshi

equation 
plane, 

dimensions, 

equa­
a line in IR2 but 
etc.) determined 

p among 

s. 

or lines in !R2 

Table 1 . 2  Equations 

Normal Form  General 
n·x =n·p 

ax+ by= c 

Form  Vector Form Parametric Form 

y = P2 + td2 

and Planes 4 1  

Section 1.3 Lines 
x = p + td  {x = P1 + td1 
Form {;: z= {;: z= 

P1 + td1 
P2 + td2 
p3 + td3 
p1 + su1 + tv1 
p2 + su2 + tv2 
p3 + SU3 + tv3 

Parametric 

Table 1 . 3  lines and Planes in !R3 

Normal Form 

{n1 · x  = n1 · p1 
n2 • x =  n2 • P2 

Lines 

Planes 

n · x  = n · p  

General 
Form 

{a1X + b1y + C1Z = d1 

azX + b1Y + C2Z = dz 
ax+ by+ cz = d 

Vector Form 
x = p + td 

X = p +SU+ tv 

the dimension 
the space 

of the object, the number 
is given by the "balancing 

formula''

: 

of equations 

required, 

and the 

dimension 

of 

(dimension 

of the 

space 

= dimension 
of the 

object) + (number of general equations) 
the dimension 
in IR3 is two-dimensiona

l, requires 

tions 
one general 
equation, 

it needs. 
For 
of the object, the fewer  equa
2 + 1 = 3. A line in IR3 is one-dimensiona
of the object also agrees 
with 
of "dimension" 
form. Notions 
intuit
ive observa­

Note that the dimension 
in its vector 
or parametric 

time being, 

but for the 

and lives 
l and so 

these 

The higher 
example, a plane 
in a three-

dimensional 
space: 

needs 3 -1 = 2 equations. 

of parameters 

the number 
will be 
tions 

clarified 
will serve us well. 

in Chapters 3 and 6, 
find the 
distance 
1.2 with the results 

We can now 
of Section 

results 

from a point to a line or a plane 

by combining 

the 

from this section. 

distance 

Find the 

from the point B = (1, 0, 2) to the line € through 
A � (3, !, I) with dimhon vedoc d � [-u  --+ 
of PB, ----> 
then AP = projd(v) and PB = v -projd ( v) (see Figure 1 .64). We do the necessary 

Solution As we have already 
where P is the 

point one at the foot of the perpendicular 

we need to calculate 

from B. If we label v = AB, 

determined, 

the length 

in several 

calcula

the point 

tions 

steps. 

Example 1 . 3 2  

42  Chapter 1 Vectors 

B 

A 

Figure 1 . 6 4  

v  -profa(v) 

___...---e 
d 
d(B,€) = llv -projd(v) II 
( v) = (:��)d 
= ((-1)·(-2) + 1·(-1) + 0·1)[-
o�l 

(-1)2+1+0 

of v onto d is 

Step 2: The projection 

projd 

Step 3: The vector 

we want is 

Step 4: The distance 

Using Theorem 1.3(b) to simplify the 

d(B, f) from B to e is 

llv -prnj,(v) II �  [ =!] 
llv -prnj,(v) II � l [ =�-

calcula

we have 

tion, 

= tv9 + 9 + 4 
= tv22 
d(B, f) = d(v, projd(v)). 

•  In terms of our earlier 

Nole 

notation, 

Example 1 .3 3  

Section 1.3 Lines 
In the case where the line £ is in IR2 and its equation 
ax + by = c, the distance 

d(B, €) from B = (x0, y0) is given 

has the 
by the formula 

and Planes 43 

general form 

d(B,€) = 

(3) 

lax0 + by0 - cl 

V az + b2 
39. 

You are invited 

to prove this formula 

in Exercise 

to the plane 

from the point B = ( 1 ,  0, 2) 

Find the distance 

is x + y - z = 1. 
<fP at the foot of the perpendicular 
'ii' and we 'ituate thr

Solution In this case, 

nurnrnl vectu' 

we need to calcula

<fP whose general equation 
of PB, where P is the point on 
[, at A, then we 

1 .65 shows, if A is any 
point on 

of the 

projection 

of AB onto n. Again we do the 

necessary 

need to find 
the length 
calculations 

in steps.  n 

te the 

length 
from B. As Figure 

n � [ _ : ] of 'ii' '° that it; tail 
I I I I I B 

11p 

Figure 1 . 6 5  

x + y - z = 1 .  A = (1, 0, O) will do. 

d(B, <!P) = llprojn(AB) II 
and error, we find any 

Step 1: By trial 

Step 2: Set 

point whose coordinates 

satisfy the 

equation 

Step 3: The projection 

of v onto n is 

projn ( V) =(�)n n·n 
=(1 ·0 +1 ·0 - 1•2)[ �1 
-t:l [=!] 

1 + 1 +( - 1)2 

- 1  

44  Chapter 1 Vectors 

Step 4: The distance 

d(B, <JP) from B to <JP is 

llrrnj"(vlll � H [ J 
�i [J 

In general, the distance 

d(B, <JP) from the 

general 

equation is ax + by + cz = d is given by the formula 

point B = (x0, y0, z0) to the plane 

whose 

d(B, <JP) 

lax0 + by0 + CZ0 - di 

Va2 + b2 + c2 
ise 40. 

You will be asked to derive 

this for

mula in Exerc

(4) 

3-6, write 

1 . 3  

..  I Exercises 
I. P  = ( 0, 0), n  = [�] 2.P = ( 1, 2), n  = [ _!] 

In Exercises 
through P with normal 
(b) general form. 

1 and 2, write the 

equation 
of the line passing 
form and 
n in (a) normal 

vector 

In Exercises 
through P with direction 
(b) parametric 
form. 

the equation 
g 
vector 

of the line passin
d in (a) vector 
form and 

3.P = ( 1, 0), d= [-�] 

S.P� (0,0,0), d� [-:l 4. p = ( -4, 4), d = [ �] 
•. p � ( 3. o. _ 2 ). d � m 
7.P� ( 0,1,0 ), n� [;] 8.P� ( 3,0,- 2) , n� m 

In Exercises 
through P with normal 
(b) general form. 

of the plane 
equation 
g 
form and 
n in (a) normal 

7 and 8, write the 

vector 

passin

of the plane pass­
s u and v in (a) 

vector 

9 and 10, write 

parametric 
form. 

the equation 
vector

In Exercises 
ing through P with direction 
form and (b) 

9. p � ( o. o. 0). u � [} � [-: l 
10. p � (6, -4, - 3), u � [l � [-:i 
11 and 12, give the vector 

In Exercises 
g through P and Q . 
passin
11. P = (1, -2), Q = (3, O) 
12. P = (O, 1, - 1), Q = (-2, 1, 3) 
In Exercises 
passing through P, Q, and R. 
(O, 1, - 1) 
13. P = (1, 1, 1), Q = (4, 0, 2), R = 
14. P = (1, 1, 
(O, 1, 1) 
O), Q = (1, 0, 1), R = 
15. Find parametric 

13 and 14, give the vector 

equation 
of the plane 

equations 
in IR2 with the following 
form for the lines 
(a) y = 3x - 1  (b) 3x + 2y = 5 

and an equation in vector 

equations: 

equation 
of the line 

Section 1.3 Lines 

and Planes 45 

points (x, y, z), where 

0 or 1 .  (See Figure 1 .
34.) 

of the plane that con­

p and q correspond 
16. Consider the vector 
or IR3. 
(a) Show that this equation 

equation x = p + t(q -p), where 
to distinct 

points P and Q in IR2 

describes 

the line segment 

25. A cube has vertices 

at the eight 

each of x, y, and z is either 
(a) Find the 

general equations 

of the planes 

that 
of the 
cube. 

gonal from the origin to ( 1, 1, 

determine 

the six faces (sides) 
(b) Find the general equation 
xy-plane. 

(c) Find the general equation 

tains 
is perpendicular 

to the 

the dia

contains 
Example 1 .

the side diagona

22. 

of the plane 
ls referred 
to in 

that 

1) and 

PQ as t varies 

from 0 to 1 .  

of PQ, 

and what is x in this case? 

(d) Find the midpoint 

(b) For which value oft is x the midpoint 

(c) Find the midpoint 
of PQ when P = (2, - 3) and 
Q = (0, 1 ) .  
of PQ when P = (1 ,  0, 1) 
and Q = (4, 1 ,  - 2). 
(e) Find the two points that divide 
(f) Find the two points that divide 

three 

equal 

with slopes m1 and m2 are perpendicular 

parts. 
equal 
a "vector 

three 
17. Suggest 
lines 
only if m1m2 = - 1. 
18. The line e passes 

h"' dfr,dion 

of all points that are 

Q to 

parts. 

of the 

proof" 

or neither: 

fact that, 

equidist

PQ in part ( c) into 

PQ in part (d) into 

ant from the points P = (1 ,  0, - 2) and 

in IR2, two 
if and 

26. Find the equation of the set 

whether e and <if' are 

27 and 28, find the distance from the point 

through the point P = (1 ,  - 1, 1) and 

<if', determine 
(h) 4x -y+ 5z=  0 

mtoc d � [ _; l Foe mh of th, 

Q = (5, 2, 4). 
the line e. 
In Exercises 
= [-�] + t[ _�] 
27. Q = (2,2),fwithequation [;] 
28. Q � (O, !, O), M h 'qoation [�] � [:J + {-�] 
29 and 30, find the distance from the point 
the plane <if'. 
Q to 
In Exercises 
2), <if' with equation x + y -z =  0 
29. Q = (2, 2, 
30. Q = (O, 0, O), <if' with equation x -2y + 2z =  1 
<if' in Exercise 18, determine whether 
P = (2, - 1) and is perpendicular 
R on e that is closest 
s to locate the 
Figure 
31. Find the po
P = (2, - 1) and is parallel 
32. Find the 
2x -3y = 1 .  
) and is 
line in IR3 
P = ( - 1, 0, 3
of the 
x -3y + 2z = 5. 
perpendicular 
P = (- 1, 0, 3
) and is parallel 
in IR3 
to 

way to use vector
to Q. 
point R on e that is closest 

4x - y + 5z = 2. For 
icular, 
form of the equation 

1.66 suggests a 

2x -3y = 1 .  
form of the equation 

to Q in Exercise 27. 
to Q in Exercise 
28. 

or neither. 
of the line in IR2 

form of the equation of the line 

int R on f that is closest 

form of the equation 

(d) 4x + 6y -2z = 0 

in IR2 
to the 

of the line 

equation 

equation 

point 

perpend

Q 

planes 
perpendicular, 

following 
parallel, 
(a) 2x + 3y - z =  1 
<if' 1 has the 

(c) x -y -z =  3 
19. The plane 
<if' 1 and <if' are parallel, 
20. Find the vector 

each of the 

planes 

that passes 
to the line with general equation 

through 

21. Find the vector 

that passes 
line with general equation 

through 

22. Find the vector 

that passes 
to the plane 

through 
with general 

vector 

23. Find the 
that passes 
the line 

through 

with parametric 

equations 

x =  1 - t 
y = 2  + 3t 
z  = -2 - t 

24. Find the normal form of the equation of the plane 

that 

P = (O, - 2, 5) and is parallel 

to the 

through 

passes 
plane with general equation 

6x - y + 2z = 3. 

I 
-----> 
r = p + PR 

Fioure 1 . 6 6  

46  Chapter 1 Vectors 

1.67 suggests a way to use vector

Figure 
R on <!J' that is closest 

to Q. 

s to locate 

the point 

the angle 
whichever 

between 
is an 

acute angle. 

(Figure 1.68) 

<!J' l and <!J' z to be either 
e or 180° -e, 

k --0 
------> 
------> 
r = p + PQ + QR 

Figure 1 . 6 1  

\lJ> I 

e \ � 1 80 -e 

Figure 1 . 6 8  

33. Find the point R on <!J' that is closest to Q in Exercise 29. 
34. Find the point R on <!J' that is closest to Q in Exercise 30. 

43-44, find the acute angle between 

the planes 

In Exercises 
with the given equations. 
43. x + y + z = 0 and 2x + y -2z = 0 
44. 3x - y + 2z = 5 and x + 4y - z = 2 
In Exercises 
given 
intersection 
45. The plane 
given 
by x = 2 + t 

equations 
between 

given 

them. 
by x + y + 2z = 0 and the 
line 

45-46, show that the plane and line with the 

intersect, and then find the acute angle of 

46. The plane given  by 

4x - y - z 

6 and the line 

ween the 

In Exercises 
parallel lin

35 and 36, find the distance bet
es. 

35. [;] [�] + s[-�] and [;] [�] + t[-�] 
36. [�] [ _�] 
37. 2x + y - 2z = 0 and 2x + y - 2z = 5 

In Exercises 
parallel planes. 

37 and 38, find the distance between 

+ {] and [;] [:J 
+ {] 

the 

y = 1 -2t 
z = 3 + t 

given by 

x = t 
y = 1 + 2t 
z = 2 + 3t 

47-48 explore one approach to the problem 
of 
Exercises 
projection 
onto a plane. 
finding the 
As Fig-
of a vector 
ure 1.69 shows, if<!J' is a plane through the origin in IR3 with 
in IR3, then p = proj9p(v) 
in <!J' such that v - en = p for some scalar 
vector 
normal 
n, and v is a vector 
is a vector 
c. 

n 

38. x + y + z = 1 and x + y + z = 3 
39. Prove Equation 
(3) on page 43. 
40. Prove Equation 
(4) on page 44. 
41. Prove that, 

in !Rz, the distance 

between parallel 

lines 

distance 

between parallel 

42. Prove that  the 

with equations n · x  = c1 and n · x  = Cz is given by 
lc1 -Czl 
llnll . 
equations n · x  = d1 and n · x  = dz is given  by 
Id, - dzl 
llnll 
and e is the angle 
If two nonparallel 
planes 
and nz 

<!J' 1 and <!J' z have normal 
vector
s n 1 
between 
n1 and Dz, then 
we define 

planes 
with 

Figure 1 . 6 9  
Projection 

onto a plane 

47. Using the fact that 
(and hence top), solve for 
sion for p in terms of v and n. 

n is orthogonal 

to every vector 
c and thereby find an expres­

48. Use the method 

of Exercise 

43 to find the projection 

of 

in <!f' 

and Planes 41 

Section 1.3 Lines 
(c) x -2z = 0  (d) 2x -3y + z = 0 

onto the planes 
(a) x + y + z = 0  (b) 3x -y + z = 0 

with the following 

equations: 

Exploration 

The Cro s s  Product 

It would be convenient if we could easily convert 
the equation of a plane 
that, 
nal to both u and v. One approach is to 
of vectors. Only valid 

the vector 
to the normal form n · x  =  n · 
u and v, produces a third 

use a 
in IR3, it is defined 
as follows: 

two nonparallel 

vectors 

given 

vector 
construction 

­
known as the cross 

n that is orthogo
product 

form x = p + s u  + t v  of 

p. What we need is a process 

by 

that can help you remember how to calculate the cross 

below. Under each complete 

Definition 
defined 

[U2V3 -U3V2] 
U X V  = U3V1 -U1V3 U1V2 -U2V1 
has no 3s.) U1 V1 U2 x Vz U3 x V3 U2V3 -U3V2 
U1 x V1 U3V1 -U1V3 
Uz Vz U1V2 -U2V1 

A shortcut 
two vectors 
is illustrated 
ponents of that vector. Ignoring 
block of four: Subtract the 
from the products of the 
the first comp
third 

onent of u X v has no ls as subscripts, the second 

components connected 

The following 

briefly explore 

problems 

the cross 

by solid 

product. 

lines. (It helps 

products of the components connected 

1 .  Compute u X v. 

product of 
first two com­
each 
lines 
by dashed 
to notice 
that 
has no 2s, and the 

vector, write the 
the two components 
on the 

top line, consider 

48 

uXv 

v 

Figure 1 . 1 0  

Figure 1 . 1 1  

ure 1. 71) is given by 

Writing Project 

The Origins 

of the Dot Product and Cross Product 

2. Show that e1 X e2 = e3, e2 X e3 = e1, and e3 X e1 = e2. 
3. Using the definition 

product, 

of a cross 

prove that u X v  (as shown in Figure 
1.70) 

is orthogonal to u and v. 

4. Use the cross 

product to help find the 

normal form of the 

equation of the plane. 

(a) The pfane 

P"'ing thrnugh P � ( 1. 0, - 2 ), parn!klto u � [:] 

and v � [ -�] 

(b) The plane 

5. Prove the 

passing 
following 

properties 

P = (0, - 1, 1), Q = (2, 0, 2), and R = (1, 2, - 1) 
of the cross 

product: 

through 

(a) v X u =  - (u X v) (b) u X 0 = 0 
(c) u X u =  0  (d) u X  kv = k
(e) u X  ku = 

0  (f) u X (v + w) = u X v 

(u X v) 

+ u X w 

6.  Prove the 
following 

properties 

of the cross 

product: 

(a) u ·  ( v  X w) = ( u  X v) · w  (b) u  X (v X w) = (u· w)v -(u· v)w 
(c) llu  X  vll2  = llull2llvll2 - ( u · v) 2  

7. Redo Problems 
8. Let u and v be vectors 

2 and 3, this time 

making use of Problems 

in IR3 and let (} be the angle 

5 and 6. 
between u and v. 
vll sin(}. [Hint: Use Problem 6(c).] 
area A of the triangle determined 

(a) Prove that llu  X vii = llull ll
(b) Prove that the 

by u and v (as shown in Fig­

A =  Hu X vii 

( c) Use the result in part (b) to 

compute the area of 

the triangle 

with vertices 

A =  (1, 2, 1), B = (2, 1, O), and C = (5, - 1, 3). 

cross 

for dot and 

by Josiah Willard 

product that we use today were in

The notations 
late 19th century 
Yale Univers
ity. Edwin B. Wilson was a graduate student in Gibbs's 
later wrote 
up his class 
1901, with Gibbs's 
of Mathematics and Physics. 
earlier 
and went by various 

as Vector Analysis: 
However, the 
notations. 
other names and 

expanded upon them, and had them published 

A Text-Book 
of dot and cross 

class, and he 
in 
s 

for the Use of Student

of mathematical physics at 

Gibbs, a professor 

blessing, 

concepts 

troduced in the 

product arose 

notes, 

Write a report on 

the evolution 

of the names and notations 

for the dot 

product 

and cross 
1 .  Florian 
2. J. Willard 

product. 
Cajori, A Histor

Use of Student
1901). Available 

Gibbs and Edwin Bidwell 

y of Mathematical Notations 

(New York: Dover, 1993). 
A Text-Book for the 
s of Mathematics and Physics (New York: Charles Scribner's Sons, 
-Guinness, 

Wilson, Vector Analysis: 
1 1 7714283. 
Encyclopedia 
of the Histor
2013). 

at http://archive
Companion 
Sciences (London: 

.org/details/

Routledge, 

online 

y 

of the Mathematical 

3. Ivor Grattan

y and Philosoph

4 9  

5 0   Chapter 1 Vectors 

Applications 

Force Veclors 
We can use vectors 
erly direction 
or the 
downward 
and a direction. 

For example, a wind blowing 
to model force. 
acting 
Earth's gravity 
by vectors 

are each best represented 

since 

on a 1 kg mass with a force of9.8 newtons 

they each consist of 
a magnitude 

at 30 km/h in a west­

It is often the case that multiple 

of 

due to grav­

is defined 

as the product 

Force 
mass and acceleration 
ity (which, on Earth, is 
of all 
the vector 

ward force of 1 kg X 9.8 
9.8 
Thus, a 1 kg mass exerts a 
ment is a newton (N). So the force 
9.8 
by a 1 kg mass is 9.8 

net result 
is simply 
act on an object, it is possible 
is clearly 
not moving in 
object is in equilibr
the result 

m/s2). 
down­
m/s2 or 
of measure­

kg • m/s2• This unit 

the forces acting 
sum of the 

is a closed 

exerted 

N. 

ium and the force vectors 
polygon (Figure 1 .73). 

forces act on an object. In such situations, 
which 
forces 

is a single force called the 
forces (Figure 1 .72). When several 

together 
individual 
that the resultant 

force is zero. 

In this case, 

the object 

resultant, 

the 

any direction 

and we say that it is in equilibr

ium. When an 

acting 

on it are arranged 

head-to
-tail, 

Figure 1 . 1 2  
The resultant 

of two forces 

Figure 1 . 1 3  
Equilibrium 

Example 1 .34 

ly direction 

Ann and Bert are trying 
in a norther
(a) What is the resultant 
(b) Carla is trying to prevent 

to roll a rock out of the 
while Bert pushes 
force on the rock? 
Ann and 
rium? 

the rock in equilib

Carla apply 

to keep 

way. Ann pushes 

with a force of 20 N 

with a force 

of 40 N in an easter

ly direction. 

Bert from moving 

the rock. What force must 

Solution (a) Figure 
lelogram 
rule, 

1 .74 shows the position 

of the two forces. Using the paral­

we add the two 

forces to get the resultant 

By Pythagoras' 

r as shown. 
b 

a 

b 

a 

Figure 1 . 1 4  
The resultant 

of two forces 

we see that llrll = V202 + 402 = V20QO  = 44.72 
a + b + c = 0. Therefore 

Section 1.4 Applications 
Bert, and Carla by a, b, and c, respec­
c = -(a + b) = -r, so Carla 

require 
a force of 44. 72 N in the direction 
opposite to r. 

N. For the direc­
force. We find that 

Theorem, 
tion of r, we calculate 

sine = 20/llrll = 0.447, so e = 26.57°. 

tively, 
needs to exert 

the forces exerted by Ann, 

r and Bert's easterly 

the angle e between 

(b) If we denote 

then we 

5 1  

in decomposing a force vector 

into other 

vectors 

whose 

Often, we are interested 
resultant 
ponents. In two dimensions, 
However, there 
are infinitely 
solve the vector into two 
idea more 
choosing the components 
y-axis. These components 
components, 
horizontal 

and vertical 

respecti
vely. 

components. 

is the given vector. This process 

resolving a vector 
into com­

is called 
we wish to resolve a vector 
many ways to do this; 

into two components. 

the most useful will be 

to re­

generally.) This is usually done by introducing 

components. (Chapters 

orthogonal 
so that one is parallel 
are usually referred 
In Figure 1 .75, f is the given vector and 

coordinate axes and by 
to the x-axis and the other 
to as 

5 and 7 explore 
this 

fx and fy are its 

to the 

the horizontal and vertical 

y 

x 

Figure 1 . 1 5  
Resolving 

a vector 

into components 

Example 1 . 3 5  

Ann pulls 
angle 
of20° 
and what force tends 

on the handle 
with the horizontal, 

to lift 

it off the ground? 

of a wagon with a force of 100 N. If the handle makes 

an 

what is the force 

that tends 

to pull the wagon forward 

Solution Figure 1.76 shows the situation and the 
consider. 

vector 

diagram that we need to 

Figure 1 . 1 6  

5 2   Chapter 1 Vectors 

We see that llfx II = 11£11 cos20° and llfy I = 1£11 sin20° 
Thus, llfxll = 1 00 (0.9397) = 93.97 and llfyll = 

1 00 (0.3420) = 34.20. 

forward with a force of approximately 

So the 
lift off 

N and it tends to 

93.97 

wagon is pulled 
the ground 

with a force of approximately 

34.20 

N. 

We solve the next 
a triangle 
of forces in equilibrium; 
the second 
ts. 
componen

example 

using 

two different 

methods. 
solution uses resolution 

The first solution consider

of forces into 

s 

Example 1 . 3 6  

Figure 1. 77 shows a painting 
painting 
has a mass of 5 kg and 
the ceiling, 

determine 

the tension 

if the two wires 
in each wire. 

that has been hung from the ceiling 

make angles 

by two 

wires. If the 
of 45 and 60 degrees 
with 

Figure 1 . 1 1  

1 We assume 
upward 

Solulion 
supply enough 
exerts 
collecti
wires 
in equilibri

and let r be their 

a downward 
vely pull upward 

that the painting 
force to balance the downward 

5 X 9.8 = 49 N on the painting, 

with 49 N of force. Let f1 and f2 denote 

is in equilibrium. Then the two wires 
force of gravity. 

must 
Gravity 
two wires must 
the 
1 .  78 ). It follows tha
we are 

t II r II = 49 since 

force of 

resultant 

so the 

(Figure 

um. 

the tensions in 

Section 1.4 Applications 

5 3  

Using the law of sines, 

we have 

llf1 ll 
llrll 
sin 45° sin 30° sin 105° 

so 

11 11 llrllsin45° 49(0.7071) 

f1 = 
Therefore, 

sin 105°  0.9659 
in the 

the tensions 

and  f2 = 

II II llrllsin30° 49(0.5) 
are approximately 

sin 105° 0.9659 
35.87 N and 25.36 

= 35.87 

wires 

N. 

= 

= 

= 25.36 

30° 

Solution 

2 We resolve 

f1 and f2 into horizontal 
as above, 

and vertical 
there 

force of 49 N 
is a downward 

components, 

say, f1 = 

h1 + v1 and f2 = h2 + v2, and note that, 
(Figure 
1.79). 

Figure 1 . 1 8  

It follows 

that 

Vz / 

60°  45° 

llvz ll = llf2ll sin 45° = I� 

Since the painting 
must the vertica
l componen
that 
which it follows 

is in equilibrium, 

ts. Therefore, 

the horizontal components 

as 
II h1 II = llh2 II and llv1 II + llv2 II = 49, from 

must balance, 

and 

into the second 

equation yields 

Substi

tuting 

equations 

the first of these 

v'3 llf2ll fil _ 
VL  v 2  

llf II _  49\/2 _ 
Thus, 11£111 = v2 llf2ll = 1.4142(25.36) = 35.87, 

35.87 N and 25.36 N, as before. 

approximately 

, ;;:;  + , ;;:; -49, or 2 - , ;;:; -25.36 

1 + v 3  

so the tensions 

in the  hires 

are 

49N 

Figure 1 . 1 9  I Exercises 

1 . 4  

1-6, determ

Force Vectors 
In Exercises 
forces. 
1. f1 acting 
f2 acting 

ine the resultant of the given 

due north with a 
due east with a magnitude of 5 N 

magnitude of 12 N and 

2. f1 acting 
f2 acting 
3. f1 acting 

due west with a magnitude of 15 N and 
due south with a magnitude of 20 N 
with a magnitude of 8 N and f2 acting 
with a magnitude of 4 N and f2 acting 

of 60° to f1 with a magnitude of 8 N 

4. f1 acting 

angle 

at an 

at an 

angle 

of 135° to f1 with a magnitude of 6 N 

5 4   Chapter 1 Vectors 

5. f1 acting 

due east with a magnitude of 2 N, f2 acting 

10. A lawn mower has a mass of 30 kg. It is being pushed 

due west with a magnitude 
angle 

of 60° to f1 with a magnitude of 4 N 

of 6 N, and f3 acting 

at an 

6. f1 acting 

due east with a magnitude of 10 N, f2 acting 

due north with a magnitude of 13 N, f3 acting 
with a magnitude of 5 N, and f4 acting 
a magnitude of 8 N 

due west 
due south with 

with a force of 100 N. If the handle of the lawn mower 
makes an angle 
horizontal 
mower to move forward? 

of 45° with the ground, 

component of the 

force that is causing 

what is the 

the 

11. A sign hanging outside 

Joe's Diner has a mass of 50 kg 

1.82). If the supporting cable makes an angle 
determine 

(Figure 
of 60° with the wall of the building, 
tension 

in the cable. 

the 

7. Resolve a force of 10 N into two forces perpendicular 

other 

to each 
of 60° with the 10 N force. 

so that one component makes an angle 

8. A 10 kg 

block 
of30° (Figure 
force, 
block 

parallel 
from sliding 

lies on a ramp that 
1.80). Assuming 
there is 
to the ramp, must be applied 

down the ramp? 

to keep the 

is inclined 

at an angle 

no friction, what 

Figure 1 . 8 0  

9. A tow truck 

is towing 

is 1500 N and the cable 

cable 
horizontal, 
cal force that tends 

as shown in Figure 

a car. The tension 

in the tow 
makes a 45° with the 
1.81. 

What is the verti­

to lift the car off the ground? 

Figure 1 . 8 2  

12. A sign hanging in the window 

of Joe's Diner has a 

If the supporting strings 

each make an 
and the supporting hooks 

of 45° with the sign 

mass of 1 kg. 
angle 
are at the same height 
each string. 

(Figure 1.83), find the tension 

in 

• 

,,• 
__ ,',f2 
_,,' 45° 
OPEN FOR BUSINESS 

Figure 1 . 8 3  

f= 1500 N 

Figure 1 . 8 1  

13. A painting 

with a mass of 15 kg is suspended 

by two 

from hooks on a ceiling. 

wires 
of 15 cm and 20 cm and the distance 
hooks is 25 cm, find the tension 

in each wire. 

between the 

If the wires have 

lengths 

14. A painting 

with a mass of 20 kg is suspended 

from a ceiling. 

wires 
If the wires 
and 45° with the ceiling, 

find the tension 

in each wire. 

by two 
make angles 
of 30° 

Review 
Chapter 
Kev Definitions 

and concepts 

properties 
10 

of vectors, 

Inequality, 

24 

, 13 

product, 48 

algebraic 
angle 
between vectors, 
binary vector
Cauchy-Schwarz 
22 
cross 
direction 
35 
distance 
dot product, 18 
equation of a line, 36 
equation of a plane, 

vector, 
between vectors, 

38-39 

23 

14- 1 6  
of a vector, 
2 0  

ail rule, 6 
head-to-t
integers 
modulo m (Zm), 
(norm) 
length 
linear 
of 
combination 
vectors, 
1 2  
normal vector, 
34, 38 
m-ary vector 
16 
orthogonal 
parallel vectors, 8 
parallelogram rule, 6 

vectors, 

26 

of a vector 
onto 
27 
Theorem, 

projection 
a vector, 
Pythagoras' 
26 
scalar multiplication, 
7 
standard 
22 
Triangle 
unit vector, 2 1  
vector, 
vector 
addition, 5 
zero vector, 
4 

unit vectors, 
Inequality, 22 

3 

1. Mark each of the following 

stions 
Review Que
U, v, and w in rr;r, ifu + w = v + w, 
U, v, and w in rr;r, ifu. w = v. w, then 

(b) For vectors 

statements 
true or false: 

(a) For vectors 

then u = v. 

----> 
and b =OB , find BC in terms of a and b. 

0, labeled 

at the origin 

ckwise 

in clo

order. If a = OA 

4. Let A, B, C, and D be the vertices of a square centered 

5. Find the angle 

between the 

vectors 

and 
[ - 1, 1, 2] 

[2,  1, 

- 1] .  

u, v, and w in IR3, if u is orthogonal 

to w, then u is orthogonal 

u = v. 

to v, and v is orthogonal 
to w. 

(c) For vectors 
(d) In IR3, if a line C is parallel 
d for e is parallel 
(e) In IR3, if a line e is perpendicular 
vector d for e is a parallel 

rection vector 
n for <:IP .  

to a plane 

<:IP, then a di­

to a normal vector 

to a plane 

<:IP ,  then 

to a normal 

are not parallel, 

then they must 

a direction 
vector n for <:IP .  
(f) In IR3, if two planes 
intersect 
in a line. 
(g) In IR3, if two lines 
intersect 

v = 0. 

a = 0 or b = 0. 

then they must 

in a point. 

are not parallel, 

(i) In l:'.5, if ab = 0 then either 

(h) If v is a binary vector such that v · v = 0, then 
(j) In l:'.6, if ab = 0 then either a = 0 or b = 0. 
2. If u = [ -� l v = [ � l and the 
with its tail at the point (1 0, -1 0), find the coordinates 
3. Ifu = [-�l v = [�land 2x + u =  3(x - v), 

of the point at the head of 4u + v. 

4u + v is drawn 

vector 

solve 

for x. 

of v � [:] 
onto u � [ -n 

in the xy-plane that is orthogonal 

6. Hnd ilie prnjection 

vector 

7. Find a unit 

tom 8. Find the general equation of the plane 

point ( 1,  1, 
parametric 

1 )  that is perpendicular 
equations 

through 
the 
to the line with 

x = 2  - t 
y = 3 + 2t 
z = -1 + t 

is parallel 

to the plane 

whose general 

9. Find the gene

ral equation of the plane through 

the 

point (3, 2, 5) that 
equation 

is 2x + 3y -z = 0. 

10. Find the gene

ral equation 

of the plane through 

the 

points A(l, 1 ,  O), B(l, 0, 1 ) ,  and C(O, 1 ,  2). 

11. Find the area of the triangle 

with vertices 
A(l, 1 ,  O), 

B(l, 0, 1 ) ,  and C(O, 1 ,  2). 

5 5  

5 6   Chapter 1 Vectors 

12. Find the midpoint of the line segment between 

A = (5, 1 ,  - 2) and B = (3, - 7, 0). 

13. Why are there 

no vectors 
llull = 2, llvll = 3, and u · v = - 7? 
from the 

14. Find the distance 

u and v in !Rn such that 

whose general 

equation is 2x + 3y -z = 0. 

point (3, 2, 5) 

to the plane 

15. Find the distance 
with parametric 

from the point (3, 2, 5) to the line 
equations 

x = t, y = 1  + t, z  = 2  + t. 

16. Compute 3 -(2 + 4)3(4 + 3)2 in Z5. 
3(x + 2) = 5 in Z7• 
3(x + 2) = 5 in Z9• 
19. Compute [2, 1, 3, 3] · [3, 4, 4, 2] in zt. 
20. Let u =  [l, 1, 1, 
O] in Zi. How many binary 

17. If possible, solve 
18. If possible, 
solve 

satisfy u · v = O? 

vectors 

v 

Systems of Lineari 
Eq uations 

was full of equations .... 
for everything, 

The world 
There must be an answer 
if only you knew how to set forth 
the questions. 

-Anne Tyler 
al Tourist 

The Accident

Alfred A. Knopf, 1985, 

p. 235 

2 . 0  Intro d u ction :  Triviali

lV 

point gives 

The word trivial is derived from the Latin root 
("road"). Thus, speaking literally, 
a triviality 
common meeting 
commonplace, ordinar
y, or insignifica
sisted 
before the quadrivium 
roads" that made up the trivium were 
examine systems 

geometry, music, 
the beginning 

(arithmetic, 

In this section, 

"common" 

we begin to 

universiti
subjects (grammar, rhetoric, 

nt. In medieval 

is a place where 

tri ("three") and the Latin word via 
three roads 
This 
meet. 
of trivial­
es, the trivium con­

and astronom
liberal 

of the 
oflinear 

taught 
y). The "three 

rise to the other, more familiar meaning 

equations. The same system 

of the 

three 

arts. 

can be viewed 
roads, 

of equations 
be our three 
threefold 
place 
(trivial

!) for you. 

the same solution. 
of linear 
way of viewing systems 

all leading to 

equations, 

ortant, ways-these 
will 
You will need 

to get 
so that it becomes 

used to this 
common­

in three 

different, yet equally imp

The system 

of equations 

we are going to consider 

is 

and logic) that were 

2x+ y =  8 

x -3y = - 3  

Problem 1 Draw the two lines 

of intersection? 

Problem 2 Consider 
grid determined 
use it as an aid in drawing 

by u and v. [Hint: 

the vectors 

the new one.] 

by these 

represented 

equations. 

What is their 
point 

u = [ �] and v = [ _ �] . Draw the coordinate 

draw the standard 

Lightly 

grid first and 

find the coordinates 

Problem 3 On the u-v grid, 
Problem 4 Another 

Problem 3 is to 
for which xu + yv = w. Write out the two equations 
equivalent 

(one for each 

way to state 

componen

t). What 

do you observe? 

Problem 5 Return 
equation 

line whose 
as line 2. Plot the point (O, O) on your graph from Problem 

is 2x + y = 8 as line 1 and the line whose 

equation 
1 and label 

it P0. Draw a 

now to the lines 

you drew for Problem 1 .  We will refer 

to the 

coordinate 

of w = [ _:] . 
is x -3y = -3 

ask for the 
to which this vector 

coefficients 
equation is 

x and y 

51 

of Linear 

Equations 

5 8   Chapter 2 Systems 
Point x 

Table 2 . 1  

y 
Po  0  0 
P1 
P2 
P3 
P4 
Ps 
p6 

horizontal line segment from P0 to line 1 and label 
2 and label 
vertical 
line segment from P2 to line 1, obtaining 
vertica
by horizonta
be happening? 

line segment from P1 to line 
l segments to line 

point P1. Next draw a 
this point P2• Now draw a horizontal 
in this fashion, 
l segments to line 
1 .  What appears to 

point P3• Continue 

this new 

2 followed 

Problem 6 Using a calcula

the first 

mate) coordinates 
solve 
of x.) Record 
separatel

of the 
equation 
your results 

y. 

tor with two-decimal-place 
points P1, P2, P3, . . .  , P6• (You will 
for x in terms of y and the second 
in Table 2 . 1 ,  writing 

the x-and y-coordinates 

find it helpful 
equation for 

to first 
y in terms 

y, find the (approxi­

accurac

of each point 

drawing 

The results of these 

problems 
in several 

show that the task of "solving" 
ways. Repeat the process 

described 

in the prob­

a system 

of linear 

equations 
lems with the following systems 

may be viewed 

of equations: 

( a )  4x -2y = 0 
x  + 2y = 5 

(b)3x+ 2y =  9 
x + 3y = 1 0  
from Problems 
Are all of your observations 
any similarities 
In this chapter, 
or differences. 

x -y = 3  
1-6 still 

(c)x + y =S 

valid 
we will explore 

for these 
ideas 

examples? 
in more detail. 

these 

Note 

(d) x + 2y = 4 

2x- y = 3  

Intro d u ction to svstems ot linear Equations 

ax + by = c 

Recall 

that the gene

ral equation 

of a line in IR2 is of the form 

and that the 
general 

equation of a plane 

in IR3 is of the form 

Equations of 

this form are called 

equations. 

ax + by + cz = d 
linear 

Definition 
that can be written 

A linear 
in the form 

equation 

in the n variables 

x1, x2, • • •  , xn is an 

equation 

where the coefficients a1, a2, . • .  , an and the constant term 
bare constan
ts. 

Example 2 . 1  

The following 

equations 

are linear

: 

3x - 4y = - 1  r -ts -lft = 9 

\/2x + :y -(sin :)z = 1 3.2x1 

-O.Olx2 = 4.6 

Observe that the 
third 
5x2 + x3 - 2x4 = 3. It is also 
in most applications) 
examples 
prime number p. 

and applications 

equation is linear 

because it can be 

important to note that, 

although 

in these 

in the form 

rewritten 
x1 + 
(and 

ers or members of "11.P for some 

examples 

the coefficients 

and constant terms are real numbers, in some 

they will be complex numb

Section 2.1 Introduction 

to Systems 

of Linear 

Equations 

5 9  

The following 

are not linear: 

equations 
xy + 2z = 1 xi -x� =  3 x -+ z = 2  

y 

equations 

do not contain 
Thus, linear 
variables; the variables 
occur 
stants. Pay particular 
attention 
fourth equation in the first list is linear 

products, 

reciprocals, or 

other 

functions 

of the 
only by con -
Why is it that the 

only to the 

first power and are multiplied 

in each list: 

to the fourth example 

but the fourth equation in the second 

list is not? 4 
equation a1x1 +  a2x2 + · · · + anxn = b is a vector 

satisfy the equation 

when we substi

tute x1 = s1, 

A solution 

[s1, s2, ••• , snl whose components 
X2 = S 2> ••• , Xn = Sn-

of a linear 

In general, the 

x = 5 and 

of3x -4y = - 1  because, 

when we substitute 
y = 4, 
3(5) -4(4) = - 1. [l, 1] is another solu
tion. 
by the given 
equa­

y correspond to the 

points on the line determined 
set of solutions 

x = t and solving for y, we see that the complete 
the two parametric 

form [t, � + �t]. (We cou

ld also set y equal 

solutions 

parametric 

Thus, setting 

(a) [5, 4] is a solution 
the equation 
is satisfied: 
solutions simpl
tion. 
can be written in the 
parameter-say, s-and solve for x instead; 
different 
(b) The linear 
as specific 
solutions. The 
in the plane determined 
parametric 
solution is 
specific solutions above?) 
three 

but would be equival
equation 

given by [3 + 

ent. Try this.) 

x1 -x2 + 2x3 =  3 has [3, 0, O], [O, 1, 2], and [6, 1 ,  - 1] 
to the set 
complete 
of points 
by the given equation. If we set x2 =  sand x3 = t, then a 
the 

set of solutions 

corresponds 

s -2t, s, t]. (Which 

of s and t produce 

to some 

would look 

values 

A system of linear 

equations 
of a system 

is a finite 
of linear 

set oflinear 
equations 

variables. A solution 
a solution of each equation in the system. The solution 
tions 
solution set of 

set of a system 
to the 
We will refer 
as "solving the 
system:' 

that is simultaneously 
process 

of the system. 
equations 

is the set of 

oflinear 
equa­
of finding 
the 

all solutions 

is a vector 

oflinear 

a system 

equations, 

each with the same 

Example 2 . 2  

Example 2 . 3  

The system 

x + 3y =  5 

2x - y =  3 

has  [2, 1 ]  as a solution, 
[ 1 ,  - 1] is not a solution of the system, since 
it satisfies 

it is a solution of both equations. On the 
other 
only the first equa
tion. 

since 

hand, 

Example 2 . 4  

Solve the following 
(a) x - y = 

systems 
(b) x - y =  2 

of linear 

equations: 

(c) x - y = 

1 

x +y = 3  

2x -2y = 4 

1 
x - y =3 

6 0   Chapter 2 Systems 

of Linear 

Equations 

together 
gives 

point of intersection 

that [2, 1] is indeed 
the only solution can be seen by observing 
( 2, 1) of the lines 

Solution 
2x = 4, so x = 2, from which we find 
(a) Adding 
the two equations 
a solution of both equations. 
that y = 1. A quick check confirms 
that this solution corresponds 
That this is 
to the (unique) 
with equations 
x + y = 3, as shown in Figure 2.l(a). Thus, [2, 1] is a unique solution. 
(b) The second 
equation in this system 
solutions 
of the 
first equa
can be represented 
[Figure 2.1 (b)]. 
solutions 
(c) Two numbers 
this system 
equation 
the lines 

so the solutions 
tion alone-namely, the points on the line 
infinitely many 

x and y cannot simultaneously 
has no solutions. (A more algebraic approach 

lly as [2 + t, t]. Thus, this system has 

from the first, 
for the equations 

absurd 
are parallel 

x -y = 2. These 

yielding the 

parametrica

conclusion 

of 1 and 3 .  Hence, 

have a difference 

x -y = 1 and 

2.l(c) shows, 

is just twice 

the first, 

in this case . 

are the 

might be to subtract the second 
0 = -2.) As Figure 

y 

4 

y 

.J 

(a) 

Figure 2 . 1  

(b) 

(c) 

A system 

oflinear 

equations 
is called 

tem with no solutions 
tems in Example 
a system 
of linear 
three 

2.4 illustrate 
equations 
hold for any system 

consistent 
is called 
inconsistent. 
Even though 
the only three 
possibilities 
with real coefficien
of linear 

one solution. 
A sys­
they are small, the three 
sys­
of 
for the number of solutions 
these 
real numbers. 

ts. We will prove later that 
equations 

possibilities 

over the 

if it has at 

least 

same 

A system 

of linear 

equations 

with real coefficients 

has either 

(a) a unique solution (a consis
(b) infinitely 
many solutions 
( c)  no 

solutions 

(an inconsistent 

tent system) 
(a consistent 

or 
system) or 

system). 

Solving 
Two linear 
example, 

a svstem or Linear 
systems are called 

Equations 

equivalent if they have the same solution sets. For 

x -y= l 
x+y= 3 
are equivalent, 

since 

and x -y= l 
y = 1 

they both have the unique 

solution [2, l ] .  (Check this.) 

� 

Example 2 . 5  

Solve the system 

Section 2.1 Introduction 

to Systems 

of Linear 

Equations 

6 1  

Our approach 

to solving 

a system 

of linear 

equations 

is to 

into an equivalent 
one tha
example 

t is easier 
above (in which the second 

to solve. The triangular 
of the 
equation has one less variable than the 

transform 
pattern 

the given 

system 
second 
first) 

is what we will aim for. 

x - y -z =  2 

y + 3z =  5 

5z = 1 0  
and working 
last equation 

Solution Starting 
that z = 2, y = 5 -3(2) = -1 ,  and x =  2  + 
[3, -1 ,  2]. 

from the 

(-1 )  +  2  = 3.  So 

backward, 

we find successi
vely 
the unique solution 

is 

ure used to solve 

The proced
We now turn 

Example 
strategy 

ion. 
2.5 is called 
for transforming 
one that can be solved easily by back substi
in greater 
example. 

detail in the next section; 

equivalent 
described 
action 

for now, we will simply 

in a single 

back substitut

general 

to the 

tution. 

a given system 

into an 
This process 
will be 

observe it in 

Example 2 . 6  

Solve the system 

2.5, 

this system 

we first need to elimina

Solution To transform 
of Example 
Observe that subtracting 
3 will do the trick. Next, observe that we are operating 
the variables, so we can save ourselves 
constant terms in the matrix 

appropriate multiples 

x from Equations 2 and 3. 
of equation 1 from Equations 2 and 
efficients, not on 
on the co
and 

te the variable 

some writing 

the triangular 

if we record 

the coefficients 

structure 

x - y -z =2 
3x -3y +  2z  = 
1 6  
2x - y  +  z  = 
9 
into  one  that 
exhibits 

- 1  2] 2 1 6  
[: - 1  

-3 
- 1  
the coefficients 

1 9 

-ix 

its entries, 
and just as 

word mater, meaning 
When the 
suffix 
the meaning becomes 
Just as a womb surrounds 

The word matrix is deri
ved from 
the Latin 
"mother:' 
is added, 
"womb:' 
a fetus, the brackets of a matrix 
surround 
the womb gives 
matrix 
functions called 
tions. A matrix 
n columns 
matrix 
The plural 
not "matrixe
s:' 

rise to a baby, 
rise to certain 
types of 
linear 
transforma­
with m rows and 

an m X n 

of matrix is matrices, 

(pronounced 

is called 

"m by n"). 

gives 

a 

three 

where the first 
columns 
column 
equal 

the constant terms, and 
is called 

in the equations. 

contains 

This matrix 

contain 

signs 

the vertical 

of the variables 
bar serves to remind 

the augmented 
ways to convert the given 
system 

of the system. 

matrix 
with the triangular 
into one 
in spirit 
to the 

more general 

we will use here are closest 

There are various 

in order, the 

final 

us of the 

we are after. 
described 

The steps 
in the next section. 
and simultaneously 
ting x from Equations 2 and 3. 

pattern 
method 
the given system 
begin by elimina
x - y -z =  2 
3x -3y + 2z = 
2x-y +z= 9  

1 6  

- 1  
- 3  
- 1  

- 1  2] 2 1 6  

1 9 

We will perform the sequence of operations 
on the 
matrix. 

corresponding augmented 

on 
We 

6 2   Chapter 2 Systems 

of Linear 

Equations 

Subtract 
3 times 
from the second 

the first equation 
equation: 

3 times 
Subtract 
second 
row: 

the first row from the 

x - y -z =  2 
5z = 1 0  
9 
2x - y +  z = 
the first 
Subtract 
2 times 
equation 
from the third 
x - y -z =  2 
5z = 1 0  

equation: 

y + 3z =  5 

Equations 2 and 3: 

Interchange 
x - y -z =  2 

y + 3z =  5 

5z = 1 0  

0 

- 1  

the first row from the 

Subtract 
2 times 
third row: 

[: - 1  
[: - 1  - 1  !�: 
[i - 1  - 1  lt 1 3 

Interchange 
rows 2 and 3: 

in Example 
where 

also the solution to the sys­

0 5 
1 3 

back substi

2.5, 

0 5 

above show that any solution 

of the 
we just performed 
are 

tions 

that we 

solved using 

in this example. Why? The calcula

This is the same system 
tution 
we found that the solution was [ 3, - 1, 2]. This is therefore 
tem given 
the steps 
given system is also a solution 
with the final 
starting 
reversible, 
So any solution 
of the final system is also a solution 
are equivalent (as 
are all of the ones obtained 
over, 
matter to reinsert 
ing with matrices is 

in the intermedia
te steps 
of equations, 
before proceeding with the back 
substi

we might just as well work with matrices instead 

of the final one. But since 
the original 
system, 

the subject of the next section.) 

the variables 

recover 

we could 

since 

above). More­
it is a simple 
(Work­

tution. 

system. (How?) 

of the given one. Thus, the systems 

with matrix capabilities 
of linear 

particu

larly 

and computer 

can 
when the systems 
As 

are large 
the case in real-life applications. 
as you can with pencil and paper 

many examples 

algebra systems 

Remark Calculators 

equations, 
as is often 

that are not "nice;' 

facilitate solving 
systems 
or have coefficients 
always, though, 
you should do as 
until 
for, think 
After 

you have an answer, 

about how you would do 
be sure to think 
into thinking 

Do not be misled 

ble with the techniq

you are 

comforta

the calcula

at all! Roundoff 

faster or more easily than calculating 
answer 
calculators 
swers 
of the problem. 

associated 
serious 
tion: 

can cause 
See Explora
been warned!) 

errors 
and computers 

to some problems. 
(You've 

ues. Even if a calculator 

or CAS is called 
before doing anything. 

tions 

manually 

about whether it is reasonable. 

that technology will 

give you the answer 

always 

by hand. Sometimes 

with the floating-point arithmetic 

you the 
it may not give 
used by 
wrong an­
Told Me for a glimpse 

and lead to wildly 

problems 

Lies My Computer 

Section 2.1 Introduction 

to Systems 

of Linear 

Equations 

6 3  

2 . 1  

1 Exercises 
3. x-1 + 7y + z =sin(;) 

in the variabl

1-6, determine 

In Exercises 
equations 
linear
1. x -'TTY + efs z = 0  2. x2 + y2 + z2 = 1 

which equations 
is not 

es x, y, and z. If any equation 

are linear 

, explain why not. 

that makes th

The systems in Exercises 
gular" pattern 
substitut
in Chapter 
25. X 

systems. 
2 26. X1 

25 and 26 exhibit a "lower trian­
again 

em easy to solve by forward 

3.) Sol

ve these 

ion. (We will encounter forward substitution 

= - 1  

4. 2x - xy -5z = 0  5. 3 cos x -4y + z = v3 
6. (cos3)x 

- 4y + z = v3 
7-10, find a linear 
equation 
on the variabl
es). 

In Exercises 
solution 
set as the given 
restrictions 
7. 2x + y = 7 -3y 

(possibly with some 

x2 _ y2 
8.--­x - y  

10. log10 x -log10 y = 2 

equation 

1  1 4 
9.- + -= -
x  y xy 

that has the same 

11-14, find the solution 

set of each equation. 

In Exercises 
11. 3x -6y = 0 
13. x + 2y + 3z = 4 

12. 2x1 + 3x2 = 5 

infinitel

systems. 

solution, 

Determine geometrica

15-18, draw graphs corresponding 
to the 
given 
lly whether 
each sys­
, or no 

In Exercises 
linear 
tem has a unique 
y many solutions
solution. Then solve each system algebraically to confirm 
your answer. 
15. x + y 
= 0 
2x + y = 3 
17. 3x - 6y= 3  
-x+ 2y =l 

16. x -2y = 7 
3x +  y = 7 
18. O.lOx -0.05y = 0.20 
-0.06x + 0.03y = -0.12 

system by back 

19-24, solve the given 

In Exercises 
substitut
ion. 
19. x -2y = 1 
21. x - y +  z = 

y =3 

20. 2u - 3v = 5 
2v = 6 
22. X1 + 2X2 + 3X3 = 0 

23. X1 + Xz - X3 - X4 = 1 

24. x -3y +  z =  5 

-5x2 + 2x3 = 0 
4X3 = 0 

= - 1  
y - 2z 

0 
2y - z = 1 

3z = - 1  

X2 + X3 + X4 = 0 
X3 - X4 = 0 
X4 = 1 

equations 

-a +  b - c -

2x + y = - 3  
- 3x -4y + z = - 10 

-!x1 + x2  5 
�X1 + 2X2 + X3 = 7 

2x+ y= 3  

Find the 
Exercises 
27. x - y = 0 

augmented matrices of the linear 
systems in 
27-30. 
28. 2X1 + 3X2 - X3 = 1 
X1  + X3 = 0 
-X1 + 2X2 -2X3 = 0 
30. a -2b +  d = 2 
3d = 1 

29. x + Sy = - 1  
-x +  y = - 5  
2x + 4y =  4 

31and32,find a system of linear 
matrix as its augmented matrix. 

In Exercises 
that has the given 

31.[� =: � :J 
3 �] 
32. [i - 1 0 3 

33-38, solve the linear 

2 
0  2 

For Exercises 
exercises. 
33. Exercise 27 
35. Exercise 
29 
37. Exercise 
31 
39. (a)  Find 
a system 

28 
30 
32 
of two linear 

34. Exercise 
36. Exercise 
38. Exercise 
equations 

- 1  

in the vari­

by the 

the system 

lution set is given 

ables x and y whose so
parametric 
(b) Find another 

equations x = t and y = 3 -2t. 
solution to 
parametric 
sandy =  s. 
which the parameter is 

40. (a) Find a system 

part (a) in 

of two linear 
ables x1, x2, and x3 whose so
the parametric 
X3 = 2 -t. 
part (a) in 

equations 
in the vari­
lution set is given by 
equations x 1 = t, x 2 = 1 + t, and 
in 
parametric 
is sand x3 = s. 

which the parameter 

solution to the system 

(b) Find another 

in 

systems in the given 

of Linear 

Equations 

of equations 
are nonlinear. 
(changes of variab
each 
system and use this linear 
system to help 

les) that convert 

6 4   Chapter 2 Systems 
41-44, the systems 

In Exercises 
Find substitutions 
system into a linear 
solve the given system. 
2 3 
41. -+ -= 0 
x y 
3 4 -+-= l 
x y 

42. x2 + 2y2 = 6 
x2 - y2 = 3 
43. tan x -2 sin y 
tan x -sin y + cos z = 2 
sin y -cos z = - 1  
44. -2a  + 2(3b) = 1 
3(2a) - 4(3b) = 1 

2 

Direct Methods for Solving linear Svstems 

we will look at a general, systematic 

procedure for solving a system 

This procedure is based on the idea 

of reducing 

the augmented 

to a form that can then be solved by back substi
in 

tution. 
(if one exists) 

directly to the solution 

that it leads 

In this section, 
of linear 
equations. 
matrix of the given system 
The method 
a finite 
work in a completely 

is direct in the sense 
way. 

different 

number of steps. In Section 2.5, 

we will consider 

some indirect methods 
that 

Form 

a n d  Echelon 

Matrices 
There are two important matrices 
associ
matrix contains 
we have already 
containing 
the con
For the system 

the coefficients 
encountered) 

stant 

terms. 

ated with a linear 

system. The coefficient 

of the variabl

is the coefficient 

es, and the augmented 
(which 
matrix augmented 

by an extra column 

matrix 

the coefficient 
is 

matrix 

and the augmented 

matrix is 

2x+ y -z =3 
x  +  Sz =  1 
-x + 3y - 2z = 

O [ 2  1 -1] -� � -� 
- 1  3] 5  1 

1 
0 
3 

-2 0 

in the appropria

Note that if a variable 
entered 
of a linear 
of the 

by A and the 
augmented matrix 

is missing (as y is in the second 
te position in the matrix. 

0 is 
matrix 
of constant terms by b, then the form 

If we denote 

the coefficient 

equation), its coefficient 

vector 

column 

system 

is [A I b J .  

Section 2.2 Direct 

Methods 

for Solving 

Linear 

Systems 6 5  

In solving 

a linear 
matrix to triangular 
a staircase 
pattern 

system, 

it will not always 
form, as we did in Example 

be possible 
2.6. 

in the nonzero 

entries 

of the final matrix. 

to reduce 

the coefficient 

However, 

we can always 
achieve 

comes from 

echelon 
word scala, meaning 
or "stairs:' 

The word 
the Latin 
"ladder" 
word for "ladder
derived 
matrix 
a staircase 

from this Latin 
A 
in echelon 
pattern. 

is also 
base. 

:' echelle, 

form exhibits 

The French 

row echelon 

A matrix is in 

Definition 
properties: 
1. Any rows 
are at 
2. In each nonzero row, the first nonzero entry 
in a column 

to the left of any leading entries 

the bottom. 
(called 
below it. 

entirely 

consisting 

of zeros 

the leading entry) 

is 

form if it satisfies 

the following 

Note that these properties 

guarantee 

that the leading entries 

form a staircase 

pat­

tern. 
leading 

entry 

In particular, in any column 

are zero, 

as the following 

containing 
entry, all entries be
te. 

a leading 
illustra

examples 

low the 

Example 2 . 1  

The following 

matrices 

are in row echelon 
form: 

4 
0 
- 1   1 
0 
0 

[: �] [: :J [i �-l� 2 0 1 - 1  �-
0  0 0  0 4 

0 - 1  1 2 
0  0 0 4 

1 2 
0 1 
0 0 

If a matrix in row echelon 

of a linear 

matrix 

sys­

form is actually 
by back substi

the augmented 
tution alone. 

tem, the system is 

quite easy 
to solve 

Example 2 . 8  

Assuming 
that each of the matrices 
the corresp
onding 

in Example 
equations 

systems oflinear 

2.7 is an augmented matrix, write out 
and solve 
them. 

Solution We first 
the vector 

of constant terms. The first 

that the last 
matrix then corresponds 

column 

in an augmented 

to the system 

matrix is 

remind 

ourselves 

2x1 + 4X2 =  1 
-x2 = 2 
that we have dropped the last 
equation 
of x1 and x2.) Back substi
satisfied 
2x1 = 1 - 4(-2) = 9, so x1 =�. The solution is[�, -2]. 
correspond
ing system 

(Notice 
clearly 

matrix has the 

for any values 

The second 

0 = 0, or Ox1 + Ox2 = 0, which is 

tution gives x2 = - 2  and then 

x, = 1 
X2 = 5 
0 = 4 

equation represents 
has no solutions. Similarly, 

The last 
the system 
has no solutions. For the system 

Ox1 + Ox2 = 4, which clearly 
has no solutions. 
corresponding 
to the fourth matrix 
the third 
matrix, we have 

corresponding to 

the system 

Therefore, 

6 6   Chapter 2 Systems 

of Linear 

Equations 

so x1 = 1 - 2(3) -x2 = -5 - x2• There are infinitely 
solution [ - 5 - t, t, 

X1 + X2 + 2X3 = 1 
X3 = 3 

x2 any value t to get 

the parametric 

assign 

3]. 

many solutions, 

since 

we may 

Elemenlarv Row Operalions 
We now describe 
echelon 
in row 
correspond 
to the 
it into an equivalent 
to transform 

form. The allowable 

operations 

system. 

the procedure by which any matrix can be reduced 

to a matrix 

operations, 

that can be performed on a system 

called elementary row operations, 
equations 

of linear 

The following 

element

ary row operations can be performed 

on a 

DefiniliOD 
matrix: 
1. Interchange 
2. Multiply a 
3. Add a multiple 

two rows. 
row by a nonzero constant. 

of a row to another row. 

a row by a nonzero 

Observe  that 
since, 

dividing 
for example, dividing 
of one row from another 

definition, 
Similar
of one row to another 
row. 
negative multiple 
nd notation 
shortha

ly, subtracting 

We will use  the 

a multiple 

following 

for  the 

constant is implied 

in the above 

a row by 2 is the same as multiplying 

it by!. 

row is the same as adding 

a 

three elementary 

row 

operations: 
1. R; �  Rj means interchange 
2. kR; means multiply 
row i by k. 
3. R; + kRj means add k times row 

rows i and j. 

The process 

of applying 

elementary 

row operations 

j to row i (and replace 

row i with the result). 
a matrix into row 
to bring 

echelon form, called 

row reduction, 

is used to reduce 

a matrix to echelon 
form. 

Example 2 . 9  

Reduce 

the following 

matrix to echelon 
form: 

4 0  0 
3  2  1 
1 3 6 

[_: 2 -4 -4 �] 

from left to right 
in a column 
entry 

a leading 

by column, 

entry 

Solution We work 
The strategy is to 
below it. 
The entry 
of the process 
to use the second 

column 

create a leading 
to become 
chosen 

is called 

pivotin

elementary 

g. Although 
row operation 

not strictly 

to make each leading 

and from top to bottom. 

zeros 
and then use it to create 
is called a pivot, and this phase 
necessar

y, it is often 
convenient 
a 1. 
entry 

We begin 

by introducing zeros 
into the first column 

first row: 

The first column 
in the second 
this by interchanging 

row, aiming 

rows. (We 

Section 2.2 Direct 

Methods 

for Solving 

Linear 

Systems 61 

2 
4 
3 

-4 
0 
2 
3 

-4 
8 
10 
- 1  
is now as we want it, so the next thing 

10 

of echelon 

-4 
9 
8 

-4 
8 
9 

-4 
0 
1 

for the staircase 
pattern 

below the leading 1 in the 

2 
- 1  
0 
3 

could also add row 3 or row 4 to row 2.) 

2 
0 
- 1  
3 
to do is to create a leading 
entry 
we do 
form. In this case, 

-4 
10 
8 
- 1  
a zero at the bottom of column 2, using 

6 5] R, -2R1 
2 -:] -5 10 
R3 -2R1 
2 R, + R1  0 5 �  0 5 0 
[ l 
2 -�] - 8 
-�] - 8 -5 
[1 2 -4 -4 -�] 
·�·[i 2 -4 -4 -�] 

-4 
9 
8 
29 
that we alread

- 1  10 9 
0 1  1 - 1  
0 0 0 24 

� 
�R,  - 1  10 9 

0 1 1 - 1  
0 29 29 -5 

a zero below it. This is easiest 
if we 

-4 
10 
8 
29 

our matrix to echelon 
form. 

entry in column 3, 

y have a leading 

a zero below it: 

first divide 

1 in row 3 to create 

The pivot 
the leading 

this time 
entry - 1  in row 
2: 

was - 1. We now create 

2 
- 1  
0 
0 
Column 2 is now done. Noting 
we just pivot 
row 3 by8: 

on the 8 to introduce 

Now we use the leading 

With this final step, 

we have reduced 

•  The row echelon 

Remarks 

form for the matrix in Example 
2.9.) 

form of a matrix is not 

unique. 

(Find a different 

row echelon 

6 8   Chapter 2 Systems 

of Linear 

Equations 

•  The leading 
•  The pivots are not necessarily the entries 
•  Once we have pivoted and introduced zeros 

tions 
1, - 1, 8, and 24. The original 
"stairc

eventually occupied 

by the leading 

entry 

ase." 

in each row is used to create 
the zeros 
that are 

. In Example 
matrix had 1, 4, 2, and 5 in those 

entries

does not change. 

In other 

words, 

column, 
that column 
from left to right, 

top to bottom. 

below it. 

below the leading 
the row echelon 

entry in a 
form emerges 

originally in the posi­
the pivots 
were 
positions on 
the 

2.9, 

Elementary row operations 

if some elementary 
operation that converts 

row operation 

B into A. (See Exercises 15 and 16.) 

are reversible-that is, they can 
is also 

converts A into B, there 

be "undone:' 
an elementary row 

Thus, 

Definition 
elementary 

row operations 

Matrices 

A and B are row equivalent if there 
that con

verts A into B. 

is a sequence of 

The matrices 

in Example 

2.9, 

0 -�] - 1  
6 �] md [� 2 

-4 
10 
1 
0 

- 1  
0 
0 

-4 
9 
1 

-4 
0 
1 

24 

2 
4 
3 

-4 
0 
2 
3 

In general, though, 

how can we tell whether 

two matrices 
are row 

are row equivalent. 
equivalent? 

Theorem 2 . 1  

Matrices 
row echelon 
form. 

A and B are row 

equivalent 

if and only if they can be reduced 

to the same 

Conversely, if A and B have the same row echelon 

then further 
Proof If A and Bare row equivalent, 
therefore A) to the (same) row echelon 
form. 
row operations, 
we can convert R into B, and therefore 
operations, 
the desired 
effect. 

we can convert 

A into R and B into R. Reversing 

form R, then, 

via elementary 

the sequence A � R � B achieves 

the latter 

sequence of 

row operations will reduce 
B (and 

Remark In practice, 

Theorem 2.1 is easiest 

to use if R is the 

reduced row echelon 

form of A and B, as defined on 

page 73. See Exercises 

17 and 18. 

Elimination 

Gaussian 
When row reduction 
equations, 
The entire process 

to the 
we create an equivalent 
system 

augmented 
of a system 
that can be solved by back substi

of linear 
tution. 

is applied 

matrix 

is known as Gaussian elimination. 

Section 2.2 Direct 

Methods 

for Solving 

Linear 

Systems 6 9  

Gaussian 

Elimination 

matrix of the system 
to reduce 

of linear 
equations. 
the augmented 

row operations 

matrix to row 

1. Write the augmented 
2. Use elementary 
tution, 
3. Using 
matrix. 

back substi
row-reduced 

echelon 
form. 

solve 

the equivalent 
system 

that corresponds 

to the 

Example 2 . 1 0  

Solve the system 

Remark When performed 

by hand, step 2 of Gaussian elimination allows quite 

a 

es: 
all zeros. 
at the top 

Here are some useful guidelin
that is not 

bit of choice. 
(a) Locate the leftmost column 
(b) Create a leading 
make this a leading 1. See Exercise 
(c) Use the 
to create zeros 
entry 
( d) Cover up the row 
containing 
cedure 
on the remaining 

below it. 
entry, 

submatrix. 

the leading 

leading 

entry 

22.) 

of this column. (It will usually 

be easiest 

if you 

Stop when the entire matrix 

and go back to step (a) to repeat 
the pro­
form. 
row echelon 

is in 

Solution The augmented 

2x2 + 3x3 = 8 
2x1 + 3x2 + x3 = 5 

X1 - Xz - 2X3 = - 5 
- 1  -�l 3 
matrix is [� 2 

echelon 
The first nonzero column 

-2 
this matrix to row 

3 

We proceed 
for step 2 of the process. 

to reduce 

form, following 

the guidelines 

given 
by creating 

is column 1. We begin 

with Archimedes 

considered 
and Newton. 

to be one of the three 

greatest 

He is often 

deserves. 
A child 

the "prince 
prodigy, Gauss reportedly 
an error 
he found the formula 

of 3, he corrected 

in his father's 
n(n + 1) /2 for the sum 
of the 

of 
do 
for 

called 

could 

Gauss 

before he could 

(1777-1855) is generally 

Carl Friedrich 
mathematicians 
along 
of all time, 
mathematicians;' 
a nickname 
arithmetic 
the company 
first n natural 
using 
dissertation, 
n zeros, 
multiple 

student, 
When he was 19, he proved 
and a compass, 
polynomial 
of degree 
zeros-the 

payroll, 
and as 
numbers. 

that he richly 
At the age 

that every 
counting 

only a straightedge 

a young 

talk. 

and at the age of21, he proved, 

in his doctoral 

n with real or complex 

coefficients 

has exactly 

Fundamental 
Theorem 

of Algebra. 

that a 17-sided polygon 

could 

be constructed 

calculations 

Gauss's 

1801 publication 
of modern number theory

Disquisitiones Arithmeticae is generally 

probably 

, but he made contributions to 

as well as to sta

foundation 
mathematics 
all of his findings, 
and was often 
to teach 
did not 
publish
The method 

-their 
called 

astrono

tistics, 
physics, 
because he was too critical 
mathematicians, 

did not 
like 
of his own work. He also did not 
perhaps because 
but 

my, and surveying. Gauss 

he discovered-

publish 

of other 
critical 
results 
before they did. 
Gaussian 
elimination was 
well known by Gauss's 
time. 

known to the Chinese 
bears 

Gauss's 

and was 
paper in which he solved 

The method 
oflinear 

a system 

equations 

to describe 

in the third 
name because of his use 
the orbit 

of an asteroid. 

century 

of it in a 

B.c. 

considered 
nearly every branch of 

to be the 

10  Chapter 2 Systems 

of Linear 

Equations 

3 

3 8 

1: 

using 

a second 

3 
- 1  

a leading entry 

We now create 

rows 1 and 3 is the best way 

zero in the first column, 

s �  2 
- s   0 

�  0 s 
0 2 

at the top of this column; interchanging 

to achieve this. [� 2 

- 1  
3 
2 
the leading 

-2 SJ Ri ++ R, [ l 
- 2   - Si 1 s 
R, - 2R1 [ l  - l 
-2 - Si s lS 
1 .  [i - 1  -2 -51 [i - 1  
- 2  -!] 3 
,, -rn, [I �  0 
-2 -5 i 

2 3 8 
zero at the bottom of column 2: 

1 3 
1 2 
0 
echelon 

We now cover up the first row and repeat the procedure. The second 
the first nonzero 
leading 

column 
Multiplying row 

2 by � will create 

The augmented 
matrix is now in row 
responding system 

s  s lS tR2 � 

form, and we move to step 3. The cor­

column of the submatrix. 

We now need another 

is 
a 

1 
0 

- 1  

3 8 

is 

2 

X1 - Xz - 2X3 = - S  

Xz +  x3 = 3 
X3 = 2 

and back substi
finally 
form as 

tution gives x3 = 2, then x2 = 3 -x3 = 3 - 2 = 1, and 

x1 = - S + x2 + 2x3 = - S +  1  + 4 

= 0. We write the solution in vector 

(We are going to 
write 
now on. The reason 

for this will become 

clear 

of linear 
in Chapter 3.) 

the vector 

solutions 

systems as column 

vectors 

from 

Example 2 . 1 1  

Solve the 

system 

w -x -y + 2z = 
2w -2x -y + 3z = 3 
-w +  x -y  = - 3  

which can be row reduced 

Solution The augmented 

Section 2.2 Direct 

Methods 

for Solving 

Linear 

Systems 11 

-2 - 1  3 
- 1  0 

matrix is u - 1  - 1  2 J 
-2 - 1  3  -----+ 
as follows: u - 1  - 1  2 J R,-2R, [: - 1  - 1  2 J 
-----+ 
[: - 1  - 1  2 il 
is now w - x - y + 2z = 1 

0 1 - 1  
0 0 0 

0  - 1  
0 - 2  2 

ated system 

- 1  0 

R3+2R2 

R, +R, 

y - z =  1 

The associ

many solutions. There is more than  one 

which has infinitely 
eters, 
but we will proceed 
ing to the leading 
entries 
free variable
In this case, 
y = 1 + 

the leading 
z, and from 

to use back substi
(the leading variables) 

writing 
in terms of the 

are w and y, and the free 

tution, 

variables 
this we obtain 

w =  1  + x + 

other 

Thus, 

s). 

way to assign param -
the variables 
corresp
ond­
(the 
variables 

variables 
are x and z. 

y -2z 
(1 + z) -2z 

= 1  + x + 
= 2 +x - z  

If we assign 
form as 

parameters 

x  = s and z  = t, the solution can be written 

in vector 

a very important property: In a consis

tent sy

that are not leading 

variables. Since 

stem, 
the 
the number 
form of 
the 

rows in the row echelon 

number of free variables 

(parameters) 

before 

Example 

variables 

2.1 1  highlights 
are just the variables 
is the number of nonzero 
we can predict the 
solution using 
back substi

free variables 
of leading 
coefficient matrix, 
we find the explicit 
although 
is the 
name to this 

number. 

the row echelon 

unique, 
a given matrix. 
same in all row echelon forms of 

form of a matrix is not 

the number of nonzero rows 
Thus, it makes sense 
to give a 

tution. 

In Chapter 3, we will prove that, 

12  Chapter 2 Systems 

of Linear 

Equations 

Definition 
echelon 
form. 

The rank of a matrix is the number of nonzero 

rows in its row 

We will denote 
matrix is 3, and in 
the coefficient 
2. The observations 
prove in more generality 

, the rank of 
the rank of a matrix A by rank(A). In Example 2.10
2.1 1, the rank of the coefficient 
matrix is 

Example 

we have just made justify the following 

theorem, 

which we will 

in Chapters 3 and 6. 

Theorem 2 . 2  

The Rank Theorem 

= n -rank(A) 

Let A be the coefficient matrix of a system 
the system 
is consistent, then 

of linear 

equations 

with n variables. 

If 

Thus, in Example 

solution), and in Example 

number 

of free variables 

2.1 1, we have 4 -2 = 2 free variables, as we found. 

2.10, we have 3 - 3 = 0 free variables 
(in other 

words, 

a unique 

Example 2 . 1 2  

Solve the system 

Solulion When we row reduce 

X1 - X2 + 2X3 = 3 
X1 + 2X2 - X3 = - 3 

2x2 - 2X3 = 

2 - 2  

3 - 3  

we have 

Ri-R1 

!.R2 
� 

matrix, 

the augmented 

2 - 1   � 

[i - 1  2 -:l [i - 1  2 -;] 
2 - 2  [i - 1  2 -�] 
�-,,, [I - 1  2 -�] 
have performed R3 -t R2 as the 
4 

0 = 5. (We could also 

�  0 1 - 1  
0  0 0 

row operation, which would have given 

form.) Thus, 

the system 

1 - 1  
2 - 2  

simplifies 

the back substi

Eliminalion 

tution phase 

done by hand on a system 
with 

Jordan (1842-1899) 

leading to the 
second 
but a different 
row echelon 

elementary 

impossible equation 

professor 

Wilhelm 
was 
of geodesy 
a German 
whose contribution 
to solving 
linear 
method 
closely related to the 
descri

was a systematic 

of back substitution 

systems 

bed here. 

method 

Gauss-Jordan 
A modification 
and is particu

larly 

helpful 

of Gaussian 

greatly 
elimination 
tions 

when calcula

are being 

us the same contradiction 
has no solutions-it is inconsistent. 

Section 2.2 Direct 

Methods 

for Solving 

Linear 

Systems 13 

infinitely 
on reducing 

many solutions. This variant, 
matrix even further. 

the augmented 

known as Gauss-Jordan 

elimination, relies 

row echelon 

form if it 

satisfies the 
following 

reduced 

A matrix is in 

Definition 
properties: 
1. It is in row 
2. The leading entry 
3. Each column 
containing 

echelon form. 

a leading 

in each nonzero row is a 1 (called 

1 has zeros 

everywhere 

else. 

a leading 1). 

The following 

matrix 

is in reduced 

row echelon 
form: 

1 2 0 0 -3 1 0 
0 0  0 4 - 1  0 
0 0  0 3 -2 0 
0 0  0 0 0  0 1 
0 0  0 0 0  0  0 
forms are 

row echelon 

For 2 X 2 matrices, 

the possible reduced 

where * can be any 
number. 
that after 

It is clear 

short 

that the reduced 

For a 
proof 
row echelon 
form of 
unique, 
Yuster, 
Form of 
Simple 
issue 

is 
see the article 
"The Reduced 
a Matrix 
Is Unique: A 
in the 
Proof;' 

March 1984 

a matrix 
by Thomas 
Row Echelon 

of Mathematics Magazine 

(vol. 57, 

no. 2, pp. 93-94). 

row operations 
will bring 
intuition may suggest 
elon form of a matrix 
In Gauss-Jordan 
the augmented 

it) is that, 
is unique. 
elimina
tion, 
matrix to reduced 

a matrix has 

been reduced 

it to reduced 
unlike 

row echelon 
the row echelon 

to echelon 

form, further 
form. What is not clear 

elementary 
(although 
form, the reduced 
row ech­

as in 
we proceed 
row echelon 
form. 

Gaussian 

elimination 
but reduce 

Gauss-Jordan 

Elimination 

matrix of the system 
reduce 

of linear 
the augmented 

equations. 
matrix 

row operations to 

1. Write the augmented 
2. Use elementary 
3. If the resulting system 

row echelon 
form. 

istent, 
solve 
of any remaining free 

is cons
variables. 

for the leading 

variables 
in terms 

to reduced 

Example 2 . 1 3  

Solve the system 

in Example 

SolUtion The reduction 

proceeds 

we reach 

the echelon 
form: 

elimina

tion. 

2.11 by Gauss-Jordan 
as it did in Example 

[: - 1  

1 
0 

0 
0 

- 1  

2.1 1  until 

2 l l - 1  1 

0  0 

14  Chapter 2 Systems 

of Linear 

Equations 

We now must create a zero above the leading 
do this by adding 

1 in the second 

row, third 

column. We 

row 2 to row 1 to obtain [: - 1  0 

0 
0  0 

The system 

has now 

been reduced 

to 

w - x  + z = 2  

y - z =I 

It is now much 

easier to solve for the 
leading 

variables: 

w = 2+ x - z  and y =I +z 

If we assign 
form as 

parameters 

x = sand z = t as before, the solution can be written 

in vector 

R e m a r k  From a computa
fewer calcula

sense 
tional point of view, it is more efficient 
tions) 
form 
to left, make each leading entry 
to 

from right 
1 s. However, for manual calcula
t to right and create the leading ls and 
the zeros 
in their 

(in the 
the matrix to row echelon 

a 1 and create zeros 
you will find it easier 

to first reduce 

tion, 

columns 

that it requires 
and then, 
working 
above these 
just work from lef
as you go. 

leading 

to the 

geometry that 

Let's 
equations 
ables 
be answered 

return 
in two variables 

IR2, so linear 
correspond 
3
in IR
. In fact, many questions 
by solving 

brought us to this point. Just as systems 
in three 
vari­
and planes 
can 

correspond 

to lines in 

an appropria

te linear 
system. 

about lines 

to planes 

equations 

oflinear 

Example 2 . 1 4  

Find the 

line of intersection 

of the planes 

x + 2 y - z 

= 3 and 2x + 3y + z = 1. 

of the two planes-

Solution First, observe that there will be a line of intersection, since the 
vectors 
lie in the intersection 
of the system 

The points that 
the solution set 

1]-are not parallel. 

[ I, 2, - I] and [2, 3, 

correspond 

of the two planes 

to the points in 

normal 

x + 2y - z =  3 
2x + 3y + z =  1 

Gauss-Jordan elimination 

applied to 

the augmented 

matrix 

yields 

Linear 

Systems 15 

Methods 

for Solving 

[� Section 2.2 Direct 

2 3  2 -1 

-11 3] 3 - 5  
s 1-7J -3 5 

0 

Replacing 

variables, we have 

z 

variable 
We set the free 
z equal to a parameter 
of intersection of the two planes: 
tions 

of the line 

t and thus obtain 

the parametric 

equa­

x  + Sz = -7 
y -3z = 5 
x = -7 -St 

y = 5 + 3t 
z =  

In vector 

form, the equation 

is 

x 

y 

Figure 2 . 2  
The intersection 

of two planes 

Example 2 . 1 5  

whcthec the lilles 

See Figure 2.2. 

point of intersection. 

and, if so, find their 

x = p + tu and x = q + tv intersect 

Lct p � [ _ H q � m' u � [:J. md v � [ J Dcte;m;ne 
SU -tv = q -p. 

Solution We need to be careful 
of both lines, 
in the equations 
parameters. 
Let's 
use a different 

x � p + su. If the lines 

simultaneously. That is, we want x = p + su = q + tv or 
p, q, u, and v, we obtain 
s -3t = -1 s +  t =  2 

the lines 
parameter-

both equations 

satisfies 

the equations 

the given 

bewmes 

tuting 

here. 

Substi

Although t has been used as the parameter 
so are their 
are independent and therefore 
so its equation 

say, s -for the first line, 

whose solution is 

easily found to bes = �, t = � .  The point of intersection is 

therefore 

s +  t =  2 

illtmed, then we wmt to find on x � [;] 

thot 

16  Chapter 2 Systems 

of Linear 

Equations 

z 

See Figure 
point.) 

2.3. 

(Check that substi

tuting 

equation 

gives the 

same 

t = i in the other 

x 

y 

Figure 2 . 3  
Two intersecting 

Remark In IR

3
, it is possible for two lines to 

intersect 

to do neither. Nonpara

llel lines 

that do not intersect 

are ca

in a point, to be parallel, 
or 
lled skew lines. 

lines  Homogeneous 
svstems 
We have seen tha
t every system 
solution, 
always 

or infinitely 

one solution. 

has at least 

many solutions. However, 

of linear 

equations 

has either 
no solution, 
there is  one 
type of system 
that 

a unique 

A system 
Definition 
term in each equation 

is zero. 

oflinear 

equations 

is called 

homogeneous ifthe constant 

In other words, a homogeneous system 

has an augmented matrix 
: 

of the form 

[A IO]. The following 

system 

is homogeneous

2x + 3y - z = 0 
-x + Sy +  2z = 0 

a homogeneous 
system 

Since 
it will have either 
many solutions. The next theorem says that the latter 
of variables 
than the number of equations. 

have no solution (forgive 
or trivial, 
case must occur if the number 

the double 
solution) 

solution (namely, the zero, 

is greater 

a unique 

cannot 

negative
or infinitely 

!), 

Theorem 2 . 3  If [A I O] is a homogeneous 

system 
has infinitely 

of m linear 
many solutions. 

equations 

with n variables, 

where 

� 

m < n, then the system 

rank(A) :::::: m (why?). By the Rank 

Proof Since the system 

has at  least 
Theorem, 
of free variables 

number 

we have 

So there is 
at least 

one free 

variable 

and, hence, 

there 

are infinitely 

many solutions. 

= n -rank(A) 2: n - m > 0 

the zero  solu
tion, 

it is consistent. 

Also, 

Note Theorem 2.3 says nothing 

you to give examples 
or infinitely 

many solutions. 

to show that, 

about the 
in this case, 

case where m 2: n. 
there 

Exercise 44 asks 
solution 

can be either a unique 

IR and ZP are examples 
the set of complex numbers C are 

numbers Q and 

The set of rational 

of fields. 

other 
examples. 
in detail 
algebra. 

Fields 

are covered 

in courses in abstract 

Section 2.2 Direct 

Methods 

for Solving 

Linear 

Systems 11 

svs1ems over Zp 

systems 

we have encountered 

linear 
Thus far, all of the linear 
and the solutions 
number systems 
respects 
numbers). Thus, we can 
coefficients 
belong to 
the linear 

For example, 
over Z2, has exactly 

like IR; in particular

ngly been vectors 

have accordi
arise-notably, "ll_P' When p is a prime number, "ll_P behaves in 
many 
multiply, and divide (b
y nonzero 
and 
equations 
over "ll_P' 

, we can add, subtract, 

when the 

also solve 

systems 

of linear 

to solving 

a system 

we refer 

some "ll_P' In such instances, 
four solutions: 

equation x1 + x2 + x3 = 1, when viewed 

have involved 
real numbers, 
in some !Rn. We have seen how other 

variables 

as an equation 

(where 

the last 

solution arises 

because 1 + 1 + 1 = 1 in Z2). 

wh,n wni,wth"q"'Hon X ;  + x, + 

X; � 1ov"2,, thno!utiom [::l m 

� 

But we need not use trial-and-error 

(Check these.) 
ces works just as well over "ll_P as over IR. 

methods; 

row reduction 

of augmented 
matri­

Example 2 . 1 6  

Solve 

the following 

system 

oflinear 

over Z3: 

equations 

X1 + 2x2 + x3 = 0 
x2 + 2x3 =  1 

X1  + X3 = 2 

are not needed; 

Solution The first thing to note in examples 
division 
we can accomplish 
tiplica
see Exercise 

(This, however, 
60 at the end of this section 

that we be working 
and Exercise 
matrix of the system, 

and 
addition 
and mul­
over "ll_P, where p is a prime; 
57 in Section 
modulo 3. 
using 

the augmented 

the same effects 

We row reduce 

requires 

calculations 

tion. 

using 

like this one is that subtraction 

1 . 1 .) 

R2+2R1 

0  � 
1 2 

[i 2 ;] [i 2 1 ;] 
R,+R2 [i 0 :J 
� 
R3+2R2  1 0 
0 2 

1 0 
1 2 

18  Chapter 2 Systems 

of Linear 

Equations 

R, +R, [ 1 Q 

�  0  1 

0  0 

0  1 l 0  2 

1  1 

Thus, the solution is Xi 

= 1, x2 = 2, x3 = 1. 

Example 2 . 11 

Solve the following 

system 

of linear 

over 22: 
equations 
Xi + Xz + X3 + X4 = 1 
Xi + X2 
= 1 
X2 + X3  = 0 
X3 + X4 = 0 
+ X4 = 1 

Xi 

Solution The row reduction 

proceeds 

as follows: 

1 1 

1 
1  0 0 1 R2+R1 0  0 
� 
0 1  0 0 R5+R, 0 1 
0 0  1 0 

1 
1 1 0 
1 0  0 
1 0 
0  0 
0 0 

0  0 

0 

� 

1 0 

R2 <--> R3 
R1 + R2  0 
R5+R2  0 0 
R2 + R3  0 
R4 + R3  0 0 

1 0 

� 

0 1  1 

0 0 
0 
1  1 0 
0 0  0 

0  0 
0 0 

0  1 
0 0 
1 1 0 
0  0 0 
0  0 0 

0  0 
0  0 

Therefore, 
we have 

X1 

+ X4 = 1 
Xz  + X4 = 0 

X3 + X4 = Q 

Setting 

the free 

variable 

x4 = t yields 

Section 2.2 Direct 

Methods 

for Solving 

Linear 

Systems 19 

Since t can take on the 

are exactly 

two solutions: 

two values 0 and 1, there 

�  solutions. 

Remark For linear 

systems 

(Why not?) Rather, when there 

of solutions is 
finite 
Exercise 

59.) 

and is a function 

over "11..P, there can never be infinitely 

many 
the number 
of the number of free variables 
and p. (See 

than one solution, 

is more 

2 . 2  

matrix is in 

whether 

ine whether 

the given 
it is also in reduced 

1-8, determ
In Exercises 
form. If it is, state 
row echelon 
row echelon 
form. 

..  1 Exercises 
• 
9. [: 0 
:J 10. [� �] 
0 
�] 2. [� 0 1 
!. [i 
0  1 -1 II.[: -!] 
�] 
0 0 
12. [� -4 -2 :J 
1 6 
4. [: 0 :J 
3. [� 1 3 �]  0 
0 0  0  13. [: -2 -1 i [-2 -4 1:] 
0 3 -4 :J 6. [: 0 :J  -1 -1 14. -� -6 
0 0  0 1  - 3  -1  2 - 3  
5. [i 
5 0  0 
7. [j 2 �: 8. [� 1 3 -�] 
0  0 1 15. Reverse the elementary 
1  0 0 Example 
0  0 0 
[� 2 -4 -4 -�] 
-1 10 9  into 
reduced  0 1  1 -1 
0  0  0 
24 

In Exercises 
the given 
row echelon 

9-14, use elementary row operations 

to reduce 
form and (b) 

row operations 
convert 

matrix to (a) row echelon 

2.9 to show 

that we can 

form. 

used in 

8 0   Chapter 2 Systems 

of Linear 

Equations 

2 
4 
3 

row operations 

row operation 

-4 
0 

"undoes" 

16. In general, 

-4 
0 
2 
3 

that 
elementary 

what is the elementary 
each of the three 

6 �] 
R; � �' kR;, and R; + kR/ 
17 and 18, show that the given 
2] B = [3  -1] 
17.A = [3
4 '  1 0 
18. A= [ � 
- 1  i [3 � , B  = � 
-�] 

In Exercises 
equivalent 
and find 
that will convert 

19. What is wrong with the following 

a sequence 
of elemen

5 
2 
"proof" that every 

matrices are row 

A into B. 

- 1  

0 

1 

1 

matrix with at least 
matrix with a zero row? 

two rows is row equivalent 

to a 

tary row operations 

R2 + R1 and R1 + R2• Now rows 1 and2 
a 

Perform 
are identical. 
row of zeros 

Now perform R2 - R1 to obtain 
in the second 
row. 

the following 

20. What is the net 

effect of performing 
row operations 

sequence of elementary 
(with at least 

two rows)? 
R2 + R,, R, - R2, R2 + R,, -R, 
frequently 
to introduce 

perform the following 
a zero into a matrix: 

type of cal­

21. Students 
culation 

on a matrix 

row operations. 

22. Consider 

Why not? Show how to achieve the same result 

3R2 -2R1 is not an elementary 
row opera­

However, 
tion. 
using elementary 

the matrix A = [ � !] . Show that any 
3 X 3 matrice

the three 
used to create a leading 
Which do you prefer 
and why? 

23. What is the rank of each of the 
24. What are the 

of 
row operations 
can be 

matrices 
row echelon 
forms of 

types of elementary 

possible 
s? 

1 at the top of the first column. 

reduced 

in Exercises 

1-8? 

25-34, solve the given system of equations 

using 

elimination. 

or Gauss-Jordan 

In Exercises 
either Gaussian 
25. X1 + 2X2 -3X3 = 9 
2X1 - X2 +  X3 = 0 
4x1 - X2 +  x3 = 4 3x +  y + 7z = 2 
28.2w + 3x - y + 4z = 1 

26. x - y + z = 0 

-x + 3y +  z = 5 

27. X1 -3X2 - 2X3 = 0 
-X1 + 2X2 + X3 = 0 
2x1 + 4x2 + 6x3 = 0 

3w - x + z = 1 
3w -4x + y -z = 2 

29.  2r +  s = 
4r +  s =  7 
2r + Ss = 
30. -X1 + 3X2 - 2X3 + 4X4 = 0 

3 
- 1  

2 

- 4X5 = 

2X1 - 6X2 + X3 - 2X4 =  - 3 
X1 -3X2 + 4X3 -8x4 = 2 
31. � X1 +  X2 - X3 - 6X4 
ix, + �Xz  -3X4 +  X5 = - 1  
32. Vlx + y + 
2z = 
Vly -3z = 
-y + Vlz = 
1 
w - x - y + z =  
0 
- 1  
w + x  + z =  2 

33. w + x + 2y + z = 

1 -V2 

x +  y 

8 

34. a +  b +  c +  d = 

4 
4d = 10 
a + 2b +  3c + 
a + 3b + 6c + lOd = 20 
a + 4b + lOc + 20d = 35 

35-38, determine 

solutio

, or no 

augmented matrix has a 

In Exercises 
performing any calculations
the given 
unique 
many solutions

by inspection 
(i.e., wit
hout 
system with 
) whether a linear 
infinitely 
n. Justify your answers. 

35. [: 
37. [i 

36. [� 
38.[� - 2  
4  OJ 8 0 

0 
- 3  
-6 
2  3 
4 
5  4 
3 
7 
7  7 

3 
7 
1 1  
if ad - be -=!= 0, 

0 
1 
0 
2 
6 
10 

39. Show that 

then the system 

2 
4 

12 0 

solution, 

ax + by = r 
= s 
ex + dy 

has a 

unique 

solution. 

Methods 

for Solving 

Linear 

Systems 8 1  

Section 2.2 Direct 

(Figure 2.6). 
(c) Give an example 

which are parallel 

of three 

planes, exactly 

two of 

figure 2 . 6  

(d) Give an example 
point (Figure 2.7). 

single 

of three 

planes 

that intersect 

in a 

ns? 

and 

40-43,

a unique 

solution, 

for what value(s) of k, if any, will the 

have (a) no solution, (b) 

In Exercises 
systems 
(c) infinitely many solutio

40. kx + 2y =  3  41. x + ky =  1 
2x -4y = -6 42. x -2y + 3z = 2 
44. Give examples 

43. x +  y + kz = 

x + ky + 
kx +  y +  z = 

x + y +  z =k 
2x - y + 4z = k2 

of m linear 
and with m > n 

in n variables with 

kx +  y =  1 

of homogeneous 

systems 

m = n 

z = 

-2 

equations 
that have (a) infinitely 
unique 

solution. 

many solutions and (b) a 

45 and 46, find the line of intersection 

of the 

In Exercises 
given 

planes. 45. 3x + 2y + z = - 1  and 2x - y + 4z = 5 
46. 4x + y + z = 0 
47. (a) Give an 

and 2x - y + 3z = 2 
planes 
(Figure 2.4). 

mon line of intersection 

example 

of three 

that have a com­

Figure 2 . 4  

(b) Give an example 

of three planes 

that intersect 

in 

have no common point of intersection 

pairs but 
(Figure 2.5). 

Figure 2 . 5  

figure 2 . 1  

n. 

find 

their 

point 

of intersectio

In Exercises 

48 and 49, determine whether the lines 

x = p + su and x = q + tv intersect and, if they do, 
48.p� [-Hq� [Hu�[_+� [-il 
49.p� [�Jq� [:J.u� [+� m 
let p � m. u � [ J. ond v � m Desaibe 
50. 
equation x = p + su. 
product of vectors u and v is a 
51. 
vector u X v that is orthogonal to 
both u and v. (See 
U � [ :: l and F  [ :: l 

all points Q = (a, b, c) such that the line 
Q with direction 
vector 

Product in Chapter 1.) If 

v intersects 

that the cross 

the line with 

through 

The Cross 

Recall 

Explora

tion: 

8 2   Chapter 2 Systems 

of Linear 

Equations 

show that there 

are infinitely 

many vectors  55. x + y = 1  over 

that simultaneously satisfy u · x = 0 and v · x = 0 

[U2V3 -U3V2] 
U X V = U3V1 -U1V3 

and that all are multiples 

of 

U1V2 -U2V1 

lines 

x = p + su and x = q + tv are 

Show that the 
skew lines. Find vector 
planes, 

equations 
each line. 

one containing 
53-58, solve the systems of linear 

of a pair of parallel 

equations 

In Exercises 
over the indicated ZP. 
53. x + 2y = 1 over 
Z3 
x +  y = 
54. x + y = 1  over 
Z2 
x  + z = 

y + z= O  

2 

1 

1 

y + z= O  

Z3 
x  + z = 
56. 3x + 2y = 1  over 
57. 3x + 2y = 1  over 
58. x1 

x + 4y = 1 

x + 4y = 1 

Zs 

X1 + 2X2 + 4x3  = 3 
2X1 + 2X2  + X4 = 1 
X1  + 3X3  = 2 

Z7 
+ 4x4 = 1  over 

Zs 

system 

equations 

of linear 

corollary 

59. Prove the following 

Let A be an m X n matrix with 
consistent 
60. When p is not prime, 
matrix A has exactly 

pn-rank(A) 

to the Rank 
Theorem: 
in Zp. Any 
entries 
with coefficient 
over ZP' 
solutions 
care is needed 
in solving 
any equation) 
over ZP' 
solve the 
arise? 

(or, indeed, 
a linear 
system 
Using Gaussian 
elimina
tion, 
over Z6• What complications 
2x + 3y = 4 
4x + 3y = 2 

following 

extra 

system 

Writing Proiect A History of Gaussian Elimination 

tion. 

sketch 

elimina

the method 

actually 

known as Gaussian 

of Gauss in this section, 

Gauss did not 
It was known in some form as 

Write a report on the history 

As noted in the biographical 
"invent" 
early as 
throughout Europe and Asia. 

the third century B.C. and appears in the mathematical writings 
F. Grear, Mathematicians 

Mathematical Monthly 94 (1987), pp. 130-142. 
(Available 
online 

linear 
name attached to the 
method? 
1. S. Athloen and  R. 

American 
Vol. 58, No. 6 (2011), pp. 782-792. 
notices
/201106/index.html) 

Elimination, 
Notices 
at http://www.ams.org/ 

What role did Gauss actually 

for solving systems of 

play in this history, and why is his 

of elimination 
methods 

Gauss-Jordan reduction: 

of Gaussian 

McLaughlin, 

of cultures 

of the AMS, 

equations. 

2. Joseph 

A brief 

history, 

3. Roger Hart, The Chinese Roots of Linear 
4. Victor 

University 
Press, 
201 1). 

J. Katz, A Histor
MA: Addison 

y of Mathematics: An Introdu
Wesley 

Longman, 
2008). 

Algebra 

(Reading, 

(Baltimore: 

Johns Hopkins 

ction 

(Third Edition) 

store real numb

and calculators 

cAs  Lies My C o mputer Told Me 
as 0.2001 X 104, and -0.00063 
g-point form of a number is :±:.M X lOk, where k is an integer 

Computers 
2001 is stored 
the floatin
mantissa 
system. 
on the 
decimal places that 
is d, we say that 
there 
8 or 12 significant 
digits; 
calculators 
to a limit. 
number 

real number that satisfies 
of decimal 
number 
calculator, 
or computer algebra 

as -0.63 X 10-3. In general, 
0.1 ::::: M < 1. 

in the mantissa 
number 
If the maximum 
Many 
are d significant digits. 
are subject 
store more 
but still 
(in which case we say that the 
to d significant 
digits. 
ng-point form is 0.3141592654 
stored as 
nt digits 
would 

store 
Any digits 
has been truncat

X 101. 
0.31415 X 

ed) or used to round the number 

ers in floatin
is stored 

form. For example, 

computers can 

M is a (decimal) 

that are not stored 

that can be stored 

computer, 

can be stored 

The maximum 

g-point 

that rounds 

and its floati

are either 

omitted 

depends 

places 

and the 

to five significant digits, 1T would be 
); a computer 
3.1416
so that it becomes 

to five significa
). When the dropped digit 

even. Thus, rounded 
to two 

is a solitary 

of 

that truncates 
yed as 
3.1415
digit is rounded 

For example,-rr = 3.141592654, 
In a computer 
1T as 0.31416 X 101 (and display 
101 (and displa
store 
5, the last remaining 
significa
nt digits, 0.735 
can have a dramatic 
effect on the 
error 
formed, 
the more the 
we can do about this. 
This exploration 
systems 

truncation or rounding 

Whenever 

becomes 

accumula

of linear 

equations. 
following 
1. Solve  the 

of linear 
numbers throughout the calculati

system 

rational 

ons). 

calculations. The more operations 
tes. Sometimes, 
illustrates 

nothing 
this phenomenon with 

there is 
very simple 

unfortunately, 

that are per­

0.74 while 0.725 
occurs, 

0.72. 
a roundoff error 

becomes 

is introduced, 
which 

2. As a decimal, 

��b = 1.00125, so, rounded 

becomes 

to five significa

nt digits, the system 

x +  y= O  
X + sooY =  1 

801 

x +  y= O  
x + 1.0012y =  1 

Using your calculator 
tion to 

five significa

nt digits. 

or CAS, solve 

this system, rounding 

the result 

of every calcula­

8 3  

equations exactly (that  is,  work with 

3. Solve the system 
significa

two more times, 
nt digits. What happens? 

then to three 

rounding 

first to four 

significa

nt digits 

and 

large 

4. Clearly, a very small roundoff 
in the solution. 

sult in very 
errors 
graphs of the various 

than or equal to 
why geometric
ally. 

error (less 
Explain 
systems you solved in 
you just worked 

with are called 

1-3.) 

Problems 

Systems such as the one 

linear 

0.00125) can re­
(Think about the 

are extremely 
We will encounter 
example 
to experiment with: 

sensitive to roundoff errors, 
is not 

and there 
again 

ill-conditioned 

systems 

in Chapters 3 and 7. Here is another 

ill-conditioned. They 
much we can do abou
t it. 

4.552x + 7.083y = 1.931 
1.731x + 2.693y = 2.001 
numbers of significa

Play around 
with eight significa

with various 
nt digits 

(if you can). 

nt digits 

to see what happens, 

starting 

Partial Pivoting 

Lies My Computer 
trouble when roundoff 

In Explora
tion: 
tems can cause 
cover 
very small changes 
Fortunately, 
problem 

error 
another way in which linear 
systems 
in the coefficients 
something 

occurs. In this exploration, 
error 

Told Me, we saw that ill-conditioned 
linear 
you will dis­
to roundoff 
and see that 
inaccuracies 
te this 

the problem with ill-conditioned 

that can be done to minimize 

can lead to huge 

or even elimina

there is 

are sensitive 

systems). 

(unlike 

in the solution. 

sys­

places. The equa­

(a) and (b) can be thought of as the 

1 .  (a) Solve 

the single linear 

(b) Suppose your calculator 
tion will be rounded 

to 0.0002x 

equation 
can carry 

0.0002lx = 1 for x. 
= 1. Solve this equation. 

only four decimal 

The difference 

effect of an error of 

2. Now extend 

in parts 

elimina

equation. 

the linear 

(a) With Gaussian 

to a system 
tion, 

between the answers 
0.00001 on the solution of the given 
equations. 
this idea 
system 

oflinear 
solve 
0.400x + 99.6y = 100 
75.3x -45.3y = 30.0 
nt digits. Begin by pivoting 
using three significa
calculation to 
significa

x = 
- 1.00, y = 1 .01. Check that the 
huge error-200% in the x value! Can you discover what caused it? 
(b) Solve the system in 
tions 
on 75.3. 
solution this 

part (a) again, 
the two rows of its augmented 
significant 

take each calculation 

Again, 
time? 

this time 

(or, equivale

matrix) 

nt digits

actual 

to three 

three 

ntly, 

on 0.400 and take each 

. You should obtain 

solution is x = 1.00, y = 1.00. This is 

the "solution" 

a 

interchanging 
the two equa­
and pivoting 

digits. What is the 

that, when using 

Gaussian 

or Gauss-Jordan 

a numerical 

The moral of the story is 
solution to a system 
choose 

to obtain 
mation), you should 
choose from among all possible pivots in 
value. Use row interchanges 
create 

where needed 

zeros 

care. 
a column 

a decimal 
approxi­

equations 
Specific
the entry 

(i.e., 
at each pivoting 
step, 
absolute 

ally, 

to bring this element 
in the column. This strategy is known 

with the largest 
into the correct position and use it to 
as partial 
g. 

pivotin

the pivots with 

oflinear 

elimination 

8 4  

with partial 
solutions are given.) 

tion, 

and then 

elimina

systems 

pivoting. 

by Gaussian 

to three significa

Take each calculation 

first without 
nt digits. (The exact 

3. Solve the following 
(a) O.OOlx + 0.995y =  1.00 
-10.2x 
Exact solution: [;] [5.00] 1.00 

+ 1.00y = -50.0 

(b) lOx -7y = 7 
-3x + 2.09y + 6z = 3.91 
Exact solution: y = -1.00 
z 1.00 

5x -y + 5z = 6 [xl [ 0.00] 

C ounting O peration s :  An Introduction 
to the Analysis of Algorithms 

Gaussian and Gaus
cedures designed 
the augmented 
matrix 
suited 
from the speed, 
are running, 

s-Jordan 
to implement 

elimination 
a particular 
of linear 

task-in 
equations. 

of a system 

are examples 

of algorithms: 
this case, 
Algorithms 

pro­
systematic 
the row reduction 
of 
are particularly 
equal. 

well 
Apart 

are created 

to computer implementa

tion, 
memory, and other 

plexit

y of an algorithm 

some algorithms 
are faster 

(a measure of its efficien

ble number of steps) is the number of basic 

but not all algorithms 
attributes 

of the computer 
than others. 
One measure 
cy, or ability 

on which they 

of the so-called com­

system 

operations 

to perform its task 
in a 
it performs as a func­

reasona
tion of the 

number of variables 

that are input. 

Let's examine this 

the basic opera

system: Gaussian and 

proposition in the case of the two algorithms 
we have for 
pur­
tion. 
For our 
that 
we will assume 
ignored. 
(This 

Gauss-Jordan 
plication 

are performed much more rapidly and can be 

elimina
and division; 

are multi

tions 

solving a linear 
poses, 
all other 
is a reasona
only systems 

operations 

performed 

of equations 

by Gaussian 

number of operations 

but rather establish 

we will not attempt to justif

so, if the 
is n. Thus, our task is to 
and Gau
that may 
l cases 

ble assumption, but 

with square coefficient matrices, 
matrix is n X n, the number of input variables 
0  a function of n. Furthermore, 
00  arise, 
the worst case that can 
9  computer 
j as long as possible. Since 
(c. 780-850) 
1. Consider 

operation), we will denote 
perform a single 
by T(n). We will typically 
by an algorithm 
n, so comparing 
this function 
which will take less time 

whose 
w'al muqabalah 
bed the use ofHindu­
rules 

(if we know how long it takes 
the number of opera

this will give us an estimate of the time it will take a 
to 

in T(n) for large 
will allow us to 

coefficient 
find the 
as 

for different algorithms 

to perform the algorithm 

we will not worry about specia

be interested 

the augmented 

to execute. 

ss-Jordan 

arise-w

Abu Ja'far 
was 
al-Khwarizmi 
a Persian 
mathematician 
book Hisab al-jabr 

(c. 825) 

descri
numerals 
arithmetic. 

Muhammad ibn Musa 

The second 

tions 

and the 

matrix 

a computer 
performed 
values 
determine 

of 

hen the algorithm 
takes 

elimination 

y it.) We will consider 

Arabic 
of basic 
word of the book's title gives 
rise 
to the 
word algebra, 
and 
the word algorithm is derived from 
al-Khwarizmi's 

English 

name. 

[A lhJ  4 9 

8 5  

to bring [A I b] to the row echelon 

required 

Count the number of operations 

3 4] - 1  0 
form  [: 2 

1  1 

1 
0 

to complete 

we mean a multiplication or a division

(By "operation" 
of operations 
nation. 
that is, to reduce [A I b ]  to its reduced 
2. Count the number of operations needed 

the back substi
number of operations. 

needed 
the total 

Record 

elimination-

row echelon form 

.) Now count the number 

tution phase of Gaussian elim

i­

to perform Gauss-Jordan 

the zeros 
into each column 
in that column). What do your answers 

are introduced 

immediately 
after 

1 is 
about the relative 

the leading 

suggest 

efficiency 

(where 
created 
of the two algori

thms? 

the algorithms 

in a general, 

from a linear 

system 

way. Sup­

systematic 
with n equations 
and 

to analyze 

We will now attempt 
pose the augmented 
n variabl

matrix [A I b ]  arises 
es; thus, [A I b ]  is n X (n + 1): [al l  
[A I b ]  =  a�1 

an] 

that row interchanges 

are never needed-tha

t we can 

always 

create a 

We will assume 
leading 

by the 
1 from a pivot by 
dividing 
pivot. 
are needed 

3. (a) Show that n operations 

a21 a22 

la., a12 
�  show that 

(Why don't 

anl an2 

we need to count an operation for the 
n operations are needed 

bn 

az2 

a2n 

anl 

ann 

aln 
a2n 

1: 

an2  ann bn 

to create 

the first leading 

* 
* :, l 
b, l l l �2 � 
a�I 
* 
* 
* 
* 
iJ 
la�, 

creation 
of the leading 
zero in column 1: 

the first 

1 ?)  Now 

to obtain 

an2  ann 

8 6  

....,.. (Why don't 

we need to count an operation for the creation 

of the zero 

itself?
) When 

the first column 

has been "swept 

out;' 

we have the matrix 

Show that the 

total 

number of operations 

(b) Show that the total 
form 

echelon 

point is n + (n - 1) n. 
needed 
to reach the row 

needed 

up to this 

0 *  *  * 

number of operations 

[� : : :] 
[! * 
* '] *  * 

0  1 * 

[ n + 

(n - l)n ]  +  [ (n - 1) + (n - 2)(n - 1)]  +  [ 

(n - 2) + (n - 3)(n - 2)] 

is 

+ . . .  + [2 +  1 . 

2 ]  +  1 

which simplifies 

to 

n2 + (n - 1)2 + · · · + 22 +  12 
· · · + (n - 1) 

number of operations 

1  + 2 + 

( c) Show that the 
phase is 

tution 

needed 

to complete 

the back su

bsti­

(d) Using summation 

formulas for the sums in parts (b) and (c) (see 

Exercises 51 and 52 in Section 2.4 and Appendix B), show 
elimination 
operations, T(n), performed 

that the total 

number of 

by Gaussian 
is 
T(n) = tn3 + n2 -tn 

every polynomial function 

is dominated by its leading 

term for large 

values 

of 

Since 
the variable, 

4. Show that Gauss-Jordan 

we see that T(n) = tn3 for large 
below the leading 

values 
of n. 
has T(n) = !n3 total 
ls as we go. (This 

elimination 

operations 
for large 
than this version 
of Gauss­

shows that, 

if we 

Gaussian 

elimination 

is faster 

create zeros above  and 
of linear 
systems 
equations, 
J ordan elimina
tion.) 

81 

8 8   Chapter 2 Systems 

Equations 

of Linear 

� S p a n n i n g  

se1s a n d  l i n e a r  Independence 

The second 
tions 
as asking 
We explore 
concepts, 

of the three roads 

in our "trivium" 

of vectors. We have seen that we can view 

whether 

a certain vector 
this idea in more detail 

is a linear 
combina
in this section. 
in later 
repeatedly 

It leads 
chapters. 

which we will encounter 

is concerned 
solving a system 

with linear 
of linear 
tion of certain other 

combina­
equations 

vectors. 
to some very important 

Spanning 
Sets of Vectors 
We can now easily answer 
a linear 

the question 

raised 

in Section 1.1: 

When is a given 

vector 

vectors? 

given 

combination 
of other 

(o) fa th, wdoc m ' linm rnmbinotion of
(b) fa m ' linemomb;mtion 

thevodorn m and [ � �} 
vectorn m and [ � �} 

of the 

Solulion 

(a) We want to find scalars x and y such that 

Example 2 . 1 8  

Expanding, 

we obtain 

the system 

whose augmented 

matrix is 

x - y =  1 
y= 2  
3x -3y = 3 

[� �� �] 

[� : �] 

(Observe that the columns of the 
the order 

of the vectors

-in particular

The reduced 

echelon 

form of this matrix is 

augmented 

matrix are 

just the given vectors; 

notice 

, which vector 

is the constant vector.) 

�  (Verif

y this.) So  the 
combination 

is 

solution is x  =  3,  y 

2, and  the 

corresponding 
linear 

Section 2.3 Spanning 

Sets and Linear 

Independence 

89 

(b) Utilizing 
matrix is 

our observation 

in part (a), we obtain 

a linear 

system 

whose augmented 

which reduces 

to 

;eve.ling that th, 'Y'"m 

h"' no ,oJotion. 

of m and [ J  4 

Th°', in thi, 

bination 

""· [ �] i' not a linm com-

The notion 

of a spanning 

set is intimately 

connected 

with the solution of linear 

Look back at Example 

systems. 
matrix [A I b] has a solution precise
of A. This is a general 

fact, summarized 

in the next theorem. 

2.18. There we saw that a system with 

augmented 

ly when b is a linear 

combination 

of the columns 

Theorem 2 . 4  

A system 
of linear 
only if b is a linear 

equations 
combination 

of the 

matrix [A I b] is consistent 
columns 
of A. 

with augmented 

if and 

Let's 

revisit 

Example 

2.4, interpreting 

it in light 

of Theorem 2.4. 

(a) The system 

x - y= l 
x + y= 3 
solution x = 2, y = 1. Thus, 

has the 

unique 

See Figure 2.S(a). 
(b) The system 

has infinitely 

many solutions 

x = 2 + t, y = t. This implies 
that 

2 

x - y = 
2x -2y = 4 
of the form 

for all values 
so all lie along 

oft. Geometrica
the same line through the origin 

lly, the vectors [ �], [ = �], and [ ! ] 

are all parallel 
and 

[see Figure 

2.S(b)]. 

9 0   Chapter 2 Systems 

of Linear 

Equations 

y 

5 

y 

5 

4 

y 

5 

4 

-1 

-2 

-3 

-2 -1 

2  3 

(a) 

Figure 2 . 8  

-3 

(b) 

-2 

-3 

(c) 

(c) The system 

has no solutions, 

so there 

of x and y that satisfy 

x - y= I  
x - y =3 
are no values 

In this case, [ �] and [ = �] are parallel, 

through the origin [see 

Figure 2.S(c)]. 

but [ �] does not lie along the same line 

We will often 

be interested 

in the collection 

of all linear 

combina

tions 

of a given 

set of 

vectors. 

Definition 
linear 

noted by span(v1, v2, ..• , vk) or span(S). If span(S) = W, then S is called 

the span ofv1, v2, . . .  , vk and is de-
a span­

If S = {v1, v2, . . .  , vk} is a set of vectors 

ofv1, v2, . . .  , vk is called 

combinations 

in u;gn, then the set 
of all 

ning set for u;gn. 

u;g

Show that 

z = span( [ _ � l [ �]). 
of [ _ �] and [�} that is, we must show that the equation 
x[ _ �] + 
1[�] 
= [:] can always 

vector [:] can be 

ved for x and y (in terms of a and b ), regardless 

Solulion We need to show 

that an arbitrary 

combination 

as a linear 

written 

be sol

of the 

values 

of a and b. 

Example 2 . 1 9  

Section 2.3 Spanning 

Sets and Linear 

Independence 

9 1  

The augmented 

matrix is [ _ � � I �]' and row reduction 

produces 

that the system 

1 a 
has a (unique) 

31b]� 1[- 1  31 b ] 
31 b ] R1-3R, [- 1  O I (b - 3a)/7] 
1 (a + 2b)/7 � 

+ 2b 
0  7  a 
solution. 
(Why?) If we con­

0 1 (a + 2b)/7 

bR1 [- 1  
---'-----+ 0 

at which point it is clear 
tinue, 

we obtain 

from which we see that x = (3a -b)/7 and y 
a and b, we have 

= (a+ 2b)/7. Thus, for any choice 

of 

�  (Check this.) 

Remark It is also true 

that  IR2 = span( [ _ � l [ � l [ �]} If, given [: l we can 

findx andysuchthatx[ _�] + y[�] 
0 [ �] = [:]. In fact, any set 

= [:lthenwealsohavex[ _�] + y[�]+ 

that contains a spanning 

of vectors 

2 will also be 

set for IR

a spanning set 

for IR2 (see Exercise 20). 
many times. 

versions 

of this example 

The next example 

is an important (easy) case of a spanning 

set. We will encounter 

Let e,, e,. and e; bethestandMd 

unit vedocs 

[;] � x[�] + ym + zm � xe, + ye, +  ze., 

in HI'. Then I°' any mtn< [; J we haw 
!Rn= span(e1, e2, .•. , en4 

seeing that, 

in general, 

have no difficulty 

Thus, IR3 = span(e1, e2, e3). 

You should 

When the 

description of 

span of a set of vectors 
the vectors' 
span. 

in !Rn is not all of !Rn, it is reasona

ble to ask for a 

Example 2 . 2 0  

Example 2 . 2 1  

9 2   Chapter 2 Systems 

of Linear 

Equations 

z 

Figure 2 . 9  
Two nonparallel 
plane 

vectors 
span a 

plane 

of all linear 

geometric
ally, 

Solulion Thinking 

combinations 

we can see that the set 

m and [ J i'iu't the 

thrnugh the migm wiili m and [ �:] OS dimti on 
� s m + { ] 
way of "ying that [� l ism th,,pan of m and [ J 

of this pfane is [ � l 

( Figme 2. 9). The vectm equotion 

Suppose we want to obtain 

which i*t anothe; 

There are several 

of this plane. 

the general 

equation 

vecto" 

of 

One is to 

ways to proceed. 
satisfied 
Substi

tution then leads to 

Another 

method 

a system 
is to use the 

of equations 

system 

of equations 

ax + by + cz = 0 must be 
vectors. 
in a, b, and c. (See Exercise 
17.) 
equation: 

by the direction 

arising 

from the 

vector 

by the points (1, 0, 3) and ( - 1, 1, -3) determined 

use the fact that the equation 

s - t = x 

t =y 
3s -3t = z 
the augmented 

If we row reduce 

matrix, we obtain 

N�� we know that this ry'1em 
[ _ �] by ossumption. 

is consistent, 

of [ �] and 
So we must have z -3x � 0 (o< 3x -z � 0, in mo<e standa<d 

since [;] is in the span 

form), giving 

us the gene

ral equation 
we seek. 

Remark A normal vector 
product 

to the plane in this example 

is also 

given by the 

cross 

e 

Linear 

Independenc

ill E"mple 218, wdound that 3 m + 2 [ �: l � m Let's 

tion as 3u + 2v = w. The vector 
linear 

combination 

of them. We say that a set 

w "depends" on u and v in the sense 

that it is a 

of vectors 

is linearly dependent 
if one 

abb<e�ate 

thi"qua­

Sets and Linear 

Independence 

93 

tion of the others. Note that we also have 
of which vector 

the question 

to 

is stated 

the formal definition 
as follows: 

v1, v2, .•. , vk is linearly dependent 

if there 
are 

Remarks 

A set 

rest, 

express 

of the 

combina

Definition 

as a linear 

of vectors 

in terms 

one of which 

is not zero, such that 

A set of vectors 

of them can be written 

that is not linearly dependent is called 

Section 2.3 Spanning 
u = - �v + tw and v = - �u + !w. To get around 
scalars c1, c2, .•• , ck, at least 
•  In the definition 
c1, c2, ..• , ck must be nonzero 
so [ ! l [ � l and [ �] are linear
•  Since Ov1 + Ov2 + . . .  + Ovk = 0 for any vectors 
+ c2v2 + · · · + ckvk = 0 only if c1 = 0, c2 = 0, . . .  , ck = 0. 

scalars 
u, v, and w are linear
In the 
in fact, all of the scalars are nonzero. 

three 
scalars 1, - 2, and 0 is nonzero. 
from the fact that the first two vectors 

pendence 
combination 
can be expressed 

ofv1, v2, . . .  , vk· Thus, linear 

says that the zero vector 

dependence, 

combination 

ly dependent, 

essentially 

of linear 

as a linear 

example 

above, 

allows 

linearly independent. 

that at least 

the requirement 

one of the 
for the possibility that some may be zero. 
ly dependent, 

since 3u + 2v - w = 0 and, 

On the other hand, 

since 

at least 
(Note that the actual 
are multiples.) (See Exercise 

one (in fact, two) of the 
dependence 
arises 

simply 

44.) 

can be expressed 

ofv1, v2, ••. , vk only in the trivial way: c1v1 

linear 
as a nontrivial 

v1, v2, . . .  , vk, 
linear 
means that the zero vector 

independence 

de­

Theorem 2 . 5  

ent! 

is given 

nition 

if at least 

as a linear 

and the formal defi­

The relationshi

can be expressed 

p between the intuitive 
notion 

of dependence 
Happily, the two notions 
are equival

in the next theorem. 

Vectors 
vectors 

if and only 
of the others. 

ly dependent 
combination 

v1, v2, •.. , vm in !Rn are linear
scalars c2, .•• , cm such that v1 = c2v2 + . . .  + cmvm. Rearranging, 
v1 - c2v2 -· · · -cmvm = 0, which implies 
Conversely, suppose that v1, v2, •.. , vm are linear
such that c1v1 + c2v2 + . . .  + cmvm = 0. Suppose 

Proof If one of the vectors-sa
there are 

one of the scalars (namely, the coefficient 

that v1, v2, . . .  , vm are linear

scalars c1, c2, . . .  , cm, not all zero, 
c1-=!= 0. Then 

y, v1 -is a linear 

we obtain 

combina

one of 
the 

at least 

since 

tion of the others, then 

ly dependent, 

1 of v1) is nonzero. 
ly dependent. Then there 

are 

and we may multiply 
other 

vectors: 

both sides 

by 1/ c1 to obtain 

v = -(�)v -· · · -(cm)v 
C1 m 

1  C1 2 

v1 as a linear 

combination 
of the 

9 4   Chapter 2 Systems 

of Linear 

Equations 

Note It may appear as if we are cheating a bit in 

combination 

of the other 
for some other 

is analogous 
things 

we can just relabel 

this proof. After all, we cannot 
vectors, 
vector 

V; or for a different 

nor that c1 is nonzero. 

scalar 
so that they work out as in the above proof. 
loss of gen­

"without 

a mathematicia
that v1 is a linear 

n might begin by saying, 
combination 

be sure that v1 is a linear 
However, 
the argument 
cj. Alternatively, 
In a situation like this, 
erality, we may assume 
proceed 

as above. 

of the other 

vectors" and then 

Example 2 .2 2  

vectors 

containing 

IJ�r, then we can find a 

the zero 
nontrivial 

Any set of 
are in 
Cm Vm = 0 by setting 

c1 = 1 and c2 = c3 = . . .  = Cm = 0. 

vector 

is linearly dependent. For if 0, v2, ..• , v m 

combination 

of the form 

c10 + c2v2 + . . .  + 

Example 2 . 2 3  

Determine 

whether 

the following 

sets of vectors 

are linearly indepen
dent: 

(a) [�]and[-�] 

(c) [-i]. [ Jand nl 
�  the other. 

Solution In answering 
determine 
little 
thought 
(a) The only way two vectors 

any question 
whether 
may save a lot of 

(Why?) These two vectors 

by inspection 

independent. 
(b) There is 
no obvious 
such that 

of this type, it is a good idea to see if you can 
. A 

combination 

is a linear 

of the others

one vector 

computation! 
can be linearly dependent is if 

are clearly 

not multiples, 

one is a multiple 
so they are 
linear

of 
ly 

dependence 

relation 
here, 

so we try to 

find scalars 
c1, c2, c3 

system 

linear 

The corresponding 
is 
C1 + 
C1 + Cz 

C3 = 0 
= O  

and the 

augmented 

matrix is [i 0 

1  0 
fundamental 
in ques
tion! 

1  OJ 0  0 

observation 

Once again, 
matrix are just the vectors 

we make the 

that the 

columns 

of the coefficient 

are linearly independent. 

Independence 

95 

that 

1 
0 

Sets and Linear 

reflection 

reveals 

( c) A little 

row echelon 
form is 

The reduced 

Section 2.3 Spanning 
0 OJ 
[i 0 
�  (check this), so c1 = 0, c2 = 0, c3 = 0. Thus, the given 
�  so the 

are linearly dependent. 
ally.] 

0  0 
1  0 

vectors 
check this algebraic
we observe no obvious 
( d) Once again, 
a homogeneous 
linear 
vectors: 

[� � : �1 �I[� 

If we let the scalars be c1, c2, and c3, we have 

0 - 1  2  0  0 

matrix has as 

1  0 l �: � �: [ 1  0 
2  0 _ R,  0  1 

whose augmented 

dependence, 

vectors 

- 1  
- 1  

[Set up a 

linear 

system 

three 

2  0 

1 

system as in 

part (b) to 

so we proceed 

directly to reduce 
its columns the given 

-� �1 0  0 

0  0 

c1 +  3c3 = 0 
c2 -2c3 = 0 

from which we see that the 
must be a nonzero solution, 
If we continue, 
we can 

has infinitely 

system 
so the given 
describe these 
of c3, we have the linear 

many solutions. In particu
are linearly dependent. 
exactly: 
dependence 

c1 = -3c3 and c2 = 2c3. 
relation 

vectors 
solutions 

Thus, for any 

nonzero value 

lar, there 

�  (Once again, 

check that 

this is correct.) 

as a theorem. 

solution. 

if and 

system 

linear 

for linear 

independence 

We summarize 

this procedure for testing 

arly 
with augmented 
matrix 

in IK£n and let A be the n X m  matrix 
vectors as its columns. Then v1, v2, .•• , vm are line
[v1 v2 · · · vml with these 
Let v1, v2, . . .  , vm be (column) vectors 
only if the homogeneous 
dependent 
[A I O] has a nontrivial 
P r o o f  v1, v2, ..• , vm are linear
c1, c2, ••. , cm, 
to sa'.i�g that the nonzero vector [ :.: ] is a solution of the system 
matnx is [v1 v2 .•. vm I OJ.  · 

c1v1 + c2v2 + . . .  + cmvm = 0. By Theorem 2.4, this is 

ly dependent 

such that 

are scalars 

only if there 

not all zero, 

if and 

cm 

equivalent 

whose augmented 

Theorem 2 . 6  

9 6   Chapter 2 Systems 

of Linear 

Equations 

Example 2 . 2 4  

The standard 
tem with augmented 

unit vectors 

e 1 ,  e2, and e3 are linearly indepen

matrix [e1 e2 e3 I OJ is already in the reduced 

row echelon 

sys­
form 

dent in IR3, since the 

0  OJ 0  0 
[: 0 

1 
0 
trivial 

and so 
linearly independent in 

clearly 
!Rn. 

has only the 

solution. 

1  0 
In general, we see that e1, e2, . . .  , en will be 

elementary 

Performing 
of the rows. We can use this fact to come up with another 
independence. 

on a matrix constructs linear 
for 

tions 
linear 

row operations 

combina­

way to test vectors 

Example 2 .2 5  

Consider 

the three 

vectors 

of Example 

2.23(d) as row vectors: 

We construct 
lon form. Each time a row changes, we denote 

1 
4 

it to 

the new 

to reduce 

vectors 

row by adding 

a matrix with these 

eche­
a prime symbol: 

as its rows and proceed 

[1,2, 0], (1,1,- 1] ,  and [1,4,2] 

OJ R;=R;+ZR; [ l 
-�J 
[: 2 
- 3(1,2, 0] + 2 [ 1, 1, - 1 ]  + (1,4, 2] =  [ O,O,O ]  

-1 �  0 

2 
-1 
0 

2 
-1 
2 

vectors, 

0 

2 

c3 = 1 in the solution to 

corresponds 

to taking 

or, in terms of the original 

From this we see that 

[Notice that this approach 
Example 

2.23(d
).] 

Theorem 2 . 1  

Thus, the rows of a matrix 

will be linear

ly depende

nt if elementary 

row opera­

tions 

can be used to create a zero row. We summarize 

this finding 

as follows: 

A bet he m X n motrix [J with 

ly dependent if and only if 

vectors 

Let v,, v , 

. . . . .  

rank(A) < m. 

these 

v m be ( rn

w) ve<to" 

in �" and kt 

as its rows. Then v1, v2, •.• , vm are linear
that v1,  v2, ••. , vm are linear

Proof Assume 
at least 

one of the vectors 

can be written 

ly dependent. 
as a linear 

Then, by Theorem 2.2, 
tion of the others. 

combina

Independence 

Sets and Linear 

the vectors, 

Section 2.3 Spanning 
if necessary, so that we can write vm = c1v1 + c2v2 + · · · + 
row operations Rm - c1R1,  Rm - c2R2, •
•
 , 
•
v1, v2, •.. , vm. Thus, v1, v2, .•. , vm are linearly dependent. 

We relabel 
cm-iYm-l· Then the elementary 
Rm - cm-iRm-I applied to A will create a zero row in row 
tions 
tution 
used in Example 

m. Thus, rank(A) < m. 
to that 
argument 
linear 
of 

Conversely, assume 
that will create 

that rank(A) < m. Then there 

used to show that 

0 is a nontrivial 

2.25 can be 

a zero row. A 

is some sequence of row opera­
analogous 

successi

ve substi

91 

combination 

In some situations, 

we can deduce 

any work. One such situation 
2.22). Another is 
theorem 

is when the zero vector 
"too many" vectors 
(We will see a sharper 

when there are 

summarizes 

this case. 

is linearly dependent with­
is in the set (as in 
The 
of this result 

version 

that a set of 

vectors 

out doing 
Example 
following 
in Chapter 6.) 

to be independent. 

Any set of 

m vectors 

in !Rn is linear

ly dependent 
if m > n. 

Proof Let v1, v2, .•• , vm be (column) vectors 
[v1 v2 .•. vml with these 

in !Rn and let A be then X m matrix 
v1, v2, •.. , vm are lin­

vectors 
only if the homogeneous 
linear 

as its columns. By Theorem 2.6, 
But, according to Theorem 2.6, 
since 

than rows; it is the case here, 

with augmented 

matrix 
this will always 
be the 
number of columns 

system 

dependent if and 

early 
[A I O] has a 
case if A has more columns 
is greater 
of rows n. 

than number 

nontrivial solution. 

m 

The vectors [ �], [ ! ] , and [ �] are linear

ly dependent, since 
in IR2• (Note that if we want to find the 
than two linearly independent vectors 
tual dependence 
three 
system 

among these 
matrix has the given 

vectors as columns. Do this!) 

whose coefficient 

we must solve 

relation 

vectors, 

cannot 
be more 

there 

ac­

the homogeneous 

Theorem 2 . 8  

Example 2 . 2 6  

...  I Exercises 

2 . 3  

In Exercises 
nation 
mainin

if the 
g vectors. 

1-6, determine 

of the re

vector 

v is a linear 

combi­

5. v � m. u, � m. u, �  [: J 
u,� m [ 3.2] [ 1.0] [ 3.4] 
GAS 6. V =  2.0 , U1 = 0.4 , ll2 =  1.4 , 
-2.6  4.8  - 6.4 
U3 = [-�:�] - 1.0 

9 8   Chapter 2 Systems 

of Linear 

Equations 

In Exercises 
of the columns of the matrix A. 

7 and 8, determine if the vector 

b is in the span 

(b) In part (a), suppose in addition 
of u1, . . .  , uk. Prove that 

a linear 
span(u1, . . .  , uk) = span(v1, . . .  , vm). 

combination 

that each v

j is also 

the span of the gi

9 10 1 1   12 

In Exercises 

( c )  Use the result of part (b) to prove that 

2.6 to deter­

[Hint: We know that IR3 = span(e1, e2, e3).] 

22-31 are linearly in­

2.23 and Theorem 

13-16, describe 

s in Exercises 
hout calculation), state why. For any 

the answer can be 
determine
d 
, find a dependence 
relation­

Use the method of Example 
mine if the sets of vector
. If, for any of these, 
dependent
by inspection 
(i.e., wit
sets that are linearly dependent
ship among the vectors. 

7. A = [� !lb =  [:J 
8. A = [ � ! �], b  = [ : l 
9. Show that IR2 =span([�], [ _�]). 
10. Show that IR2 = span( [ _ � l [ �]). 
1 1 .Showthot�' � 'P""([H [J [:]) 
23. [:]. m. [-�] 
12. Show thot �' � 'P""( [J m. [ _:]) 
24• m. [!J Hl 25 [:H�lE 
::r!fFrd (b)alg,bm::�fa [:J  26. [-;i. [-!J [:J m 27. [;]. [H [�: 
15. m. ui 16. [ _�irn [-:i 28r�H�J 
u1 
r�� �rt���1�f���,�1;���:�r�;::!n�f��: [ l l [-11 [ l l [ o l 
17. ;�i:�;�;,
20 �a;;:�vethatifu1, . . .  , umarevectors
30 [�l [�l [�l [:l 
(b) Deduce that if [Rn = span(S), then [Rn = span(T)  [ 3 
, -3� 
l [-l 1 [ 1 l [-l 1 

, �
�
�
31. -
, 
combination 
-
-
32-41, determine if the sets of vector

cz = 0. Solve for 
a, b, 
18. Prove that u, v, and w are all in span(u, v, w). 
19. Prove that 
u, v, and w are all in span(u, u + v, u + 

also. 
Suppose that vector 
of vectors 
combination 
a linear 
combination 
span(u1, . . .  , uk) � span(v1, . . .  , vm). 

U1, U2, . . .  , Uk ,  n = U1, . . .  , Uk, Uk+!> . . .  , 
{ 
um}, then span(S) � span(T). [Hint: 
question 

·0
Rephrase 
this 
1
combinations

In Exercises 
given 
exercise 
are linear

0 1 
, - 1  
, 1 
1 - 1  1 

u1, . . .  , uk and that each u; is a linear 

ofv1, . . .  , vm and therefore 

by converting the 

v1, . . .  , vm. Prove that w is 

ly independent 

form ax + by + 

and c.  29 

in terms oflinear 

- 1  1 
, 0 

ven vector

. 1 
0 

w is a linear 

of vectors 

s in the 

} a dT { 

in!Rn, S= 

21. (a) 

'2
1

'2
1

l 
l 

.] 

'2

s 

l 

. 

1 

Section 2.4 Applications 

9 9  

s to row vector

vector
and Theorem 
find a dependence 
32. Exercise 
34. Exercise 
36. Exercise 
38. Exercise 
40. Exercise 
42. (a) If the columns 

s and using the method of Example 
2.25 
2.7. For any sets that are linearly dependent
, 
vectors. 
relationshi
23 
25 
27 
29 
31 

p among the 
33. Exercise 
35. Exercise 
37. Exercise 
39. Exercise 
41. Exercise 

22 
24 
26 
28 
30 

of an nX n matrix A are linearly in­
in !Rn, what is the rank of A? 

dependent as vectors 
Explain

. 

(b) If the rows of an n X n 
vectors 

matrix A are linear
in !Rn, what is the rank of A? 

ly inde­

pendent as 
Explain

. 

(b) If vectors 

u, v, and w are linear

ly independent, 

will 

u -v, v - w, and u - w also be 
dent? 

Justify your answer. 
are linear

44. Prove that two vectors 

linearly indepen­

and only if one is a scalar multiple 
[Hint: Separately 
the case where one of the 
vectors 
is O.] 

consider 

ly dependent if 
of the other. 

proof" 

of Theorem 2.8. 

45. Give a "row vector 
46. Prove that every subset of a linearly independent set is 

47. Suppose that S = {v1, .•. , vk> v} is a set of 

ly independent. 

vectors 

linear

in 

some !Rn and that v is a linear 
vk. If S' = {v1, . . .  , vd, prove 
[Hint: Exercise 
2 l (b) is helpful 
here.] 

combina
that span(S) = span(S'). 

tion of v1, . . .  , 

48. Let 

tors in !Rn, and let 
v = c1v1 + c2v2 + · · · 

{v1, . . .  , vd be a linearly independent 
+ ck vk with c1 * 0. Prove that 

{v, v2, .•. , vd is linearly independent. 

v be a vector in !Rn. Suppose that 

set of vec­

43. (a) If vectors 

u, v, and w

are linear

ly independent, 

will 

u + v, v + w, and u + w also be linearly indepen 
dent? 

y your answer. 

Justif

-

11 f Applications 

There are too many applications 
single 
settings 

section. 
in which they arise. 

This section 

of systems 

oflinear 
a few applications, 

do them justice 
to illustrate 
the diverse 

equations to 
in a 

will introduce 

o f  Resources 

Allocation 
A great many applications 
a set of constraints. 
resources 

subject to 

of systems 

of linear 

equations 

involve 

allocating limited 

Example 2 . 2 1  

has placed 

A biologist 
three 
where they will feed on three 
of A, 800 units 
rium consumes 
How many bacteria 
food? 

strains 
different 

of bacteria 
food sources 
of Care placed 

of B, and 1500 units 
a certain number of units of 

(A, B, and C). Each day 2300 
units 
in the test tube, and each bacte­
each food per day, as shown in Table 2.2. 

I, II, and 

III) in a test tube, 

(denoted 

of each strain 

can coexist in the test tube and consume 

all of the 

Table 2 . 2  

Food A 
FoodB 
FoodC 

I 

2 
1 
1 

II 

2 
2 
3 

III 

4 
0 

Bacteria Bacteria Bacteria 
Strain 

Strain 

Strain 

1 0 0   Chapter 2 Systems 

of Linear 

Equations 

Solulion Let x1,  x2, and x3 b e  the numbers of bacteria 
respecti
vely. 
strain I consumes 
total of 2x2 and 4x3 units 
of A, we have the equation 

I consumes 
strains 
we want to use up 

Since each 
a total of 

I ,  II, and III, 
of A per day, 
2 units 
II and 
III consume 
all of the 2300 units 

of the x1 bacteria 

of food A daily. Since 

per day. Similarly, 

of strains 

of strain 

2x1 units 

a 

Likewise, we obtain 

equations 

consumption 

of B and C: 

of three 

2x1 + 2x2 + 4x3 = 2300 
correspond
ing to the 
X1 + 2X2  = 800 
X1 + 3X2 + X3 = 1500 
linear 
equations 
matrix 

4 2300] [l 
gives [: 2 
0 0 100] 0 350 

0 800 �  0 

1 1500  0 

in three 

1 350 

2 
3 

0 

we have a system 

Thus, 
the corresponding 

augmented 

variables. Row reduction 

of 

x1 = 100, x2 = 350, and x3 = 350. The biologist 
should 
I and 350 of each of strains 

II and III in the test tube if she wants all the food 

place 

100 bacteria 

Therefore, 
of strain 
to be consumed. 

Example 2 . 2 8  

2.27, 

Repeat Example 
shown in Table 2.3. Assume 
units 

of C are placed 

using the data on daily 
this time 

in the test tube each day. 

that 1500 units 

consumption 

of food (units per day) 
ofB, and 4500 

of A, 3000 units 

I 

Table 2 . 3  

Food A 
FoodB 
FoodC 

Bacteria 
Strain 

Bacteria 
Strain 

Bacteria 
Strain 

2 
3 

1 
3 
5 

III 
II 
- 1  OJ 2 1500 

0  0 
given by 

numbers of bacteria 

of each type. The aug­
and the corresponding reduced 

echelon 

matrix 

system 

linear 

3 again be the 
lting 

Solulion Let x1, x2, and x
for the resu
mented 

form are  [: 1 

� ����i �  [� � 

We see that in this case we have more than one 
solution, 

5 4500  0  0 

2 
3 

X1  - X3 =  0 
X2 + 2X3 = 1500 

Letting 
we must be careful to interpret solutions 

x3 = t, we obtain 

x1 = t, x2 = 1500 -2t, and x3 = t. In any applied problem, 

properly. Certainly the number of bacteria 

Section 2.4 Applications 

1 0 1  

t 2: 0 and 1500 - 2t 2: 0. The 

Therefore, 

cannot be negative. 
that t :::::: 
whole number, so there 
751 solutions 
are of the form 

750, so we have 0:::::: t :::::: 

are exactly 

750. Presuma
751 values 

latter 

inequality 

implies 

bly the number of bacteria 
must be a 

oft that satisfy 

the inequality. Thus, 
our 

many.) .+ 

one for each integer 
this system has 

value oft such that 0 :::::: t 

:::::: 750. (So, 
although 

mathematic

ally 

infinitely 

many solutions, 

physically there 

are only finitely 

(the products). A balanced chemical equation 

certain molecules 

Equations 
Balancing  Chemical 
When a chemical reaction 
occurs, 
new molecules 
that gives 
the relative 
same number of atoms of 
usually written with 
in between to show 
For example, 

nts on the 
the reacta
left, 
the direction 
of the 
reaction. 
in which hydrogen 

each type on the 

numbers of reacta

nts and 

to form 
(the reactants) combine 
is an algebraic 
equation 
and has the 
products in the reaction 
The equation is 

left-and right-hand sides. 

the products on the 

gas (H2) and oxygen 

and an arrow 

right, 

(02) com­

bine to 

form water 

for the reaction 
(H20), a balanced 

is 

l equation 

chemica
2H2 + 02 -----+ 2H20 
of hydrogen 
Observe that the 
atoms on each side. 

combine 

atoms and two oxygen 

ting that two molecules 

of water. 

indica
form two molecules 
four hydrogen 
be a unique 
a balanced 
balanced. 
reaction

. 

equation for a reaction, 

balanced 
equation will also be balanced. 
Therefore, 

we usually 

since 

For example, 

with one molecule 
since 
Note that 
there 
any positive integer 

to 
of oxygen 
equation is balanced, 
there 
are 
will never 
multiple 
6H2 + 302-----+ 6H20 is also 
balanced 

equation for a given 

of 

look for the simplest 
the process 
work in simple examples, 
system 
oflinear 
solving a homogeneous 
the guesswork. 

to remove 

of balancing 
equations, 

and error 

While trial 
really 
chemical 
so we can use the techn
iques 

equations 

will often 
involves 

we have developed 

(NH3) in oxygen 

Example 2 . 2 9  

The combustion 
Find a balanced chemical 

of ammonia 

equation for this reaction. 

produces nitrogen 

(N2) and water. 

Solution If we denote 
water 

by w, x, y, and z, respect

the numbers of molecules 

oxygen, 
then we are seeking an equation 
of the form 

of ammonia, 

ively, 

nitrogen, 

and 

Comparing 
and products, 

we obtain 

the numbers of nitrogen, 

and oxygen 

atoms in the reacta
nts 

wNH3 + x02 -----+ yN2 + zH20 

hydrogen, 
three linear 
equations: 
w = 2y 
Nitrogen: 
Hydrogen: 
3w = 2z 
Oxygen: 2x = z 
in standard 

form gives 

Rewriting 
these 
linear 
equations 

equations 
in four variables. [Notice 

us a homogeneous 
system 

of three 

that Theorem 2.3 guarantees 
that such a 

1 0 2   Chapter 2 Systems 

of Linear 

Equations 

tion. 

w 
3w 

matrix 

the corresponding 

by Gauss-Jordan elimina

system 
augmented 

will have (infinitely many) nontrivial solutions.] We reduce 

- 2z = 0-----+ 3  0 
-2y = 0  [ 1  0 
2x - z=O 0 2  

0 -� �]-----+[� � 

0 
0 
- 1  0  0  0 
1 

-t 0] -! 0 -t 0 

positive value of z that will produce 

for all four variables is the least 
the 

w = 4, x = 3, y = 2, and z = 6. Therefore, 

Thus, w = tz, x = !z, and y 
= tz. The 
t, !, and t-namely, 6-which gives 

integer values 
balanced 

common denomina

chemical 

smallest 

equation is 

-2 
0 

tor of the fractions 

j20 
f +2 
i30 
f1 + f2 = 50 

Figure 2 . 1 0  
Flow at a 

node: 

Analysis 

Network 
situations 
Many practical 
cations 
, and economic 
networks
the possible flows through networks
of roads, 
through 

network. 

information flows through a data network, 
an economic 

networks

, to name 
. For example, 

a few. Of particular 
vehicles flow 
and goods and services 

through 

flow 

give rise to networks

: transportation 

networks

, communi­
interest are 
a network 

For us, a network will consist 

connected 

or vertices) 
branch 
that can 
traveling 
through a network 

will be labeled 
flow along 
along a network 

of a finite 

number of nodes (also called junctions 
by a series of directed 
edges 
with a flow that represents 

known as branches 
the amount 

or arcs. 
Each 

in the indicated 
.) The funda

of some commodity 
of cars 
mental rule governing flow 

direction. 
(Think 

or through 

that branch 

of one-way streets
is conservation 

of flow: 

At each node, 

the flow in equals 

the flow out. 

with two branches 
a node and two 

2.10 shows a portion 
The conservation 

of a network, 
of flow rule implies 

Figure 
leaving. 
units, must match the 
20 + 30 units. Thus, we have the linear 
ing to this 
node. 
equation 
an entire 
of linear 

network 
the resulting system 
equations. 

f1 + f2 = 50 correspond
the flow through 

entering 
incoming 
flow, 
f1 + f2 

total outgoing 
flow, 

by constructing 

We can analyze 

that the total 

and solving 

such equa­

tions 

Example 2 .3 0  

through 

the network 

of water pipes 

shown in Figure 2.1 1 ,  

flows 

the possible 

Describe 
where flow is measured in liters 
Solution At each node, 
of flow there. 
constant on the right, 

We then rewrite 

per minute. 

we write out 

the equation 

that represents 
the variables 
on the 

the conservation 

left and the 

form. 

system 

in standard 

each equation with 

to get a linear 
15 = !1 + 14 
!1 = !2 + 10 
!2 + f3 + 5 = 30 

f4 + 20 = f3 -----+ !1  + f4 = 15 

= 10 
!2 + f3  = 25 
f, -f4 = 20 

!1 -!2 

Node A: 
NodeB: 
Node C: 
Node D: 

1 0 3  

h 

Section 2.4 Applications 
B  10 -
1d 
5 +--
30! 
1 15] 1 5 

5! jQ. A  !1 -
J4! 20 -
1 15] [1 0 10  0 
[i 0  0 

f4, so we have infinitely 
in terms of f4, we obtain 

0 25  0 
- 1  20  0 

elimination, 
we reduce 

the augmented 

variable, 

1 
0  1 

many 
Settingf4 = t and expressing the leading variables 

0 
0 
1 
0 

matrix: 

one  free 

0  0 

- 1  20 

0 
0 

- 1  0 

� 

Figure 2 . 1 1  

0 

c 

D 

f, = 15 -t 
!2 = 5  -t 
f3 = 20 + t 
!4 = 
flows and allow 
describe 

Using Gauss-Jordan 

.......,.. (Check this.) We see that there is 

solutions. 

These equations 
all possible 
example, we see that if we control 
the other 

are f1 = 1 O,f2 = 0, and f3 = 25. 

flows 

We can do even better: We can find the 
Each of the flows must be 
in turn, 

on each branch. 
ond equations 
(otherwis
than the first, 
t, so we have deduced 
on our 
parameter 
the four equations, 
we see that 

we see that t :s 15 (otherwise 
negative). The second 
so we must use it. The third 

ef2 would be 

us to analyze 

the network. 

For 
the flow on branch AD so that t = 5 L/min, 
then 
and maximum possible 
flows 
sec­
tive. Examining 
and t :s 5 
f1 would be negative) 
inequalities 
ve 
restrictions 

minimum 
nonnega

the first and 

is more restricti

no further 

of these 

butes 

equation contri
that 0 :s t :s 5. Combining 

this result 
with 

10 :s f, :s 15 
0:Sf2:S5 
20  :s f3 :s 25 
0:S.fi:S5 
of the possible flows through 
description 

this network. 4 

We now have a complete 

1 0 4   Chapter 2 Systems 

of Linear 

Equations 

Networks 
networks 

Electrical 
Electrical 
such as batteries, 
power sources, 
bulbs or motors. A power source 
network, 
amount 

where it encounters 
of force be applied 

are a specialized 

type of network 

providing 

and devices 
"forces" 

powered by these 

a current 

of electrons 

such as light 

information about 
sources, 
to flow through 
that a certain 

the 

The fundamenta

force E is needed 

to drive 

various 

resistors, 

each of which requires 

in order 
l law of electri

for the current to flow through it. 
city is Ohm's law, which states 
R. 

how much 
with resistance 

a resistor 

exactly 

a current I through 

O h m 's Law 

force = resistance 

X current 

or 

E = RI 

Force is measured 
short). Thus, in terms of these 
it tells us 
how much voltage 
is used up. 

in volts, 
what the "voltage 

resistance 

in ohms, and current in 

units, Ohm's law becomes 
drop" is when a current passes 

(or amps, for 
amperes 
= ohms X amps;' and 
t is, 

"volts 
through 

a resistor-tha

of the positive terminal 

of a battery 

flows out 

terminal, 

around 
traveling 
of an electrical 
network, 

Current 
negative 
a diagram 
positive terminal is 
the longer 
The following 
works. The first is a "conservation of flow" 
of voltage" 

law around 

two laws

each circuit. 

one or more closed circuits 
batteries 
bar) and resistors 

are  represented 

and flows back into the 

by �I-(where the 

process. 

in the 

In 

vertical 

are represented 
, whose discovery we owe to Kirchhoff
, govern 
the second 

by -'VV'v- . 
electrical 
net­
is a "balancing 

law at each node; 

Kirchhoff's Laws 

flowing 

into any node is equa

l to the 

sum of the currents 

of the currents 
out of that node. 

Current Law (nodes) 
The sum 
flowing 
Voltage Law (circuits) 
The sum of the voltage 
the circuit (provided 

by the batteries

). 

drops around any cir
cuit is equal 

to the total voltage 

around 

Figure 2.12 illustrates 

Kirchhoff's laws. In part (a), the current 

(or 11 - 12 -13 = 0, as we will write it
Ohm's law to compute 
can set up a system 
an electrical 

the voltage 
equations 

network. 

of linear 

); part (b) gives 

drop 41 at the resistor

law gives 

11 = 12 + 13 
41 = 10, where we 
have used 
. Using Kirchhoff's laws, we 

that will 

allow 

us to 

determine 

the currents in 

Example 2 . 3 1  

Determine 

the currents 

11, 12, and 13 in the electrical 
network 
has two batteries 
12 flows across 

and four resistors. 

Current 11 flows 

through 

the middle branch AB, and current 13 

shown in Figure 2.13. 

Solution This network 
the top branch BCA, current 
flows 
the bottom branch 
BDA. 

through 
At node A, the current law gives 1

1 + 13 = 12, or 

(Observe that we get the 

11 - 12 + 13 = 0 
same equation at node B.) 

lY',, 
1� 

1 +---
1 +---
(b) 41 = 1 0  

4 ohms 

10 volts 

1 0 5  

Section 2.4 Applications 
+---
+---
11  c  11 
--+ 
--+ 
I ohm 
12 
lz 
+---
+---
4 ohms  h  D  h 

8 volts 

2 ohms 

2 ohms 

B 

A 

1 6  volts 

Figure 2 . 1 2  

Figure 2 . 1 3  

Next we apply the voltage 

law for each circuit. 

cuit CABC, the voltage 

drops at the resistors 

are 2Ii, I2, and 2Ii. Thus, we have the equation 

For the cir

4Ii + I2 = 8 
I2 + 4I3 = 16 

Similarly, 

for the 

circui

t DABD, we obtain 

we must treat the voltages 

is actually 

2Ii + 2Ii -4I3 = 8 -16 = - 8  or 4Ii -4I3 = - 8, which we observe 

a third circuit, CADBC, if we "go 

and resistances on 

the flow:' 
tive. 

against 

the "reversed" paths as nega

In this 

ts. Thus, we can 

that there 

(Notice 
case, 
Doing so gives 
is just the difference 
omit this equation, 
ing it does no harm.) 

We now have a system 

hand, includ­

variables: 

two circui
On the other 

tions 

of three 

in three 

of the voltage 
as it contributes 

equations 
for the other 
no new information. 

4I1 + I2  8 

linear equa

I1 -I2 + I3 =  0 
I2 + 4I3 = 16 
-1 1 ol  [00
1 o
1 l 
� 1! � 
o1 �1 43
[� 
are Ii = 1 amp, I2 = 4 amps, and I3 = 3 amps. 

produces 

Gauss-Jordan 

elimination 

Hence, 

the currents 

networks
Remark In some electrical 
even be negative. A negative 
value simply 
ing branch 

in the direction 

may have fractional 
values 
or may 
ond­
means that the current in 
the corresp
network 
. 
diagram

opposite that shown on the 

, the currents 

flows 

CAS 

Example 2 . 3 2  

shown in Figure 2.14 has a 

The network 
the curr
as a Wheatstone bridge circuit. 

ents I, Ii, . . .  , I5• This is 

an example 

single power source A and five resistors
. Find 
is known in electrical 

of what 

engineering 

1 0 6   Chapter 2 Systems 

of Linear 

Equations 

1ii 
14 --B 

2 ohms 

2 ohms 

c  l ohm 
13 ! 15 --
I ohm 
--1 
--1 

E  2 ohms 
A 

1 0  volts 

Figure 2 . 1 4  
A bridge 

circuit 

!Ii 

D 

Solulion Kirchhoff's current law gives  the 
nodes: 

following 

equations at the four 

For the 

three 

basic circuits, 

Node B: I - I1 - I4 =  0 
Node C: I1 - I2 - I3 =  0 
Node D: I - I2 -Is =  0 
Node E: I3 + I4 -Is =  0 
the voltage 

law gives 

Circuit ABEDA: I4 + 2Is = 10 
Circuit BCEB: 2I1 + 2I3 - I4 =  0 
Circuit 
CDEC: I2 -2Is -2I3 =  0 

drop; thus, 

there 
signs 

times 

branch 

of current 

and therefore 

the current:' 

We now have a system 

because we went "against 

change 
no problem, since 

DAB has no resistor 

determine 
of seven equations 

This poses 
the direction 

(Observe that 
no voltage 
is no I term in the equation for circuit ABEDA. Note also that we had to 
three 
flow.) 
will let the sign of the answer 
in six variab
1  0  0 0  0 0 7 
1 -1 0 0 -1 0 0 
0 1 0 0  0 0 3 
0 1 - 1  - 1  0 0 0 
- 1  0 �  0 0  0 1 0 0 - 1  
1 0 - 1  0 0 - 1  0 
0  0 1 0  0 0 4 
0 0 0 
0 0 0  0  2 10  0 0  0 0 1 0 4 
0 0  0 0  0 1 3 
0 2 0 2 - 1  0 0 
0  0 -2 0 - 2  0 
0 0  0  0  0  0 
0 

we 

les. Row reduction 
gives 

(Use your calculator 
Is =  3, I
the current 
the diagram

2 = I4 =  4, and I

or CAS to 
3 = - 1. The 

through branch CE is flowing 
. 

check this.) Thus, the solution (in amps) is I =  7, I1 = 

significance 

of the negative 

value here is that 

in the direction 

opposite that marked on 

� 

-+ 

Remark There is 
tery sends a current of 7 amps through 

only one power source in this example, so the 

the network. 

If we substi

single 
tute these 

10-volt bat-
values into 

Section 2.4 Applications 

101 

Ohm's law, E =RI, we get 
there 
the network. 

were a single 1?--ohm resistor

10 = 7R or R = 1?-. Thus, 

network behaves as if 
. This value is called the effective resista
of 

the entire 

nce (Reff) 

Economic Models 

linear 
An economy is a very complex system 
ous sectors of the economy and the goods and services 
Determining 
of production 
goals requires 
powerful tool in developing and analyzing 

prices and 
levels 
ted mathematical 

optimal 
sophistica

they produce 
subject to 

desired 
models. Linear 
algebra has proven 
to be a 
models. 
such economic 

with many interrela

tionships 

among the vari­
and consume. 

economic 

In this section, 
Leontief 

mist Wassily 
analysis, 
corporations, 

are now standard tools 

and entire 

countries 

We begin with a simple example. 

we introduce 

two models based on the work 

in the 1930s. His methods, 

often referred 

of Harvard 
to as input-output 

econo­

in mathematical economics 
for economic 

and are used by cities, 
and forecasting. 

planning 

Example 2 . 3 3  

ty, 
service, 
ty, we assume 
produces a single 
in a given year and that income (output) is gener­

that each industry 

or sectors: 

electrici

of three 

industries, 

consists 

industries, 

production. 

The economy of a region 
and oil 
For simplici
commodity (goods or services) 
ated from the sale of this commodity. Each industry 
other 
purchased from outside 
the region 
more, for each industry, we assume 
put equals 
that is in equilibrium. Table 2.4 summarizes 
consumed 

, in order to 
and no output 
that production 

including 
itself

by each industry. 

expenditur

income 

equals 

input, 

purchases 

commodities 

generate its output. No commodities 

is sold outside 
equals 
exactly 
e). In this sense, 
this is 

from the 
are 
the region. 
(out­
consumption 
a closed 
economy 

Further­

how much 

of each industry's output is 

Produced 

by (output) 

Service 1/4 
Electricity 1/4 
1/2 
Oil 

Service  Electricity Oil 
1/2 
1/4 
1/4 

1/3 
1/3 
1/3 

Table 2 . 4  

Consumed by 
(input) 

his Ph.D. 

was 
He 

He emigrated 

ofLenin­
from the 

Leontief (1906-1999) 

Wassily 
born in St. Petersburg, 
Russia. 
studied 
at the University 
grad and received 
University 
of Berlin. 
to the United 
States 
ing at Harvard 
University 
and later 
at New York Univers
Leontiefbegan 
task of 
monumental 
an 
input-output 
analysis 
States 
were published 
an early 
user of computers, 
needed to solve 
For his pio­
systems 
neering 
was awarded 
the Nobel Prize 

in 1931, teach­
ity. In 1932, 
in 1941. 
in 1973. 

in his models. 
work, Leontief 

of which 
He was also 
which he 

conducting 
of the United 

economy, the results 

in Economics 

compiling 
data for the 

From the first column 

of its own output, 
the service 
Notice 
industry 

electrici
industry's 
output. 
of each column 

that the  sum 
is consumed. 
Let Xi, x2, and x3 denote 
respecti
vely, 
tricity, and t x3 on oil. This 
ture is �Xi + .!. x2 + t x3. Since 

and oil 
to expenditure, 

the service industry 

3 

industries, 
the large-scale linear 

of the table, 
ty consumes 

we see that the 
another 

industry 
1I4 
industry 
1/4, and the oil 
uses 1/2 of 

service 

consumes 

The other two columns 

have similar 

interpreta
ting that all of the  output 

tions. 
of each 

is 1, indica

the annual 
in millions 

(income) 
. Since 

output 
of dollars

ty, 
of the service, electrici
consumption 
spends� Xi on its own commodity,.!. x2 on elec-
expendi-

corresponds 
3 
service industry's total 

annual 
the economy is in equilibrium

, the service industry

means that the 

's 

1 0 8   Chapter 2 Systems 

of Linear 

Equations 

each equation, 
.......... which we then solve. (Check this!) 

expenditure must 
equations; 
electri

the other 
city and oil 

industries. 

equal its annual 
two equations 

income x1. This gives 
are obtained 

the first of the following 

by analyzing 

the expenditures 

of the 

Service: 
Electrici
Oil: 

ty: 

equations, 

3 
3 
3 

of linear 

we obtain 

Rearranging 

a homogeneous 
system 

ix1 -�x2-ix3 = 0 
tx1 + l x2-� x3 = 0 

-�x1 + l x2+ tx3 = 0 ---7 n I  l "] [� 0 -: "] 
-4 0 
4 

3 
-3 -i 0 

x1 = t and x2 = � t. Thus, 
ty, and oil industries 

we see that the relative 
outputs 

2 l 0 � 
l 
3 

Setting x3 = t, we find that 
the service, electrici
for the 

economy to be in equilibrium

1 
0 0  0 

•  The last example 
•  Since 

Leontief closed 
illustrates 
what is commonly called the 
to income, 

we can also 

think of x1, x2, and x3 as the 

corresponds 
commodities. 

output 

model. 

of 

Remarks 

. 

prices of the three 
We now modify the model in 
is an external 
in which there 
are produced. 
Not surprisin

Example 
as well as an internal 
gly, this version 

is called 

2.33 to accommodate an open economy, one 

demand 

for the commo
that 

dities 

the Leontief open model. 

need to be in the ratios x1 : x2 : x3 = 4 : 3 : 4 

Example 2 . 3 4  

the three  industries 

2.33 but with consumption 

produced by the 

service industry

, 40% by the electri
service industr

y's output 

city industry

, and 

is consumed 
of output 

is an excess 

of this calculation 
is that there 

(income) 

given by 
, 20% are 
10% by the oil 
by this econ­

by the 

service industry

of Example 
of the commodities 

Consider 
Table 2.5. We see that, 
consumed 
industry. Thus, only 70% of the 
omy. The implication 
over input (expenditure) 
productive. Likewise, the oil industry 
is pro
productive. (This 
is reflected 
are less than 1 but the sum of the 
be applied 
to satisfy an external 

second 
demand. 

for the service industry. 

ductive but 

We say that the service industry 

is 
is non­
the electrici
columns 

ty industry 

in the fact that the sums of the first and third 

column 

is equal to 

1). The excess 

output 
may 

Table 2 . 5  

Produced 

by (output) 

Service 0.20 
Electricity 0.40 
Oil 

Service  Electricity Oil 
0.50  0.10 
0.20 
0.30  0.30 

0.10 

0.20 

Consumed by 
(input) 

Section 2.4 Applications 

1 0 9  

For example, 

suppose there is 

an annual 

for 10, 10, and 30 from the service, 
equating 
(internal 
put), we obtain 

the following 

expenditures 

equations: 
output internal 

external demand 
ity, and oil 

electric

industries, 
demand and external demand) 

of dollars) 
(in millions 
respecti
Then, 
vely. 
(out­
with income 

demand external 

demand 

CAS 

Rearranging, 

system 

linear 

Service Xi = 0.2X1 + 0.5X2 + O.lX3  + 10 
Electricity X2  = 0.4X1 + 0.2X2 + 0.2X3  + 10 
X3  = O.lx1 + 0.3x2 + 0.3x3  + 30 
Oil 

matrix: 

and augmented 

we obtain the following 

- 0. lx1 -0.3X2 + 0.7X3 = 30  -0.l -0.3 0.7 30 

0.8X1 -O.SX2 -O.lX3 = 10 [ 0 8  - 0.5 -0.1 10] 
- 0.4X1 + 0.8X2 -0.2X3 = 10 ---+  - 0.4 0.8 - 0.2 10 
[� 0 0 61.74 l 

1 0 63.04 
0  78.70 

Row reduction 
yields 

from which we see that the service, electrici
nual production 
and $78. 70 (million), respecti
vely, 
both the internal 

of $61. 7 4, $63.04, 
and external demand 

industries 
must have an an­
in order 

commodities. 

ty, and oil 

for their 

to meet 

We will revisit 
these 

models in 

Section 

3.7. 

in which we must consider 

a physical system that 

many outcomes. 
bulb from on to 

of the light 

For example, a light 

has only a 
by applying 
certain 
pro­
bulb can be 
vice versa. 

off and 

can be altered 

Games 

Linear 

states 

of states. 
each of which produces 
can change 

Finile 
There are many situations 
finite number 
cesses, 
on or off and a switch 
that arise 
Digital 
computer games 
feature 
switches 
a desired 
to analysis 
Problems 

to produce 
using modular 
involving 

Sometimes 
these 
finitely 
the state 
in computer 
puzzles 

systems 

outcome. 

arithmetic, 
this type of situation 

science 

are often of this type. 

More frivolousl
y, many 

in which a certain 

device 

The finiteness 
and often linear 
are often called 

must be manipulated 

over some ZP play a role. 

of such situations 
is perfectly 
systems 
finite linear 

by various 
suited 

games. 

Example 2 . 3 5  

light 

is controlled 

directly above it and 

A row of five lights 
of the 
the states 
off) of the 
the left and right. For example, if the first and 
third 
then pushing 
Ifwe next push switch 

of the system 
changes 
C, then the resu
lt is the state 

the state 
(on or 
tely adjacent to 
lights 
2.lS(a), 
are on, as in Figure 
to that shown in Figure 2.lS(b). 
shown in Figure 2.lS(c). 

by five switches. Each switch 
lights 

changes 
immedia

the state 

switch A 

1 1 0   Chapter 2 Systems 

of Linear 

Equations 

B  c  D  E 

A  B  c  D 

E 

(a) 

Figure 2 . 1 5  

(b) 

(c) 

'-" A  B  c 

D  IH E 

Suppose 

that initially 

all the lights 

and fifth lights will 

are off. Can we push the 
in 

be on? Can we push the switches 

in some order 

switches 

so that only the first light 

so that only the first, third, 
some order 
Solution The on/off nature 
and that we should 
a vector 

will be on? 
of this problem 

work with 22• Accordingl

in Z�, where 0 represents 

suggests 
that binary 
y, we represent 

notation 
the states 
by 
on. Thus, for example, 

will be helpful 
of the five lights 
the vector 

off and 1 represents 

corresponds 

to Figure 2.IS(b). 

We may also use vectors 
of a light, 

the state 

changes 
With this convention, 

in Z� to represent the action 
the correspond
of the five switches 

of each switch. If a switch 
ing component is a 1; otherwise, 

the actions 

it is 0. 

a=  0 ,  b =  1 ,  c =  1 ,  d = 

0  0 
1  0 

are given by 
0 
0 

, e = 0 

1 

0  0  1 
0  0 
depicted 
in Figure 

0 

esponds 
1 
0 

The situation 

2.IS(a) corr

to the initial 
state 

followed 

by 

0 

1 
0 
0 

s =  1 

0 
0 

a=  0 

1 

0 
0 

Section 2.4 Applications 

1 1 1  

It is the vector 

sum (in Z�) 

0 
1 
s+ a= 1 
0 
0 

.-

Observe that this result 

agrees 
Starting 
with any initial 
C, B. 

with Figure 2.15(b). 

A, C, D, A, 
in Z�, addition 

rresponds 
This co
is commutative, 
so we have 
s + a + c +  d  + a + c + b = s + 2a + b + 2c +  d 

to the vector sum s + a + c +  d + a + c + b. But 

configuration 

s, suppose we push the switches 

in the order 

= s + b + d  

where we have used the fact that 2 = 0 in z:'.2. Thus, 
by pushing 
need to push 
Hence, 
a target 

only Band D-and the order 
in this example, we do not 

So, to see if we can achieve 

does not matter. 
any switch 

we would achieve the same result 

correct

(Check that this is 
.) 
more than once. 
t starting 

are scalars x1, .•• , x5 in z:'.2 such that 

from an initial 

configuration

configuration 
s, we need to determine 
whether 
there 
+ x5e = t 
s + x1a + x2b + · · · 
we need to solve (if possible) 
the linear 

system 

equation 
x1a + x2b + · · · 

+ x5e = t - s = t  + s 

In other 
sponds 

words, 
to the vector 

over z:'.2 that corre­

In this case, 

s = 0 and our first 

target configura
tion is 

The augmented 

matrix of this system 

has the given 

vectors 

as columns: 

We reduce 

it over z:'.2 to obtain 

1 
0 
t = 1 
0 

1  0  0  0 1 
1  0  0 0 
0 1  0 1 
0  0 1 1  1 0 
0  0  0 

1 0  0 0 1 0 
0  0  0 1 
0 0 1 0  0 
0 0  0 1  1 1 
0  0  0 0  0 0 

1 1 2   Chapter 2 Systems 

of Linear 

Equations 

Thus, x5 is a free variabl
x5 = 0 and x5 = 1). Solving 

e. Hence, 

there 

are exactly 

for the other variables 

two solutions 
in terms of x5, we obtain 

(corresponding 

to 

So, when x5 = 0 and 

x5 = 1, we have the solutions 

X1  1 
X1  0 
X2  0 
X2 
X3  and X3  1 
X4  1  X4  0 
X5  0 
(Check that these 

both work.) 

X5 

respect

ively. 

_... 

Similar

ly, in the second case, we 

have 

X1 =  X5 
X2 = + X5 
X3 = 
X4 = + X5 

1 
0 
t =  0 
0 
0 

The augmented 

matrix reduces 

as follows: 

0 1  1 0 0 -----+  0  0 1 0  0 

0 0  0 1 
1 1  1 0  0 0 

1 0  0 0  0 
0  0 0 

0 0 1  0 
0 0  0  0 

0  0  0 1  1 
0 0  0 0  0 

showing that there 
of the lights 

is no solution in 

this case; 

that is, it 

is impossible to start 

with all 

off and turn only the first light 

on. 

Example 

2.35 shows the power of 
found out by trial 
that there 
push the 
fact that no switch 

and error 
would have been extremely 
need ever be pushed 

linear 
was no solution, 

more than once. 

switches 

tedious

algebra. Even though 

we might have 

checking all possible ways to 
. We might also have missed 
the 

Example 2 .3 6  

a row with only three 

Consider 
are three 
Below the lights 
lights 
particular 
to the next state, 
of the first two lights, 
the states 

in the 
switch 

order 
B all three 

switches, A, B, and C, each of which changes 
2.16. Switch 

shown in Figure 

lights, each of which can be off, light 

blue, 

lights, and switch 

C the last two 

or dark blue. 
the states 

of 

A changes 

Section 2.4 Applications 

1 1 3  

Dark blue  Light blue 

� 

Figure 2 . 1 6  

Figure 2 . 1 1  

j B  c 

lights. If all three 
so that the lights 

are initially 

lights 
are off, light 

off, is it possible to push 
in that 
and dark blue, 
order 

the switches 
(as in Figure 2.17)? 

blue, 

in some order 

Solution Whereas 
"1!..3• Accordingl

Example 

2.35 involved 
correspond to the vectors 

"1!..2, this one clearly 

y, the switches 

(is it clear?) involves 

in Zl. and th, final wnfigmation w' "' <riming 

fod, t � [ n ( Offo 0, light 

bl"' i' 1, 

and dark blue 

is 2.) 

We wish to find scalars Xi, x2, x3 in "1!..3 such that 

X1a + X2b + X3C = t 

(where X; represents 
gives 

rise to the 

augmented 

the number of times 

the ith switch 

matrix [ab c It], which reduces 

. This equation 

is pushed)
over "1!..3 as follows: 

Hence, 
switch 

there is 
A twice 

a unique 
and the other 

Xi = 2, x2 = 1, x3 = 1. In other 
(Check this.) 

two switches 

once each. 

solution: 

words, 

we must 
push 

I Exercises 
.. 
1. Suppose that, 

of R e s o urces 

Allocation 

2 . 4  

2.27, 

in Example 
of B, and 600 units 

of food A, 
of C are placed 

400 units 

600 units 
tube each day and the data on daily 
tion by the bacteria 
in Table 2.6. How many bacteria 
coexist in the test tube and consume 

(in units 

per day) are as shown 
can 

of each strain 
all of the food? 

food consump­

in the test 

Table 2 . 6  

II  Strain 
III 

Bacteria 
Strain 

Bacteria Bacteria 
Strain 

I 

Food A 
1 
2 
Food B 
FoodC  1 

2 
1 

0 
1 
2 

2. Suppose that in Example 

2.27, 

500 units 
the test tube each day 

of B, and 600 units 

and the data on daily 
food 

400 units 
of C are placed in 

of food A, 

consumption 
as shown in Table 2.7. How many bacteria 

per day) are 
of each 

by the bacteria 

(in units 

1 1 4   Chapter 2 Systems 

of Linear 

Equations 

Table 2 . 1  

III 
I  Strain 

II  Strain 

the specia
of the gourmet 
many bags of each type should 
Bacteria  Bacteria 
ifhe wants to use up all of the beans and maximize 
What is 
profit? 

a profit of$1.50, and one bag 
How 

l blend produces 

blend produces 

the maximum 

profit? 

a profit of $2.00. 
the merchant prepare 

his 

Bacteria 
Strain 
2 

Food A 
Food B  2 
FoodC  1 

0 
3 
1 

the chemical equation 

for each 

E q u a t i o n s  

C h e m i c a l  
7-14, balance 

B a l a n c i n g  
In Exercises 
reaction. 
7. FeS2 + 02-----+ Fe203 + S02 
8. C02 + H20 -----+ C6H1206 + 02 (This 

when a green plant 

place 
water to glucose and oxygen 

converts 

during 

carbon 

reaction 
dioxide 

takes 
and 
photosynthesis.) 

and water.) 

9. C4H10 + 02-----+ C02 + H20 (This reaction 
occurs 
of oxygen 

when butane, 
to form carbon 

C4H10, burns in the presence 
dioxide 
10. C7H602 + 02-----+ H20 + C02 
11. C5H110H + 02-----+ H20 + C02 (This 
the combustion 
12. HC104 + P4010----+ H3P04 + Cl207 
13. Na2C03 + C + N2-----+ NaCN + CO 

resents 

of amyl alcohol.) 

equation rep­

cA514. C2H2Cl4 + Ca(OH)z-----+ C2HC13 + CaC12 + H20 

Network A nalvsis 
15. Figure 

2.18 shows a network of water pipes 

with flows 

of linear 

equations 

to find 

in liters per 
minute. 

measured 
(a) Set up and solve a system 
(b) If the flow through 
the other 

the possible flows. 

will the flows 

through 

AB is restricted 
to 5 L/min, 
what 
two branches 
be? 
maximum possible 

flows 

through each 

(c) What are the minimum and 
branch? 
( d) We have been assuming 
What would negative 

tive. 
ing we allowed it? 
example. 

that flow is always 
posi­
flow mean, assum­
Give an illustration 
for this 

arrangements 

strain can coexist in 
of the 
food? 
3. A florist 

the test tube and consume 

all 

one rose, 

sizes of 
flower 

chrysanthemum

and chrysanthemums. Each 

daisies, 
s. Each medium arrange

three 
­

daisies, 
contains 

offers three 
containing 
roses, 
small arrangement 
and three 
ment contains 
santhemums. Each large 
roses, 
day, the florist noted 
50 daisies, 
for these 
arrangements 

two roses, four daisies, and six chry­
four 
eight daisies, and six chrysanthemu
ms. One 
of 24 roses, 

that she used a total 

and 48 chrysanthemums 
orders 
three 

in filling 
. How many 

types of arrangements
of each type did she make? 

arrangemen

t contains 

There are 20 coins 

4. (a) In your pocket you have some nickels, 
dimes, and 
and exactly 

quarters. 
twice 
coins 

nickels. 
as many dimes as 
The total 
of coins 
is $3.00. 
(b) Find all possible combinations 
of 20 coins 

altogether 
value 
of the 
of each type. 
(nickels, 
that will make exactly 
$3.00. 

dimes, and quarters) 

Find the number 

5. A coffee merchant 
sells 

roast 

three 

beans. 

beans, 

grams of Colombian 
and 100 grams of French 

of coffee. 
A bag 

blends 
of Colombian 
300 grams 
beans. 

A bag of the 

l blend contains 200 

house blend contains 
200 grams of French 

of the 
beans and 
specia
200 grams of Kenyan 
roast 
100 grams of Colombian 
beans, 
chant 
15 kilograms 
roast 
many bags of each type of blend can be made? 

and 200 grams of French 
has on hand 

of Kenyan 
Ifhe wishes 

blend contains 
gourmet 
beans, 200 
grams of Kenyan 
beans. 

roast 
The mer­
30 kilograms 
of Colombian 

and 25 kilograms 

to use up all of the beans, 

beans, 
of French 
how 

beans, 

beans, 

beans. 

that the house blend contains 
beans, 50 grams of 
beans and the 
Colombian 

100 grams of 

beans, 
beans, and 50 grams of French 

Kenyan 

roast 

roast 

6. Redo Exercise 

5, assuming 
300 grams of Colombian 
beans, and 150 grams of French 
gourmet blend 
contains 
350 grams 
beans. 
beans, 
Colombian 
15 kilograms 
of French 
the house blend 

of Kenyan 
This time 

the merchant 

produces 

has on hand 30 kilograms 

15 kilograms 
beans. 

roast 

of Kenyan 

a profit of 

Suppose one bag 
$0.50, 

one bag 

of 
beans, 
and 
of 

of 

A bag of the 

!1 --+ 

A 

B 

Fioure 2 . 1 8  

c 

Section 2.4 Applications 

1 1 5  

16. The downtown 

minute entering 

and 

For the city block 

core of Gotham City consists 
and the 

of 
traffic flow has been 
at each intersection. 
2.19, the numbers represent the 
numbers of vehicles per 
intersections 
A, B, C, and D during 

one-way streets, 
measured 
shown in Figure 
average 
leaving 
hours. 
(a) Set up and solve a system 
the possible flows f1, . . .  ,f4. 
(b) If traffic 
cles per minute, 
other 

(c) What are the 

is regulated 

minimum 
on each street? 

streets 

oflinear 

on CD so that f4 = 10 vehi­
on the 

what will the average flows 

and maximum possible 

equations 

flows 

be? 

to find 

business 

equations 
to find 

oflinear 

solve a system 
(a) Set up and 
the possible flows 
f1, . . .  ,f5• 
(b) Suppose DC is closed. 
DB? 

(c) From Figure 2.20 it is  clear 

What range of flow will need 
to be maintained through 
that DB cannot 
How does your solution 

be 
in part 

(Why not?) 

closed. 
(a) show this? 

(d) From your solution 
mum and maximum flows 

in part (a), determine 

the mini­

18. (a) Set up and solve 

a system of

possible flows 

find the 
Figure 2.21. 

DB. 
through 
linear 
equations 

to 
in the network shown in 

(d) How would the solution change 

if all of the direc­

tions 

were reversed? 

forf1 = 100 andf6 = 150? [Answer 
first with reference 
to your solution 

(b) Is it possible 
this question 
in part (a) and then directly from Figure 2.21.] 
each of 

the range of flow be on 

( c) If f4 = 0, what will 

the other 

branches? 

1 0!  20f 
--
!1  .2.... 
J.Q. 
A 
B 
h!  13 t 
+---
.£ 
.£ 
f4 
1 0!  isf 
D 
c 

200 

1 0of 150 ! 20of 
--
l.QQ.. 
Ji 
A 
c 
13 t  f4 !  fs t 
+---
.l2Q. 
16 
D 
F 
1 0of 1 00 !  1 0ol 

--
h 
B 
+---
h 
E 

200 

Figure 2 . 1 9  

Figure 2 . 2 1  

17. A network of irrigation ditches 

is shown in Figure 2.20, 

with flows 

measured 

in thousands 

of liters 

per day. 

l.QQ.. A 

.l2Q. 
!" 
..!22. 
c 

Figure 2 . 2 0  

200 

D 

the currents for the 

Networks 

electrical 

Electrical 
For Exercises 
networ
given 
ks. 

19 and 20, determine 
+---
--
/2 
+---

+---
19.  /1 c  /1 
--
/2 
+---
4 ohms  /3  D  /3 

8 volts 

1 ohm 

A 

B 

1 ohm 

1 3  volts 

R ----
eff - 1  1 

-+­R1 Rz 

11 --12 --
1f 

(a) 

E 
Ri R2 

E 

(b) 

Equations 

B 

A 

1 ohm 

2 ohms 

8 volts 

5 volts 

of Linear 

-
--
12 
-

1 1 6   Chapter 2 Systems 
-
20.  Ii  c  11 
--
h 
-
4 ohms  13  D  13 
21. (a) Find the currents 
branch CE becomes O? 
(c) Can you change 
l ohm 
!h 
Iii 
/4 --B 
13 ! 15 --E  1 ohm 
D 
14 volts -I 
-I 

leave everything 
rent through 

(b) Find the effective 

c  2 ohms 

in Figure 2.22. 

A 

2 ohms 

1 ohm 

Figure 2 . 2 2  

22. The networks 

in parts (a) and (b) of Figure 
2.23 

I, I1, . . .  , I5 in the bridge circuit 
resistance 
of this network. 
the resistance 
else unchanged) 

in branch BC (but 

so that the 
cur­

Figure 2 . 2 3  
Resistors 

in series 

and in parallel 

Farming consumes 1/2 

l i n e a r  E c o n o m i c  
23. Consider a simple economy with just two industries: 

M o d els 

consumes 

and manufacturing. 
and 1/3 of the 

farming 
the food 
1/2 of the 
turing 
Assuming 
factured goods. 
in equilibrium, 
find the relative 
and manufacturing 

of 
Manufac­
manufactured 
goods. 
food and 2/3 of the 
manu­
the economy is closed and 

outputs 

of the farming 

24. Suppose the coal and 

industries 
form a closed 

industries. 
steel 

in series and in parallel, 
a general formula 

for the 
of each network-that is, find Reff 

show two resistors 
coupled 
respect
ively. 
We wish to find 
effective 
resistance 
such that E = Refl 

(a) Show that the effective 

resistance 

Reff of a network 

economy. Every $1 produced by the coal industry 
requires 
$0.30 
produced by steel 
steel. 
steel if 

of steel. 
of coal and $0.20 
(output) 
of coal and 
is $20 million. 

Find the annual 
the total 

of coal and $0.70 
requires 
$0.80 

production 
production 

Every $1 
of 

annual 

25. A painter, a plumber, and an electrician 

enter 

into a 

with two resistors 
is given by 

coupled 

in series [Figure 2.23(a) J 

(b) Show that the effective 

resistance 
coupled 

Reff of a net­
in parallel 

resistors 

work with two 
[Figure 

2.23(b)] is given by 

in which each of them agrees 

arrangement 

of 10 hours per week according 

cooperative 
to work for himself/herself and the other 
total 
must 
shown in Table 2.8. For tax purposes, 
establish 
do 
a value 
this so that they each come out even-that 

for his/her 

to the schedule 
each person 

services. They agree to 

is, so that the 

two for a 

Section 2.4 Applications 

1 1 1  

services 

produced by the other 

they produce, each department uses a certain amount 
of the 
departments 
and itself
ing the year, 
other 
in Administra
tive services, 
services, 
What does the annual 
produced by each department 
meet the demands? 

, as shown in Table 2.10. Suppose that, 
dur­
require 
$1.2 million 
in Health 
in Transportation 

value of the 
services 
need to be in order 

city departments 

and $0.8 million 

$1 million 

services. 

dollar 

to 

Table 2 . 1 0  

Department 

A  H  T 

Buy  H  0.10 0.10 0.20 
A  $0.20 0.10 0.20 
T  0.20 0.40 0.30 

total amount paid 
out by each person 
he/she receives. 
charge 
if the rates 
are all whole 
and $60 per hour? 

What hourly 

equals 

the amount 
rate should each 
person 

numbers between $30 

Table 2 . 8  

Supplier 

Painter 

Plumber 

Electrician 

Painter 

5 
1 
Consumer  Plumber 
4 

Electrician 

2 
4 
4 

5 
4 

26. Four neighbors, each with a vegetable 

garden, 
to 

agree 

their 

produce. One will grow beans (B), one will 
(L), one will grow tomatoes (T), and one 

(Z). Table 2.9 shows what fraction 

share 
grow lettuce 
will grow zucchini 
of each crop each neighbor will receive. What prices 
should the neighbors charge 
person 
a value 

break even and the lowest-priced 
crop has 

is to 
of $50? 

for their 

crops 
if each 

Table 2 . 9  

Producer 

B  L  T  z 
B  0  1/4 1/8 1/6 
Consumer L 1/2  1/4 1/4 1/6 
T 1/4  1/4 1/2 1/3 
z 1/4  1/4 1/8 1/3 

industries 
form an open 

27. Suppose the coal and steel 
$0.15 of coal and $0.20 
by steel 
requires 
$0.25 

economy. Every $1 produced by the coal industry 
requires 
produced 
steel. 
for $45 
satisfy 
(a) How much should each industr
by $5 million 
(b) If the demand for coal decreases 
by 
increases 

of steel. 
Every $1 
of coal and $0.10 of 
outside 

Suppose that there is an annual 

of coal and $124 million 

y produce to 

the demands? 

of steel. 

demand 

million 

per year while the demand for steel 
per year, how 
$6 million 
industries 
adjust their 

production? 

should the coal 

and steel 

28. In Gotham City, the departments 
tion (A), Health (H), and Transportation 
(T) are 
interdep

For every dollar's 

endent. 

worth of services 

of Administra­

are initially 

l i n e a r  G a m e s  
Finite 
In Example 
29. (a) 
off. Can we push the 
that only the second 

2.35, 

suppose all the lights 
switches 
so 
and fourth lights 
in some order 

in some order 
will be on? 
so that 

(b) Can we push the switches 

30. (a) In Example 

will be on? 

light 
suppose the fourth light 

only the second 
2.35, 
initially 
on and the other 
we push the switches 
the second 

and fourth lights 

in some order 
will be on? 

is 
four lights 
are off. Can 
so that only 

(b) Can we push 
only the second 

light 

the switches in some order 

so that 

will be on? 
all possible 

that can be obtained 

if we start with all the 

of lights 
lights 

off. 

31. In Example 

2.35, 

describe 

configurations 

32. (a) In Example 

suppose that all of the lights 

2.36, 
off. Show that it is possible to 

are initially 
push the switches 
lights 

are off, dark blue, and 

in some 

order so that the 
light 
push the switches 

blue, in 

that order. 

(b) Show that it is possible to 

in 
blue, 
off, 

some order 
and light 

so that the lights 
in that order. 

blue, 

are light 

(c) Prove that any configuration 

of the three 

lights 

can be achieved. 

33. Suppose 

the lights 

in Example 

2.35 can be off, light 

blue, 

or dark blue and the switches 

work as described 

1 1 8   Chapter 2 Systems 

of Linear 

Equations 

2.36. 

2.35 but cycle 

as in Example 

in Example 
lights 
in Example 
all of the lights 
so that the lights 
light blue, 

(That is, the switches control 
as 
to start 
with 
in some order 
dark blue, 

off and push the switches 
are dark blue, 

2.36.) Show that it is possible 

and dark blue, 

through 

the colors 

the same 

blue, 

light 
in that 
order. 
configurations 
all possible 
starting 
with all the 

34. For Exercise 33, describe 

that can be obtained, 

of lights 
lights 

off. 

cAs 35. Nine squares, each one either 

black 

or white, 
are ar­

ranged 

in a 3 X 3  grid. 

Figure 2.24 shows one possible 

the square 

changes 

work. (Touching 

marked* to chan
turn all nine squares 

how the state 
whose number is circled causes the 
states of the 
squares 
is to 
are adapted 
the interacti
(Trilobyte 
(a) If the initial 

from puzzles 
ve CD-ROM game The Sevent
h Guest 
Software/Virgin 
Games, 1992).] 

ge.) The object of the game 
black. 
that can be found in 

configuration 

[Exercises 
35 and 36 

is the one shown in 

2.24, show that the game can be won and 

Figure 
describe 

a winning 
(b) Prove that the 

of moves. 
sequence 
game can always 
l configuration. 

what the initia

be won, no matter 

CAS 

36. Consider 

puzzle. 

a variation 

on the nine squares 
same as that described 
in Exercise 35 
states 
for each 
change 
te changes 

but now the sta

gray, or black. 

The squares 

are three 

possible 

as 
follow 

The 

that there 
white, 

game is the 
except 
square: 
shown in Figure 2.25, 
the cycle 
all-black 
the winning 
from the initial 

white� gray� black� white. Show how 
can be achieved 

configuration 

configura

tion shown in Figure 2.26. 

Figure 2 . 2 4  
The nine squares 
puzzle 

arrangement. 
its own state 
(black� white and white� black). Figure 2.25 shows 

When touched, 
and the states 

each square changes 

of some of its neighbors 

l  2  3 

7  8  9 

4  5  6 

* 
* 
CD  2  3 
* 
* 
* 
* 
@  5  6 
* 
* 
* 
* 
* 
(j)  8  9 

4  5  6 

7  8  9 

l  2  3 

7 

8 9 

l  2  3 

4  5  6 

* 
* 
* 
l @  3 
* 
* 
* 
* 
4 G)  6 
* 
* 
* 
* 
7 ®  9 

4  5  6 

7  8  9 

l  2  3 

1  2  3 

7  8  9 

4  5  6 

* 
* 
1  2 ® 
* 
* 
* 
4  5 ® * 
* 
* 
* 
7  8 ® * 
* 

4  5  6 

7  8  9 

2 3 

l 

Figure 2 . 2 5  
State 

changes 

for the nine squares 

puzzle 

F i g u r e  2 . 2 6  
The nine squares 
with more states 

puzzle 

M i s c e l l a n e o u s  
P r o b l e m s  
In Exercises 
of linear 
37. Grace is three 
will be twice 
now? 

37-53, set up and solve an appropriate system 

equations 

to answer the questions. 
as old as 
times 
as old as 
Hans is then. 

Hans, but in 5 years 
she 
How old are they 

38. The sum of Annie's, Bert's, and Chris's 

Annie is older 
that Bert is older 
than Chris. 
Annie is now, Annie will 
as old as Chris 
is now. What 
ages? 

than Bert by the same number of years 
When Bert is as 
be three times 

are their 

ages is 60. 
old as 

two pr

The preceding 
oblems 
popular books of mathematical 
their origins 
vives from about 

have 
in antiquity. A Babylonian clay tablet 
that sur­

300 B.c. contains the following problem. 

of those found in 
However, they 

puzzles. 

are typical 

Section 2.4 Applications 

1 1 9  

39. There are two fields 

whose total area 

is 1800 square 

yards. One field produces 
per square yard; 
rate of
t bushel 
1 100 bushels, what is the 

grain 
field produces grain at the 
yield 

If the total 
size of each field? 

the other 
per square yard. 

at the rate of� bushel 

is 

44. Generalizing 

Exercise 
of a 3 X 3 addition 

42, find conditions 
table that will guarantee 
that 

on the en­

45. From elementary 

for a, b, c, d, e, and fas previously. 
we know that there 

geometry 

tries 
we can solve 

until the 

that did not become 

y. (There is no evidence 

equations, including 
in 
that 

ago, the Chinese developed 
Over 2000 
years 
methods for 
solving systems 
a version of 
of linear 
Gaussian elimination 
well known 
Europe 
19th centur
Gauss was aware of the Chinese methods when he devel­
oped what 
we now call Gaussian elimination. 
clear 
that the Chinese knew the essenc
e of the method, even 
The followin
g problem 
though they did not justify its use.) 
(Nine 
suanshu 
is taken 
Chapters 
Art), written 
Han Dynasty, about 
types 
40. There are three 

from the Chinese text Jiuzhang 
in the Mathematical 

200 B.C. 

during the 
early 

However, it is 

of corn. 

Three bundles 
of the 
first type, two of the second, and one of the third 
make 39 measures. Two bundles 
of the first type, 
of the second, 
and one of the third 
And one bundle 
and three 
measures 
each type? 
41. Des

of the third 
of corn are contained 

make 26 measures. 
How many 
in one bundle 

all possible values 

cribe 

of a, b, c, and d that 

of 

of the first type, two of the second, 

three 

make 34 measures. 

[Problems 

addition 
will make each of the following 
41-44 are based on the article 
table. 
''An Application 
of Matrix 
The Mathematics Teacher, 

Theory" by Paul Glaister 
85 (1992), 

in 
pp. 220-223.] 

a valid 

(a) 

(b) �b 

c 3  6 
d 4  5 

42. What 

conditions 

on w, x, y, and z will guarantee 
is a valid 
c, and d so that the 

following 

that 

we can find a, b, 
addition 

table? 

noncollinear 

any three 

parabola 
a plane. 

straight line through any two points 
is a 

is a unique 
in a plane. Less 
well known is the fact that there 
unique 
through 
points in 
For each set of points below, find 
an equation of the form y = ax2 + 
a parabola with 
bx+ c that passes through 
the given 
the resulting 
answer.
(a) (O, 1), (- 1, 4), and (2, 1) 
(b) ( - 3, 1), ( - 2, 2), and (- 1, 5) 

to check the validity 
of your 

parabola 

) 

points. (Sketch 

46. Through any three 

noncollinear 

points there also 
(whose general 
circles 

a unique 

circle. 

Find the 

are of the form x2 + y2 + ax + by + c = 

passes 
equations 
that pass through the sets of points in Exercise 
check the validity 
radius 

of your answer, 
of each circle and draw a sketch.) 

center and 

find the 

45. (To 

0) 

g it as 

(ratios of polyno­
denominator is 
numbers. The reverse 
function apart by writin
functions is useful in several 

The process of adding rational functions 
mials) by placing them over a common 
the analogue of adding rational 
process of taking a rational 
a sum of simpler rational 
areas of mathematics; for example, it arises in calculus 
when we need to integrate a rational function and in dis­
crete 
s when we use generatingfunctions to 
The decompositio
solve recurrence relations. 
fractions 
function as 
of 
linear equations. 
s 47-50,find the partial 
fraction 
letter

mathematic
a sum of partial 

In Exercise
decomposition 

leads to a system 

form. (The capital 

of the given 

n of a rational 

3 

= --
=- +--

B 

A 

+ --

48. 

+---

s denote constants.) 
3x +  1 

x  + 2x - 3 x - 1  x + 
x2 -3x + 3 A  B  C 
x3 + 2x2 + x x x +  1  (x 

47. -2 ----
CAS 49. ----------
=--
CAS 50 

(X +  l)(x2 +  l)(x2 + 4) 
+  +---
x +  1  x
x3 + x +  1 A  B 

A  Bx+ C Dx + E 

2 +  1  x2 + 4 

x - 1 

+ 1)2 

43. Describe 

all possible 

that will make each 
table. 

values 
of a, b, c, d, e, and f 
of the following 

a valid 

addition 

(a) +  a  b  c 

(b) +  a  b  c 

d 3  2 1 
e 5  4  3 
f 4  3 

d  1 2  3 
e 3  4  5 
f 4  5  6 

)(x2 + 1)3 x x - 1 
• x(x - l)(x2 + x +  l
Cx + D  Ex + F  Gx + H 
+ 
+ 
x2+ x+ l x2+ 1  (x2+ 1)2 (x2+ 1)3 

= - + --
+ +----

Ix + J 

sums of powers of 

rs: 

of Linear 

Equations 

useful formulas for the 

Following 
consecut

are two 
ive natural numbe

1 2 0   Chapter 2 Systems 
1+ 2+·· · +n =----
12 + 22 +·. · + n2 = -------6 
tion (see Appendix B ). One way to make an educat

The validit
even n 2:: OJ can be 

formulas for all 
established 

n(n +  1) 

using mathemat

n(n + 1) (2n +  1) 

y of these 

as to what the formulas are, though, is to observe 
can rewrite 

the two formulas above 

and 

as 

2 

ical induc­
ed guess 
that we 

values of n 2::  1 (or 

to the 

conjectur
respectively. This leads 
powers of the first n natural numbe
degree 
n. 
51. Assuming that 

in the variable 
1  +  2  + 
· · · 
tuting 

p + 1 

find a, b, and c by substi
thereby obtaining 
b, and c. 

a system 
that 12 + 22 + · · · 

e that the sum of pth 
rs is a polynomial of 
+  n  = an2 + bn + c, 
values 
three 
for n and 
in a, 
of linear 
equations 
+ n2 = an3 + bn2 + en + d. 

52. Assume 

te to 
Find a, b, c, and d. [Hint: It is legitima
What is the left-hand side in that case?] 
+ n3 = (n(n + 1)/2)2. 

53. Show that 13 + 23 + · · · 

use n = 0. 

Vignette 

The Global  Positioning System 

(GPS) 

System 

Positioning 

locations. The military, surveyors, airlines, 

The Global 
ing geographical 
is becoming so commonplace 
and hikers all make use of it. GPS technology 
that 
handheld 
some automobiles, 
devices are now equipped 
with it. 

is used in a variety 

ons for determin­

and various 

cellular 

phones, 

of situati

shipping companies, 

dimensiona

l triangulation: 
its distances 

A point 
from three 

by knowing 

other 

is the location of the GPS receiver, the 
are computed 

using the 
travel 

times of 

on which we impose an xyz-coordinate 
through 

and with the positive z-axis running 

The basic 

idea of GPS is a variant 

on three-

is uniquely 

on Earth's surface 
determined 
points. Here the point we wish to determine 
other 
satellites, and the distances 
radio 

points are 
signals from the satellites 

to the receiver. 
that Earth is a sphere 

We will assume 

with Earth centered 

pole and fixed relative to 

at the origin 
Earth. 

system 
the north 
For simplic

ity, let's 

take one unit to be equal to 

the unit sphere with equation 

a radio 

becomes 

surface 
measured in hundredths 
takes 
signal to get from 
speed of light, 
which is approxim
a second)
Let's 

. 
imagine 

that you are a hiker 

the radius 

x2 + y2 + z2 = 1 .  Time will be 

of Earth. 

Thus Earth's 

of a second. 

GPS finds distances by knowing 

one point to another
ately 

equal to 0.47 (Earth radii 

. For this we need to know 

per hundredths 

how long it 
the 
of 

lost in the woods at point (x, y, z) at some time 

t. You don't know where you are, and furthermore, 
know what time it is. However, 
from four satellites, 
signals 
giving 
their 
(Distances 
in Earth radii 
midnight.) 

you have your G PS device, and it receives simul
and times as shown in Table 2.1 1 .  
of a second 
past 

positions 
and time in hundredths 

you have no watch, 

are measured 

so you don't 

taneous 

on the 

is based 

This application 
article 
Linear 
Dan Kalman in 

by 
for GPS" 
The College 

"An Underdetermined 
System 

t of the ideas 

Mathematics Journal, 33 (2002), 
pp. 384-390. 
1997). 

treatmen
here, 
Linear 
(Wellesley-

see G. Strang 
Algebra, 

Geodesy, and GPS 
Cambridge 

For a more in-depth 

introduced 

and K. Borre, 

2 
3 
4 

Press, 

MA, 

Table 2 . 1 1  sa1em1e Dara 
Satellite 

Position Time 

( 1 . 1 1 ,  
2.55, 
(2.87, 0.00, 
(0.00, 
( 1.54, 1.01, 

2.14)  1.29 
1.43) 1.31 
1.08, 2.29) 2.75 
1.23) 4.06 

121 

Let (x, 

y, z) be your position, 

and let 
for x, y, z, and t. 
Your distance 

goal is to solve 
follows. The signa
1.29 and arrived at time t, so it took t -1 .29 hundredths 
Distance 

t be the time when the signals arrive. The 
as 
10-2 sec, was 
of a second 

from Satellite 
rth radii/

at a speed of0.47 Ea

velocity multiplied 

sent at time 
to reach you. 

l, traveling 

time, so 

1 can be computed 

equals 

by (elapsed) 
d = 0.47(t 
-1.29) 

Combining 

Expanding, 

We can also express d in terms of (x, y, z) 
using 

the distance 

and the 

satellite

's position 

( 1 . 1 1 ,  

2.55, 
2.14) 

formula: 
d = V(x -1 . 1 1)2 + (y  -2.55)2 + (z  -2.14)2 
results leads to 

the equation 

these 
(x -1 . 11)2 + (y -2.55)2 + (z -2.14)2 = 0.472(t -1.29)2  (1) 
simplifying, 
2.22x + 5.lOy + 4.28z 
we can derive 

we find that Equation 
= x2 + y2 + z2 -0.22t2 + 1 1 .95 
for each of the 

a corresponding equation 

and rearranging, 

(1) becomes 

-0.57t 

three 

satel­

of four equations 

Similarly, 
lites. We end up with a system 
= x2 + y2 + z2 -0.22t2 + 1 1 .95 
2.22x + 5.lOy + 4.28z 
-0.57t 
= x2 + y2 + z2 -0.22t2 + 9.90 
5.74x  + 2.86z -0.58t 
2.16y + 4.58z -1.2 1 t  = x2 + y2 + z2 -0.22t2 + 4.74 
3.08x + 2.02y + 2.46z -1.79t = x2 + y2 + z2 -0.22t2 + 1.26 

in x, y, z, 
and t: 

other 

These are not 
If we subtract 
linear 
system: 

equations, 

linear 
nonlinear 
terms 
the first equation from each of the other 

are the same 
three 

in each equa
tion. 
we obtain 

equations, 

but the 

a 

3.52x -5.lOy -1.42z -O.Olt =  2.05 
- 2.22x -2.94y + 0.30z -0.64t 
0.86x -3.08y -1.82z -1 .22t = - 10.69 

=  7.21 

The augmented 

matrix row reduces 

as 

[ 3.52 

- 2.22 

0.86 

- 5.10 
- 2.94 
- 3.08 

- 1.42 

0.30 

- 1.82 

2.97] 0.81 
- 1.22 =�:��1 � [� � � �:�: 

- 10.69 0  0  1 0.79 

- 0.01 
- 0.64 

5.91 

1 2 2  

from which we see that 

x = 2.97 -0.36t 
y = 0.81 -0.03t 
z = 5.91 -0.79t 

(2) 

with t free. Substi

into (1), we obtain 

these 
-0.36t 

tuting 
(2.97 
+ (5.91  -0.79t 

equations 
-1 . 1 1 )2 + (0.81 

-0.03t 
-2.14)2 = 0.472(t -1.29)2 

-2.55)2 

which simplifies 

to the quadratic equation 

There are two solutions: 

0.54t2 -6.65t + 20.32 = 0 

t = 6.74 and  t = 5.60 

into (2), we find that the first solution corresponds 

solution 
and the second 
not on the unit sphere (Earth), so we reject it. The first solution produces 

tuting 

Substi
0.61, 0.56) 
is clearly 
x2 + y2 + z2 = 0.99, 
have located your coordinates 
In practice, 

solution to (x,y, z) = (0.96, 
within 
0.61, 0.56). 
ntly more factors 

so we are satisfied 
that, 

GPS takes significa

as (0.55, 

to (x, y, 
1.46). The second 

into account, 

acceptable 

roundoff 
error, we 

0.65, 

z) = (0.55, 

is not 

such techniques 

that Earth's surface 
volving 
the results of 
the GPS calcula
dinates 
yet other 

into latitude 
branches 

and longitude, 
of mathematics. 

exactly spherical, 
as least 
squares 

so additional 

tion are converted 

approximation (see Chapter 7). In addition, 
(Cartesian) 
and one involving 

from rectangular 
in itself 
an interesting 
exercise 

refinements 

coor­

such as the 
are needed in­

fact 

1 2 3  

1 2 4   Chapter 2 Systems 

of Linear 

Equations 

'� � llerative 

Melhods for Solving 

linear svstems 

we explore 

due to roundoff 

elementary 

row operations, 

generating sequences of vectors that 

lead 
and other 
a different 

using 

systems, 

linear 
but are subject to errors 
road in our "trivium" 
takes 

methods 

for solving 

In this section, 

in many cases 
as we have seen. 
The third 

The direct 
to exact solutions 
factors, 
path indeed. 
sively 
many instances 
zero entries
), iterative 
Also, 
erate 
curacy: 

iterative 
is sufficiently 

Roundoff 

methods 

(such as 

error 
We will explore 

methods 

that proceed 
approach a solution 

us down quite 
iterativel
to a linear 
many 

y by succes­
is sparse-that is, contains 
methods. 
they gen­

system. In 

accurate. 

methods 
can be stopped whenever 

can be faster and more accurate than direct 
the approximate 
methods 
convergence 
toward 

In addition, 
can actually 
accele
two iterative 

iterative 
rate their 
for solving 

solution 
often benefit from inac­
a solution. 
systems: Jacobi's method 
we will be consid­

methods 

linear 

when the coefficient 

matrix 

and a refinemen
ering linear 
that there is a unique 
assume 
iterative 
methods. 

t of it, the Gauss-Seidel method. In all examples, 
with the same number of variables 
Our interest 

systems 

solution. 

is in 

as equations, 
finding this 

and we will 
solution using 

Example 2 . 3 1  

Consider 

the system 

Jacobi's 
method 
for x2, to obtain 

begins with solving 

the first equation 

for x1 and the second 

equation 

(1) 

7x1 - x2 = 5 
3x1 - 5x2 = - 7  

5  + Xz 

X1 = 7 
Xz = 5 

7  + 3x1 

was 

It turns 

values 

5  +  0  5 

of x1 and x2: 

out that it 
does not 

approximation 
approximation 

to the solution. 
is, so we might as well take x1 = 0, x2 = 0. We 

We now need an initial 
what this initial 
matter 
use these 
in Equations 

who made 
to many 
of mathematics 
, 

(1) to get new values 
-- =   - =  0.714 
Jacobi (1804-1851) 
into (1) to get 
X1 = ---
= 0.914 
.2-
Xz = --- 7 = 1 .829 
of x1 and x2), producing the sequence of approxima

Carl Gustav 
mathematician 
a German 
important contributions 
fields 
and physics, 
including 
analysis, 
dynamics. 
work was in applied mathematics, 
Jacobi 
doing 
, he held positions 
A fine teacher
at the Universities 
Konigsb
erg and was one of the 
famous mathematicians 

(written 
and x1 to get the new 
given 

geometry, number theory
mechanics, 
Although 

believed 
of 
mathematics 

places). We repeat this process 

5 
values 

decimal 
values 

and fluid 
much of his 

(using the old 

in the importance 

in Table 2 . 1 2 .  

Now we substi

tute these 

of Berlin and 

for its own sake. 

to three 

values 

7 
7 
5 

in Europ

most 
e. 

7  +  3 

7  + 3 .  0 

Xz = 

1 .400 

5  + 1 .4 

7 

7 

5 

of x2 
tions 

Linear 

Table 2 . 1 2  

Systems 1 2 5  

Methods 

for Solving 

2 3 4 5 6 

Section 2.5 Iterative 
1 
n  0 0 0 
0.714 0.914 0.976 0.993 0.998 0.999 
1.400 1.829 1.949 1.985 1.996 1.999 
ve vectors [::] are called iterates, 
is [0·993]. We can see that the iterates 
1.985 

so, for example, when n = 4, 

in this  example  are 

the fourth iterate 

The successi

......... approaching [�l which is the  exact 

We say in this case 

that Jacobi's 

Jacobi's 

method 

ing to the crisscross 

calculates 
pattern 

method 

system. (Check this.) 

solution of the given 
converges. 

4 
shown in Table 2.13. 

in a two-variable system 

ve iterates 

the successi

accord­

Table 2 . 1 3  

n  0  1 2  3 

C. F. Gauss 
von Seidel (1821-18

96). Seidel 

method 
and Philipp 

The Gauss
after 

probability 

Ludwig 

is named 

-Seidel 

in analysis, 
worked 
y, and opt
ics. 
, astronom
theory
y, he suffered from 
Unfortunatel
and retired 
eye problems 
age. The paper 
in which he described 
the method 
Seidel was publish
it seems, 
method! 

ed in 1874. 

now known as Gauss­
Gauss, 

was unaware 

at a young 

of the 

Before we consider 

modification 
is the same as the 
So in our example, 
we now use this 

of it that often 
Jacobi 
we begin 

value of x1 to get 

by calculating 

Jacobi's 
converges 
except 

method 

we will look at a 

in the general 
to the solution. 

case, 
The Gauss-Seidel method 
n. 

method 
faster 
that we use each new value as soon as we ca

x1 = (5 + 0)/7 = * =  0.714 as before, but 
5 

the next value of x2: 7 + 3. 2. 
Xz = --- 7 =  1.829 

are 

method 

has converged 

this time 

this time 

according to the 

are calculated 

tion. The iterates 

We observe that  the Gauss-Seidel 

We then use this value of x2 to recalculate x1, and so on. The iterates 

shown in Table 2.14. 
Table 2.15. 
n  0 1 2 3 4 5 
0 0.714 0.976 0.998 1.000 1.000 
0 1.829 1.985 1.999 2.000 2.000 

faster to the solu­
pattern 

zigzag 

Table 2 . 1 4  

shown in 

1 2 6   Chapter 2 Systems 

of Linear 

Equations 

Table 2 . 1 5  

n  0 

1  2 3 

Xz 

method 

also has a 

The Gauss-Seidel 

nice geometric 
of x1 and x2 as the coordinates 

two variables. We can think 
starting 
calculation 

x2 = � =  1 .829, which moves us to 

x1 = �, so we move to the point ( �, O) =  (0.714, 
the point(�,�) =  (0.714, 

point is the point corresp

to our initial 

approximation, 

in the case of 
interpretation 
of points in the plane. 
Our 
(0, O). 
Our first 

0). Then we compute 
1.829). Continuing 
give rise to a sequence 

onding 

method 

gives 

in 

our calculations 

from the Gauss-Seidel 
from the preceding 

point in exactly 
7x1 - x2 = 5 and 3x1 -5x2 = - 7  correspond

ing to the two given 

we find that the points calculated 

Moreover, 
they approach 
to the solution of the system 

of equations. This is what 

above fall alternately 

two lines, 
the point of intersection of the 

on the 

this fashion, 
of points, each one differing 
we plot the lines 
equations, 
as shown in Figure 2.27. 
lines, 
convergence means! 

which corresponds 

one coordinate. If 

2 

0.5 
- 0.5 

-1 

0.2 0.4 

Figure 2.21 

Converging 

iterates 

The general cases 

two methods are 

analogous

of the 

. Given a system 

of n linear 

equations 

in n variables, 

ai 1X1 + ai2X2 + · · · 
az1X1 + az2X2 + · · · 

+ ainXn = bi 
+ aznXn = bz 

(2) 

we solve 
with an initial 

the first equation for x1, the second 

for x2, and so on. Then, beginning 

approxim

ation, we use these 

new equations 

to iteratively 

update each 

Section 2.5 Iterative 

Methods 

for Solving 

Linear 

Systems 121 

uses all of the values 
method 
the Gauss-Seidel 

Example 2.39 

always 
later 

uses the most recent value 
illustrates the 
Gauss-Seidel 

tion. 

at the 

kth iteration 

to compute the 

in a three-

method 
variabl
e. Jacobi's 
whereas 
(k + l)st iterate, 
of each variable 
in every calcula
variable 
method 
problem. 
At this point, you should 
methods. (Do you?) Several 
when do they converge? 
answer 

to the 

have some questions 
come to mind: Must these methods 
solution? 
The 

about these 
converge? 

must they converge to the 

and concerns 

If they converge, 

iterative 
If not, 

first question is 

no, as Example 

2.38 illustrates. 

Example 2 . 3 8  Apply the Gauss-Seidel 

method 

to the system 

X1 -Xz = 1 
2x1 + Xz = 5 

with initial 

approximation [ �]. 

Solution We rearrange 

the equations 

�  The first few iterates 

to get 
X1 = 1 + X2 
X2 = 5  -2x1 

are given 

in Table 2.16. (Check these.) 

is [::] = [ �]. Clearly, the iterates in 

2.28 makes graphic

ally clear 
in an 

The actual 

solution to the  given 
system 

2.16 are not approaching 

this point, as Figure 

Table 
example 

of divergence. 

Table 2 . 1 6  

n 0 1 2 
3 - 2  

4 
Xi  0 
X2 0  3 - 3  

9 

4 5 

10 -14 

- 4  

- 15  33  F i o u r e  2 . 2 8  

Diverging 

iterates 

1 2 8   Chapter 2 Systems 

of Linear 

Equations 

these 

S o  when do 

iterative 

methods 
is rather tricky. We will 
question 
answer 
will give 
a partial 
proof. 
Let A be the 

answer, without 
n X n 

matrix 

converge? Unfortunately, 
it completely 

the answer 
in Chapter 7, but for now we 

to this 

We say that A is strict

ly diagonally dominant 

a12 a22 
a21 
an2 

A =  . ["" an) 
a,, l a2n 
la11I > la12I + la13I + · · · 
+ la1nl 
la22I > la21I + la23I + · · · 
+ la2nl 

ann 

if 

That is, the absolute 
sum of the 
absolute 

value of each diagonal 
values 
of the remaining 

entry a11, aw . . .  , ann is greater than the 
entries 

in that row. 

Theorem 2 . 9  

If a system 
nant coefficient 
Gauss-Seidel method 

of n linear 
matrix, 

converge 

to it. 

equations 
then it has a unique solution 

in n variables 

has a strictly diagonally domi­

and both the Jacobi 

and the 

Remark Be warned! This theorem is a one-way implica

tion. 
is not strictly diagonally dominant does not mean that the 

They may or may not converge. 

(See Exercises 

15-19.) Indeed, 

The fact that a 
meth­
iterative 
there 
are 

system 
ods diverge. 
examples 
either 
of these 
converge 

in which one of the 
methods 
converges, 
point. 

to some other 

methods 

converges 

and the other 
diverges. However, 
converge to the solution-it cannot 

then it must 

if 

Theorem 2 . 1 0  

If the Jacobi 
equations 

or the  Gau
in n variables, 

ss-Seidel 
then it must converge 

method 

converges 

for a system 

of n linear 

to the solution 

of the system. 

Proof We will illustrate 
the idea 
Jacobi's method, using 
the system 
of equations 
is similar. 

the proof 

behind 

by sketching it out for the case of 

in Example 

2.37. 

The general proof 

Convergence 

means that "as iterations 

closer 
respecti
vely, 

and closer to a limiting value:' This means that x1 and x2 converge 
to r and s, 

as shown in Table 2.17. 

increase, 

the values 

of the iterates get 

We must  prove 

that [::] [:] is the solution of the system 

of equations. In 

of x1 and x2 must stay the same as at 

other 

words, 

at the (k + l)st iteration, 

the values 

Section 2.5 Iterative 

Methods 

for Solving 

Linear 

Systems 1 2 9  

Table 2 . 1 1  
n 

k k+I k+2 
give x1 = (5 + x2)/7 = (5 + s)/7 and x2 = 
(7 + 3x1)/5 = (7 + 3r)/5. 
7 + 3r 
5 + s 
and ---
-- = r  

r  r 
s  s 

But the calculations 

the kth iteration. 

Therefore, 

= s 

r 

s 

5 

7 
Rearranging, 

we see that 

7r - s =  5 
3r - 5s = 
Thus, x1 = r, x2 = s satisfy the original 
equations, 
If iterative 

By now you may be wondering: 

methods 

we just use Gaussian 

are they? 

what good 

solution, 
Why don't 
have seen that Gaussian elimination 
inaccur
ity can lead to 
nation does not 
go astra
example, if we use Gaussian 
there is 
and work with increas
In contrast, 

ate or even wildly 
y, we cannot 

no way to obtain the 

improve 

ed accuracy. 

to calcula
elimination 
solution to 
four decimal 

is sensitive to roundoff 
wrong answers. Also, 

places 

don't 

- 7  
as required. 
always 
elimina
errors, 
and this sensitiv­
even if Gaussian 
have found it. For 

converge 
tion? 

to the 

elimi­

on a solution once we 

except 

to start 

over again 

we can achieve additional accuracy 
with iterative 
larly 

methods 
by 
with sparse 

simply 
coefficient 

systems, particu

those 

te a solution to two decimal places, 

First, we 

iterative 

doing more iterations. For large 
matrices, 
on a computer. 
dominant, and 
illustrates 

In many applications, 
thus iterative 
methods 

one such application. 

methods 

are much faster than direct methods 

when implemented 

the systems 
are guaranteed to converge. 

that arise 

are strictly diagonally 

The next example 

Example 2 . 3 9  

Suppose we heat each edge of a metal plate 
Figure 
2.29. 

to a constant temperature, 

as shown in 

50° 

Figure 2 . 2 9  
A heated metal 

plate 

oo 

1 3 0   Chapter 2 Systems 

of Linear 

Equations 

Eventually 

the temperature 

at the interior points will reach equilibrium, 

where the 

following 

property can be shown to hold: 

The temperature at each interior 
tures 

on the circumference 

point P on a plate 
centered 

of any circle 

is the average 
the plate 

at P inside 

of the tempera­

(Figure 

2.30). 

figure 2 . 3 0  

To apply this property in an 

an alternative, 
or mesh, 

that has a finite 

we can approximate 

actual 

example 

ues from calculus. As 
by overlaying 
the plate 
number of interior points, as shown in Figure 2.31. 

the situation 

requires 

techniq

with a grid, 

Figure 2 . 3 1  
The discrete version 
plate 

problem 

of the heated 

The discrete analogue 

of the averaging 

property governing 

equilibrium 

tempera­

tures 

is stated 

as follows: 

The temperature 
points adjacent to P. 

at each interior 

point P is the average 

of the temperatures 
at the 

For the example 
four other 

adjacent to 

shown in Figure 2.31, there 

points. Let the equilibrium 

are three 
interior 
temperatures 

and each is 
of the interior points 

points, 

be t1, t2, and t3, as shown. Then, by the 

Linear 

Systems 1 3 1  

Methods 

for Solving 

temperature-averaging 

Section 2.5 Iterative 
t  = --------
100 + 100 + G + 50 
t1 = ____ 
t1 + t3 + 0 + 50 
100 + 100 + o + G 

property, we have 

(3) 

4 

__ 

! 

_ 
4 
t3 =  4 

or 

250 
- t1+ 4t2- t3= 50 
- t1 + 4t3 = 200 

Notice 

(3) are in 

that this system 

is strictly 
the form required 
for Jacobi 

tions 
approximation of ti = 0, t2 = 0, t3 = 0, the Gauss-Seidel method 
iterates. 

diagonally dominant. Notice 
tion. 
gives 

also that Equa­
With an initial 
the following 

or Gauss-Seidel itera

Iteration 1 :   t1 = 
t1 = 
t3 = 

100 + 100 + 0 + 50 

= 62.5 

62.5 + 0 + 0 + 50 

= 28.125 

4 

4 

100 + 100 + 0 + 28.125 

= 57.031 

Iteration 

2: 

t1 = 

100 + 100 + 28.125 + 50 

= 69.531 

69.531 + 57.031 + 0 + 50 

= 44.141 

100 + 100 + 0 + 44.141 

= 61.035 

t1 = 

t3 = 

4 

4 

4 

4 

listed 
two successi

we find the iterates 
and stop when 

Continuing, 
digit 
accuracy 
variables. 

�  0.001) ti = 74.108, t2 = 46.430, 

Thus, the equilibrium 

and t3 = 61.607. 
tion as we like about the equilibrium 

temperatures 

in Table 2.18. 

We work with five-significant­

ve iterates 

agree within 

0.001 in all 

at the interior 

points are 
calcula
tions.) 

(Check these 

(to an accuracy 

of 

By using a finer grid (with more interior points), we can get as precise 

temperatures 

at various 

points on the 

informa­
plate. 

Table 2 . 1 8  

n 0 1 2 3  7 8 
t3 0  57.031  61.035 61.536  61.607 61.607 4 

ti 0  62.500 69.531  73.535 74.107 74.107 
t1 0  28.125 44. 141  46.143 

46.429 46.429 

of Linear 

Equations 

1 3 2   Chapter 2 Systems 
..  I Exercises 
GAS 

2 . 5  

agree 

method to the given system. 

In Exercises 
1-6, apply Jacobi's 
Take the zero vector as 
with four-significant-digit 
iterates 
compare your answer with 
any direct method you like. 
1. 7X1 - X2 = 6 

the initial 

approximation and work 
, 

accuracy until two successive 
each case
0.001 in each variable. In 
the exact solution 
found using 

2. 2X1 + X2 = 5 
X1 - 5X2 = -4  X1 - X2 =  1 

within 

method to obtain an approximate solution 
to within 
15. X1 - 2X2 = 3 
3x1 + 2x2 =  1 

16. X1 - 4X2 + 2X3 = 2 
2x2 + 4x3 =  1 
6X1 - X2 - 2X3 =  1 
of the 
the divergence 

17. Draw a diagram to illustrate 
in Exercise 

Gauss-Seidel 

method 

that is accurate 

15. 

0.001. 

coefficient 

matrix is not strictly 
18 and 19, the 
In Exercises 
dominant, nor can the equations 
diagonally 
be rearra
nged 
to make it so. However, both the Jacobi and the Gauss-Seidel 
ge anyway. Demonstrate 
method conver
that this is true of 
g with the zero vector 
the Gauss-Seidel method, startin
the initial 
and obt
aining a 
solution 
accurate 
18. -4x1 + 5X2 = 14 
X1 -3x2 = - 7  
19. 5x1 -2x2 + 3x3 = - 8  

approximation 
to within 
0.01. 

as 
that is 

X1 + 4X2 - 4X3 = 102 

20. Continue 

-2x1 -2x2 + 4x3 = -90 
obtain 

performing 

iterations 

in Exercise 18 to 

a solution that is accurate to within 0.001. 

21. Continue 

performing 

iterations 

in Exercise 19 to 

obtain 

a solution that is accurate to 

within 0.001. 

es shown 

the metal plate 

on its boundaries. 

at each of the indicat

In Exercises 
22-24, 
peratur
temperature 
settin
the Jacobi or the Gauss-Seidel method. Obtain a solution 
that is accurate to within 
22. 

has the constant tem­
Find the equilibrium 
ed interior 
and applying either 

points by 
equations 

g up a system of linear 

0.001. 

3. 4.5X1 -0.5X2 =  1 
X1 -3.5X2 = - 1  

4. 20X1 + X2 - X3 = 17 
X1 -10x2 + x3 = 13 
-x1 + X2 + 10x3 = 18 

5. 3X1 + X2  =  1 
X1 + 4X2 +  X3 =  1 
X2 + 3X3 =  1 

6. 3X1 - X2 

=  1 
-X1 + 3X2 - X3 = 0 
-x3 + 3X4 =  1 

-X2 + 3X3 - X4 = 

agree 

using 

the Gauss­

exercise 
as the initial approxi­
within 0.001 in each variabl

accuracy until 
e. 

iterates 

In Exercises 
7-12, repeat the given 
Seidel method. Take the zero vector 
mation and work with four-significant-digit 
two successive 
Compare 
the number of iterations 
and Gauss-Seidel methods to reach 
solution. 
7. Exercise 
1 
9. Exercise 
3 
1 1 .  Exercise 5 

8. Exercise 
10. Exercise 
12. Exercise 

required by the Jacobi 
such an approximate 

2 
4 
6 

13 and 14, draw diagra

In Exercises 
vergence of the Gauss-Seidel method wit
13. The system 
14. The system 

in Exercise 
in Exercise 

ms to illust
h the 

rate the con­
system. 
given 

1 
2 

15 and 16, compute the first four iterates, 

In Exercises 
approximat
using the 
zero vector 
that the Gauss-Seidel method diverges. Then show that 
the equations 
dominant coefficient matrix, 

nged to give a strictly diagonally 
and apply 

as the initial 

can be rearra

the Gauss-Seidel 

ion, to show 

50  50 

23. 

oo 

100° 

24. 

oo 

40° 

oo 

100° 

oo  oo 

100° !00° 
oo 20° 

t2 
t4 
t2 
t4 

t1 
t3 
f 1 
f3 
25 and 26, we refine the grids 

40° 100° 

20° 

100° 

cises 22 and 24 to obtain 

In Exercises 
the equilibriu
m temperatures 
Obtain solutions 
e to within 
either the Jacobi or the Gauss-Seidel method. 
25. 

Exer­
more accurate information 
about 
points of the plates. 

that are accurat

used in 

at interior 

0.001, using 

oo 

oo t2 
oo t4 ts 
50 50 50 
oo f 1  t2  t3 t4  20° 
oo 15  16  I] lg  20° 
40° 19 110 111 112  100° 
40° t13 t14 t15 t16  100° 

26. oo oo 20° 20° 

40° 40° 100° 100° 

may allow us 

to use a 

Section 2.5 Iterative 

Methods 

for Solving 

Linear 

Systems 1 3 3  

27 and 28 demonstrate that sometimes, if we are 

Exercises 
lucky, the form of an iterative 
problem 
little 
27. A narrow strip of paper 1 unit long is placed 

insight to obtain an exact solution. 

along a 
that its ends are at 0 and 1. The paper 
right end over left, 
so that its ends 

number line so 
is folded in half, 

are now at 0 and i. Next, 

it is folded in half again, 
this 

so that its ends are at ! and t. 

We continue 
right-over-left and left­

folding 

time left end over right, 
2.32 shows this process. 
Figure 
the paper in half, 
alternating 
over-right. If we could continue 
that the ends of the 
is this point that we want to find. 

(a) Let x1 correspond 

paper would converge 

to a point. It 

to the left-hand 

end of the paper 

indefini

tely, it is clear 

the 

values 

(b) Find two linear 

and x2 to the right-hand end. Make a table with 
of [x1, x2] and plot the 
first six values 
correspond­
ing points on x1, x2 coordinate 
axes. 
equations 
and x1 = cx2 +  d that determine 
at each iteration. 
of the endpoints 
sponding 
on your coordinate 
lines 
that this diagram would 
Gauss-Seidel method to the 
system 
tions 
you have found. (Your diagram 
-
ble Figure 

of the form x2 = ax1 + b 
the  new 
Draw the corre­
axes and show 
the 
of linear 
equa­
should resem 

2.27 on page 
126.) 
to decimal representation, 
method 
applying the 
Gauss-Seidel 
the point to which the ends 
verging 

(c) Switching 

result from applying 

of the paper 

continue 

to within 
(d) Solve the system 

0.001 accuracy. 
of equations 

and compare 

exactly 

are con­

your answers. 

to approximate 

again, 

28. An ant is standing 

tely. 

on a number line at point A. It 
around. Then it 
around 

walks halfway to point B and turns 
walks halfway back to point A, turns 
to do this 
and walks halfway to point B. It continues 
indefini
The ant's walk is made up of a sequence of overlap
ping line 
the left-hand endpoints of these 
their 

Let point A be at 0 and point B be at 1 .  
of 
the positions 
segments and x2 

right-hand endpoints. (Thus, we begin with 

x1 =  0 and x2 = t. Then we have x1 =!and x2 = t, 
(a) Make a table 
of [x1, x2] and 

and so on.) Figure 2.33 shows the 
walk. 

Let x1 record 

with the first 

segments. 

six values 

­

start of the ant's 

plot the correspond
axes. 

ing points on x1, x2 coordinate 

linear 

(b) Find two 

andx1 = cx2 + dthatdetermine the new
valuesofthe 
endpoints 

of the form x2 = ax1 + b 
Draw the corresponding 

at each iteration. 

equations 

axes and show that this 

on your coordinate 

lines 
gram would result from applying 
method to 
found. (Your diagram should resemble 
on page 

dia­
the Gauss-Seidel 
you have 

the system of 
linear 

equations 

(c) Switching 

to decimal representation, 

126.) 
the Gauss-Seidel method 
to which x1 and x2 are converging 

continue 

to approxim

applying 
the values 
within 

ate 
to 

Figure 2.27 

(d) Solve the system 

0.001 accuracy. 
of equations 

exactly 

and comp
are 

your answers. Interpret your results. 

Equations 

0 

of Linear 

1 3 4   Chapter 2 Systems 
I !"tf 0 l 4  2 
I 3 4 
� 
) 
0 l 
l 
3 4 
4  2 
� 
_5 
0 l  l 5 
3 4 
4  2 8 
� 

I ! I 
........----.. 
l 2 D 
0 l  l  3 
4  2  4 

0 

Figure 2 . 3 2  
Folding 

a strip 

of paper  The ant's 

walk 

Figure 2 . 3 3  

Chapter Review 
Kev Definitions 

and concepts 

augmented 
matrix, 
61,64 
tution, 
back substi
61 
coefficient matrix, 
64 
consis
tent system, 
60 
convergence, 125-126 
divergence, 
127 
elementary 
free variable, 
71 
Gauss-Jordan 
elimina
Gauss-Seidel 
method, 124 

row operations, 
66 
tion, 
73 

elimina

tion, 68-69 

Gaussian 
system, 
homogeneous 
76 
inconsistent 
system, 60 
iterate, 
125 
method, 124 
Jacobi's 
leading 
variable 
equation, 58 
linear 
linearly dependent vectors, 93 
ly independent system 
linear
vectors, 93 

pivot, 66 
rank of a matrix, 72 
Rank Theorem, 
72 
reduced row echelon 
row echelon 
row equivalent 
68 
span of a set of vectors, 90 
set, 90 
spanning 
equations, 59 

matrices, 

(leading 

1), 71-73 

of linear 

form, 73 

form, 65 

1. Mark each of the following 
Review Questions 

(a) Every system 
(b) Every homogeneous 

oflinear 

statements 
true or false: 
equations has 
system 

a solution. 
equations 

oflinear 

oflinear 
than equations, 

has more vari­
equations 
then it has infinite
ly many 

has a solution. 

(c) If a system 

ables 
solutions. 
(d) If a system 

than variables, 

oflinear 

equations 
then it has no solution. 

has more equations 

whether b is in span(a1, . . .  , an) is 

to determining 

whether the system 

where A = [a1 . . .  anl· 

equivalent 

(e) Determining 
[A I b] is consistent, 
(f) In IR3, span( u, v) is always 
(g) In IR3, if nonzero 
(h) In IR3, if a set of vectors 
then they are linear

vectors 

ly independent. 

the origin. 
a plane 
through 
u and v are not 
parallel, 

can be drawn head to tail, 

one after the other 
is formed, then the 

so that a closed 
vectors 
are linear

path (polygon) 
ly dependent. 

2. Find the

(j) If there 

(i) If a set of 

Chapter 

Review 1 3 5  

by 

3  4 

is linear

is line

has the 

vectors 

spanned 

of entries 

multiples 

are more vectors 

mdependent. 

arly dependent. 

the number 
of vectors 

1 1 .  Find the general equation of the plane 

property that no 
of 
two vectors in the set are 
scalar 
one another, 
then the set of vectors 
ly 
independent. 

in a set of vectors 
than 
in each vector, then the set 

[}ndm 
whether [ �], [-�], [ ! ] are linearly 
rnAA of the ma1'U [ i �; _ � 3  2] -3 2 . 
12. ?etermine 
- 3 -2  -2 
IR3 = span(u, v, w) if: 
(a) u� [}� [Hw� [:] 
(b) u� [-}� [-nw� [-:] 
(a) The reduced 
(c) The system 
(e) (a) and (b) are both true, 

3w + 8x -18y + z 
= 35 
w + 3x - 7y + z = 10 
system 
2x + 3y = 4 
x + 2y = 3 

dent vectors 
let A = [ a1 a2 a3]. Which of the following 
are true? 

of A is 13. 
solution 
[A I b] has a unique 
for any 

w + 2x -4y = 1 1  

x + y -2z = 4 
x + 3y- z = 7 
2x + y -Sz = 7 
system 

b in IR3. 
vector 
and (c) are all true. 

row echelon form 
is 3. 

14. Let a1, a2, a3 be linearly indepen

(d) (a), (b), 

(b) The rank of A 

13. Determine 

the linear 

whether 

linear 

6  2 

but not (c). 

3.  Solve the 
system 

linear 

4. Solve the 
linear 

5. Solve 

over 27. 

6. Solve the 

system 
3x + 2y = 1 
x + 4y = 2 

in IR3, and 
statements 

over 25. 

2
augmented 

matrix [ � 2

7. For what value(s) of k is the linear 

system 
with 

k I �] inconsistent? 

8. Find parametric 

equations 

x + 2y + 3z = 4 and Sx + 6y + 7z = 8. 

of intersection 
u and v. 

for the line 

of the planes 
9. Find the 

point of intersection 

of the following 
if 

lines, 

it exists. 

15. Let a1, a2, a3 be linearly dependent vectors 

all zero, 
values 

and let A = [a1 a2 a3]. What are the possible 
of the rank of A? 

in IR3, not 

matrix? 
What is 

the minimum 

rank of a 5 X 3 

16. What is the maximum rank 

of a 5 X 3 
matrix? 
ly independent 
vectors, 

17. Show that if u and v are linear

then so are u + v and u -v. 
18. Show that span(u, v) = span(u, u + v) for any vectors 

matrix 

for a linear 

19. In order 

with augmented 
what must be true about the 

system 
[A I b] to be consistent, 
ranks of A and [A I b]? 

20. Areiliematci= U ! -lnd [i O -:i 

ent? Why or why not? 

row equival

Matrices 

We [Halmos and Kaplansky J share 

3 . 0  Intro d u ction :  M atrices i n  Acti o n  

algebra: 

with matrices 

-Irving 

free, but when the chips 

In Paul Halmos: Celebrating 

a philoso
phy about linear 
we think basisjree, we write 
basis-
are 
down we close the 
office door and 
like fury. 
compute 
Kaplansky 

50 Years of Mathematics 
and F. W Gehring, 
J. H. Ewing 
p. 88 
r-Verlag, 1991, 

In this chapter, 
matrices
help streamline 
calcula
that matrices 
rules 
with them, subject to the 
are not static 
objects, 
matrices 
certain 
types of functions 
These "matrix transforma
algebra 
systems 
augmented 
end of this chapter. 
In this section, 

and will shed new light 
of linear 
equations. 

that "act" 
tions" 

eds. Springe

matrices; we will explore 

we will study matrices 

in their 

own right. 

We have alread

y used 

tions 

-in the form of augmented 

matrices-to record 
systems 
involving 
of their 
properties 
have algebraic 
algebra. Furthermore, 
of matrix 
recording 
information and data; 

own, which enable 

oflinear 

you will observe that 
rather, 

information about and to 

us to 

equations. Now you will see 

calculate 

on vectors, transforming 

they represent 
vectors. 
will begin to play a key role in our study 
of linear 
learned 
and 
on what you have already 
about vectors 
in many forms other 
than 
at the 

some of the many applications 

of matrices 

them into other 

matrices 
arise 

Furthermore, 

we will consider 
vectors. In the process, 

a few simple 

examples 

to illustrate 

how matri­

you will get your first glimpse of "matri

x 

ces can transform 
arithmetic:' 

Consider 

the equations 

We can view 

Y1 = X1 + 2Xz 
Yz =  3x2 
y = [;:] 
by F, then F = [ � �], and we can rewrite 

as describing 

. If we denote 

into the 

equations 

vector 

these 

a transformation of the vector 

the matrix of coefficients 

of the right-hand side 

(1) 

x = [::] 

the transforma
tion as 

tly, y = Fx.  [Think of this expression 

or, more succinc
notation 
dent "variable;' 

y = f(x) you are used to: x is the indepen

and F is the name 

of the "function:'] 

as analogous 

to the functional 

dent "variable" 

here, 

y is the depen­

1 3 6  

Matrices 

in Action 1 3 1  

Section 3.0 Introduction: 
Thus, if x = [ -�], then the Equations ( 1) give 
y1 = -2 + 2 · 1  = 0 [o3] or y = 
y2 =  3 · 1  = 3 
as [ �] = [ � �] [ -�]. 
(a) x =  [�] (b) x = [ _ �] (c)x =[=�J 

Problem 1 Compute Fx for the following 
x: 

this expression 

vectors 

We can write 

y1 and y2), draw the four points determined 

of 

Problem 2 The heads of the 

four vectors 

x in Problem 1 locate the four corners 

A, B, C, and D, 

Draw this square and label 

its corners 

and (d) of Problem 1. 

ing to parts (a), (b), (c), 

of a square in the x1x2 plane. 
correspond
coordinate 
axes (labeled 
On separate 
these 
1. Label 
that the line 
segment AB is transformed 
three 

by Fx in Problem 
assumption 
likewise 
for the other 
resented by A'B'C'D'? 
Problem 3 The center 
C' D'? What algebraic calculation 
Now consider 

the equations 

sides 

of square 

A' B' 

A '  B ', and 

segment 

confirms this? 

into the line 

make the (reasonable) 

points A', B', C', and D'. Let's 
of the square ABCD. What geometric figure is rep­

ABCD is the origin 0 = [ �]. What is the center 
z = [::J. We can abbreviate 

vector 

this 

Z1 = Y1 -Yz 
Zz = - 2yl 
y = [�:] into  the 
G =[ 1 -1] 

- 2  0 

(2) 

a vector 
that transform 
transformation 
as z = Gy, where 

z =  G(Fx). You may recognize 

Problem 4 We are going to find out how G transforms 
this expression 

the figure A'B'C'D'. 
as being analogous 
with which you are familiar.] Call the correspond

y that you computed in Problem 1. [That 
to 
ing 
the figure A"B"C"D" on z1z2 coordinate 
axes. 
for 

Compute Gy for each of the four vectors 
is, compute 
the composition of functions 
points A", B", C", and D", and sketch 
z1 and z2 in terms of x1 and x2• If we denote 
we have z = Hx. Since 

Equations (1) into Equations (2), obtain 
equations 

we also have z = GFx, it is reasona

equations 
by H, then 

Problem 5 By substi

the matrix of these 

ble to write 

tuting 

H =  GF 

Can you see how 
Problem 6 Let's 

do the 

the entries 

of H are rela
above process 

of F and G? 

ted to the entries 
the other 
A* B*C* D*. Then transform 

way around: First 

the 
transform 
lting 
the resu

square ABCD, using G, to obtain figure 
figure, 

A** B**C**D**. [Note: 

using F, to obtain 

Don't worry about the "variables" 

x, 

1 3 8   Chapter 3 Matrices 

of A, B, C, and D into Equations (2) 

substitute 

the coordinates 

about the order 

does this tell you 

in which we perform the transfor­

(l).] AreA **B**C**D** andA"B"C"D" 

and then substi
the same? What 

Simply 
tute the results into Equations 

y, and z here. 
mations F and G? 
Problem 1 Repeat Problem 5 with general matrices 
F = [ill !12], G = [gll g12], and H = [hll h12] 
!21 !22 g21 g22  h21 h22 
That is, if Equations (1) and Equations (2) 
by F and G, 
H = GF. 
(a) F = [ � -� l G = [ � �] 
(c)F=[� �],c=[_� -�] 

of F and G. The result will 
�l G = [� �] 
(b) F = 
-2] G = [2 4 
1 �] 
(d) F = 

Problem 7 may help to speed up the algebraic calcula
differences 

Problem 8 Repeat Problems 1-6 with the following 

matrices. (Your formula from 
tions.) Note any similarities 
or 

find the entries 
for the "product" 

that you think 
are significa

have coefficients 

of H in terms of the 

as specified 

entries 

be a formula 

nt. 

, 

M atrix O p e rations 

Although 
definition 

we have already 
and recording 

encountered 

matrices, 
some facts for future reference. 

we begin by stating 

a formal 

Defi n ition A matrix 
elements, of the matrix. 

is a rectangular 

array 

of numbers called 

the entries, 

or 

s: 

[ 7 ]  

is a descri

of matrice

[ 1  1], 

The following 

are all examples 

1  1  6.9 0  4.4 , 
- 7.3 9  8.5 

[� �l [� �l Ul  [ 51 1.2 - 1  l 

'TT 
- 1  
"m by n") if it has 
m X n (pronounced 
A 1  X m matrix is called 
of sizes 2 X 2, 2 X 3, 3 X 1, 1 X 4, 3 X 3, and 1 X 1, 
by aij· Thus, if A=[� 9 
then a13 = - 1  and a22 = 5. (The notation 

The size of a matrix 
matrix is called 
the examples above 
respect
matrix is called 

a row matrix (or row vector), and an n X  1 
. 
notation to 
refer 
cript 
j is denoted 

m rows and 

5 
Aij is sometimes 
a matrix 

We use double-subs
A in row i and column 

ption of the numbers of rows and columns it has. A 

a column matrix (or column vector)

used interchangea

n columns. Thus, 

are matrices 

to the entries 

therefore 

of a matrix 

A. The 
of 

denote 

ively. 

bly with 

entry 

aij.) We can 
tant to specify the size of A, although 

compactly 

the size will usually 

A by [a;j] (or [a;jlmx n  if it is impor­
be clear from 
the con
text). 

numbers will usually be 

Although 
chosen from the set� of real num-
bers, 
from 

the set C of complex 
from "ll_P' where 

p is prime. 

numbers or 

they may also be taken 

not be­

is a distinction 

lly, there 

matrices 

but we will 
We will, 
between 
and column 

Technica
between row/column 
and vectors, 
labor this distinction. 
however, 
row matrices/vectors 
matrices/vectors. 
is important-at the very 
least­
for algebraic 
computati
ons, as we 
will demonstrate. 

distinguish 

This distinction 

If the columns 

With this notation, 

If the rows 

of A are A1, A2, . . .  , Am, then we may represent A as 

1 3 9  

A =  

a21 azz 

Operations 

aml am2 
of A are the vectors 
a1, a2, . . .  , an, then we may represent A as 

Section 3.1 Matrix 
a general m X n matrix A has the 
form [ a., a12 
A� []J 
m = n (that is, if A has the 

A  = [ a1 a2 · · · a"] 

of A are a11, aw a33, . . .  , and if 
), then A is called 
are all zero 

same 
se 
A square matrix who
A diagonal 
matrix 
all 
are the same is called a scalar matrix. 
If the scalar on 
the 

a square matrix. 
is called a diagonal matrix. 

is called an 

identity matrix. 

of rows as columns

The diagonal entries 
number 
nondiagonal 
entries 
of whose diagona
diagonal 

l entries 
is 1, the scalar matrix 

For example, let 

A =  [ 2 

5 
4 
- 1  
entries 
with diagonal 

The diagonal 
2 X 2 
trix. Then X n identity 
Since 
can and should 
many of the convent
way) to 
. 

matrices

we can view 

Two matrices 

m = r and n = s and aij = b;j for all i and j. 

are equal. 

entries 

0 
6 
0 

of A are 2 and 4, but A is not square; 
entries 
3 and 5; C is a diagonal 
by In (or simply 
as generalizations 

matrix is denoted 
matrices 

ma­
I if its size 
od). 

matrix; D is a 3 X 3 identity 
of vectors 

B is a square matrix of size 

is understo

be thought of as being made up of both row and column 

ions and operations 

for vectors 

carry 

through 

(in an obvious 

(and, indeed, 
matrices 
vectors), 

are equal if they have the same size and if their 
Thus, if A  = [aijlmx n  and B  = [bijlrxs' 

ing 
then A  =  B if and only if 

correspond

Neither A nor B 
Bare 2 X 2 
and d =3. 

3 

,  and C = 

matrices 

can be equal 

to C (no matter 

what the values 

and C is 2 X 3. However, A = B 

5 3 y 
of x and y), since 

the matrices  OJ [2  0  x] 
R � I 1  4 3 ]  •nd c � [r 

if and only if a = 2, b = 0, c = 5, 
4 

the matrices 

A and 

Example 3 . 1  

Consider 

Example 3 . 2  

Consider 

1 4 0   Chapter 3 Matrices 

the fact that R 

Despite 
and C have the same entries 
and C is 3 X 1. (If we read R and C aloud, 
R is 1  X  3 
) Thus, our distinction be
"one, four, 
three:'
matrices
/vectors 
is an important one. 

in the same order, 

they both sound the 

R of- C since 
same: 
and column 

tween row matrices/

vectors 

a n d  scalar M u ltiolicalion 

Matrix Addilion 
Generalizing 

from vector 

[a;) and B =  [

b;) are m X n 

the corresp
onding 

by adding 

their 
matrices, 
entries. 
Thus, 

addition, 

we define matrix addition 

component

If A = 
wise. 

sum A + Bis them X n matrix obtained 

[We could equally 
that each 
column 
rows) of A and B.] If A and Bare not the same 

well have 
defined 
(or row) of A  + B is the sum 
size, 

A + B in terms of vector 

of the correspond
(or 
then A + B is not defined. 

addition 

ing columns 

by specifying 

Example 3 . 3  Let 

A  = [ 1  4 OJ B = [- 3  

-2 6  5 ,  3  0 
2 

Then 

,  and C = 

- 1] [4  3] 
-�] 

2  1 

but neither 

A + C nor B + 

C is defined. 

The componentwise definition 

of scalar multiplication will come as no 

surprise. 

If 
multiple cA is the m X n 
matrix 

A is an m X n 
obtained 

matrix and c is a scalar, then the scalar 

by multiplying 
each entry 

of A by c. More formally, 

we have 

[In terms of vectors, 
cA is c times 
the corresp

onding 

column 

(or row) of A.] 

we could  equivalently 

stipulate 

that each column 

(or row) of 

Example 3 . 4  

For matrix 

A in Example 

3.3, 

[ 2  8 

-4 12 

2A = 

-4 

-6 -�J 4 

as -A and 
this fact to define the difference 

The matrix ( -l)A is written 
can use 
then 

called the negative 
of two matrices: 

of A. As with vectors, 
are the same size, 

If A and B 

we 

A  - B = A+ (-B) 

Example 3 . 5  

A and B in Example 

3.3, 

Operations 

1 4 1  

Section 3.1 Matrix 
0 -�] 3 

1 

6 �] 
by 0 (or 

and denoted 
that if A is any matrix and 

For matrices 

A - B=[ l -2 
0 is the zero 

matrix of the 

4 
6 

A matrix all of whose entries 

are zero is called a zero matrix 

Omxn if it is important to specify its size). It should 
same size, then 

be clear 

and 

A +O =A =O + A  

A -A =  0 =-A+ A 

Mathematicians 
are sometimes 
like 
Lewis Carroll's 
Humpty 
Dumpty: 
"When I use a word;' 
Humpty 
Dumpty said, "it means just what 
I choose it to mean-neither 
more 
nor less" 
ing Glass). 

(from Through the Look­

M alrix M ultiplication 
The Introduction 
analogous 
to the com
The definition 
Problems 5 and 7 in Section 3.0. 
the definition 
multiplication, 
there is 
definition. 
Of course, 
in a componen
fashion; 
is not as 

"natural" 

twise 

in Section 3.0 suggested 

that there 

is a "product" of matrices 
that is 

position of functions. We now make this notion 

more precise. 

we are about to give generalizes what you should 

Unlike 

the definitions 

of the product of two matrices 
to stop us from defining 
nothing 
unfortuna
tely such a definition 

have discovered in 
of matrix addition 
and scalar 
is not a componen
a product of matrices 
and 
has few applications 

twise 

as the one we now give. 

Defi n ition If A is an m x n matrix 

and B is an n x r matrix, 

matrix. 

The (i, j)  entry 

then the product 
of the product is computed 

as 

C  = AB is an m  X  r 
follows: 

•  Notice 

Remarks 

same size. 

that A and B need not be the 

umns of A must be the same as the 
and AB in order, we can see at 
over, 
we can predict the 
number of rows of AB is the same as the 
columns of AB is the same as the 

whether this requiremen
size of the product before doing any calcula

However, 
number of rows of B. If we write 

the number of col­
the sizes of 
A, B, 
More­
t is satisfied. 
since 
the 
tions, 
number of rows of A, while the number of 

number of columns of 

B, as shown below: 

a glance 

B  AB 
A 
mXn nXr mXr 
Size of AB 

1 4 2   Chapter 3 Matrices 

•  The formula 

it is. It says that 
the (i, j) entry 
and the jth 
column 
of B: 

Example 3 . 6  

Compute AB if 

Notice 
on each 
agree 
mation 

that, in the expression 
ab term in the sum are always 
and increase 
notation: 

and indeed 

the "inner subscripts" always 

subscripts" 

clearly 

i and j whereas 

for the entries of 

if we write ciJ using sum­

ai1 ai2  ain 
aml am2  amn 

the product looks like a dot product, 
of the matrix AB is the dot product of the ith row of A 

b21  b2j  b2r 
bnl bnj  bnr 

a11 a12  aln [b"  bl)  b,,: 
ciJ = ai1 b11 + ai2b21 + · · · + ainbn)' the "outer 
from 1 to n. We see this pattern 
Ci)= �aikbkj 
-�] and B = [-: -� - 1  2 
C14 = l(- 1) +3(1) + (- 1)(6) =-4 

the product AB is defined 
by taking 
row of the product C = AB 
columns 
Thus, 

C11 = 1(-4) + 3(5)  + (- 1)(- 1) = 12 
C12 = 1(0)  + 3(-2) + (- 1)(2) = - 8  
C13 = 1(3)  + 3(- 1) + (- 1)(0) = 0 

is computed 
of B in turn. 

3 
- 1  
0 

3 
- 1  

n 
k�I 

and will be a 

the dot 

-2 

A =  [ 1 

Solulion Since A is 2 X  3 
and B is 3 X  4, 
2 X  4 matrix. 
The first 
product of the first row of A with each of the 

The second 
with each of the columns 

row of C is computed 
of B in turn

: 

by taking 

the dot product of the second 
row of A 

Thus, the product matrix is given by 

Cz1 = (-2)(-4) + (- 1)(5)  + (1)(- 1) = 2 
C22 = ( -2) ( 0) + ( - 1) ( -2) + (1) ( 2) = 4 
Cz3 = ( -2) ( 3) + ( - 1) ( - 1) + (1) ( 0) = - 5 
c24= (-2)(- l) +(- l)(l)  + (1)(6)  =7 
4 - 5  -�] 

[12 

-8 0 

AB= 2 
practice, 
of the details 
with matrix capabili

(With a little 
writing 
calculator 

out all 

you should be able to do 

as we have done here. 
ties or a computer 

tions 

calcula

mentally 
these 
For more complicated 
examples, a 
algebra 

is prefera

system 

without 

ble.) 4 

Section 3.1 Matrix 

Operations 

1 4 3  

Before we go further, we will consider 

two examples 

that justify our chosen 

definition 

of matrix multiplication. 

Example 3 . 1  

to go shopping 

Bert are planning 

Ann and 
to buy some apples, oranges, 
what they intend 
their 
shopping at 

are given in Table 3.2. 
each of the two 

and grapefruit, 
to buy. There are two fruit markets 

How much will 

for fruit for the 

prices 

markets? 

but in differing 

amounts. Table 3.1 lists 
nearby-Sam's and Theo's-and 
it cost Ann and Bert to do 
their 

next week. 

They each want 

Table 3 . 1  

Table 3 . 2  

Ann 
Bert 

Apples Grapefruit 
10 
5 

6 
4 

3 
8 

Oranges 

Apple 
Grapefrui
t 
Orange 

Sam's  Theo's 
$0.10 
$0.40 
$0.10 

$0.15 
$0.30 
$0.20 

Solution If Ann shops 

she will spend 

at Sam's, 
6(0.10) + 3(0.40) 

+ 10(0.10) = $2.80 

If she 

shops at Theo's, she will spend 

6(0.15) + 3(0.30) + 10(0.20) = $3.80 

Bert will spend 

at Sam's and 

4(0.10) + 8(0.40) + 5(0.10) = $4.10 

4(0.15) + 8(0.30) + 5(0.20) = $4.00 
Ann will shop at 
The "dot product form" of these ca

Sam's while Bert goes to Theo's.) 
lculations 

suggests 

at Theo's. (Presumably, 

that matrix multiplication 

is at work here. 
price 

matrix 

P, we have 

If we organize 

matrix 
D and a 

the given information into a demand 

4  8  [0.10 0.15] 
D = [6  3 
4  8 ] [0.10 0.15] [  ] 
[6  3 

and  P = 0.40 0.30 
0.10 0.20 
the product 

- 2.80 3.80 

0.40 0.30 -
0.10 0.20 

to computing 

4.10 4.00 

10 
5 

above are equivalent 

DP= 

The calcula

tions 

Thus, the product matrix DP tells 
each store 

(Table 

3.3). 

us how much each person's purchases 
will cost at 

Table 3 . 3  

Ann 
Bert 

Sam's 
$2.80 
$4.10 

Theo's 
$3.80 
$4.00 

1 4 4   Chapter 3 Matrices 

Example 3 . 8  

Consider 

the linear 
system 

X1 - 2X2 + 3X3 = 5 
-X1 + 3X2 + X3 = 1 
2x1 - x2 + 4x3 = 14 
Observe that the left-hand side 
arises 

from the matrix product 

(1) 

so the system 

( 1) can be written 

as 

or Ax = b, where A is the coefficient matrix, x is the 
b is the (column) 

of constant terms. 

vector 

(column) vector 

of variabl
es, and 

You should 

have no difficulty 

form Ax= b. In fact, the notation 
is just shortha
nd for the matrix equation 
mendously useful way of expressing 
it often 

a system 

seeing that every linear 
[A I b] for the augmented 
Ax = b. This 

in the 
can be written 
matrix of a linear 
system 
form will prove to be a tre­

system 

of linear 

equations, 

and we will exploit 

from here on. 
Combining 

this insight with 

and only ifb is a linear 
There is another 

ful: Multiplication 
"reproduce" a column 
or row of a matrix. 
products Ae3 and e2A, with the unit vectors 
make sense. Thus, 

the 

0  5 - 1  

and consider 

so that the products 

can be used to "pick 

that Ax = b has a solution if 

we see 
of the columns 
of A. 

Theorem 2.4, 
combination 
fact about matrix operations 
of a matrix by a standard 

that will also prove to be quite 
use­
out" or 

unit vector 
Let A = 
e3 and e2 chosen 

[4  2  l] 
5 -�J 
5  [ _ �] and e2A = [ 0 l ]  [ � 
= [O  5 - 1] 
and ej an n X  1 standard 
e; a 1  X m standard 

us the third 
result 

of A and e2A gives 

unit vector, 

as a theorem. 

us the second 

column 

row of A. 

2 

2 

Notice 
We record 

that Ae3 gives 

the general 

Let A be an m  X n matrix, 

Then 
unit vector. 
ith row of A and 
a. e; A is the 
jth column 
of A. 
b. Aej is the 

Theorem 3 . 1  

Operations 

1 4 5  

Proof We prove (b) and leave 
of A, then the 

41. If a1, . . .  , an are the columns 

proving 

product Aej can be written 

(a) as Exercise 

Section 3.1 Matrix 
Aej = Oa1 + Oa2 + · · · + laj + · · · + Oan = aj 
calculation: [ a1 1  
Ae. = } 
. 

a21 

0 

aml 

0 

We could also prove (b) by direct 

since 

the 1 in ej is the jth 

entry. 

Partitioned Matrices 
It will often be convenient 
smaller submatrices. 
can partition it into 
arising 
ticularly those 

a matrix as being composed of a number of 
into a 
we 
par­

By introducing vertical and horizontal lines 
blocks. 
way to partition 
in certain applications. For example, 

There is a natural 

matrix, 
many matrices, 

consider 
the matrix 

to regard 

It seems natural 

to partition 

A =  

0  2 
1  0 
- 1  
3 
0  1 
0 
1  4 
0  0 
0 
0  1 
0  0 
7 
0  0  0  7  2 

A as 
1  0  0 
i 1 3 
0  1  0 

! 2 - 1  
-�-----�-----�-L�--------�-
o 0  0 ! 1  7 

0  0  0!7 2 

[� �] 

B is 3 X 2, 0 is the 2 X 3 zero matrix, and 

C is 2 X 2. 

matrix, 

identity 

When matrices 

where I is the 3 X 3 
In this way, we can view A as a 2 X 2 matrix 
are being multiplied, 
viewing 
ing structur
large and have many blocks of zer
matrices is 

them as partitioned 

up computa
os. It turns 

es, but it often 

matrices. 

speeds 

is often 

whose entries 
there 

are themselves 
an advantage 
Not only does this frequently 

matrices. 
to be gained 
by 
reveal 
underly­
when the matrices 

are 
out that the multiplication 

especially 

tion, 

of partitioned 

rise to a different way 

just like ordinary 

We begin by conside

matrix multiplication. 

Suppose A is m  X n and B is n X r, so the product AB exists. If we partition 

as B = [b 1 : b2 : . . . 

the product of two matrice

of partitioned 

ring some specia

terms of its column 

of viewing 

s. Each gives 

: br] ,  then 

vectors, 

matrice

l cases 

B in 

s. 

1 4 6   Chapter 3 Matrices 

Example 3 . 9  

If 

This result 
The form on the 

is an immediate consequence of the definition 

of matrix multiplication. 

right 

is called 

the matrix-

column representation of the product. 

then 

A=[� 3 -1 �] [ 4 -1 i 
and  B = �  � 
-1  and Ab2 = [� 3 :J[-�] � [ _�] 
Ab = [1 I  Q  3 :t] � ['�] 
-1 
[13 : 5] 
e, AB = [Ab1 :Ah2] =  .  . (Checkbyordinarymatrixmultiplica
� 

Remark Observe that  the 

matrix-column representation 
of the columns 

us to 
of AB allows 
entries 

of A with 

combination 

2:-2 

tion.) 

Therefor

write each column 
from B as the coef

of AB as a linear 
ficients. For example, 

3 -1 

(See Exercises 

Suppose A is m X n and B is n X r, so the 

23 and 26.) 

terms of its row 
as 

vectors, 

product AB exists. If we partition 

A in 

then 

Once again, this result 
The form on the right 

is a direct conseq
uence of the 
is called 

definition 
of matrix 
the row-matrix representation 
of the 

product. 

multipl

ication. 

Example 3 . 10 

Using the row-matrix representa
tion, 

compute AB for the matrices 

in Example 

3.9. 

Solution We compute 

A,B � [ 1  3 2 ][: -�] � [13  5 

141 

Operations 

Section 3.1 Matrix 
and A2B = [ 0  -1 [4 - 1] l ]  �  � 
�-] as before. 

[2 -2] 

Therefore 

AB = [-���-] = [-�-�--------

'  A2B  2 -2 ' 

The definition 

of the matrix product AB uses the natural 

of A into 
rows 
and B into columns; this form might well be called the row-column representation of 
the product. We can also partition 
and B into rows; this form is called 
the column-row representation of the 

A into columns 

product. 

partition 

In this case, 

we have 

so 

(2) 

Notice 
dividual 

term a;B; is the product of an m  X  1 and a 1  X r matrix. Thus, 
each a;B; is an m  X r 

that the 
terms are matrices, 
Let's 

is that the in­
make sure that 
Each 

a dot product expansion; 

sum resembles 

the difference 

not scalars. 

this makes sense. 

matrix-the same size as AB. The products a;B; are called 
called the outer product 

expansion 
of AB. 

outer product

s, and (2) is 

Example 3 . 1 1  

Compute the outer 

product expansion 

of AB for the matrices 

in Example 

3.9. 

Solution We have 

The outer 

products are 

and 

1 4 8   Chapter 3 Matrices 

(Observe that computing 
table.) Therefore, 

each outer 
the outer prod

ication 

a multipl

like filling in 

product is exactly 

uct expansion 
of AB is 

0  - 1  -�J + [: 0] = [ 13 5] = AB 
- 1] + [ 3 
0  2 -2 4 

We will make use 

of the outer 

discuss 

the Spectral Theorem 

product 
and the singular 

partitions 

Each of the foregoing 

is a specia
trix A is said to be partitioned 
if horizontal 
subdividing 
called 
cks. 
are its blo
matrix whose entries 

A into submatrices 

blocks. 

expansion 

in Chapters 

5 and 7 when we 
vely. 
value decomposition, 
respecti
l case of partitioning 
in general. 
A ma­
and vertica
have been introduced, 
Partitioning allows 

A to be written 

l lines 

as a 

For example, 

are partitioned 

A =  0  0 li4 0 

1  0  0 

0  0 i 1  3 
and B = ---------=�-_!_} ___ } __ :_}_ 
4 3 ! 1  2 
! 2 - 1  
_ 
________________ .. ____________ 
- 1  2 ! 2  1 
1 0 ! 0  0 i 2 
0 1 i 0  0 i 3 
have the block structures [Bll 

0  0  0  1  7 
0  0  0  7  2 
matrices

and B = B21 
by scalars block by block. 
can be multiplied 

and have been partitioned 

matrices 

. They 

If two matrices 
are the same size 
that they can be added and multiplied 
with suitable 
fact that, 
next example 
illustrates 

partitioning, 
this process. 

is clear 
in the same way, it 
is the 
Less obvious 
The 
as well. 

blockwise 

Their 

the matrices 

are matrices, 

A21 A22 B21 B22 B23 

and B a 2 X 3 matrix. 

If we ignore 
be a 2 X 2 matrix 

A and B above. 
Consider 
entries 
then A appears to 
product should thus be a 2 X 3 matrix given by 

A21B11 + A22B21 A21B12 + A22B22 A21B13 + A22B23 

AB= [ A11 A12][B11 B12 B13] 
[ A11B11 + A,2B21 A11B12 + A12B22 AllB13 + A,2B23] 
- 1] [6 3  =  0 
A,,B,, + A ,,B,, � I,B,, + A  ,,I, � B,, + A" � [-: �i + [� - 5  4 

But all of the products in this calcula
make sure that they are all defined. 
since the 
in the 
block multiplication. 

A and B are said to be partitioned conformably for 
indicated 

the case, 
numbers of columns in the blocks of A (3 and 2) match the numbers of rows 

matrix products, 
so we need to 
indeed 

tion are actually 
A quick check reveals 

us the product AB in partitioned 

blocks of B. The matrices 

calculations 
gives 

Carrying out the 

that this is 

0  5 

form: 

Example 3 . 12 

for the moment the fact that their 

Section 3.1 Matrix 

Operations 

1 4 9  

(When some of the blocks are zero matrices or 
these 
blocks of AB are similar. 

can be done quite quickly.) 

Check that the 
result 

calcula

matrices, 

as is the case 
identity 
here, 
The calculations 
for the 

other five 

tions 

is 

6 2:1  2  2 

0 5 i 2  1 12 
5 - 5  i 3  3 ' 9 
' 
' 
---------
7-r-0----o-T-i3 
7 2 i 0  0 i 20 

�  (Observe that the block 

Check that you obtain 

in the upper-left corner 
the same answer 

by multiplying 

is the result 

of our calcula
A by B in the usual 

tions 
way. 

above.) 

M atrix Powers 
When A and B are two n X n matrices, 
their 
A special case 
to define 

A k as 

occurs when A = B. It makes sense 

product AB will also be an n X n matrix. 

to define A 2 = AA and, in general, 

� k factors 

Ak = AA · · ·  A 

Before making too many assumpt

to define A 0 = Iw 
. Thus, A 1 = A, and it is convenient 
ask ourselves 

if k is a positive integer
matrix powers behave like powers of real numbers. The following 
immediately 
of 
the corresp

we have just given and are 

from the definitions 

for powers of real numbers. 

properties 

we should 

onding 

ions, 

properties follow 

to what extent 

the matrix analogues 

If A is a square matrix 
1. ArAs = Ar+s 
2. (Ar) s = A'" 

and r and s are nonnega

tive integers, 
then 

In Section 3.3, we will 
powers. 

extend 

the definition 

and properties to include 

negative 

integer 

Example 3 . 13 

(a)IfA =[� �], then 
A2 = [� �][� �] [� 

2 , 

and, in gene
ral, 

�] [! :J 

2  2  1 

2] A3 = A2A =  [2  2][1 
2n-l] foralln 2: 1 

2n-1 

The  above 

statemen
infinite collection 

t can be proved by mathematical 

induction, 

of statements, 

one for each natural 

number n. (Appendix B gives 

since 
it is an 
a 

1 5 0   Chapter 3 Matrices 

brief 
holds 

review 

of mathematica

for n = 1. In this case, 
[21-1 
21-JJ 21-1  [� �J = A  

A1= 21-1 

l induction

.) The basis 

step is to 

as required. 

The induction 

hypothesis 
is to 

assume 

that 

prove that the formula 

for some integer k 2: 1. The induction 
k + 1. Using the definition 

step is to 

of matrix powers and the 

prove that the 
formula holds 
hypothesis, 

induction 

we compute 

for n = 

�J 

Thus, the formula 

(b) If B = [01 

we find 

and 

= 

holds 

princi

2(k+J)-1 

induction. 

then B2 = 

ple of mathematical 

[2(k+J)-J 
for all n 2: 1 by the 
-1J [o -1J[o -1J [-1 
0J. Continuing, 
o' 1 o 1 o  o 
-1 
B3 = B2B = [-1 oJ[o -1J 0 -1 1 0 [ _ � �J 
B4 = B3B = [ 0 lJ [O -lJ = [l OJ 
[o -1J [-1 oJ [ o lJ [1 oJ [o -1J 
1 0 ' 0 -1 ' -1 0 ' 0 1 ' 1 0 ' 

sequence of powers of B repeats in a cycle 
of four: 

Thus, B5 = B, and the 

The Transpose of  a M atrix 
Thus far, all of the matrix 
real numbers, although 
tion has no 

such analogue. 

operations 

we have defined 

are analogous 

to operations on 

they may not always 

behave in the sa

me way. The next opera­

-1 0 1  0  0 1 

Section 3.1 Matrix 
The transpose of an m x n matrix A is the 

n x m  matrix AT 

Operations 

1 5 1  

by interchanging 

the rows and columns 

of A. That is, the 
of 

ith column 

Definition 
obtained 
AT is the ith row 

of A for all i. 

Example 3 . 14  Let 

C = [ 5 - 1 2 ]  
A  = [ � � � l B = [: � l and 

Then their 

transposes are 

The transpose 

uct of two vectors 

is sometimes 
in terms of matrix mul

ternative definition 
of the 
tion. 

used to give an al

tiplica
If 

dot prod­

then 

l [ �: j 

u  . . .  u 
2  n . 
vn 

A useful alternative 

definition 

of the transpose is given componen

twise: 

(A T)ij = Aji for all i and j 

In words, 
the entry 
column 
i of A. 

in row i and column j of AT is the same as the entry 

in row 
j and 

The transpose is also used to define a very important 

type of square matrix: 

a 

symmetric 

matrix. 

Definition 
its own transpose. 

A square matrix 

A is symmetric 

if AT= A-that is, if A is equal 

to 

Example 3 . 15  Let 

and B = [ 1 2] - 1  3 

1 5 2   Chapter 3 Matrices 

Then A is symmetric, 
since 

AT = A; but B is not symmetric, 
since 

BT = [ � -�] * B. 4 

its 

Figure 3 . 1  
A symmetric 

matrix 

A symmetric 

matrix has the property that it is its own "mirror image" across 

Figure 3.1 illustrates 

main diagonal. 
ing shapes represent equal 
arbitrary. 

entries; 

this property for a 3 X 3 matrix. The correspond­
line) 
are 
the diagonal 

entries 

(those 

dashed 

on the 

A componen
algebraic 

definition 
the "reflection" 

of a symmetric 

property. 

description of 

twise 

matrix is also 

useful. 

It is simply 
the 

A square matrix A is symmetric 

if and only 

if Aij = Aji for all i and j. 

Let 

3 . 1  

..  I Exercises 
A =  [ _� �l B = [� -2 
[ 0 - 3] 
, E = (4 2 ] ,  F =[-�] 
D = _2 l 

1-16, compute the indicat

ed matrices (if 

In Exercises 
possibl
e). 
1. A  + 2D 
3.B -C 
5.AB 
7. D + BC 
9. E(AF) 
11. FE 
13. BTCT -(CB)T  14. DA -AD 
15. A3 
16. U2 -D)2 
17. Give an example 

2. 3D - 2A 
4. c -BT 
6.BD 
8.BBT 
10. F(DF) 
12.EF 

2 X 2 matrix A such 

of a nonzero 

18. Let A = [ � �]. Find 2 X 2 matrices 
that A2 = 0. 
thatAB = ACbutB *- C. 

B and C such 

2  c � [� :J 

19. A factory 
gizmos, 
houses 
uct shipped to each warehouse is given by the matrix 

manufactures 
three 
and widgets) and ships 
for storage. The 

number of units 

them to two ware­

products (doohickies, 

of each prod­

75] 100 
[200 

A =  150 
100 

alphabetical 

into a matrix 

and $1.00. 

of product i sent to 

per widget. 

125 
aij is the number of units 
(where 
warehouse j and the products are taken in 
order). The cost of shipping 
by truck 
$2.00 
by train 
costs 
tion to 
shipping 
truck and 

one unit of each product 
is $1.50 per doohickey, $1.00 per gizmo, 
and 
The corresponding 
unit costs 
to ship 
these 
are $1.75, $1.50, 
Organize 
Band then use matrix multiplica­
show how the factory 

its products to each 
by train. 
20. Referring to 
of distributing 
each product but varies 
distances 
involved. 
one unit 
from warehouse 1 and $1.00 to distribute 
into a matrix 
from warehouse 2. Organize 
C and then use matrix multiplication 
to compute the 
total 

Exercise 19, suppose that the unit cost 
by warehouse because of the 
to distribute 

It costs $0.75 
these 

can compare 
the cost of 
of the two 

the products to stores 

cost of distri

each product. 

is the same for 

buting 

one unit 

costs 

warehouses by 

Section 3.1 Matrix 

Operations 

1 5 3  

6 

of A. 

- 1  1 

0  0  Oi4 

of AB as a linear 

write 
rows of B. 

combination 

2x1 +  x2 -5x3 = 4 

system of linear 
equa­

In Exercises 

of the product to 
of the 

to write 
the columns 

X1 - Xz  = - 2  
Xz +  x3 = - 1  

In Exercises 
tions 

each row of AB as a linear combination 

the given 
of the form Ax = b. 
as a matrix equation 

matrix-column representation of the product 
of 
each column 

21-22, write 
21. X1 -2X2 + 3X3 = 0 
22. -X1  + 2X3 = 1 
23-28, let 
A =  H 
0 -�i 0 - 1  
and  B =  u 
3 :J - 1  
23. Use the 
24. Use the row-matrix representation 
25. Compute the outer 
26. Use the matrix-column representation of the 
27. Use the row-matrix representation 
28. Compute the outer 
29 and 30, assume that the product 
29. Prove that 
30. Prove that if the rows of A are linear
31-34, compute AB by block 
31.A�[�Hi�J n� o of1 0  0: 1 

to write 
of BA as a linear 
the columns of B. 

if the columns 
of B are linear
then so are the columns 
of AB. 

In Exercises 
using the 
ioning. 

(a) Compute A2, A3, . • .  , A7• 
(b) What is A2015? Why? 

0 �]. B � +!t-� -2 : 3  2 
32.A = 
4 5  [ o : 1 ol 
[2  3 
33.A � [! �L! J· 
34· A  = [L:jH]' B = 
35. LetA  = [ O  1]. 
36. Let B = [ 0 -0]. Find, 
37. Let A  = [ � �].Find a formula for An (n 2 1) and 
38. LetA  = 
- sine]. 
[costJ 
costJ [cosW 
- sinW]. 
n [cosntJ - sinntJ] c 
(a)Showthat A2= . smW 
cosW 
(b) Prove, by mathematical 
39. In each of the following, 
u= j -i 
(c) aij = (i -lY  ((i + j -1)7T) 
that satisfies 
(a) aij = ( - l)i+j (b)  a
(d) aiJ = sin 
40. In each of the following, 
{ i + j if i ::; j  { 1 
if Ii -jl ::::: 1 
{ 1 if 6 ::; i + j ::; 8 
if Ii -jl >  1 
(c) a = 11  0 otherwise 
41. Prove Theorem 3.l(a). 

the given condition
0 if i > j (b)  aij = 
0 

that satisfies 
(a) aij = 

[ 2  3: ol 

In Exercises 
sense. 

indicat
1 -1:0  0 

find the 
the given condition

induction, 
1or n 2 1 

4 X 4 matrix 
A = [ a;j] 
: 

multiplication, 

write 
A. 
rows of 

each row of BA as a linear 

combination 
of the 

combination 

ly dependent, 

product expansion 
of AB. 

product expansion 

verify your formula 

product 
of 

so are the rows of AB. 

ly dependent, then 

mathematical 

ed partit

: 
- 1  1: 0 

each column 

AB makes 

of the product to 

induction. 

sin ntJ cos ntJ 

using 

B2015. 

that 

v2  v2 

[aij] 

of BA. 

A = 

sintJ 

: 

4 

: 

with justification, 

find the 6 X 6 matrix A = 

1 5 4   Chapter 3 Matrices 

Matrix Algebra 

that of vectors. 

arithmetic 

of matrices 
to addition 

In some ways, the 
any surprises 
with respect 
none. This will allow 
familiar 
ning sets, 

several 
to matrices 
with from our work with vectors. In particul
and linear 

generalizes 
and scalar multiplication, 

us to extend 

independence 
carry 
operations, 

we are already 

concepts that 
ar, linear 

combina
no difficulty. 
such as matrix multiplication, 

over to matrices with 
that vec­
to behave like multi­

tions, 
span­

We do not exp
ect 
there 
are 

and indeed 

not expect matrix multiplication 
we can 

in fact, it 
and prove some of the main properties 

In this 
does not. 
of matrix operations 

prove that it does; 

However, 

matrices 

have other 
tors do not possess. 
We should 
plication of real numbers unless 
section, 
and begin 

we summarize 

to develop 

an algebra 
. 

of matrices

Prooerlies of Addilion 
All of the algebraic 
(Theorem 1.1) carry 
ties in the next theorem. 

and scalar M u lliolicalion 

properties of addition 
over to matrices

and scalar multiplication 
proper­

ess, we summarize these 

. For completen

for vectors 

Theorem 3 . 2  

Algebraic 

Properties 

of Matrix Addition 

and Scalar Multiplication 

of the 

same size 

and let 

c and d be scalars. Then 

Let A, B, and C be matrices 
a. A +  B = B + A  
b. (A + B) + C = A  + (B + C) 

c. A +  0 = A 
d. A +  ( -A)= 0 
f. (c + d )A  = cA + dA 

e.  c (A + B) = cA +  cB 

g. c (dA) = (cd )A 
h. IA= A 

Commutativity 
Associativity 

Distributivity 
Distributivity 

properties are direct 
and are 
valid 

left as exer
here, and you should 

analogues 
cises. 

of the corresponding proofs 

Likewise, the comments following 

The proofs of these 

properties 

of the vector 
Theorem 1.1 are equally 
properties 
and see Exercises 

have no difficulty 

using these 
(Review Example 
1.5 
scalar multiplica­

to perform algebraic manipulations 

with matrices. 

17 and 18 at the end of this section.) 
The associativity 
without 

us to unambiguo
usly combine 
If A, B, and C are matrices 

property allows 

tion and addition 

parentheses. 

(2A + 3B) -C = 2A + (3B -C) 
2A + 3B -C. Generally, 

if A1, A2, •.. , Ak are matri­

and so 
ces of 

we can simply 
the same size and c 1, c2, . . .  , ck are scalars, we may form the linear 

then, 

write 

combination 

of the 

same size, then 

c1A1 + c2A2 + · · · 

+ ckAk 

We will refer 
ask and 

answer 

to c1, c2, . . .  , ck as the coefficients of the linear combina
We can 
now 

tion. 

questions 

about linear 

combinations 
of matrices

. 

Example 3 . 16 

Algebra 1 5 5  

Section 3.2 Matrix 
LetA1 = [ _� �lA2 = [� �land A3 = [� �]. 
(a) IsB = [� �] a linearcombination
(b) Is C = [� !] a linear 

combination 
3? 

of A1, A2, and A

ofA1, A2, andA3? 

Solution 

(a) We want to find scalars c1, c2, and c3 such that c1A1 + c2A2 + c3A3 = B. Thus, 

The left-hand side of this equation can be rewritten 

as 

Comparing 
equations: 

entries 

and using the definition 

of matrix equality, we have four linear 

C2 +  c3 = 1 
C2 +  C3 = 1 

C1  +  c3 =  4 
- cl  +  c3 =  2 

Gauss-Jordan 

elimination 

easily gives [ � � � :j----+ [� 0 � - �j 
-1 0 1 2  0 0 1 
0  1 0 0 0  0 

3 

3 = 3. Thus, A1 -2A2 + 3A3 = B, which can 

�  (check this!), so c1 = 1, c2 = - 2, and c

be easily checked. 
(b) This time we want to solve 

Proceeding as in part (a), we obtain 

the linear 

system 

c2 +  C3 = 1 

C1  +  c3 =  2 
- c1  +  c3 =  3 

C2 +  C3 = 4 

1 5 6   Chapter 3 Matrices 

Row reduction 
gives 

We need 
this case, 

go no further: The last row implies 
combination 
C is not a linear 
of Ai, A2, and A3• 

that there 

is no 

solution. 

Therefore, 

in 

as "O, 1, - 1, O," which corresponds 

if we simply "straightened 

out" 

Remark Observe that the 

augmented 
we are given. If we read the entries 

columns 

of the 

matrix contain 
of each matrix from left 
of 

the entries 
to right 
in the columns 

appear 

we get the order 

in which the entries 

of the matrices 
and top to bottom, 
the augmented 
to the first 
column 
the given matrices 
the same system 
of linear 

matrix. 

For example, we read Ai 
It is as 

matrix. 

of the augmented 
into column 

equations 

ended up with exactly 

vectors. Thus, we would have 

as in part (a) if we had asked 

{] a linemombillation 

of [-l}[ � l and [ '.} 

from now on. In Chapter 6, we will 

such parallels rep

eatedly 

We will encounter 
explore 

them in more detail. 

We can 

define the span of a set of matrices 

to be the set of 

all linear 

combinations 

of the matrices. 

Example 3 . 11 

Describe 

the span of the 

matrices 

linear combination 

of 

Ai, A2, and A3 in Example 
simply 

to write out a general 

3.16. 

Solulion One way to do this is 
Ai, A2, and A

3• Thus, 

�] 
matrix[; :J is in span(Ai, A2, A3). From the representa­

representation of a plane). But suppose we 

to the parametric 

(which is analogous 
want to know when the 
tion above, 

we know that it is when 

for some choice 
whose left-hand side is exactly 

of scalars Ci, c2, c3. This gives 

the same as in 

rise to a system 

of linear 

equations 

Example 

3.16 but whose right-hand side 

Section 3.2 Matrix 

Algebra 151 

is general. 

The augmented 

matrix of this system 

is 

sists 

The only 

row, where clearly 

and row reduction 

comes from the last 

restriction 
have a solution. 

we 
Thus, the span of A 1, A2, and A
3 con-

0 !x -tJ l 0 -!x -tJ + w 
produces [-l � : fl �  [� 0 0 0 
1  !x + h 0 w - z  
__.  (Check this carefully.) 
must have w - z = 0 in order to 
of all matrices [; : ] for which w = z. That is, span (A 1, A2, A3) = { [; : ] } . 4 
of A[1,
that B = [� �] is a linear 
a]nd A3, since 
A1, A2, ••• , Ak of the same size are linearly independent 
trivial one: c1 = c2 = ·  ·  · = ck = 0. If there 
then A1, A2, •.. , Ak are called linearly dependent. 

y form (take w = 1, x = 4, and y = 2), but C = 3 4 cannot 
form (1 i= 4). 

3.16, we would have seen 
combination 
A2,
1
2 

immediately 
the necessar
combina

tion of A1, A2, and A3, since 
independence 

Nole Ifwe had known this before attempting 

. We say that matrices 
if the only 

(1) 
coefficients 
that satisfy 

also makes sense for matrices

it does not have the proper 

is the 
(1), 

are nontrivial 

be a linear 

solution of the 

Example 

Linear 

equation 

it has 

Example 3 . 18 

Determine 
independent. 

whether 

the matrices 

A1,  A2,  and  A

3 in Example 

3.16 are linear

ly 

c1A1 +  c2A2 +  c3A3 = 0. Writing 

out the 

Solulion We want to solve the 
matrices, 

equation 

we have 

This  time 
we get a homogeneous 
in Examples 
3.16 and 3.17. (Are you starting 
reduces 
matrix row 

linear 

system 

to give 

whose left-hand side is the  same as 

to spot a pattern 

yet?) The augmented 

1 5 8   Chapter 3 Matrices 

Example 3 . 19 

Consider 

the matrices 

Thus, c1 = c2 = c3 = 0, and we conclude that 
indepen
dent. 

the matrices 

A1, A2, and A3 are linearly 

of M alrix M ulliolicalion 
a new operation, 

such as matrix 

Prooenies 
Whenever we encounter 
be careful 
behaved like multiplica
are some significa

nt differences. 

not to 

assume too 

much about 

it. It would be nice if matrix 

multiplication 
in many respects 
it does, 

there 

tion of real numbers. Although 

multipl

ication, 

we must 

4 

A = [ _ � _ �] and B = [ � �] 

gives 

Multiplying 

AB = [ _ � _ �] [ � �] [ _: _ �] and BA = [ � �] [ _ � _ �] 
Thus, AB -=fa BA. So, in contrast 
not commutative- the order 
that A = 0 (unlike 
A2 = 0 does not imply 

[� �] 
check that A2 = [� �] (do so!). So, for matrices, 

of real numbers, matrix multiplica­
in a product matters! 

the situation for real numbers, where the 

to multiplication 

It is easy to 

of the factors 

tion is 

equation x2 = 0 has only x = 0 

as a solution). 

the equation 

However 

gloomy things 

might appear after 

the last example, 

the situation is not 

bad at all-you 
yourself 

really 
remind 
properties 

of matrix multiplication. 

just need to get used to working with 

and to const
antly 
that they are not numbers. The next theorem summarizes 
the main 

matrices 

Theorem 3 . 3  

Properties 

of Matrix Multiplication 

(whose 

sizes 

are such that the indicated 
operations can 

and let k be a scalar. Then 

Let A, B, and C be matrices 
be performed) 
a. A(BC) = (AB)C 
b. A(B + C) =AB + AC 
c. (A + B)C =AC+ BC 
d. k(AB) = (kA)B = A(kB) 
e. ImA = A = 

Aln if A is m X n 

Associativity 
Left distrib
Right 

distrib

utivity 

utivity 

Multiplicative 

identity 

Proof We prove (b) and  half 
Section 

3.6. The remaining 

of (e). We defer the proof of property (a) until 

properties 

are considered 

in the exercises. 

(b) To prove A(B + C) = AB +AC, we let the rows of A be denoted 
columns of 
tion is defined 

Band C by bj and cf Then the jth column of 

by A; and the 
B + C is bj + cj (since addi­

componentwise), and thus 

Algebra 1 5 9  

Section 3.2 Matrix 
= A; . bj + A; . CJ 
[A(B +  C)J u = A;· (bj + c) 

= (AB);1 + (AC);1 
= (AB+ AC);j 
have A(B + C) = AB + AC. 
identity 

this is true for all i and j, we must 

Since 
(e) To proveAin = A, we note that the 

matrix In can be column-partitioned 

as 

where e; is 

a standard unit vector. Therefore, 

Ain =  [Ae1: Ae2: • • ·: Aen l  
=  [ a1 : az : · · · : an] 

= A  

by Theorem 3 .1 (b). 

We can use 

these 

resembles 

multiplication 

properties 
to further 
of real numbers. 

explore 

how closely matrix multiplication 

Example 3 . 2 0  

If A and B are square matrices 

of the same size, 

is (A + B)2 = A 2 + 2AB + B2? 

Solution Using properties 
(A  +  B)2 = (A  +  B)(A  +  B) 

of matrix multiplication, 
we compute 

= (A  +  B)A  + (A  + B)B 
= A 2 + BA + AB + B2 

by left distrib

utivity 

2. Subtracting 

utivity 
(A + B)2 = A2 + 2AB + B2 if and 
only if A2 + BA + AB + B2 = A2 + 
A 2 and B2 from both sides 
BA = AB. Thus, (A + B) 2 = A 2 + 2AB + B2 if and 
only if A 
Can you find 

(Can you give an example 

of such a pair of matrices? 

Therefore, 
2AB + B
AB from both sides 
and B commute. 
two matrices 

that do not satisfy this 

BA + AB = 

property?) 

gives 

gives 

distrib

by right 

2AB. Subtracting 

Properties of the Transpose 

Theorem 3 . 4  

Properties 

of the Transpose 

sizes 

are such that the indicated 

operations can be 

(whose 

and let 

k be a scalar. Then 

Let A and B be matrices 
performed) 
a. (AT)T = A  
c. (kAl = k (AT) 
e. (Arl = (AT) r for all nonnega

b. (A + B)T = AT+ BT 
d. (AB)T = BTAT 

tive integers 

r 

1 6 0   Chapter 3 Matrices 

generalized to sums and 

clear 

( e) is a good exercise 

(a)-(c) are intuitively 

Proof Properties 
30). Proving 
property 
We will 
pected that (AB)T = ATBT might be true?] 

in mathematical 
it is not what you might have expected. 

if A ism X n and B is n X r, then BT is r X n and AT is n X m. Thus, the product 

and straightforward 
31). 
have sus­

[Would you 

induction 

prove (d), since 

to prove (see Exercise 

First, 

(see Exercise 

and is 

BT AT is defined 
have the same size. 

r X m. Since 
We must now prove that their 
We denote the 
ith row 
conventions, 
we see that 

of a matrix X by 

Using these 

AB is m X r, (AB) Tis r X m, and so (AB) T and BT AT 
corresponding 

row;(X) and its jth 

entries 
column 

are equal. 
by col/X). 

[ (AB)T] ;j = (AB)ji 

= row/A) · col;(B) 
= col/AT) · row;(BT) 
= row;(BT)· col/AT) = [BTAT]ij 

(Note that we have used the definition 
transpose, and the fact that the dot product is 
trary, this result implies 

that (AB) T = BT AT. 

of matrix multiplication, 

the definition 
of the 

commutative.) Since 

i and j are arbi­

Remark Properties 

products of finitely many matrices: 

(b) and (d) of Theorem 

+ Akf = A[ + Af + ·  ·  ·+ A[ and (A1A2 • · · Akf 

3.4 can be 

= A[- · · AfA[ 

(A1 + A2 + ·  ·  · 

that the sizes 
You are asked to prove these 

assuming 
formed. 
and 33. 

of the matrices 

are such that all of the operations 

facts by mathematica

l induction 

can be per­
in Exerc
ises 32 

Let 

ThenAT = [� We have 

matrix. 

4 ' 
3] so A  + A

5 : ] , a symmetric 
BT= H �l 

A  = [� !] and B = [� - 1  �] 3 
T = [ 2 
BBT = [� - 1  �i[-r n � [': l:J 
H :J[� �i � n 2 �] 

10 
3 
even though 

mmetric, 

- 1  
3 

B is not 

BTB = 

3 

so 

and 

Thus, both BBT and BTB are sy
that AA 

T and AT A are also symmetric.) 

even square! (Check 

Example 3 . 2 1  

Section 3.2 Matrix 

Algebra 1 6 1  

The next theorem says that the results of Example 

3.21 are true in general. 

Theorem 3 . 5  

a. If A is a square 
b.  For 

matrix, 

then A + AT is a symmetric 

matrix. 

any matrix A, AA T and AT A are symmetric 

matrices. 

Proof We prove (a) and leave 
(A  + A Tf 
of the transpose 
to its own transpose and so, by definition, 

(using properties 
A + AT is equal 

proving 
(b) as Exercise 34. We simply 
= AT + (A Tf = AT + A  = A  + A

commutativity 

and the 

of matrix addition). Thus, 
is symmetric. 

check that 

T 

3 . 2  

I Exercises 
1-4, solve the equation 
A = [ � ! ] and B  = [ -� �]. 
1. X -2A + 3B = 0 
2. 2X = A  -B 

In Exercises 

3. 2(A  + 2B) = 3X 
4. 2(A  - B + X

) =  3(X - A) 

for X, given 

that 

9-12, find the general form of the span of the 

3.17. 

ed matrices, as in Example 

In Exercises 
indicat
9. span(A1, A2) in Exercise 
10. span(A1, A2, A3) in Exercise 
1 1 .  span(A1, A2, A3) in Exercise 
12. span(A1, A2, A3, A4) in Exercise 

6 
7 

8 

5 

-4 

of the 

combination 

B as a linear 

In Exercises 
5-8, write 
other 

matrices, if possible. 

5. B  = [� :l A1 = [ _� �l 
Az= [� �] 
A  = [l I  Q �l 
Az= [� -1] 0 , 
6. B = [ 2 
�l 
�] 
A3 = [� 
- 1] 0 , 
�l A = [l I  Q 0 
7. B = [� [- 1  
- 2  -n A, �[� 
2 �l A3 = [� 
0 n 1 
8.B � [: 
A,� [: 
0 - 1  i 0  , 
1 n n 

1 �] 0 

0  A3= 
0 

A  = 
2  0 

0 - 1  

0 
0 

0 

1 

13-16, determine whether 

the given matrices 

In Exercises 
are linearly independent

. 

13. [� !l [� �] 
14. [ � � l [ -� � l [ � �] 
15. [ � �], [� �], [-� -�i, [-� -!] 
I •. [� -� m: ! m� : n 
[-� - 1  �i 

- 1  0 1  1 0 2 

0 0 -4 

4  5 

n X n matrices, 

n X n matrices, 

3  4 

3.3(d). 

on a, b, 

AB = BA if 

3.2(a)-(d). 

A and B, 

on a, b, 

for square matrices 

on a, b, c, and d such that B = 

commutes with both 0 

commutes with every 2 X 2 

and only if (A -B)(A + B) = A2 - B2• 

Prove the half of Theorem 3.3(e) that was not proved 

In Exercises 23
c, and d such that AB = BA. 

1 6 2   Chapter 3 Matrices 
17. Prove Theorem 
18. Prove Theorem 3.2(e)-(h). 
19. Prove Theorem 3.3(c). 
20. Prove Theorem 
21. 
in the text. 22. Prove that, 
-25, if B = [: �],find conditions 
25.A = 
23.A  = [� 1] 24. A  = [ l 
-1 -1] [1 2] 
1 
1 
26. Find conditions 
[1  oo] and [oo 
27. Find conditions 
c, and d such that B = [a  b] 
28. Prove that if AB and 
* 
* 
* 
* 
* 
* 
0 * 
0  0 *  * 
0  0 0 * 
29. Prove that the 
30. Prove Theorem 3.4(a)- (c). 
31. Prove Theorem 3.4(e). 
32. Using induction, prove that for all n 2: 1, 
33. Using induction, prove that for 
all n 2: 1, 
34. Prove Theorem 3.S(b). 

entries marked * are arbitrary. A more formal 
of such a matrix A = [ aij] is that aij = 0 if i > j. 

called upper triangular if all of the en­
the form of an 
matrix is 

A square matrix is 
the main diagonal ar
tries 
upper trian
gular 

(A1 + A2 + · · ·  + A
(A1 A1· · · A

product of two upper triangular 
n X n 

BA are both defined, then 

where the 
definition 

n)r = Af + Af + · · ·  +

matrix. c  d 

BA are both square matrices

is upper triangular. 

n)T =A�· · · AfAf. 

matrices 

e zero. 

below 

Thus, 

AB and 

A�. 

. 

are skew-symmetric? 

if Ar =  -A. 

so is kA 

for any scalar k. 

to show 

then so is A + B. 

matrices 

that if A and Bare 

then AB need not be 

if and only if AB =BA. 

then 
n X n matrix, 

then AB is symmetric 

n X n matrices, 

if A and B are symmetric 

symmetric 
symmetric. 

(b) Prove that if A is a symmetric 

(b) Prove that if A and B are symmetric 

A square matrix is called skew-symmetric 

35. (a) Prove that 
36. (a) Give an example 
37. Which of the following 
(a) [ _� �] (b) [� -�] 
(<) [-: -� -�] (d) [-: � �] 
38. Give a componen
matrix. 39. Prove that the main diagonal 
40. Prove that if A and B are skew-symmetric 
41. If A and Bare sk
42. Prove that 
43. (a) Prove that any square matrix 

ew-symmetric 
is AB skew-symmetric? 
then A - Ar is 

if A is an n X n matrix, 

trix must consist 
entirely 
of zeros. 

of a skew-symmetric 

of a skew-symmetric 

skew-symmetric. 

A can be written 

what conditions 

definition 

matrix and a skew­

matrices, under 

matrices, 

then so is A + B. 

2 X 2 

twise 

n X n 

ma­

denoted by tr(A). That is, 

5 
8 

Theorem 3.5 

(b) Illu,tmte 

matrix. [Hint: Consider 

on its main diagonal and is 

as the sum of a symmetric 
symmetric 
and Exercise 42.] 

The trace of an n X n matrix A = [ aij] is the sum of the en­
tries 

pact (o) fo, the nrnt'ix A � [ � 2 
44. If A and B are n X n matrices, 
45. Prove that if A and B are n X n matrices, 
46. If A is any matrix, 
47. Show that there 

properties of the trace: 
(a) tr(A + B) = tr(A) + tr(B) 
(b) tr(kA) = ktr (A), where k is a scalar 

(AA T) equal? 
matrices 

tr(A) = a11 + a22 +  · · · 

that AB -BA = 12• 

tr(AB) = tr(BA). 

to what is tr 

are no 2 X 2 

A and B such 

+ a"" 

then 

prove the following 

Section 3.3 The Inverse 

of a Matrix 1 6 3  

The Inverse 

of a M atrix 

ption Ax= b of a system 
to solve 

the system. 

By way of analogy

, 

of linear 
equa­

to the matrix descri

and look for ways to use matrix algebra 

the equation ax = b, where a, b, 

we return 

In this section, 
tions 
consider 
to solve for 
must remind 
ing that a * 0, we will reach 

x. We can quickly 
ourselves 

that this is true 

Proceeding 

only if a * 0. 

and x represent real numbers and we 
want 
figure out that we want x = b/a as the solution, 
but we 
the solution by the following 
b 

b 
-(b) =:> -(a) x = -=:> l · x = -=:> x = -
a 
our head and how many 

1 (1 ) b 

a 
properties 
of arith­

more slowly, assum­

sequence of steps: 

we do in 

a 

ax= b =:>-(ax) = 
a 

1 
a 

(This example 
metic and algebra we take for granted!) 

shows how much 

matrix equation Ax= b, what do we need? We 
t A' A = I, an identity 
to 1 /a) such tha
to the requirement 
that a * O), 
(analogous 

matrix 

matrix exists 

sequence of calcula
tions: 

procedure for the 
A' (analogous 

To imitate this 
need to find a matrix 
(analogous to 1). If such a 
then we can do the following 

�  (Why would each 
�  AA' = I. 

Our goal 

of these 

Ax= b =:> A'(Ax) = A 'b =:> (A 'A)x = A 'b =:> Ix = A 'b =:> x = 

A 'b 

ely when we 
A'. In fact, we are going to insist on 

be justified?) 
can find such a matrix 
precis
determine 
more: We want not only A' A = I 
but also 

steps 
in this section 
is to 

forces A and A' to be square matrices

If A is an n x n matrix, an inverse 

of A is an n x n matrix A' with 

This requirement 

. (Why?) 

a bit 

AA' = I 

invertible. 

Definition 
the property that 

where I = In is the n X n identity 
If A = [� 5]  , [ 3 
, thenA = 3 - 1  
AA ,  = [ 2 5] [ 3 -5] 
1 3 - 1  2 [� 

1 

-5] 2 is an inverse of A, since 
o]andA 'A =[ 3 -5][2 5] 
- 1  2 1 3 

matrix. If such an A' exists, 

then A is called 

and A '  A = I 

Show that the 

following 

matrices 

(a) 0 = [� �]  (b) B = [� �] 

are not invertible: 

Solution 
(a) It is easy to see that the zero 
would be a matrix 
with any 
other 

O' such that 00' = I= O'O. But the product of the zero matrix 
and so 00' could never equal the identi

zero matrix, 

matrix is the 

matrix 0 does not 

ty 

have an inverse. If it did, then 

there 

Example 3 . 2 2  

Example 3 . 2 3  

1 6 4   Chapter 3 Matrices 

is true for n X n matrices 

matrix I. (Notice 

that this proof 

in general.) 

(b) Suppose B has an 

makes no reference 

to the 

size of the matrices 

and so 

inverse B' = [; : ] . The 

[� !][; :J = [� �] 

equation 

BB' = I gives 

from which we get the 
equations 

w  + 2y  = 1 
x  + 2z = 0 
2w + 4y  = O 
2x + 4z = 1 
yields 
from the 
Subtracting twice 
equation 
absurd.  Thus, there 
(Row reduction 
gives 
really 
ible. 

needed 
(In fact, it does not even have an inverse that works on one 

is no solution. 
here.) We deduce 

the  first 

third 

the same result 
that no such matrix B' exists; tha

but is not 

0 = -2, which is clearly 
two!) 4 

t is, B is not invert­
side, 
let alone 

Remarks 

A' (if it exists) 

•  Even though 
•  The examples 
•  We have not ruled 

mutative, 

must satisf

(1) How can we know when 
(2) If a matrix does have an inverse, 

y A' A = AA'. 
two questions, 
a matrix has an inverse? 
how can we find it? 

we have seen that matrix multiplication 

is not, in general, 

com-

above raise 

which we will answer 

in this section: 

one inverse. 

The next 

theorem assures us 

that this cannot 

happen. 

out the possibility 

that a matrix A might have more than 

Proof In mathematics, 
to show that there 
cannot 
A' and A". Then 

way to show that there 

a standard 
be more than one. So, suppose that A has two 

is 
inverses-say, 

is just one of something 

AA' = I = A '  A and AA" = I = A" A 

A '  = A 'I = A'(AA") = (A'A)A" =IA" =A" 

Thus, 
Hence, 

A' =A", and the 

inverse is unique. 

Thanks to this 

theorem, 

we can now 

refer 

From now on, when A is invertible, 
nounced 

"A inverse"). 

we will denote 

to the inverse of 
its (unique) 

an invertible 
matrix. 
inverse by A-1 (pro­

Warning 

Do not be 

tempted 

"division 

by a matrix." Even if there 

1 

to write A -l = A! There is 
were, how on earth could we divide the 
scalar 

no such operation as 
1 by 

Theorem 3 . 6  

If A is an invertible 
matrix, 

then its 

inverse is unique. 

Section 3.3 The Inverse 

of a Matrix 1 6 5  

the matrix A? If you ever feel tempted to 
is multiply 

by its inverse. 

"divide" by a matrix, 

what you really 

want to do 

We can now 

complete 

the analogy 

that we set up at 

the beginning 

of this section. 

Theorem 3 . 1  

If A is an invertible 
Ax= b has the unique solution x =A -lb for any b in !Rn. 

n X n matrix, 

then the system 

of linear 

equations 

given by 

Theorem 3 . 8  

formalizes 

Proof Theorem 3.7 essentially 
of this section. We will 
go through 
asked to prove two 
(In mathematics, 

things: 
such a proof 

more carefully 

We are 
that Ax = b has a solution and that it has only one solution. 
we need only verify that x =A -lb works. We check 

is called an "existence 

and uniqueness" 

the observation we made at the beginning 

To show that a solution exists, 

it again, 

a little 

this time. 

proof.) 

that 

So A -lb satisfies 

To show that this solution is 

and multiplying 
both sides 
implica

the equation Ax= b, and 

hence there is 
suppose y is another 
by A -l on the left, 

A-1(Ay) = A-1b => (A-1A)y = A-1b => Iy = A-1b => y = A-1b 

of the equation 

at least 

unique, 

tions 

this solution. 
Then A y = b, 

solution. 
we obtain the chain of 

Thus, y is the same solution 

as before, 

and therefore 

the solution 

is unique. 

to the questions 

So, returning 
can we tell if a matrix 
ible? 
sufficiently 

We will give 

a general procedure shortly, 
being singled 
out. 

simple to warrant 

we raised 

in the Remarks before Theorem 3.6, how 
is invertible and how can 
we find its inverse when it is invert­
is 

but the situation 

for 2 X  2 matrices 

0, in which case 

if ad - be * 

If A = [: �], then A is invertible 
A-1 - 1 [ d -
[a  b]  1 

b] 
-- times 

inverse of  (when it exists) is thus 
dctA 
and changing 

-e 
a
invertible. 

If ad -be = 0 ,  then A is not 

ad -be is called 

the determinant 

The expression 

ad - be 

e  d 

for the 
interchanging 
the entries 
two entries. In addition 
if and 
A is invertible 
can be defined 
for all 
is no simple 

formula 

on the main 
to giving this 

the matrix obtained 
diagonal 
the signs on 
formula, Theorem 3.8 says that a 2 X  2 

by 

the other 
matrix 
4 that the determina
there 

nt 
although 

true, 

only if det A * 0. We will see in 
square matrices 
. 
for the inverse oflarger 

square matrices

and that this result remains 

Chapter 

Proof Suppose that det A = ad - be * 

[a  b] [ d  - b] = [ad - be 

e  d  -e  a 
ed -de 

-ab+ ba] =[ad - be 0 ] = detA[l OJ 

-eb + da  0  ad - be 0  1 

0. Then 

of A, denoted 

det A. The formula 

1 6 6   Chapter 3 Matrices 

Similarly, 

Since 

det A * 0, we can multiply 

of each equation by 1 / det A to obtain 

0  1 

and 

[Note that we have used property (d) of Theorem 3.3.] 

Thus, the matrix 

- e  a  e  d 

[ d -b] [a b] = detA[l OJ 
[: �](de�A[-� -�]) = [� �] 
(de�A[-� -�])[: �] = [� �] 

both sides 

detA - e 
a
so A is invertible. 
Since 

b] 
1 [ d  -
A-1 --- 1 [ d -b] 

detA -e a 
= 0. We will consider 

satisfies 
the definition 
by Theorem 3.6, we must have 

of an inverse, 

the inverse of A is unique, 

Conversely, assume 

the cases 
and where a = 0. If a * 0, then d = be/a, so the matrix can be written 

separately 
where 
as 

that ad - be 

a * 0 

where k = e/a. In other 
to Example 

3.23(b), we see that if A has an inverse[; ;], then 

row of A is a multiple 

words, the second 

of the first. 

Referring 

and the corresponding system 

of linear 

(Why?) 

kax + kbz = 1 

equations 
aw  + by  = 1 

ax  + bz = 0 
kaw + kby =O 
form  [� �] or [� �] 
case, [ � �] [; ; ]  [ � �] * [ � �]. Similar

(Verif

y this.) 

�  has no solution. 

Thus, 

A is of the 

�  have an inverse. 

In the  first 

Consequently, if ad - be = 0, then A is not invertible

. 

If a = 0, then ad - be = 0 implies 

that be = 0, and therefore 

either 

b or e is 0. 

ly, [ � �] cannot 

Example 3 . 2 4  

Example 3 . 2 5  Use the inverse of the coefficient matrix to solve 

of a Matrix 1 6 1  

Find the 

Section 3.3 The Inverse 
[l 2] [12 
inverses of A = 3 4 and B = 4 -15] -5 , if they exist. 
Solution We have <let A =  1(4) -2 (3) = -2 * 0, so A is invertible, 
A-1 __ 1_[ 4 -2 - 3  -2] = [-:  �] 1 ---2 
hand, <let B = 12 (-5) -(-15) 

On the other 

with 

2 

�  (Check this.) 

system 

the linear 

(4) = 0, so B is not invertible. -+-
x + 2y =  3 
3x +  4y = -2 
the matrix A = [� !l whose inverse we com­

Ax= b has the unique 
to the given system 

solution x = A-1b. 
is 

Solution The coefficient matrix is 
3.24. By Theorem 3.7, 
puted in Example 

Here we have b = [ _ �]; thus, the solution 

x = [-� -tJ [ _�] [-�] 
for 2 X 2 coefficient 
(See Exercise 13.) Furthermore, 

Ax= b via x = A-1b would appear 
faster 

matrix is square 

with cer­
n or Gauss-Jordan 
elimi­
of 

to use Gaussia

matrices 

system 

and invertible, 

to be a good 

the coefficient 
be applied. 

Remark Solving a linear 
Unfortunately, 
except 

method. 
tain special forms, it is almost always 
nation 

Example 3.25 works only when 

to find the solution 

elimination 
methods 

can always 

directly. 

while 

and matrices 

the technique 

Properties of Invertible Malrices 
The following 
matrices

theorem records 

. 

some of the most 

important 

properties 

of invertible 

Theorem 3 . 9  

a.  If 

A is an invertible 
matrix, 

then A -I is invertible 

and 

b. If A is an invertible 

matrix and c is a nonzero 

scalar, 

then cA is an invertible 

matrix and 

(cA)-1 = _l_
A-1 
c 
matrices 
invertible 
of the 
same size, 

c.  If 

A and B are 

then AB is 

invertible 

and 

1 6 8   Chapter 3 Matrices 

d. If A is an invertible 
matrix, 

then AT is invertible 

and 

e. If A is an invertible 
matrix, 

then An is invertible 

for all nonnegative 
inte­

gers n and 

(A")-1 = (A-1)" 

14 and 15. 

in Exercises 

properties 
(b) and (d) to be 

Proof We will prove properties 
proven 
(a) To show that A -i is invertible, 
we must argue that there 
A-1x = I= XA-1 

(a), (c), and (e), leaving 

X such that 
of X, so A -l is invertible 
X such that 
that (A-1)-1 = A. 
(AB)X = I= X(AB) 
tuting B-1A -l for X works. We check that 

equations 
are unique, 
is a matrix 

in place 
this means 

But A certainly 
satisfies 
these 
inverse of A-1. Since 
inverses 
( c) Here we must 

The claim 

show that there 

is that substi

is a matrix 

and A is an 

ly, (B-1A -l) (AB) = I 

parentheses. Similar

where we have used associativity 
�  (check!), so AB is invertible 

to shift the 

(e) The basic idea here is easy enough. 

Similarly, 
argument 
a similar 
induction 
is the 

The basis 

For example, when n = 2, 
we have 

A2(A-1)2 = AAA-IA-I = AL4. -I = AA-I = I 

(A -I) 2 A 2 = I. Thus, 
way to carry 
step is when n = 0, in which case we are being asked to prove that A 0 is 

and its inverse is B-1A-1. 
(A -I) 2 is the inverse of A 2. It is not difficult 
r 1 = I, which is clearly 

works for any higher 
out the proof. 

value of n. However, 

mathematical 

and that 

integer 

true. 

to see that 

This is the same as showing that I is invertible 

invertible 

and that 

�  (Why? See Exercise 

16.) 
Now we assume 
that the result 

is true when n = k, where k is a specific 
hypothesis 

nonnega­
that A k is invertible 
and that 

is to assume 

tive integer. 

That is, the induction 

The induction 

(Ak+1)-1 = (A-1)k+1. Now we know from 
step requires 
Ak are both invertible

that we prove that Ak+I is invertible 
and that 
(c) that Ak+1 = AkA is invertible, 
since A 
. Moreover, 

and (by hypothesis) 

(A-1)k+1 = (A-1)kA-1 
= (Ak)-1A-1 
= (AAk)-1 
= (Ak+1)-1 

by the induction 

hypothesis 

by property (c) 

. 

Remarks 

of a Matrix 1 6 9  

tive integers 

ple of mathematical 

for all nonnega
induction

An is invertible 

Therefore, 
princi

n, and (A n)-1 = (A -l) n by the 

Section 3.3 The Inverse 
•  While all of the properties of Theorem 3.9 are useful, 
•  Property 

highli
is also the 
counterexample 
general. 
shoes 
in the reverse order. 
A2, . . .  , An are invertible 

t is easiest 
property, (AB)-1 = B-1A-1, is sometimes 
called 
we put our socks on before our shoes, 

ght. It is perhaps the most important 
to get wrong. 
contrary 

you should 
( c) is the one 
property of matrix 
inverses. 
It 
ise 17, you are asked to give a 
to what we might like, (AB)-1 * A -l B-1 in 
the socks-and­
we take 

If A1, 
many invertible 
and 

to products 
of the same size, 

algebraic 
In Exerc

then A 1A2• • • 

(c) generalizes 

because, although 

The correct 

matrices: 

to show that, 

An is invertible 

of finitely 

them off 

one tha

rule, 

matrices 
(A1A2 ·  · · An)-1 = A;;-1 ·  · · A21A�1 

(See Exercise 

18.) Thus, we can state: 

The inverse of a product of invertible 
the reverse order. 

matrices 

is the product of their 
in 

inverses 

•  Since, 
•  Property (e) allows 

19). In fact, except 

square matrices, 
Exercise 

for  real 

matrix: 

-- * -+ -, we should 

1  1  1 

a +  b a b 

numbers, 

(A +  B)-1 = A-1 + B-1 (and, indeed, 
for special matrices, 
us to 

define negative integer 

there 

not expect that, 
for 
this is not true 
see 
is no formula for (A + B)-1. 

in general; 

powers of an invertible 

Definition 
defined by 

If A is an 

invertible 

matrix and 

then A -n is 
integer, 

n is a positive 

definition, 

With this 
and (AT = A'5, hold for all integers 

it can be shown that the rules 

r ands, provided 

One use of the algebraic 

properties of matrices is 

matrices
attention 

. The next example ill
to the order 

of the matrices 
in the product. 

ustrates the 
process. 

tion, Ar As = A'·+s 
. 

for exponentia
A is invertible
to help solve 
involving 
equations 
Note that we must pay particular 

Example 3 . 2 6  

the following 

Solve 
such that all of the indicated 

matrix equation 
for X (assuming 

that the matrices 

involved 

are 

operations 
are defined): 
A-1(BX)-1 = (A-1B3)2 

110  Chapter 3 Matrices 

Solulion There are many ways to proceed 

here. 

One solution is 

A-1(BX)-1 = (A-1B3)2 ==? ((BX)A)-1 = (A-1B3)2 

=} [((BX)A)-1]-1 = [(A-1B3)2]-1 
==? (BX)A = [ (A-1B3)(A-1B3) J-1 
==? (BX)A = B-3(A-1)-1B-3(A-1)-1 
==? BXA = B-3AB-3A 
==? B-1BXAA-1 = B-1B-3AB-3AA-1 
==? IXI = B-4AB-3I 
==? X = B-4AB-3 
careful 

�  (Can you justif

of (A -1B3) 2• We have 
tion to 

simplif

y the placeme

y each step?) Note the 

use of Theorem 3.9(c) 

and the exp

ansion 

also made liberal use of the associa

tivity 

of matrix multiplica­

nt (or elimina

tion) 

of parentheses. 

Elemen1arv Malrices 
We are going to use 
reduction 
of matrices
insights 

into the nature 

matrix multiplication 
. In the process, 
of invertible 
matrices

. 

to take a different 

perspective 
you will discover many new and important 

on the row 

If 

we find that 

words, 

In other 
multiplying 
rows 2 and 3 of A. What is significa
applying 
the same elementary 
turns 

out that this always 
works. 

A by E (on the left) 

has the 
nt about E? It is simply 

row operation, 

R2 � R3, to the identity 

same effect 

we obtain 
by 
matrix J3. It 

the matrix 

as interchanging 

DefiniliOD 
forming 

An element

ary matrix 
row operation 

is any matrix that can be obtained 
on an identity 

matrix. 

by per­

an elementary 

Since 

there 

are three 

types of elementary 

row operations, 

there 

are three cor­

responding 

types of elementary 
matrices

. Here are some more elementary 

matrices. 

Example 3 . 2 1  Let 

Section 3.3 The Inverse 

of a Matrix 1 1 1  

to 3R2, E2 to R1 � R3, 

a 

matrix 

is performed on 

from the 

identity 

has been 

row operation. 

the corresponding 

matrices, 
For example, 

matrices 
elementary 

left-multiply 
elementary 

I4 by applying 
a 4 X n matrix by one of 
these 
row operation 

A =  a31 a32 a33 
a41 a42 a43 

Each of these 
obtained 
single 
The matrix E1 corresponds 
and E3 to R4 -2R2. Observe that when we 
elementary 
the matrix. 

if  ["" a12 ""] a21 a22 a23 
then  [ au a12 ""] ["" a32 a,,: 
E3A  = [ a,,  a22  a,, l 

' 
3a21 3a22 3a23 ' E2A  = az1 a22 az3 

au  a12  a13 
a31  a32  a33 
a41 -2a21 a42 -2a22 a43 -2a23 

a31 a32 a33  a11 a12 a13 
a41 a42 a43 

a41 a42 a43 

Example 

E1A  = 

and 

row operation 
elementary 

3.27 and Exercises 
on any matrix 

24-30 should 

convince 
can be accomplished 
this fact as a theorem, 

by left-multiplying 
the proof 

of which is omitted. 

you that any elementary 

by a suitable 

We record 

matrix. 

T h e o r e m  3 . 10 

obtained by performing 
Let E be the elementary 
matrix 
row operation 
tion on In-If the same elementary 
is the same as the matrix 
EA. 
the result 

row opera­
an elementary 
is performed on an n X r matrix 

A, 

R e m a r k  From a computational point of view, 

it is not 

a good idea to use el­

ementary matrices to 
However, elementar
matrices 

solution 

and the 

perform 

y matrices 

of systems oflinear 

equations. 

elementar
y row operations-just do 
can provide some valuable 
insights 

them directly. 
into invertible 

We have already 

observed that every 

elementary 

row operation 
to elementary 

matrices 
shows us that 

can be "undone:' 

or "reversed:' 
they are invertible. 

This same observation 

applied 

3 = [ � � �] 
by doing R2 � R3 again. 
�  E1 -l = E1. (Check by showing that E l  = E1E1 = I.) The matrix E2 comes from 4R2, 

0 �] ,  and  E
to R2 � R3, which is undone 

Then E1 corresponds 

- 2  0  1 

0 
0 

0 
4 

Thus, 

1 

Exam p l e  3 . 2 8  Let 

112  Chapter 3 Matrices 

which is undone 

by performing iR2• Thus, 

which can be easily checked. 
tion R3 -2R1, which can be undone 
in this case, 

to the 
Finally, E3 corresponds 
row operation 
by the elementary 

elementary 
R3 + 2R1. So, 

row opera­

it is easy 

(Again, 
in both orders, 

to check this by confirming that 
is I.) 

the product of this matrix and E3, 

Notice 

elementary 

that not only 
matrix of the 

is each 
same type. We record 

elementary 

matrix invertible, 

but its inverse is another 
as the next theorem. 

this finding 

Theorem 3 . 1 1  

matrix is invertible, 

and its inverse is an elementary 
matrix 
of the 

Each elementary 
same type. 

Malrices 

The Fundamen1a1 Theorem ol lnvenible 
We are now in a position to prove one of the main results 
equivalent 
much of linear 
these 
characterizations 
application. 
duction, we will use this theorem a great deal. 

of what it means for a matrix 
theorem, 
As you might expect, given this intro­
Make it your 

to be invertible. 
in the development of 

is connected 
or in their 

rizations 
algebra 

in this book-a set of 
In a sense, 

to this 

characte

either 

friend! 

We refer 

to Theorem 3.12 as the 

first version 

of the Fundamental 

Theorem, 
since 

we will add to it in subsequent chapters. You are reminded 
that, 
of statements 
we mean that, 
ments are either 

about a matrix A are equivalent, 

all true or all false. 

when we say that a set 
for a given A, the state­

Theorem 3 . 12 

Theorem of Invertible 

Matrices: Version 1 

The Fundamental 

Let A be an n X n matrix. 

The following 

statements 

are equivalent: 

a. A is invertible. 
b. Ax = b has a unique solution 
c. Ax = 0 has only the 
d.  The reduced 
e. A is a product of elementar

solution. 
form of A is Iw 
y matrices. 

for every b in !Rn. 

row echelon 

trivial 

Section 3.3 The Inverse 

of a Matrix 113 

Proof We will establish 

the circular 

chain of implica

tions 

the theorem by proving 
(a) =} (b) =} (c) =} (d) =} (e) =} (a) 

(b) =} (c) Assume 

(a)=} (b) We have already 

shown that if A is invertible, 

unique solution x = A-1b for any b in !Rn (Theorem 3.7). 
in particu
has x 
always 

lar, that Ax = 0 has a unique solution. 
x = 0 must be 
= 0 as one solution. 
(c) =} (d) Suppose that Ax= 0 has only the trivial solution. 

solution for any b in !Rn. This implies, 
But a homogeneous 
system 
the solution. 
The corresponding 

Ax = 0 

that Ax= b has a unique 

then Ax = b has the 

So in this case, 

of equations 

system 

is 

+ a1nxn = 0 
a11x1 + a12x2 + · · · 
a11X1 + a21X2 + . . .  + a1nXn = 0 

and we are assuming 

that its solution is 

= O 
= O 

In other words, 
system 

gives 

Gauss-Jordan 

elimination 

applied to 

the augmented 

matrix of the 

[Alo J  = a�, a21  a2n 
["" a12  aln fl [! 0  0 !] 

an] an2  ann 
Thus, the reduced row echelon 

form of A is In-

[Inlo J  

0  1 

that the reduced 
row echelon 
sequence of elementary 

� 

0 

row operations. By Theorem 3.10, 

form of A is In, then A can be 
by an 
is 

elementary 

sequence of 

matrices 

reduced to 
each one 
appropriate 

( d) =} (e) If we assume 
In using a finite 
of these 
elementary 

E1, E2, ••. , Ek (in that order), then we have 

elementary 

matrix. 

If the appropriate 

row operations 

can be achieved 

by left-multiplying 

According to Theorem 3.1 1, these 
so is their 

product, and we have 

E2E1A = In 
Ek· · · 
elementary 

matrices 

are all invertible. 

Therefore, 

each E;-1 is another 

Again, 
A as a product of elementary 

elementary 
matrices, 

matrix, by Theorem 3.1 1, so we have 
as required. 

written 

(e) =} (a) If A is a product of elementary 

matrices, 

then A is invertible, 
since 

elementary 

matrices 

are invertible 

and products of invertible 
matrices 

are invertible. 

114  Chapter 3 Matrices 

Example 3 .2 9  

If possible, 

express 

Solution We row reduce 

. 

matrices

A  = [ � � J as a product of elementary 
A  = [2 3J � [l 3J �· [l 3J 
1  3 2 3 
� [l 0 J � [l OJ =  I 

0 - 3   0  1 2 

A as follows: 

0 - 3  

Thus, the reduced 
Theorem assures 
matrices

row echelon 
us that A is invertible 
E4E3E2E1A = I, where 

form of A is the identity 
and can be written 

. We have 

matrix, so the 
Fundamental 
as a product of elementary 

are the elementary 
used to reduce 
A to I. As in 

corresponding to the four elementary 
the proof 

theorem, 
we have 

matrices 

of the 

row operations 

[O lJ [l OJ [l -lJ [l OJ 
1  0 2 1  0  1  0 

- 3  

as required. 

Remark Because 

the sequence of elementar
neither 
into I 
matrices. (Find a different 

that transforms 
of A as a product of elementar

y row operations 
A 
y 
.) 

A as a product of elementar

is the representation 

is not unique, 

way to express 

y matrices

"Never 
bring 
Act I unless 
the last act:' 

a cannon 
in 
you intend to fire it by 
-Anton Chekhov 

o n  stage 

The Fundamental 

Theorem is surprisi

two of its consequences. 
matrix states that a matrix A is invertible 

consider 
vertible 
AB = I and BA = I are satisfied, 
can cut our work in half! 

if there is 
we need only check one of these 

The first is that, 

ngly powerful. 
although 

its power, we 
To illustrate 
of an in -
the definition 
that both 
a matrix B such 
equations. Thus, we 

Theorem 3 . 13 

Let A be a 
and B = A -1• 
then A is invertible 

square matrix. 

If B is a square matrix such 

that either 

AB = I or 
BA = I, 

BA= I. Consider 
implies 

that x = Ix= 0. Thus, 
x = 0. From the equivalence 

that A is invertible. 
both sides 

Ax= 0. Left-multiplying 

the system 
of (c) and (a) in the Fundamental 

by Ax= 0 has the 
AA - i = I = A 
- i A.) 
- i 

and satisfies 
of BA = I by A -i, we obtain 

multiply 
BAA -i = IA -i =} BI = A -i =} B = A 

(That is, A -1 exists 

the equation 

by B, we 

represented 

Theo­

have 

Proof Suppose 
BAx = BO. This 
unique solution 
rem, we know 

now right-

If we 

(The proof 

in the case of AB = I is left as Exercise 
The next consequence of the Fundamental 

41.) 

method 

of computing 

the inverse of a matrix. 

Theorem is the basis 

for an efficient 

Theorem 3 . 14 

Section 3.3 The Inverse 

of a Matrix 115 

Let A be a square matrix. 
to I, then the same sequence of elementary 

If a sequence of elementary 

row operations 

A 
-i. 
transforms 

reduces 
I into A 

row operations 

by a sequence E1, E2, ••• , Ek of elementary 

to I, then we can achieve  the 

matrices. Therefore, 

by left­
reduction 
we have 

Proof If A is row equivalent 
multiplying 
Ek· · · E2E1A = I. 
ible and A -l = B. Now applying 
I is equivalent 

B = Ek· · · E

Setting 

to left-multiplying 

the same sequence of elementary 
I by Ek· · · E2E1 = B. The result is 

2E1 gives 

BA = I. By Theorem 

3.13, A is invert­
to 
row operations 

Thus, I is transformed 
into A 

same sequence of elementary 

row operations. 

Ek· ·  · E2E1I = BI =  B  = A-1 
-i by the 

Melhod for Compuling 

The Gauss-Jordan 
We can perform row operations 
augmented 
by the Fundamental 
row operations will yield 

Theorem (d) � (a), means that A is invertib

[A II ] .  Theorem 3.14 shows that if A is row equivalent 

on A and I simultaneously by constructing 
to I [which, 

matrix" 

lhe Inverse 

le], then elementary 

a "super­

If A cannot 
not invertible. 

be reduced 

to I, then the 

Fundamental 

Theorem guarante

es us 

that A is 

is to 

is simply 

look at the problem 

Gauss-Jordan 
matrix. 

The procedure just described 
of an n X (n + 1), augmented 
is sufficient, 

n X 2n, instead 
cedure 
for an n X n matrix 
of A must be a two-sided 
inverse 
equation 
matrix 
then this 
of In are the standard unit vectors 
columns 
Since the 
of linear 
equations, 
all with 

elimination 
performed on an 
way to view this pro­
Another 
the matrix 
AX = In 
a right 

of X by x1, ... , Xn' 

A -I as solving 
by the Fundamental 

the columns 
columns 

e1, . . .  , en, we thus have n systems 

inverse.) If we denote 

is equivalent 

coefficient 
matrix 

of X, one at a time. 

equation 

to solving 

of finding 

X. (This 

for the 

A: 

Theorem, since 

Since 

the same sequence of row operations 
form in each case, 
combined 

the augmented 

echelon 
[A I en] ,  can be 

as 

is needed 

to bring 

A to reduced 
systems, [A I e1] , . . .  , 

row 

matrices 

for these 

We now apply row 
taneously solve 

to try to 
reduce 

for the columns 
this use of Gauss-Jordan 

A to In, which, 
of A -i, transforming 
In into A -i. 
elimination 
with three 

We illustrate 

operations 

examples. 

if successful, 
will simul­

Example 3 . 3 0  

Find the inverse of 

if it exists. 

A =[�� -�i 1  3 - 3  

116  Chapter 3 Matrices 

Solulion Gauss-Jordan elimination 

produces 

1 - 2  - 1  0 

1 -2 - 1  0 

3 - 3  0  0 

[: 2 - 1  0 �: 
[A IIJ  2  4 0 
R2-2R, [: 2 - 1   0 �] 
R3-R1  - 2  6 - 2  1 
� 
- 3   I -2 
[: 2 - 1   0 �] 
H:JR, 
-2 
0  - 2  I 2 
[: 2 - 1   0 �: 
R3-R2  1 - 3  1 I 
� 
2 
0  -2 ! 2 3 [: 0  0 9 -2 -:i 
R,+R3 [: 2 0 - 1  I 1] 
R2+3R3 
� 
1 ! 2 
R,-2R2  1 
� 
A-1= - 5   3 [ 9 -t - 5] 

0 - 5  
0 1 - 2  

1 0 - 5  1 

multiplica

tion. 

By Theorem 
3.13, 

Therefore, 

- 2  t 1 
(You should 
we do not need to check that A -i A = I too.) 

check that AA - I  = I by direct 

always 

that we 

Remark Notice 

have used the 

and then creates 

all of the zeros below 

first introduces 
bottom, 
top. This approach 
it easier, 
The answer, 

when working 
of course, 

variant 
the leading 
zeros above the leading 
tions, 
by hand, to create 
will be the same. 

to right 
to left and bottom to 
as we noted in Chapter 2, but you may find 
as you go. 
all of the zeros 

of Gauss-Jordan elimination 
ls, from left 

saves on calcula

in each column 

ls, from right 

and top to 

that 

Example 3 . 3 1  

Find the inverse of 

A =[-! - 1  -:i 

-2 2 - 2  

if it exists. 

At this point, we see that it is not 
on the left-hand side of the augmented 

As the next example ill

p is prime. 

Example 3 . 3 2  

Find the inverse of 

if it exists, 
over "11..3. 
1 We use the 

Solution 

Gauss-Jordan 

in "11..3. 

Solution We proceed 
then trying to manipulate 

as in Example 

Section 3.3 The Inverse 

of a Matrix 111 

[A IIJ  - 1  6 0 1 

there 

3.30, 

to reduce 

matrix to A and 

identity 

possible 

is a row of zeros 

A to I, since 

- 2  2 - 2  0  0 

R3+R1  1 -2 2 1 

0  0 0 - 5   - 3  

adjoining the 

ustrates, everything 

� 
�  0  - 3  2 1 

[A I I] into [I I A-1 ] .  [-: 1 -4 1 0 �] 
R2 +2R1 [: 1 -4 0 �] 
3 -6 1 0 ,,-,., [I 2 - 1   0 �] 
matrix. Consequently, A is not invertible. 4 
A = [� �] 
[A I I] = [� 211 
0  0 �] 
2R1 [� 1 12 �] 
R1+R1 [� 1 12 �] 
R1+2R2 [� 0 10 �] 

same way over "11..P, where 

remembering 

that all calculations 
are 

works the 

method, 

0  0 

1  2 

1  2 

is easy to check that, 

over "11..3, AA -l = I. 
also compute 

A -l using the formula 

� 
� 
� 
�], and it 

Thus  A-1 = [0 

,  2 

2 Since 

matrix, 
Solution 
given in Theorem 3.8. The determina
nt of A is 

A is a 2  X  2 

we can 

detA = 2(0) - 2(2) = - 1  = 2 

in "11..3 (since 2 +  1 
We must be careful 

= O). Thus, 
here, 

A -l exists 

and is given by the formula 

in Theorem 3.8. 

though, 

since the 
formula 

introduces 

the "fraction
" l/det A 

118  Chapter 3 Matrices 

in Z3. We must use multiplicative 
rather 

inverses 

than 

and there 
division. 

Instead 
the equation 2x 
T1 = 2, since 

= 1 in Z3• It is easy to see that x = 2 is the solution 
2(2) = 1 .  The formula 

are no fractions 
of l/det A = 1/2, we use T1; that is, we find the 
for A-1 now becomes 

A-1 = T1[ _� -�] = 2[� �] = [� �] 

with our previous 

solution. 

which agrees 

number x that satisfies 

we want: In Z3, 

3 . 3  

matrix (if it 

3.8. 

In Exercises 
exists) 
using Theorem 

1-10, find the inverse of the given 

6. - 1;V2  l/V2] 1/\/2 

..  I Exercises 
1. [� �]  2. [� -�] 
3. [! !]  4. [ _� �] 
5. [i n  [ l/V2 
9. [� -�] [l/a IO. 1/ c 
-4.2] 2.4  8. [3.55 0.25] 8.52 0.60 
[- 1.5 
7. 0.5 
l/b] l/ d 
11 and 12, solve the given 
3.25. 
13.LetA = [� !lh1 = [;], h2 = [-�J, andb3 = [�]. 

In Exercises 
method of Example 
11. 2X +  y = - 1   12. X1 - X2 = 1 
2x1 + x2 = 2 

(a) Find A-1 and use 

system using the 

it to solve the three 

, where neither 

matrix [A I h1 h2 h3] using 

Ax = h1, Ax = h2, and Ax = h3. 
ducing 
Gauss-Jordan elimina

three 
the augmented 
tion. 
(c) Carefully count the total 
multiplications 
that you performed in (a) and in 
(b). You should discover 
example, one method 

uses fewer operations. 

even for this 2 X 2 

same time 

systems at the 

(h) Solve all 

of individual 

a, b, c, nor d is O 

5x + 3y =  2 

number 

that, 

by row re­

systems 

For larger 
pronounced, 
systems 
do not use 
linear 
systems. 

systems, the difference 

is even more 

and this 

explains 
one of these 

why computer 
methods 
to solve 

14. Prove Theorem 3.9(b). 
15. Prove Theorem 3.9(d). 
16. Prove that the n X n identity 

and 

(h) Under what conditions 

In is invertible 

matrix 
thatln-l = Iw 
17. (a) Give a counterexample 
to show that (AB)-1 * 
A-1B-1 in general. 
on A and B is (AB)-1 = 
A-1B-1? Prove your assertion. 

prove that if A1, A2, ••• , An are invertible 
and (A1A2 • • • An)-1 = A ;;-1 ·  ·  · A21A�1• 
matrices 
is invertible 
19. Give a counterexample 
A-1 + B-1 in general. 
20-23, solve the given 

then the product 
to show that (A + B)-1 * 

matrix equation 

18. By induction, 

same size, 

A 1A2 ·  ·  · An 

of the 

for X. 

of 

as much as possible. 

21. AXB = (BA)2 

.") Assume that all matrices 

(In the words 
"Everything should be made as simple 
as pos­
le. 

In Exercises 
Simplif
y your answers 
Albert 
Einstein, 
sible, but not simpler
20. XA2 = A-1 
22. (A-1X)-1 = A(B-2A)-1 23. ABXA-1B-1 = I+ A 
In Exercises 

24-30, let [: 2 - 1] [1 - 1  �l 
A =  1 � ,  B = � 1 
c � [: 2 - 1] H 2 -: ] 

1 ' D =   - 1  
- 1  

2 - 1  

- 1  

- 1  

are invertib

Section 3.3 The Inverse 

of a Matrix 119 

and AB is 
47. Prove that if A and Bare square matrices 
. 

then both 

A and B are invertible

invertible, 

In Exercises 
inverse of the given matrix (if it exists). 

Gauss-Jordan 

method to find the 

49. [-2  4] 3 - 1  

1 
0 

tary 

Why or why not? 

an elementary 

equation. 

In each case, find an elementary matrix E that satisfies the 
given 
24.EA = B 
27.EC = A  
30. Is there 

26.EA = C 
29.ED = C 

25.EB = A  
28.EC = D 

matrix E such that EA = D? 

31-38, find the inverse of the given elemen

In Exercises 
matrix. 

31. [� �]  32. [� �] 
33. [� �]  34. [ _� �] 
35. [i 0 -�] [: 0 :J 
37. [i 0 n'H 38. [i 0 lH 
39 and 40, find a sequence 
E1, E2, .•. , Ek such that Ek· · · E2E1A = I. Use this 
both A and A - i as products 
39.A = [ _� -�J 40. A =  [� �] 

In Exercises 
matrices 
sequence to write 
matrices. 

42. (a) Prove that if A is invertible 
43. (a) Prove that 

and AB = 0, then 
to show that 
the result in 

0 
of elementary 

(b) Give a counterexample 

if A is invertible 

if A is not invertible. 

3.13 for the case of AB= I. 

part (a) may fail 

41. Prove Theorem 

36. 1 
0 

of element

ary 

B =  0. 

c 
0 

and BA = CA, then 
to show that the result in 

(b) Give a counterexample 

B = C. 
part (a) may 

44. A square matrix 

fail if A is not invertible. 
if A 2 = A. 
idempotent 
A is called 
(The word idempotent 
and potere, 
meaning "same;' 
Thus, something 
that is idemp
power" when squared.) 

:' 
otent has the "same 

comes from the Latin idem, 

(a) Find three 

idempotent 2 X 2 matrices. 

(b) Prove that the only invertible 
n X n 

idempotent 

meaning "to have power

0 - 1  

48-63, use the 

:J -�] 
48. [� 
50. [� 
-� -�] 
52. [: 
� :J 
54. [: 
� �] [ V2 
56. [: 
60. [0
62. [: 
61. [! !] over Z5 
5 OJ 2 4 over Z7 
63. [: 

0 2V2 
V2  0 
0  1 
0  3 

-4V2 
58. 0 
0 

6  1 

1 

matrix is the identi

ty matrix. 

45. Show that if A is a square matrix 

equation 

A2 -2A +I =  0, then A-1 = 2I -A. 

that satisfies the 

46. Prove that if a symmetric 
also. 

inverse is symmetric 

matrix 

is invertible, 
then its 

ioning large square matrices can sometimes make their 

Partit
inverses easier 
to compute, particul
a nice form. In Exercises 
tion that the inverse of a matrix, 
as claimed. (Assume that all inverses 

64-68, verify by 

multiplica­
if partitioned as shown, 
exist as needed.) 

arly if the blocks have 

block 

is 

C(BC)-1 I -C(BC)-1B 
- C(I - BC)-1 I + C(I - BC)-1B 

1 8 0   Chapter 3 Matrices 
65.[� �rl [-(BC)-1 (BC)-1B ] 
66. [� �rl [ (I -BC)-1  -(I - BC)-1B ] 
67. [� �rl 
(BD-1C)-1BD-1 ] 
[ -(BD-1C)-1 
B]-1
p Q] S , where P =(A -BD-1C)-1, 
68.[A
--[R

- D-1c(BD-1c)-l 

C  D
Q = -PBD-1, R = -D-1CP, and S = D-1 
+ D-1CPBD-1 

In Exercises 
can apply one of the formulas from Exercises 
then calculate the inverse using that formula. 

the given matrix so that you 

64-68, and 

0  0 
1  0 
3  1 

69-72, partition 
2  0 �] 
71. [� �: 0 1 

- 1  1 
1 0 

72. 

0 

D-1 - D-1c(BD-1c)-1BD-l 70. The matrix in Exerc
ise 58 

• 

The LU Factorizati

o n  

natural 

(and illuminating) 

Just as it is natural 
other 
tor matrices 
of two or more other matrices is 

number into a product of 
helpful 
frequently 
to fac­
s. Any representation of a matrix as 
a product 
a matrix factorizat

numbers-for 
as products of other 

ion. For example, 

to factor a natural 

example, 30 = 2 · 3 · 5-it is also 
[3 - 1] = [l  0][3 - 1] 

matrice
called 

9 - 5  3  1  0 - 2  

is a matrix factoriza

tion. 

ations 

to say, some factoriz
a matrix 

Needless 
we introduce 
equations 
mentation. 
factoriza
tions. Indeed, 
to it. 
devoted 

factorization 
that arises 
by Gaussian 
is particular
elimination and 
In subsequent chapters, we will encounter 
is a rich one, and entire 

the topic 

to computer 
imple­

ly well suited 
other equally 
useful matrix 
books and courses 

have been 

are more useful than others. In this section, 

in the solution of systems 

of linear 

Consider 

a system 

Our goal is to 

matrix. 
uct of matrices that 
with the same coefficient matrix) 
easily. 

of linear 
show that 
then enable us to 

equations 
Gaussian 
solve 

of the form Ax = b, where A is an n X n 
elimination 
implicitly 
factors 
(and any other 
the given system 

A into a 
prod­
system 

The following 

example 

illustrates 

the basic 

idea. 

Example 3 .3 3   Let 

1 

- 1  

5 

Row reduction 

of A proceeds 

as follows: 

- 1  

5 

1 8 1  

Section 3.4 The LU Factorization 
u (1) 

The three elementary 
echelon 

form U are (in order): 

matrices 

£1,  E2,  E3 that accomplish 

this reduction 

of A to 

0 
1 
0 

0 
1 
0 

Hence, 

Solving for A, we get 

A  = E1-1E2-1E3-1U = 

Thus, 

A can be 
as 

factored 

0 �Ju 1 

0 

-2 

- 2  

0 
1 

[� u 
0 �JU 
0 �m 1 
0 �Ju� LU 
r�[: :  !] 

A =  LU 
(see the exercises 
matrix 

for Section 

where U is an upper triangular 
triangular. That is, L has the form 
lower 

3.2), 

and L is unit 

Alan M. Turing 

was introduced 

The LU factorization 

mathematician 

in 1948 by the great English 
(1912-1954) 
1 (1948), 
pp. 287-308). 

"Roundin
Proces
Mechanics and Applied 

in a paper 
g-off Errors 

Mathematics, 
During 

ses" (Quarterly Journal of 

with zeros 

entitled 

in Matrix 

was 

above and 

ls on the main diagonal. 

The preceding 

example motivates 

the following definition. 

II, Turing 
the 
However, 
'' code. 

in cracking 

"Enigma

World War 
instrumental 
German 
best known for his work in 
he is 
mathematical 
that laid the 
theoretical 
for the 
development 
and the 
modern 
ence. 
intellig

logic 

The "Turing 

that he proposed in 1950 

test" 
is still 

used as one of the benchmarks in 
addressi
question 
ng the 
a computer 
"intelligent:' 

can be considered 

of whether 

groundwork 
of the digital 

Definition 
L is unit lower triangular 
of A. 
field of artificial 

computer 

Let A be a square matrix. 

A factorization 

of A as A  = LU, where 

and U is upper triangular

, is called 

an LU factorization 

•  Observe that the matrix 

Remarks 

row interchanges were needed 
matrices 

A in Example 
in the row reduction 

3.33 had an LU factorization 

because no 

of A. Hence, 

all of the elementary 

that arose were unit lower triangular. 

Thus, L was guaranteed to be unit 

1 8 2   Chapter 3 Matrices 

and products of unit lower triangular 

29 and 30.) 

If a zero had appeared 

inverses 
(See Exercises 
in a pivot 

because 
lower triangular 
also unit lower 
triangular. 

rows to get a nonzero pivot. This would have resulted in 

�  triangular. 
•  The notion 
•  Some books define an LU factorization 

of an LU factoriza
U to be a matrix in row echelon 

trix for which row interchanges 
ry?) 

by simply requiring 

position at any step, 

We will comment 

will be necessa

form. (See Exercises 13 and 14.) 

on this observation below. (Can you find a ma­

of a square matrix A to be any 

we would have had to swap 
L no longer 

being unit lower 

tion can be generalized to 

nonsquare matrices 

matrices 

further 

factor­

ization 

A = LU, where L is lower triangular 

and U is upper triangular. 

are 

The first remark above is essentially a proof of the 

following 
theorem. 

Theorem 3 . 15 

If A is a square matrix that 
row intercha

can be reduced 
tion. 

then A has an LU factoriza

nges, 

to row echelon 

form without 

using 
any 

Example 3 .3 4  

Ax= b, where 

system 

a linear 

consider 

To see why the LU factorization 

is useful, 
has an LU factoriza

tion A  = LU. We can rewrite 
the coefficient matrix 
Ax= b as LUx = b or L(Ux) = b. Ifwe now define y = Ux, then we can solve 
two stages: 
1. Solve 
2. Solve Ux = y for x by back substitutio
Each of these 
ces L and U are both triangular. 

Ly= b for y by forward substitution 
(see Exercises 

systems is straightforward 

illustrates 

The next example 

the method. 

linear 

to solve 

n. 

for x in 

because the coefficient 

matri­

25 and 26 in Section 

2.1). 

the system 

!] to,olveAx � b, whece b � [-: l 

1 

5 

- 2  

- 1  

found that 

- 1  -2 1  0  0  2 

Use an LU factorization 

Solution In Example 3.33, we 

of A  = [ ! 
A  = [ 2 � �i [� -� -�] = LU 
Ly� b fm y � [f :J Thi''' ju,tthe Hnem-'l"tem 
Yi= l,y2 = -4 -2yi = -6,y3 = 9 +Yi+ 2y2 = - 2  

Y1 
2y1 + Y2  -4 
-y1 -2y2 + y3 = 9 
working 

to solve Ax = b (which is 

As outlined above, 

tution (that is, 

Forward substi

from top 

to bottom) yields 

the same as L( Ux) = b 

), we first solve 

Section 3.4 The LU Factorization 

1 8 3  

2x1 + x2 + 3x3 = 

- 3x2 -3x3 = - 6  
2x3 = -2 

and back substitution quickly 

produces 

x2 = 3, and 

X3 = - 1, 
- 3x2 = - 6  + 3x3 = - 9  so that 
2x1 =  1 - x2 -3x3 =  1 so that x1 = t 
· 

the given 
'Y'tem 

Ax � b ;, x � [ _ ! l 

Thmfoce, 

the rn

lution to 

A n  Easv wav to Find L U  Factorizations 
In Example 
Fortunately, 
needing to compute elementary 
A can be 
the case, 

we computed 
3.33, 
L can be computed 
to row echelon 

reduced 
then the entire 

ii>-"'-- row operations 

. 
process 
our 
that 

from the 
row reduction 
at all. Remember 
any row intercha

the matrix L as a product of elementary 
matrices
directly 
without 
that we are assuming 
matrices 
using 
form without 
process 
can be done using 
R; - kR1. (Why do we not need to use  the 
multiplying 
to the scalar k as the multiplier. 
the elementary 

remaining 
a row by a nonzero scalar?) In the operation 

elementary 
R; - kRj, we will refer 
3.33, 

row operation, 

row reduction 

row operations 

that were used were, 

of the  form 

In Example 

only elementary 

nges. If this is 

in order, 

R2 -2R1 
R3 + R1 = R3 - (- l)R1 
R3 + 2R2 = R3 -(-2)R2 

= 2) 
= -1) 
=  -2) 

(multiplier 
(multiplier 
(multiplier 

The multipliers 

are precis

ely the entries 

of L that are below its diagonal

! Indeed, 

and L21 = 2, L31 = - 1, and L32 = -2. Notice 

R; - kR1 has its mul

tiplier 

k placed 

in the (i, j) entry 
of L. 

that the elementary 

row operation 

Example 3 . 3 5  

Find an LU factorization 

of 

A =  r ! 4 ! -��1 

- 9  5 - 2  -4 

3  2 5 - 1  

1 8 4   Chapter 3 Matrices 

Solulion Reducing 

A to row echelon 

4 8 
A=  2 5 
5 - 2  

[j 3 

2 

R3-R1 

form, we 
have 

- 10 R2-2R1  Q 
- 1  R4-(-3)R1 0 

-4] [' 
1 3 -4] 2  2 -2 
- 1! 8  7 
3 -4] 2  2 - 2  
-4 -----+ 0 [; R,-!R2 
R4-4R1 -----+ .. -HJ•,[: -----+ 0 
3 -4] 2  2 - 2  

4 
0 
0 - 1 - 8 

0 1 4 
0  0 -4 

=U 

0 

The first three 
of the first column 

multipliers 
of L. So, thus far, 

are 2, 1, and -3, and these 

go into the subdiagonal 
entries 

The next two multipliers 

we continue 

to fill out L: 

are t and 4, so 

The final multiplier, 

-1, replaces 

the last * in L to give 

Thus, an LU 

of A is 

factorization 

A= [ ! 4 ! -��i [ � 0 � �i [� 2 � =�i 

3  2  5 - 1   1 t  0  0  0  1  4 

-9 5 - 2  -4  - 3  4 -1 1  0  0  0 

easily checked. 

= LU 

-4 

as is 

To illustrate 

Section 3.4 The LU Factorization 

1 8 5  

Remarks 

by column 

these rules, 
consider 

it is important to note that the 

from top to bottom within 

elementary 
row opera­
each column 
(using the 
from left to right. 
the following 

this method, 
tions 
R; -kRj must be performed 
diagonal entry 
as the pivot), and column 
what can go wrong if we do not obey 

•  In applying 
A=[� � �i �' [� � �i � [� -� -�] U 
.,._..  but A * LU. (Check this
•  An alternative 

in L as follows: L32 = 2, L21 = 1 .  We would 
tion of A.) 

L�[i: �] 

! Find a correct LU factoriza
way to construct L is to 

observe that the 
the intermedia

2  2  1 0  0 - 1   0  0 - 1  

This time 
get 

the multipliers 

would be placed 

row reduction: 

obtained 
reduction 
umns of the matrices 

directly from the matrices 
process. 

obtained at 
3.33, 
examine the pivots 
in the row reduction 

multipliers 
can be 
of the 
te steps 
row 
and the corresponding 
col­

The first pivot 

is 2, which occurs in the first column 
that are on or below the diagonal 

of 
by the pivot 

the entries 

this column 

vector 

In Example 
that arise 
1 

- 1  

5 

±[ J [ _�-

u 

produces 

of A. Dividing 
of A1. Dividing 

by the pivot, 

The next pivot 
of this column 

is - 3, which occurs in the second 
vector 

that are on or below the diagonal 

column 

the entries 

we obtain 

The final pivot 
ing the entries 
we obtain 

(which we did not 
of this column 
vector 

need to use) is 2, in the third 

column 

or below the diagonal 

by the pivot, 

of U. Divid­

that are on 

±[J [J 
u -� J 

entries 

vectors 

If we place 

the resulting 

three 

column 

side by side in a matrix, 

we have 

which is exactly L once the above-diagonal 

are filled 

with zeros. 

1 8 6   Chapter 3 Matrices 

In Chapter 2, we remarked that the row echelon 
matrix A has an LU factoriza

form of a matrix is not 
tion A =  LU, then this 

unique. 
factoriza­

However, 
tion is unique. 

if an invert

ible 

Theorem 3 . 1 6  If A is an invertible 

matrix that has an LU factoriza

tion, 

then L and U are unique. 

Proof Suppose A  = LU and A  = L1 U1 are two LU factorizations 
L1 U1, where Land L1 are unit lower 
fact, U and U1 are two (possi

and U and U1 
echelon 

of A. Then LU = 
are upper tria

bly different) row 
forms of A. 

triangular 

ngular. 

By Exercise 30, L1 is invertible. 

Because A is invertible, 

its reduced 

row echelon 

In 

matrix I by the Fundamenta

l Theorem of Invertible 

Matrices. 

is invertible 
also. Ther
Lj1(LU)u-1 = Lj1(L1 U1)u-1 so (Ll1L)(uu-1) = (Ll1L1)(UI u-1) 

to I (why?) and so U 

efore, 

form is an identity 

�  Hence U also row reduces 
�  (Why?) It follows 

Hence, 

(Ll1L)I =  I(UI u-1) so Li1L  =  U1 u-I 

But Lj1L is unit lower triangular 

by Exercise 29, and  U1 u-1 is upper triangular. 

that Lj1L  =  U1 u-1 is both unit lower triangular 
LU factorization 
1 and U =  U1, so the 

The only such matrix is the identity 
that L = L

and upper tri­
so Lj1L  =  I and U1 u-1 = I. It 

of A is unique. 

angular. 
follows 

matrix, 

The pr LU Factorization 

the problem 
We now explore 
row interchanges 
are necessa

of adapting 
ry during 

the LU factorization 
the matrix 

to handle cases 
Consider 

Gaussian 

elimina

tion. 

where 

A straightfo

rward row reduction 
produces 

which is not 
upper triangular 

However, 
form by swapping rows 2 and 3 of B to get 

an upper triangular 

matrix. 

we can easily convert 

this into 

Alternat
matrix 

ively, 

we can swap 

rows 2 and 3 of A first. To 

this end, let P be the elementary 

Section 3.4 The LU Factorization 

181 

Now this handles 

that then reduce 

E be the product of the 

rows 2 and 3, and let 

PA to U (so that E-1 = L is unit lower triangu­

ing to interchanging 
matrices 

correspond
elementary 
lar). Thus EPA= U, so A =  (EP)-1U = P-1E-1U = P-1LU. 
the product P =  Pk· · ·P2P1 of all the row 
P1 is performed 
serve that a permutation matrix arises 
in some order. For example, the following 

only the case of a single row interchange. 
Ob­
and so on). Such a matrix 
matrix 

P1,P2, ••• , Pk (where 

matrix. 
the rows of an identity 

are all permutation matrices: 

P is called a permutation 

interchange 

from permuting 

In general, 
P will be 

matrices 

first, 

[� �]. [: 0 

0 

Fortuna
lations 

tely, 
are needed 
at all! 

the inverse of a permutation matrix is easy to compute; 

in fact, no calcu­

Theorem 3 . 1 1  

If P is a permutation matrix, 

then p-l = PT. 

Proof We must 
column 
is a permutation matrix. 

show that pTp  =  I. 
are both equal 
So 

of P, and these 

But the ith row of pT is the same as the  ith 
to the same standar

e, because 

vector 

d unit 

P 

(prp);; = (ith row of pr)(ith column 

ows that diagonal 

entries 

of pTp are all ls. On the 

This sh
the jth column 
off-diagonal 

of P is a different standard 

unit vector 

of prp is given by 

entry 
(prp)iJ = (ith row of p
matrix, 

r)(jth column 
as we wished to show. 

Hence pT P is an identity 

of P) =  ere' =  e · e' =  0 

of P) =  ere =  e 

· e =  1 
hand, if j i= i, then 
from e-say e'. Thus, a typical 

other 

of A as A = PTLU, where 

Thus, in general, 

we can factor a square matrix A as A = P-1LU = Pr LU. 

Let A be a 

and U is upper triangular, is 

A factorization 

a Pr LU factorization 

square matrix. 
Definition 
P is a permutation matrix, 
L is unit lower triangular, 
of A. 
called 

Find o pT LU fadmizotion 

of A � [: � n 
interchange. [: 0 !] [i 2 !] [i 2 -�l 

A =  2  R1 +-*Rz  0  R,-2R1  0 
- 3  

form. Clearly, we need at least 

Solution First we reduce 

echelon 

A to row 

� 

� 

1 

� 
Rz+-*R3  - 3  
0 

[: 2 -!] 

one row 

Example 3 . 3 6  

1 8 8   Chapter 3 Matrices 

The discussion 

Theorem 3 . 18 

Every square matrix has a PTLU factoriza

We have used two row interchanges 
permutation matrix 

is 

required 

We now find 

PA=  0 
0 

Hence L21 = 2, and 

so 

of PA. 

0 
0 

0 
0 

2 
1 

p =  P2P1 =  0 

an LU factorization 

1  R2-2R1  - 3  
0 
0 

(R1 �  R2 and then R2 �  R3), so the 
[i 0 �m 1 :J [: 1 �] 
-!] u 
[: 1 �m 0 !] [� 2 !] [: 2 
A� P'LU � [� : rn� � :J[i -� -!: 
R1 �  R3 also would have worked, leading 
of A) is T(n) =  n3 /3, the same 

the PTLU factorization 
In 
to 

once P has been determined, 

(multiplications 

a single row interchange 

L and U are unique. 

of operations 

above justifies 

the following 

is not unique. 

theorem. 

and divisions) 

tion. 

Remark Even for an invertible 
matrix, 

3.36, 
Example 
a different 
P. However, 

required 

Operations;' 

"Counting 
the forward 
elimination 
both forward 
and backward substitution 
of n, the n3 /3 term is dominant. From 

phase produces 

and the LU factorization 
advantages: 

are equivalent. 

is very compact because 

of L and U as they are computed. In 

ations 

values 

hardly 

a linear 

factoriza

tion has other 

required 

2.) This is 

then, 
the LU 

an LU factorization 

number 
Ax= busing 

of view, 
However, 

system 
for Gaussian 

for large 
elimination 

elimination. 
surprising 

we can overwrite 
we found that 
Example 

(See the Exploration 
since 

Computational Consider
If A is n X n, then the total 
to solve 
as is 
in Chapter 
the LU factorization 
require 
Therefore, 
this point 
Gaussian 

in =  n3 /3 steps, whereas 
=  n2 /2 steps. 
•  From a storage point of view, the 
A =  [ � -� �i [ � � �i [� -� -�] = LU 
[ � =� -�] 

- 2  5  5 - 1  - 2  1  0  0  2 
as 

of A with the entries 

LU factorization 

the entries 

This can be stored 

- 1  - 2  2 

3.33, 

Section 3.4 The LU Factorization 

1 8 9  

(3,2), 
correspond

(2,3), 
ing 

by the 

(1,2), (1,3), (2,1), (3,1), (2,2), 

other 

placed 

in the 

order (1,1), 

(Check that this works!) 

of A are replaced 

words, the subdiagonal entries 

many linear 
of Example 

with the entries 
(3,3). In 
multipliers. 

tion of A has been computed, 

•  Once an LU factoriza
•  For matrices 
•  For an invertible 

Ax = b 

the vector b each time. 
elimination in solving Ax = b. 

with certain special forms, especially 
off the 
tion. 

systems of the form 
3.34, varying 

entrated 
of an LU factoriza

(so-called "sparse" matrices) conc

of zeros 
that will simplif
is faster 

A, an LU factoriza

tion of A can be 

than Gaussian 

putation 

as we like. 

y the com

matrix 

with a large 

those 
diagonal, 
there 
cases, 
In these 
used to find A-1, 
a 

if necessary. Moreover, this can be done in 
factoriza

tion of A-1. (See Exercises 

15-18.) 

it can be 

number 

such a way 

that it simultaneously yields 

We just need to apply the method 

are methods 
this method 

used to solve as 

Remark If you have a CAS (such as MATLAB) that has the LU factorization 
in, you may 
output. 
to reduce 

between your hand calculations 
most CAS's will automatically 
. (See the Exploration "Partia
discussion 

notice 
This is because 
roundoff 
errors
paper is an extended 

try to 
l Pivoting;' 

in Chapter 
of matrix 

some differences 

in the context 

perform partial 

of such errors 

built 
computer 
pivoting 
2.) Turing's 
tions. 
factoriza

and the 

This section has 

one of 
In subsequent chapters, we will encounter 
other 

served to introduce 

the most useful matrix factoriza
equally 
tions. 

useful factoriza

tions. 

3 . 4  

of A. 

using the given 

1-6, solve the sy

In Exercises 
LU factorization 

I Exercises 
stem Ax = b 
1. A= 
1  OJ[- 2  l] b =[SJ 
[-2 l]  [ 
2  5 - 1  1  0  6 ' 1 
[� -�] = [t �][� -!lb=[�] 
2.A = 
[-! _: =�J H _; �l 
3.A = 
x [� 4 =!], b = [-�1 
[ � =� �1 [ ; 0 �1 
- 1  2  2 -� 0  1 
4.A = 
x [� -� H b � U_ 

0  0 -�  0 

5.A = 

6.A = 

7-12,find an LU factoriza

In Exercises 
7· - 3  - 1  

[ 1  2] [2 -4] 

s. 3  1 

tion of the given 
matrix. 

1 9 0   Chapter 3 Matrices 

­

0 
0 

matrix as a 

13. 0  3 3 

6  9 5  8 

of the given 

0  0 1 0  0 

to nonsquare 

-1 - 2  -9 0 

0 1 
1 0 
0 0 

19-22, write the 

find an LU factorization 
14. 

of LU factorization 

9. [: ; : l 
11.[ � : _; -:i 
12. [-� ! -� �i 4  4 7  3 
[1 c  -�] 
0  0  0 [ 1 2 0 - 1  -�] 

Generalize the definition 
matrices by simply requiring U to be a matrix in row ech
elon form. With this modification, 
of the matrices in Exercises 
13 and 

14. � 1 3 5 

e matrix with an LU factorization 

A = LU, 
e and A -l = u-1 L -1. In 

invertibl
15 and 16,find L-1, u-1, and A-1 for the given 

For an 
both L and U will be invertibl
Exercises 
matrix. 
15. A in Exercise 1  16. A in Exercise 
be computed 
The inverse 
sev­
3.34. 
eral systems 
the method of Example 
For an n X n matrix A, to find its inverse 
we need to solve 
as 
this equation 

AX= In for the n X n matrix X. Writing 
A [ x1 x2 • · · xn]  =  [ e

4 
of a matrix can also 
by solving 
of equations 

that we need to solve n systems 
of linear 
Ax1 =  e1, Ax2 =  e2, . . .  , AXn =  en· Moreover
tion A = LU to solve each one of these 

n], using the matrix-column 
, we 

form of AX, we see 
equations: 
can use the factoriza
systems. 

0  0 
0  0 
0 1 

In Exercises 

22. 0  0  0 1 0 
1 

0  0  0  0 
0  0  0  0 

given permutation 
nge) matrices. 

23.A =  2  24. A =   2 

In Exercises 
product of elementary (row intercha

23-25, find a PTLU factorization 

�] 

19. [: �] 20.[� 0  0 �] 
21.[ 1 0 ;] 1 0  0  0  0 
matrix A. H !] [-; 0 1 
[-; - 1  il 1 
27-28, solve the sy
27. A  = [ � � -� l [ � � � l [ � C : l 
1  - 1   0  0  1 2 -t 1 
x [: 3 �l l � P'LU, b � m 
[! 3 �-[: :J [: 0 :J 
x [� 1 r [ �:i 

1 - 1  
0  1 
26. Prove that 
are exactly 

In Exercises 
factorization 
be rewritten 
using the 
3.34. 

stem Ax= busing the 
given 
ppT = I, PTLUx = b can 
system can then be 
solved 

A = PTLU. Because 
as LUx = Pb. This 

- 1  = PTLU, b = 
0 

method of Example 

In Exercises 
find A -l for the 
given 
Exercises 
17. A in Exerc

to 
method of 
matrix. Compare with the 

17 and 18, use the approach just outlined 

ise 1  18. A in Exercise 4 

n! n X n permutation 

1 e2 · · · e

-2 - 7  3 8 

using 

matrices. 

3 - 3  -6 

15 and 16. 

0  - 1  

there 

28.A = 

25.A = 

1 
0 

- 1  

0 

1 

0 

3 

3 

Section 3.5 Subspaces, 

Basis, 

Dimension, 

and Rank 1 9 1  

29. Prove that a product of unit lower triangular 

matrices 

is unit lower 

triangular. 

30. Prove that every unit 

lower triangular 
matrix is 
and that its inverse is also 

unit lower 

invertible 
triangular. 

tion A = LDU, where L is a unit lower 
An LDU factorization 
of a square matrix A is a factoriza­
triangular matrix, 
D is a diagonal matrix, 
and U is a unit upper triangu-
lar matrix (upper triangular with ls on its diagonal). In 
Exercises 

31and32,find an LDU factorization 

of A. 

tion, 

factoriza

4 
and invertible 
and has 
an LDU 

31. A in Exercise 
1  32. A in Exercise 
show that U = Lr. 
33. If A is symmetric 
and A = LDL T (with L 
34. If A is symmetric 
this 
have A = L1D1L[ (with L1 unit lower triangular 
That is, prove that if we also 
and D1 

unit lower triangular 
factorization 

and invertible 

and D diagona

is unique. 

l), then L = L1 and D = D1• 

diagona

l), prove that 

Subspaces, Basis, Dimensio n ,  and R a n k  

introduces 

This section 
already 
seen that there 
use geometric 
intuit
algebra 
will often allow 
in which they first arose. 

perhaps the most important 
is an interplay 
between geometry 
to obtain 
findings 

reasoning 
extend our 

in the entire 
and algebra: We can often 
results, and the power of 

algebraic 
well beyond the geometric 

book. We have 

ideas 

us to 

settings 

ion and 

In our study of vectors, we have alread

y encountered 

all of the 

concepts 
in this 

we will start 

to become 
Here, 
. As you'll see, the  notion 

more formal by giving definitions 

examples 
of lines 
for a subspace 

an algebraic 

of a subspace is simply 
is then 

and planes 
derived 

through 
from the idea 
give a 
us to 
geometric 
idea of the 

of direc­
pt of a basis will allow 

with an intuitive, 

the origin. The 

that agrees 

and planes. The conce

to allow gene
see that these 

ralization 
ideas 

to other 
shed more light 

settings. 

on what you already 

informally. 

tion of the geometric 

concept 
for such lines 

section 
for the key ideas
generaliza
fundamental 
tion vectors 
precise 
term, 

of a basis 
definition of 
dimension 
enough 
You will also begin to 
know about matrices 
we will encounter 
section 
a "getting 
A plane 

through the origin 

yet is flexible 

Consider 
this 

and the solution of systems 

equations. In Chapter 6, 
all of these fundamental ideas 
to know you" session. 

of linear 
again, 
a copy of IR2• Intuit

in IR3 "looks like" 

in more detail. 

that they are both "two-dimensiona

we would 
further, we might also say that 

agree 
any calculation that 
the origin. 
form linear combina
tors in the same plane. We say that, 
to the operations 
respect 

ively, 
l:' Pressed 
in IR2 can also be done in a plane 
can be done with vectors 
in such a plane, 
vec­
and the results 
like IR2, a plane through 
the origin 
with 
and scalar multiplication. 
3.2.) 
dimensional ob
because they live in IR3 and therefore 

of addition 
in this plane two-or three-
dimensional 

are other 
is closed 
jects? We might 

can add and 
of vectors 

In particular, we 
tions) 

take scalar multiples 

(and, more generally, 

the vectors 

(See Figure 

But are 

through 

have three 

ts. On the other 

that they are three-

argue 
hand, 
componen
two vectors-direction 
vectors 
ing in a two-dimensional 
conundrum

. 

they can be described 

of just 
for the plane-and so are two-dimensional 
resolving 
this 

is the key to 

of a subspace 

combination 

plane. The notion 

as a linear 

objects liv­

z 

x 

Figure 3 . 2  

2u + v 

y 

1 9 2   Chapter 3 Matrices 

A subspace of !Rn is any collection 

Definition 
0 is in S. 
1 .  The zero vector 
S, then u + v is in S. (Sis closed 
2. If u and v are in 
3. If u is in Sand c is a scalar, then cu is in S. (Sis closed 

under additio
n.) 

S of vectors 

in !Rn such that: 

under scalar 

multiplication.) 

We could have combined 
closed 

under linear 

properties 

(2) and (3) and required, 

equivale

ntly, 

that S be 

combinations: 
If u1, u2, • . •  , uk are in S and c1, c2, . • .  , ck are scalars, 

then c1u1 + c2u2 + ·  ·  · 

+ ckuk is in S. 

Example 3 . 3 1  

Every line and plane through 
geometri
in the case of a plane through 
proof 

for a line in Exerc

cally 

ise 9. 

that properties (1) through 

the origin 

in IR3 is a subspace 
(3) are satisfied. 
You are asked to give the corresponding 

of IR3. It should 
Here is an algebraic proof 

be clear 

the origin. 

Let <JP be a plane 

through the origin 

span (v

1, v2). The zero vector 

0 is in <JP, since 

with direction 

vectors 
0 = Ov1 + Ov2• Now let 

v1 and v2. Hence, 

<JP = 

be two 

vectors 

in <JP. Then 

Thus, u + vis a linear 

combination 

ofv1 and v2 and so is in <JP. 

Now let c be a scalar. Then 

which shows that 
cu is also 
a linear 
properties ( 1) through 
have shown that <JP satisfies 

ofv1 and v2 and is therefore in <JP. We 
( 3) and hence is a subspace 

combination 

of IR3 4 

of Example 
3.3 7, you will notice 
no role at all in the verification 

that the 
fact 
of the 

prop­

at the details 
in IR3 played 
. Thus, the algebraic 
method 
where we can no longer 

If you look carefully 
that v1 and v2 were vectors 
erties
in situations 
of Example 
method 
generalize 
Example 
is important 
enough 

3.37 can serve as a "template" 
3.3 7 to the span of an arbitrary 
to be called 

we used should 

a theorem. 

beyond IR3 and apply 

generalize 
geometry. It does. 
in more general settings. When we 
set of vectors 

Moreover, 
in any !Rn, the result 

the 

visualize the 

Theorem 3 . 19 

Let v1, v2, . . .  , vk be vectors 

in !Rn. Then span(v1, v2, . . .  , vk) is a subspace 
of !Rn. 

Proof Let S 
observe that the zero 

1, v2, . . .  , vk)· To check property (1) of the definition, 
vector 

= span (v

0 = Ov1 + Ov2 + ·  ·  · 
+ Ovk. 

S, since 

0 is in 

we simply 

Section 3.5 Subspaces, 

Basis, 

Dimension, 

and Rank 1 9 3  

Now let 

be two vectors 

in S. Then 

u + v = (c1v1 +  c2v2 + ·  ·  · 

+ dkvk) 
+ ckvk) + (d1v1 +  d2v2 + ·  ·  · 

= (c1 +  d1)v1 + (cz +  dz)V2 + ·  ·  · 

tion of v1, v2, •.. , vk and so 

+ (ck +  dk)vk 

Thus, u + v is a linear 
erty (2). 

combina

To show property (3), let c be a 

scalar. Then 

is in S. This verifies 

prop­

+ ckvk) 
cu = c(c1v1 +  c2v2 + ·  ·  · 

which shows that cu is also 
in S. We have 
ofll�r. 

a linear 
shown that S satisfies 

= (cc1)v1 + (cc2)v2 + ·  ·  · 

combination 

+ (cck)vk 
of v1, v2, . . .  , vk and 

is therefore 
(1) through (3) and hence is a subspace 

properties 

to span (v1, v2, .•. , vk) 

as the subspace spanned by v1, v2, ..• , vk. 

to save a lot 

of work by 

recognizing 

when Theorem 3.19 can be 

We will refer 

We will often be able 
applied. 

Example 3 . 3 8  

Show that the set of all vectors [;] that satisfy the conditions 

forms a subspace 

of IR\3.  z 

x = 3y and z  = -2y 

Solullon Su bstitufog thetwo conditions 

into [ �] yields 
is span ( [ �] ) and is thus a 
4 

-2 

, the given set of 
Since y is arbitrary
oflR\3, by Theorem 
3.19. 

vectors 

subspace 

Geometrica

lly, the set 

in Example 

3.38 represents the line through 

the 

migin in GI' with dimtion 

of vectors 

vectm [ _ n 

1 9 4   Chapter 3 Matrices 

Example 3 . 3 9  

....,.. The zero vector 

Example 3 .4 0  

[H)H'""' 

x = 3y +  1 

of IR3. 

of the form 

the set 

whether 

whether 

the set of 

Determine 

Determine 

Solulion This time, 

we have all vectors 

property (1) does not hold, 

so this set cannot be a subspace 

and z = -2y is a subspace of IR3.  z 

all vectors [;] that satisfy the conditions 
[3y: l l 
-2y 
is not of this form. (Why not? Try solving [ 3y: 1 l -2y 
of all vectors [;], where y = :x2-, is a subspace 
of the form [:2 ]-call this set S. 
u = [:�] and v  = [:�]be in S. 
u + v  = [x� + x�J X1 + Xz 
u = [ �] and v  = [ ! ] 
of IR2. [ ! ] is not in S since 
(1) through (3) hold in general. However, for S 

5 * 32. Thus, .+ 

of some !Rn, we must 
that 
prove 
of!Rn, 
to fail to be a subspace 
is 
of the property. 
the failure 

x� + x? * (x1 + x2) 2. To be specific, 
property (2) 

then both u and v are in 
fails and 

belongs to 
Then 

properties 
it is enough 
to find a single, 
usually 
Once you have done so, 

one of the three 
specific 
there is no need to consider 

counterexample 
the other 

S (take x = O), so property (1) holds. Let 

S, but their 
S is not a 

sum u + v  = 
subspace 

S o lulion These are the vectors 

it does not have the correct 

Remark In order for a set S 

we look for a counterexample. If 

properties fails to hold. 

to illustrate 

to show that 

is not in S, since 

to be a subspace 

in general, 

This time 

form; that is, 

properties. 

which, 

of IR2. 

0 = [ �] 

Subspaces Associaled 
A great many examples 
encountered 
notion 

of a subspace 

the most important 

wilh Malrices 
in the 

in Chapter 2; we now revisit 

of matrices

context 

of subspaces 
arise 
of these 

in mind. 

them with the 

. We have already 

The easiest course 

Section 3.5 Subspaces, 
Let A be an m x n matrix. 

Basis, 

Dimension, 

and Rank 1 9 5  

Definition 
1. The row space of A is the subspace 
2. The column space of A is the subspace 

row(A) of !Rn spanned 

col(A) of !Rm spanned 

by the rows of A. 
by the 

columns 

of A. 

Remark Observe that, 

by Example 

3.9 and the Remark that follows 

it, col(A) 

Exa m p l e  3 . 4 1  

Consider 

consists 
precis

ely of all vectors 

of the form Ax where x is in !Rn. 

the matrix  [ 1 - 1  i 
b � [ �] ;, in the column 
(') Dctmnine 

A =  0 1 
3 - 3  

'P"' of A. 

wheth" 

(b) Determine 
whether 
(c) Describe row(A) and col(A). 

w =  [ 4 

5 ]  is in the row space 
of A. 

Solution 
(a) By Theorem 2.4 and the 
columns 
the augmented 

of A if and 

only if the linear 

discussion 

preceding it, b is a linear 
system 
Ax =  b is consistent

matrix as follows: [� �: �] �  [� � �] 

combination 

of the 

. We row reduce 

is consistent 

(and, in fact, has a 

is just Example 

unique 
2.18, phrased 

solution). Therefore, b is 
in the terminology 
of this 

Thus, the system 
in col(A). (This example 
section.) 
(b) As we 
combinations 
space of 
rows of A, so if we augment 

the matrix. 

also saw in Section 

operations 
row operations 

___..... to bottom in each 

of the 
column. (Why?) 
In this example, we have 

to this augmented 

matrix to reduce 

only elementary 

of the rows of a matrix. 

2.3, elementary 

row operations 
That is, they produce 

simply 
vectors 

linear 

create 
only in the row 
of the 

If the vector 

w is in row(A), then w is a linear combination 

elementary row 

A by w as [ �], it will be possi

ble to apply 

it to form [ �'] using 

form R; + kRj, where i > j-in other 

words, workin

gfrom top 

[ �] [� -�i �:=!�: [� -�i R4-9R2 [� -�i 

�� 
3 - 3  �  0  0 �  0  0 

4  5  0  9 

0  0 

�� 

�� 

1 9 6   Chapter 3 Matrices 

�  Therefore, 

w is a linear 

[ l  -1 ]  +  9 [ O  l ] -how?), and thus w is in row(A). 

tion of the rows of A (in fact, these 

combina

fo r  any vector 

w =  [ x 

y] , the augmented 

calcula

tions 

show 

matrix [ �] 

that w =  4 
( c) It is easy to check that, 
reduces 

to 

in a similar 

fashion. 

Therefore, 

every vector 

Finding col(A) is identical to solving 

Example 
with the plane (through the origin) 

it coincides 
will discover other 

ways to answer 

this type of question 

shortly.) 

), and so 

in IR2 is in row(A

2.21, wherein 

in IR3 with equation 3x -z = 0. (We 

we determined 

row(A) = IR2. 
that 

Remark We could 

also have answered 

part (b) and the first part of part (c) by 

that any question 

observing 
the columns of AT. So, for example, w is in row(A) if and 
is true if and 
part (a). (See Exercises 

about the rows of A is the corresponding 
A Tx = wT is consistent. 

only if the system 
2 1-24.) 
we have made about the relationshi

The observations 

We can 

operations 

and the row space are 

summarized 

in the following 

theorem

. 

only if wT is in col(A T). This 

question 
about 

now proceed 

as in 

p between elementary 

row 

Theorem 3 . 2 0  

Let B be any 
matrix that 

is row equivalent 

to a matrix A. Then row(B) = row(A). 

Proof The matrix A can be transformed 
Consequently, the rows of B are linear 
combina
cise 21 in Section 

2.3.) 
On the other 
hand, reversing 

It follows 
that row(B) � row(A). 
these 

tions 

combinations 
of the 

row operations 

into B by a 

argument 

shows that row(A) � row(B). Combining 

transforms 
B into A. There­
these 

results, we 

sequence of row operations. 
of the rows of A; hence, 

linear 

of the rows of B are linear combinations 

rows of A.  (See Exer­

fore, the above 
have row(A) = row(B). 

Theorem 3 . 2 1  

of solutions of the homogeneous 

y encounter
ed: the set 

It is easy to 

prove that 

this 

There is 

of solutions 
subspace satisfies the 

three 

subspace 

that we have alread
equations. 
of linear 
properties. 

another 
of a homogeneous 
system 

important subspace 

and let N be the set 
Ax = 0. Then N is a subspace 

of !Rn. 

Let A be an m X n matrix 
linear 

system 

in N. Therefore, 

Proof [Note that x must be a 
that 0 = Om is the zero vector 

(column) vector 
in !Rm.] Since 

Au= 0 and Av= 0. It follows 

that 

AOn = Om, On is in N. Now let u and v 

and 
for Ax to be defined 
be 

in !Rn in order 

A(u + v) 

= Au + Av = 0 + 0 = 0 

Dimension, 

and Rank 1 9 1  

Hence, 

Basis, 

u + vis in N. Finally, 

Section 3.5 Subspaces, 
in N. It follows 
Let A be an m  X n matrix. 

for any scalar c, 
A(cu) = c(Au) = cO = 0 

cu is also 

that N is a subspace 

of !Rn. 

and therefore 

Definition 
!Rn consisting 
by null(A). 

of solutions of the homogeneous 

system 

The null space of A is the subspace 
Ax = 0. It is denoted 

linear 

of 

The fact that the null space 

tuition and examples 
They have either 

no solution, 

have led us to 

of a matrix is a subspace allows 
about the 
or infinitely 

prove what in -
solutions of linear 

understand 

many solutions. 

solution, 

us to 

a unique 

systems: 

Theorem 3 . 2 2  

Let  A  be  a 
equations 

Ax = b, exactly 

one of 

the following 

is true: 

matrix whose entries 

are real numbers.  For  any 
system 

of linear 

a. There is no solution. 
b. There is a unique 
c. There are infinite

solution. 
ly many solutions. 

it is not entirely 
clear 

glance, 
reflection 

At first 
rem. A little 
prove is that if (a) and (b) are not true, then 
there 
but there 

persuade you that 
( c) is the 
then there 
cannot 

to prove this theo­
really 
being asked to 
possibility. That is, if 
be just two or even finitely 
many, 

is more than one solution, 
many. 

what we are 
only other 

must be infinitely 

how we should 

proceed 

should 

Proof If the system 
done. 
then, 
and x2. Thus, 

Assume, 

Ax = b has either no solutions 
that there 

are at least 

two distinct 

solutions 

solution, 
of Ax = b-say, x1 

or exactly one 
we are 

Ax, = b and Ax2 = b 

that 
with x1 * x2. It follows 
A(x,  -x2) = Ax1 -Ax2 = b -b = 0 
Set Xo = x1 - x2. Then Xo * 0 and AXo = 0. Hence, 

and since 
null(A) is closed under scalar multiplication, 
scalar c. Consequently, the null space of A contains 
contains 

every vector 
of the form CX
the (infinitely many) vectors 

Now, consider 

at least 

o and there 

infinitely 

many vectors 
(since it 
many of these). 

are infinitely 
1 + CXo, as c varies 

of the form x

through 

the null space 

of A is nontrivial, 
CXo is in null(A) for every 

the set of real numbers. We have 

A(x, + cx0) = Ax, + cAXo = b  + cO = b 

Therefore, 

there are infinitely 

many solutions 

of the equation 

Ax = b. 

Basis 
We can extract 
of planes 

a bit more from the intuitive 

idea that subspaces 

through 

the origin 

in IR3. A plane is spanned 

are generaliza
that are 

tions 

by any two vectors 

1 9 8   Chapter 3 Matrices 

parallel to the plane but are not parallel 
such vectors 
not work; more than 
subspace. 

span the plane and are 
two vectors 

to each other. In algebraic 

parlance, two 

linearly independent. 
is not necessary. This is the 

will 
Fewer than two vectors 
of a basis 
for a 

essence 

Definition 

A basis 

for a subspace 

S of ll�r is a set of vectors 

in S that 

1. spans S and 
2. is line

arly indepen
dent. 

Example 3 .4 2  

2.3, we saw that the 

dent and span !Rn. Therefore, 

In Section 
indepen
basis. 

Example 3 .4 3  

2.19, we showed 

In Example 
also linearly indepen

dent (as they are not multiples), they form a 

ly 

for !Rn, called the standard 

e1, e2, . . .  en in !Rn are linear

standard unit vectors 
they form a basis 

4 
that IR2 =span([_�],[�]). Since [_�]and[�] are 
for IR2. 4 
basis { [ � l [ �]} and the basis { [ _ � l [ �] } . How-

have more than  one 
basis. 

For example, we 
have just 

in a basis for a given 

subspace 

basis 

will 

A subspace 

can (and will) 

seen that IR2 has the standard 
ever, 
always 

we will prove 

be the same. 

shortly that the number of vectors 

Example 3 .44 

Find a basis 

for S = span (u, v, w), where 

any linear 

�  linear

It is easy to determine 

y span S, so they will be a basis 

u, v, and w alread
we can ignore 
to involve 

for S if they 
w = 
that they are not; indeed, 
involving 
combinations 
u, v, 
u and v alone. (Also see Exercise 47 in Section 
2.3.) 
y 

Solution The vectors 
ly independent. 
are also linear
2u -3v. Therefore, 
w, since 
and w can be rewritten 
This implies 
ly indepen

basis 
lly, this means that 
u, v, and w all lie in the same plane and 
u and v can serve as a set of direction 
for this plane.) 

that S = span (u, v ,  w) = span (u, v), and since u and v are certainl
4 

dent (why?), they form a 

for S. (Geometrica

vectors 

Example 3 .4 5  

Find a basis 

for the row space 

Section 3.5 Subspaces, 

Basis, 

Dimension, 

and Rank 1 9 9  

Solution The reduced 

- 1  0 

form of A is 

A =   2 1 - 2  

of [-! 3 

1 6 
row echelon 
0 
0 
1 
0 

1 
2 
0 
0 
is enough 
by its nonzero 
three 
forces the first 
need to establish 

rows of R to be linear

0 
1 
0 
0 

row(A) = row(R), so it 

By Theorem 3.20, 
of R. But row(R) is clearly 
pattern 
the staircase 
is a general 
fact, one that you will 
a basis 

for the row space of 

spanned 

A is 

to find a basis for the 
rows, and it is easy to check that 

row space 

ly independent. 
to prove Exercise 33.) Therefore, 

(This 

{ [  1  0 0 - 1 ] ,[ 0  1  2  0 3 ] ,[ 0  0  0 4 ] }  

We can use the method 

of Example 

3.45 to find 

a basis 

for the subspace 

spanned 

by a given 

set of vectors. 

Example 3 .4 6  

Rework 

Example 

3.44 using 

the method 

from Example 

3.45. 

Solution We transpose u, v, and w to get row 
these 

vectors 
as its rows: 

vectors 

and then form a matrix with 

Proceeding as in Example 

3.45, 

we reduce 

row echelon 

form 

B to its reduced 

[ 1  0 !>] � � -! 
{[HUll 

again. Thus, a basis 

for the 

as a basis 

and use the 
column 

vectors, we must 

nonzero row vectors 

transpose 

row space. 

Since 
for span(u, v, w) is 

we started 
with 

•  In fact, we do not need to go 

Remarks 

elon form is far enough. 

If U is a row echelon 

form of A, then the 

nonzero 

row vectors 

all the way to reduced 

row echelon 

form-row ech­

2 0 0   Chapter 3 Matrices 

of U will form a basis 
(often) allowing 

for row(A) (see Exercise 

s. In Example 

33). This approach has the 
3.46, B can be reduced 

advantage 
to 

us to avoid 

fraction

of 

which gives 

us the basis 

\HH-rn 

•  Observe that the methods 
for span (u, v, w). 

used in Example 

3.44, Example 

3.46, 

and the 

Remark 

bases. 

above will generally produce different 
of finding 

problem 
a basis 
We now turn to the 
is simply 
One method 
to transpose the matrix. 
AT' and we can apply the method 
row vectors of 
row(A T). Transposing these 
do this in Exercises 
21-24.) 
on AT. 
of row operations 
Instead, 

then gives 

vectors 
us a basis 
This approach, however, 

for the column space of 
a matrix A. 

The column 

vectors 

the 
of A become 
of Example 3.45 
for 
to find 
a basis 
for col(A). (You are asked to 

use the 

to take an approach that allows 
us to 

computed. Reca
combination 

we prefer 
of A that we have already 
tor corresponds 
to a linear 
coefficients
among the columns 
set, if A is row equivalent 
ships as the columns of R. This important observation is the basis 
for the technique we now use 

form 
ll that a product Ax of a matrix and a vec­
of x as 
of the columns 
a dependence 
relation 
affect the solution 
to R, the columns of A have the same dependence 
(no pun intended!) 

. Thus, a nontrivial solution to Ax = 0 represents 

of A with the entries 

to find a basis for col(A). 

elementary 

row reduced 

row operations 

of A. Since 

do not 

relation­

requires 

performing 

a new set 

Example 3 .41 

Find a basis for the column space 

of the matrix from Example 

3.45, 

Solution Let a; denote 
reduced 

echelon 

form 

a column 

vector 

of A and let r; 

-1 0 1 
A =   2  - 2  
6 1 

[-� 1 3 1 -:i 
R � [� 0 1 0 -il 1 2 0 
that r3 = r1 + 2r2 and rs= -r1 + 3r2 + 4r4• (Check 

0  0 1 
0 0 0 

a column 

vector 
of the 

denote 

see by inspection 

We can quickly 
that, 
dence relations.) Thus, 

as predicted, 

the corresponding 
r3 and rs contribute 

column vectors 
to col

nothing 

of A satisfy the same depen­

(R). The remaining 

column 

Example 3 .4 8  

Section 3.5 Subspaces, 

Basis, 

Dimension, 

and Rank 2 0 1  

vectors, 
tors. The correspond

r1, r2, and r4, are linear
among the column 

ing statements 
vectors 

they are just standard unit vec­

since 
are therefore 
true of the column 
of A, we elimina

te the dependent ones ( a3 and a5), 

ly independent, 

vectors 

Thus, 

of A. 

and the remaining 
What is the fastest 
columns of 

ones will be linearly independ
way to find this basis? 
R containing the leading 

ls. A basis 

ent and hence form a basis for col(A). 
to the 

of A that correspond 

Use the columns 

for col(A) is 

r .... ,..,} � { [-;H-�H-m 

row operations change the column 

vector 

Warning Elementary 
every 

col(A) * col(R), since 
this is certainly 
the column 
vectors 
do not form a basis 

for a basis of 
for the 
column 

not true of col (A). So we must go back to the 

in col(R) has its fourth component equal 
original matrix 
r1, r2, and r4 

col(A). To be specific, 

in Example 

to 0 but 
A to get 

3.47, 

space of 

A. 

space! In our example, 

3.47. 

nothing 

for the null space 

of matrix A from Example 

Find a basis 
really 
Solution There is 
to find and describe 
the solutions 
ready computed 
the reduced 
form R of A, so all that remains 
elimination is to solve 
in Gauss-Jordan 
matrix is 
l augmented 
variables. 
The fina
0 

new here except 
of the homogeneous 
system 

gy. We simply 
have 
Ax = 0. We have 
to be done 
in terms of the free 

leading variables 

the terminolo

row echelon 

for the 

al­

1 
2 
0 
0 

0 
0 
1 
0 

0 
0 

If 

ls are in 

then the leading 
the free 
Setting 

variables 
x3 = s and x5 = t, we obtain 

columns 

x3 and x5• We get x1 = -x3 + x5, x2 = - 2x3 -3x5, and x
4 = -4x5. 

1, 2, and 4, so 

we solve for x

1, x2, and x

4 in terms of 

X1  -s + t - 1   1 
X2  -2s -3t -2  -3 

x= X3  s  = s 1 + t 0 = su + tv  

X4  -4t  0  -4 
X5 

0 

Thus, u and v span null(A ), and 
for null(A). 

since 

they are linearly independent, 

they form a 

basis 

4 

2 0 2   Chapter 3 Matrices 

Following 

is a summary 
the column 

the row space, 

space, 

and the 

null space of 
a matrix A. 

of the most effective 

procedure to use to find bases 

for 

1. Find the reduced 
2. Use the nonzero 

row echelon 
row vectors 

form R of A. 
of R (containing 

the leading ls) to form a basis 

for 

row(A). 

3. Use the column 

vectors 

of A that correspond 

to the columns 

of R containing 

the 

leading 

ls (the pivot 

columns) to form a basis for col(A). 

4. Solve for the le

free variables 
a linear 
f vectors 

combina
form a basis 

for null(A). 

ading variables 
of Rx = 0 in terms of the free variables, set the 
tute back into x, and write the result as 
to parameters, substi
equal 
f is the number of free variables
(where 

tion off vectors 

). These 

Theorem 3 . 2 3  

The Basis Theorem 

Holmes 

whatever 

noted, 
the impos­

"When 
Sherlock 
you have eliminated 
sible, 
remains, 
however 
improbable, 
(from The Sign of Four by Sir 
Arthur 

Conan Doyle). 

must be the truth" 

If we do not need to find the null space, 

echelon 
valid 

form to find 
(with the substi

bases 
tution of the 

for the row and column 

word "pivots" 

then it is faster 

to simply 
spaces. 
Steps 
for "leading ls"). 

reduce 

A to row 

2 and 3 above remain 

and Rank 

Dimension 
We have observed that although 
the same number of vectors. This fundamental 
here on 

will have different 
fact will be of vital 

a subspace 

in this book. 

has 
bases, each basis 
importance 
from 

Let S be a subspace 
vectors. 

of !Rn. Then any two bases for S have the same number of 

Proof Let B = {u1, u2, . . .  , u,.} and C = {v1, v2, .•• , v,} be bases 

of the other 

prove that r = s. We do so by showing that neither 
or r > s, can occur. 
of vectors. To this end, 

Since B is a basis for S, we can write each V; as a linear 

let 

elements 

"f 

Suppose that r < s. We will show that this forces C to be a linearly dependent set 

for S. We need to 
two possibilities, 
r < s 

(1) 
of the 

combination 

V1 = a1 1  u, + a12u2 + ·  ·  · 
+ a,rur 
Vz = a11 u, + az2U2 + ·  ·  · 
+ azrUr 

(2) 

Substi

tuting 

the Equations 

(2) into Equation 

(1), 

we obtain 

Section 3.5 Subspaces, 

Basis, 

Dimension, 

and Rank 2 0 3  

Regrouping, 

we have 

(c1all + C2a21 + . . .  + c,as1)U1 + (c1a12 + C2a22 + . . .  + csasz)Uz 

+ · · · + (c1a1r + c2a2r + · · · + c,a)u, = 0 
the u/s are linearly independent. 
c1a11 + c2a21 + · · · + c,a,1 = 0 

Now, since 
parentheses must be zero: 

B is a basis, 

C1a12 + C2a22 + . . .  + csas2 = 0 

So each of the expressions 

in 

c1, c2, •.. , c5• (The 

in the s 

equations 

of r linear 

This is a homogeneous 
system 
appear to the left of the coefficients 
fact that the variables 
r < s, we know from 
Theorem 2.3 that there are 
infinitely 
a nontrivial 
lar, there is a nontrivial 
tion (1). 
vectors. 
fact that C was given to 
r < s is not possible. Similar
to a contradict
leads 

variables 
makes no difference.) Since 
many solutions. 
In particu­
dependence 
But this finding 

be a basis and hence linearly independent. We 

relation in Equa­
the 

C is a linearly dependent set of 

of Band C), we find that r > s 

ly (interchanging 

contradicts 
conclude that 

have r = s, as desired. 

ion. Hence, 

we must 

solution, 

the roles 

giving 

Thus, 

Since 
attach 

a name to this 

number. 

all bases 

for a given 

subspace 

must have the same number of vectors, 
we can 

Definition 
is called 

the dimension 

of S, denoted 
dim S. 

If S is a subspace 

of !Rn, then the number of vectors 

in a basis for S 

Remark The zero vector 

0 by itself 

is always 

a subspace 

of !Rn. (Why?) Yet any set 

containing 
have a basis. We 

the zero vector 

define dim {O} to be 0. 

(and, in particular

, { 0}) is linearly dependent, 

so { 0} cannot 

for !Rn has n vectors, dim !Rn = n. (Note that this resu

for n :s 3.) 

of dimension 

lt agrees 

Example 3 .4 9  

Since 
with our 

basis 
the standard 
intuitive 
understanding 

Example 3 . 5 0  

3.45 through 3.48, we 

In Examples 
col(A) has a basis 
dim(row(A)) = 3, dim(col(A)) = 3, and dim(null(A)) = 2. 

found that row(A) has a basis with three 
and null(A) has a basis 

vectors, 
vectors. Hence, 

with two 

vectors, 

with three 

is not enough 

A single example 

spaces 

and column 
the fact that the sum of dim(col(A)) and dim(null(A)) is 5, the 
A. We now prove tha

relationships 

in Example 

are true in general. 

t these 

on which to speculate, 
3.50 have the same dimension 

is no accident. 
Nor is 
number of columns 

of 

but the fact that the row 

2 0 4   Chapter 3 Matrices 

Theorem 3 .2 4  

The row and column 

spaces 

of a matrix 

A have the same dimension. 

Proof Let R be the reduced 
row(R), so 

row echelon 

form of A. By Theorem 3.20, 

row(A) = 

dim(row(A)) = dim(row(R)) 

= number 
= number 

of nonzero 
rows of R 
ofleading 
ls of R 

Let this number be called 

Now col(A) * col(R), but the columns of 

r. 
A and R 
dim(col(A)) = dim(col(R)). Since 

ps. Therefore, 

!Rm if A and Rare m X n matrices

relationshi
has r columns 
remaining 
follows 

that dim(row(A)) = r = 

that are standard 

columns 

there 

ctors, 

e1, e2, . . .  ,  er. (These 

unit ve
.) These r vectors 
dim(col(A)), as we wished 
to prove. 

are r leading ls, R 
in 
will be vectors 
and the 
are linearly independent, 
of them. Thus, dim(col(R)) = r. It 

of Rare linear combinations 

have the same dependence 

we 

first de­

was a German 

(See Chapter 

The rank o f  a matrix was 

fined in 1878 by Georg Frobenius 
(1849-1917), although 

he defined 
determinants 
and not as 

4.) 

it using 
have done here. 
Frobenius 
mathematician 
doctorate 
at the University 
known for his contributions 
group theory, Frobenius 
matrices 
representations. 

from and later 
of Berlin. Best 
to 

in his work on group 

who received 

taught 

used 

his 

Definition 
spaces 

and is denoted 

by rank(A). 

The rank of a matrix A is the dimension 

of its row 

and column 

Remarks 

3.50, 

For Example 

we can thus write 
rank(A) = 3. 

•  The preceding definition 
•  The rank of a matrix simultaneously 

was introduced 
more flexible. 

in Chapter 2. The 

advantage 

agrees 

with the more informal 

definition 
is that it 
is much 

definition 

of rank that 

of our new 

gives us information about linear 

among the row vectors 

dependence 
particular, it tells us 
(and this number is the same in each case!). 

of the matrix and among its column 

the number of rows and columns 

that are linear

vectors. 

In 
ly indepen

dent 

Since 

following 

the row vectors 
immedia

te corollar

y. 

of A are the column 

vectors 

of Ar, Theorem 3.24 has the 

Theorem 3 . 2 5  

For any matrix 

A, 

P r o o f  We have 

rank(A T) = rank(A) 

rank(AT) = dim (col(AT)) 

= dim (row(A)) 
= rank(A) 

Definition 
denoted 
(A). 

by nullity

The nullity of a matrix 

A is the dimension 

of its null space 
and is 

Section 3.5 Subspaces, 

Basis, 

Dimension, 

and Rank 2 0 5  

In other 

words, nullity

is the 
Rank Theorem (Theorem 2.2), 

(A) is the dimension 
number of free variables 
rephrasing 

same as the 

of the 

in the solution. 

We can now revisit 

the 

it in terms of our new 

definitions. 

solution space of 

Ax= 0, which 

Theorem 3 . 2 6  

The Rank Theorem 

If A is an m  X n matrix, 

then 

Example 3 . 5 1  

Find the 

nullity 

of each of the following 

matrices

: 

(A) = n 
rank(A) + nullity

form of A, and suppose that rank(A) = r. 
in the 

and n -r free variables 

variables 

Proof Let R be the 
reduced 
Then R has r leading ls, so there 
solution to 

Ax = 0. Since 

row echelon 

are r leading 

dim(null(A)) = n -r, we have 
(A) = r + (n -r) 

rank(A) + nullity
= n 
of a matrix, we do not need to know the 

Often, when we need to know the nullity 
extremely 

solution of Ax = 0. The Rank Theorem is 
following 

illustrate

actual 
as the 

example 

s. 

useful in such situations, 

Solution Since 
Thus, 

the two columns 
nullity

by the Rank Theorem, 

There is no obvious 
to reduce 

row operations 

dependence 

4 - 3  
7 

of M are clearly 

linearly independent, rank(M) = 2. 
rank(M) = 2 - 2 = 

(M) = 2 -
among the rows or columns 

M =  [I �: and 
N � [� 1 -2 -:1 
it to [: 2 
0 -�] 
(N) = 4 -rank(N) = 4 -2 = 2. 4 

(we do not need reduced row echelon 
are only 
for the null space). We see that there 

the Fundamental 
Theorem of 

allow us to extend 

nullity

-2 
1 

form 

0. 

0 

of N, so we apply 

We have reduced 
here, since 
two nonzero 

the matrix far enough 
we are not looking for a basis 

rows, so rank(N) = 2. Hence, 

The results 
Invertible 

Matrices 

of this section 

(Theorem 3.12). 

2 0 6   Chapter 

3 Matrices 

Theorem 3 .21 

statements 

are equivalent: 

Matrices: Version 2 

Theorem of Invertible 

The Fundamental 

an n X n matrix. The following 
b in ll�r. 

Let A be 
a. A is invertible. 
b. Ax = b has a unique 
c.  Ax 
d.  The reduced 
e.  A is a product of element

row echelon 

invaria

(A) = 0 

for every 

for ll�r. 

of A are 
linear

of A form a basis 

m. The row vectors 

ly independent. 

of A span ll�r. 

Proof We have alread
be shown that 

solution 
= 0 has only the trivial 
solution. 
form of A is In-
ary matrices. 

y established 
statements 
rank(A) + nullity

g. nullity
h. The column 
i. The column 
j. The column 
k. The row vectors 

vectors 
vectors 
vectors 
arly independent. 
of A are line
of A span !Rn. 
basis for !Rn. 
of A form a 

f. rank(A) = n 
1. The row vectors 
The nullity 
of a matrix 
was defined 
(f) � (g) Since 
in 1884 by James 
Sylve
Joseph 
ster 
(1814-1887), who was interested 
in 
the Rank Theorem that rank(A) = n if and 
nts -proper
ties 
of matrices 
(f) ==> (d) ==> ( c) ==> (h) If rank(A) = n, then the reduced 
that 
do not change 
under 
certain 
n leading 
of transformations. 
Born 
types 
in England, 
became the 
Sylve
ster 
president of the 
second 
London 
(h) ==> (i)  If 
Society. 
In 1878, 
Mathematical 
Thus, by (c) ==> (b), Ax= b has a unique 
while 
teaching 
at Johns 
Hopkins 
University 
in Baltimore, he 
founded 
the 
Mathematics, the first 
mathematical 
(i) ==> (j) If the 
journal 
in the 
United 
State
s. 
so rank(A) = dim(col(A)) = n. This is (f), and we have alread
(f) ==> (h). We conclude 
(j) ==> (f) If the column vectors 
n 
ls, and thus rank(A) = n. 
shows that (f) ==> (d) ==> (c) ==> (h) ==> (i) ==> (j) ==> 
(f) � (g). Now recall that, by 

(A) = n when A is an n X n matrix, 

which implies 
that the col
combination 
vectors 

the trivial 
!Rn. This means that every 
column 

of A, establishing 
column 

solution, 
Ax is just a linear 

The above discussion 

(f) to (m) are equivalent 

umn vectors of 

linear
leading 

for !Rn, since, 

dent. It follows 

the equivalence 

form a basis 

as a linear 

the column 

reduced 

vectors 

written 

ly indepen

solution. 

vector 

that the 

American 

Journal of 

A. 

of the column vectors of 
of A are linearly independent, 
b in !Rn can be 
(i). 
of A span W, then col(A)  =  !Rn by definition, 
vectors 

then Ax = 0 has only 
solution for every b in 
of the 

combination 

column 

vectors 
of A are linearly indepen
!Rn. 
by assumption, 
they also span 
in particu
form of A contains 

of A form a 
that the 

basis for !Rn, then, 

row echelon 

lar, they are 

y established tha

t 
dent and so 

ls and so is In-From (d) ==> (c) we know that Ax= 0 has only the 

(A) = 0. 
row echelon 

trivial 
A are linearly independent, since 

only if nullity

form of A has 

of (a) through (e). It remains 

to 
to the first five 
statemen
it follows 
from 

ts. 

Theorem 3.25, 
just proved gives 
onding 
are then results 
the row vectors 
about 
equivalences 

us the corresp

and completing 
such as the Fundamental 

Theorems 

the proof. 

bor-saving devices 
us to cut in half the work 

est. They are tremendous la
has already allowed 
ces are inverses. 
bases 
!Rn if either 
The next example 

of linear 
of the necessary properties 
shows how easy the calcula
tions 

for !Rn. Indeed, 

when we have a set of 

It also simplifies 

the task 

of A, bringing 

column 

vectors 

of Ar_ These 

results about the 
of 

rank(AT) = rank(A), so what we have 
(k), (1), and (m) into the network 
n vectors 

inter­
The Fundamental 
to check that two square matri­
are 
sets of vectors 
in !Rn, that set will be a basis 
for 
or spanning 

of showing that certain 

not merely of theoretical 

independence 

Theorem are 

needed 

as well. 

set is true. 

Theorem 

can be. 

Section 

3.5 Subsp

aces, 
Basis, 

Dimension, 

and Rank 2 0 1  

Example 3 . 5 2  

Show that the vectors 

Theorem 3 . 2 8  

form a basis for IR3. 
Solution According to the Fundamenta
IR3 if and 
perform just enough 

only if a matrix with these 
row operations 

l Theorem, 

mrn and m 
A � [� -� fl--7 [� -� J 
4 

vectors 
to determine 

will form a basis for 
We 

of both the Rank 

Theorem and the 

as its columns 

the vectors 

vectors 

Funda­

this: 

lt in Chapters 

5 and 7. 

(or rows) has rank 3. 

for IR3 by the equivalence 
We see that A has rank 3, so the given 
(f) and (j). 

are a basis 

of 

We will require 

mental Theorem. 

The next theorem is an application 
this resu

Let A be an m  X n matrix. Then: 
b. The n X n matrix AT A is invertible 

a. rank(A TA) = rank(A) 

if and only if rank(A) = n. 
(A) = n = rank(A TA) + nullity

same number of columns 

(A TA) 

AT A is n X n, it has the 

Proof 
(a) Since 
that 
then tells us 
rank(A) + nullity

as A. The Rank Theorem 

to show that rank(A) = rank(A 
TA). We will do so by 

Hence, 
nullity(A 
same. 

TA), it is enough 

to show that nullity
(A)  = 
T A are the 

of A and A

null spaces 

establishing 

that the 

To this end, 

let x be in null(A) so that Ax = 0. Then AT Ax= A To = 0, and thus 

x is in null(AT A). Conversely, let x be in null(AT A). Then A TAx = 0, so xTA T Ax = 
xTO = 0. 

But then 

and hence Ax = 0, by Theorem 1 .2(d). Therefore, 
null(ATA), as required. 
(b) By the Fundamental 
rank(A 

TA) = n. But, by (a) this is 

only if rank(A) = n. 

the n X n matrix AT A is invertible 

x is in null(A), so null(A) 

Theorem, 

so if and 

if and only if 

Coordinates 
We now return 
How should 
two-dimensiona
clarif

y things. 

to one of the questions 

posed at the 

very beginning 

of this section: 

we view vectors 

in IR3 that live 
dimensional? 

The notions 

in a plane through the origin? 

Are they 
and dimension 
will help 

of basis 

l or three-

2 0 8   Chapter 

3 Matrices 

A plane through the origin 

is a two-dimensiona
serving as 
a basis. 
us to 
in turn allowing 

two direction vectors 
plane/subspace, 
illustrate 
in this way are unique. 

this approach, we prove a theorem 

l subspace 

Basis vectors 
view the plane as a "copy" 

of IR3, with any set of 
locate coordinate axes in the 
of IR2• Before we 

eing that "coordinates" that arise 

guarante

Theorem 3 . 2 9  

Let S be a subspace 
vector 
v in S, ther
in B: 
vectors 

of !Rn and let B = {v1, v2, . . .  , vk} be a basis for S. For every 
combination 
of the basis 

one way to write 
v as a linear 

e is exactly 

Proof Since 
combination 

B is a basis, 
of v1, v2, . . .  , vk. Let one 

it spans 

S, so v can be written 

in at least 

of these 

linear 

combinations 

one way as a linear 
be 

that this is the only way to write v as a linear 

combination of 

Our task is to show 
v1, v2, . . .  , vk. To this end, suppose that we also have 
+ dkvk 
v = d1v1 + d2v2 + ·  ·  · 

Then 
Rearranging 

(using properties 
of vector 

algebra), we obtain 
+ (ck - dk)vk = 0 

Since 

B is a basis, 

(c1 - d1)v1 + (c2 - d2)v2 + ·  ·  · 
v1, v2, . . .  , vk are linearly independent. 
(c1 - d1) = (c2 - d2) = ·  ·  · 

words, c1 = d1, c2 = d2, ••• , ck = dk, and the 

= (ck - dk) = 0 

two linear 

Therefore, 

is exactly one 

way to write v as a linear 

In other 
actually 
the basis 

the same. 
Thus, there 
in B. 
vectors 

Definition 
S. Let v be a vector 
are called 

the coordinat

in S, 

and write v = c1v1 + c2v2 + ·  ·  · + ckvk. Then c1, c2, . . .  , ck 
of!Rn and let B = {v1, v2, . . .  , vk} be a basis 

es of v with respect to B, and the column 

vector 

Let S be a subspace 

for 

combinations 
combination 

are 
of 

is called 

the coordinate vector 

of v with respect to B. 

Example 3 .5 3  

Let E = { e1, e2, e3} be the 

standard 
basis 

for IR3. Find the coordinate 
of 

vector 

with respect 

to E. 

Example 3 . 5 4  

and Rank 2 0 9  

Section 
3.5 Subsp
aces, 
Basis, 
Dimension, 
27] 
[v]E = [4

that the 

vector 

vector 

be clear 

It should 

itself. 

respect 

coordinate 

of every (column) 

to the standard basis is just the vector 

Solution Since v = 2e1 + 7e2 + 4e3, 
ID Exompk 3.44, we<aw iliat u � [-} � m and w � [-;lace th"ev" 
for S. Since w = 2u -3v, we have 
[w]B = [ _�] 
-3v ,w = 2u - 3v 

tors in the same subspace 
basis 

S of IR3 and that B = { u, v} is a 

the origin) 

See Figure 3.3. 

through 

(plane 

z 

in !Rn with 

x --- . 

Figure 3 . 3  The coordinates 
respect 
to a basis 

of a vector 

with 

y 

3 . 5  

of vector

prove 
property. In each case, either 
that 
to show 

In Exercises 
IR3. 
that satisfy the given 
S forms a subspace of IR2 or give a counterexample 
that it does not. 

I Exercises 
1-4, let S be the collection 
s [;] in IR2 
1.x = 0  2.x 2 O, y  2 0 
3.y = 2x  4.xy 2 0 
of vato" [;] ;n D;l' 
2x,y = 0 

that satisfy the given 
S forms a subspace of IR3 or give a counterexample 
that it does not. 
5.x = y = z  6. z = 

property. In each case, either 
prove that 
to show 

S be the collea;on 

In Exm;,,, 5-8, let 

7. x - y + z =  1 

9. Prove that every 

line through 

8. Ix - y I = IY - z I 
the origin 

space of 

in IR3 is a sub­

of 

10. Suppose S consists 

of all points in IR2 that are on the 
the union 
of IR2? Why or why not? 

axes.) Is S a  subspace 

w is in row(A), as in 

x-axis or the y-axis (or both). (Sis called 
the two 

In Exercises 
and whether

11 and 12, determine whether 
3.41. 
11.A=[� � -�lb=[�], w=[ - 1  1 ]  
12. A� [� -� �}� [i}w� [2 4 - 5  

Example 

b is in col( 

A) 

2 1 0   Chapter 

3 Matrices 

13. In Exercise 
the method 

whether 
described 

1 1 ,  determine 

w is in row(A), 

in the Remark following 

using 
Example 
3.41. 

mateix 

16. If 

in the Remark following 

w is in row(A), 

15. If A ;, tho 

for row( A), col( 

12, determine 

17-20, give bases 

In Exercises 

using 
Example 
3.41. 

whether 
described 

14. In Exercise 
the method 

in F.xmi" 1 1, b � [ �: }n null (A)? 
A i'1he mate ix in Exmi" 12, i<v � [ -: }n nu!l(A)? 
0 -�] 
17.A  = [�  [0
19.A  = [� 1 0 J 1 - 1  
1 - 1  H -4 0 2 !] 2  2 

1 l 

21-24,find bases for row( A) and col( 

A), and null(A). 

A) in the 

1 - 3
2

1 - 1  -4 

20.A  = 

- 2  4 

18. A =  

1 

exercises 

In Exercises 
using AT. 
given 
17  22. Exercise 18 
21. Exercise 
19  24. Exercise 20 
23. Exercise 
25. Explain 
carefully 

why your answers 
even though 

to Exercises 17 
there appear to be 

and 21 are both correct 
differences. 

26. Explain 

carefully 

why your answers 
even though 

to Exercises 18 
there appear to 

and 22 are both correct 
be differences. 

find a basis 

for the span of the given 

In Exercises 
27-30, 
vectors. 

27. [-il [-�J [ J 28. [-:J m. [:J m 

29.  [2 

- 3  1 ] ,  [ 1 - 1  O ] ,  [4 -4 1 ]  

30. [ 0  1 -2 1 ] ,  [ 3 

1 - 1  0 ] ,  [ 2  1  5 

1 ]  

31 and 32, find bases for the 

spans of the 

exercises 

from among the 

vectors 

For Exercises 
vectors 
in the given 
themselves. 
31. Exercise 29 
33. Pr

32. Exercise 30 

ove that if R is a matrix in echelon 
of the nonzero rows of R. 

for row(R) consists 

form, then a basis 

34. Prove that ifthe columns 

of A are linearly indepen­

dent, 

then they must form a basis for col(A). 

35-38, give the rank and the nullity of the 

exercises. 

For Exercises 
matrices in the given 
35. Exercise 17 
36. Exercise 18 
37. Exercise 19 
38. Exercise 20 
39. If A is a 3 X  5 

40. If A is a 4 X 2 
be linearly dependent. 
41. If A is a 3 X  5 

nullity

(A)? 

matrix, explain 

why the columns 

of A 

must be 

linearly dependent. 

matrix, explain why 

the rows of A must 

matrix, 

what are the possible values 

of 

42. If A is a 4 X 2 

matrix, 

what are the possible values 

of 

(A)? 

- 2 1 

nullity

vector

-2 -1 a 

values of rank( A) as 

Exercises 

by considering 

the matrix with the 

Answer 
given 

43 and 44, find all possible 

45-48 
s as its columns. 

In Exercises 
a varies. 

2 al 4a 2 
44. A  = [ � � = � l 
45. Do [J [ H [:J form a b"i' foe�'? 
46. Do [-J [-n [-1] focm a bM1'fm �'? 
rn focm a bMi' foe�'' 
47. Do r i l m m 

Section 

3.6 Introduction 

Transforma
tions 

to Linear 

2 1 1  

every 

3.1.] 

in null(A) is 

57. If 

has rank n. 

of rank n, prove that AB 

A ism  X n, prove that every 
vector 
orthogonal to 
vector 

in row(A). 
58. If A and B are n X n matrices 

Exercise 29 
in Section 

(b) Give an example 

59. (a) Prove that rank(AB) :::::: rank(B). [Hint: Review 
60. (a) Prove that rank(AB) :::::: rank(A). [Hint: 
61. (a) Prove that if U is invertible, 
in which rank(AB) < rank(A). 
A = U-1( UA).] 

then rank( UA) = 
(b) Prove that if V is invertible, 
then rank(A V) = 

in which rank(AB) < rank(B). 
Review 

Exercise 30 in 
Exercise 
59(a).] 

rank(A). [Hint: 
rank(A). 

(b) Give an example 

3.1 or use transposes and 

Section 

62. Prove that an m X n matrix 
as the outer 

A can be written 
in !Rm and v in !Rn. 

A has rank 1 if and only if 
product uvT of a vector 

u 

63. If an m X n matrix A has rank 
r, prove that A can be 
as the 
each of which has 
[Hint: Find a way to use Exercise 62

sum of r matrices, 
.] 

written 
rank 1. 

64. Prove that, for 

m X n matrices 

rank(A) + rank(B). 
65. Let A be 

an n X n matrix such 

A and B, rank (A + B) :::::: 

that A 2 = 0. Prove that 

rank(A) :::::: n/2. [Hint: Show that col(A) 
use the Rank Theorem.] 

� null(A) and 

66. Let A be a skew-symmetric 

(See page 162). 

(a) Prove that xT Ax= 0 for all x in !Rn. 

n X n matrix. 

(b) Prove that 

I +  A is invertible. 

[Hint: 

Show that 

null(I + A) = {O}.] 

49. Do m [: J m 
50. Do m [: J [ � l 

Zl? 
fmm a bn<i• fo, Zj? 
51 and 52, show that w is in 

form a b.,i, fo, 

span(B) and find 

In Exercises 
the coordinate vector 

[w]8. 

1 

0 �] omZ, 

3 
0 

1 
0 

53-56, compute the rank and nullity of the 
"ll..P" 
:J ov°' z, 

In Exercises 
given matrices over the indicated 

54. [� 1 
53. [� 1 
55. [� 3 
�]om Z; 
0 �] omZ, 
56. [! 4 
0 

1 
0 
4 
0 
5 
2 

1 
2 
1 

3 
0 

Intro d u ction to linear Transformations 

one of the themes 
can be used to transform 

we begin to explore 
There we saw that matrices 

In this section, 
chapter. 
of "function
pendent variable 
at several 
examples 
transformation-a powerful idea that we will encounter 

'' of the form w =  T(v), where the independent 
this notion 
of such matrix transformations, 

vectors, acting 
variable 
v and the de­
now and 
more precise 
look 
to the concept 
from here on. 

w are vectors. We will make 

to this 
as a type 

leading 

repeatedly 

from the introduction 

of a linear 

2 1 2   Chapter 

3 Matrices 

some of the 

with most 
of these 

by recalling 
ideas 

We begin 
be familiar 
tions 
bers. What 
is new 
that are "compatible" 
with the 

of the form f: IR ---+ IR [such as f(x) = x2] that transform 

basic concepts 
courses 

are involved 
operations 

here is that vectors 

from other 

vector 

associated 

with functions. 
You will 

in which you encountered 

real numbers 

func­
into real num­

and we are interested only 

in functions 

of addition 
and scalar 

multiplica
tion. 

Consider 

an example. Let 

Then 

generally. 

We can describe 

this transformation more 

Thi,,hows that A tmmfmmn into w  � [ _ :J 
[ � -� l [;] = [ 2x � y l 
vectm [ 2x "_ y] in D;l'. We denote 
3x + 4y  r,([ x]) � [ 2x � Y l 

3 4  3x + 4y 
that shows how A transforms 

an arbitrary 

gives a 

formula 

this tmmfmmotion 

y  3x + 4y 
some writing. The 

the parentheses 

(Although 
technica
is a common convent

lly sloppy, omitting 

ion that saves 

The matrix equation 

vector [;] in IR2 into the 

by T, and w'ite 

in definitions 
description of 

such as this one 
TA becomes 

vector 

With this example 

in mind, we now consider 

some terminolo
assigns 

gy. A transformation 
v in !Rn 
to each vector 
of T is !Rn, and the codomain of T is !Rm. We 
T(v) 

with this convention.) 
(or mapping or function) T from !Rn to !Rm is a rule that 
a unique 
v in the domain 
indica
in the co domain 
images 

T(v) in !Rm. The domain 
te this by writing 
is called 
T(v) (as v varies 

T : !Rn---+ !Rm. For a vector 

T,, D;l'--. D;l'. The image of v � [ _ :J is w  � T,(v) � [ _ i J 

of T) is called 
t the domain 
of TA is IR2 and its codomain 

the image of v under (the action 
throughou

of T, the vector 
of all possible 

is IR3, so  we write 

In our example, the domain 

What is thecange of 

of) T. The set 

the range of T. 

3.6 Introduction 

Transformations 

to Linear 

2 1 3  

TA? It consists 

of all vectors 

Section 
TA[x] = [ 2x � y] = x[�] + y[-�] 

y  3x + 4y  3  4 

codomain IR3 that are of the form 

in the 

whi<h d�mib"  the "t of •ll linm wmbimtiom of the  wlumn ve<to" [ �] 
and [-�i of A. In othe; wo'd',  the 

'ange of T IB the  wlumn 'P"'e of A! (We 

say about this later-for now we'll 

will have more to 
observation.) Geometric
ally, 
origin 
range of 

in IR3 with direction 
TA is strictly smaller than 

note it 
this shows that the range of TA is the plane 
vectors 

given 
the co domain of TA. 

by the column vectors 

simply 

through the 
that the 

of A. Notice 

as an interesting 

Transformations 

linear 
The example 
a linear 
essence 
of addition 

TA above is a special case of a more general 

type of transforma

tion called 

transformation. We will consider 
are the transformations 
of it is that these 

the gener

al definition 

in Chapter 6, but the 

that "preserv

e" the vector 

operations 

and scalar multiplication. 

Defi n ition A transforma
1 .  T(u + v) 
2. T(cv) = cT(v) for all v in !Rn and all scalars c. 

= T(u) + T(v) for all u and v in !Rn and 

tion T :  !Rn---+ !Rm is called 

a linear 

transformation if 

Example 3 . 5 5  

Consider 

once again 

the transforma

by 

tion T :  IR2 ---+ IR3 defined 

r[;] � [ :: ; � l 

u = [;:J and  v = [;:J 

transformation. 

To verify (1), 

we let 

Let's 

check that T is a linear 

Then 

2 1 4   Chapter 

3 Matrices 

To show (2), we let v  = [;] and let c be a scalar. Then 
T(cv) = r(c[;]) = r([:;]) 
(cy) ] = [ c(2
[ 2(cx) 
:x__ 
c� 
y) ] 
3(cx) + 4(cy) c(3x + 4y) 
= cT(v) 

Thus, T is a linear 

transforma

tion. 

bining 

Remark The definition 
(1) and (2) as shown below. 

T :  !Rn � !Rm is a linear 

transformation if 

of a linear 

transforma

tion can be 

streamlined 

by com­

In Exercise 53, you will be asked to show that the statemen
this equivalent 
to the original 
writing-try it! 
transformation T in Example 3.55 
origina
the matrix 

tion TA, it is a simple matter to recover 

definition. In practice, 

transforma
T given in the example. We observe that 

Although 

the linear 

t above is equivalent 

formulation 

can save some 

lly arose as a matrix 
A from the definition 

of 

(N otkethat when the vmiahles 

x ond y "' J;ned 

rn T � TA, whm A � [: -� J 
Let A be an m X n matrix. 

that a transformation is a matrix 
as the next theorem shows, all matrix transforma

Recognizing 

Then the matrix 

coefficient matrix.) 

up, the matrix 

A is just their 

transforma
tions 

are linear 

tion is 

important, 

since, 

transforma

tions. 

transformation 

TA : !Rn � !Rm defined 

by 

is a linear 

transformation. 

Theorem 3 . 3 0  

Section 

3.6 Introduction 

Transformations 

to Linear 

2 1 5  

Proof Let u and v be vectors 

in !Rn and let 

c be a scalar. Then 

Example 3 . 5 6  

tion that sends each point to its ref

lection 
in the 

and 

TA(cv) = A(cv) = c (Av) = cTA(v) 

tion. 

Hence, 

transforma

TA is a linear 

Let F : IR2 ---+ IR2 be the transforma

x-axis. Show that F is a linear 

transforma

tion. 

Solution From Figure 3.4, it is clear that 
Thus, we may 

write 

F sends the 

point (x, y) to the point (x, -y). 

y 

(1 ,  2) T  (x, y) T I I I 
I 
I 
I 
I 
I 
�+----+1�--+-�� :1--•x 
I 
• 
I 
•  (x, -y) 
in the x-axis 
Figure 3 . 4  Reflection 

Therefore, 
now follows, 

(1 ,  -2) 

Example 3 . 5 1  

We could proceed 
to check!), but it is faster 

to observe that 

to check that F is linear, as in 

Example 

3.55 (this 

one is 

even easier 

F[;] = A[;], where A = [ � _ �], so F is a matrix 

that F is a linear 

transforma

tion. 

by Theorem 3.30, 

transforma
It 

tion. 

Let R : IR2 ---+ IR2 be the transformation that rotates 

about the origin. 

Show that R is a linear 

each point 90° counterclo

ckwise 

transformation. 

Solution As Figure 3.5 shows, 
we have 

R sends the point (x, y) to the point (-y, x). Thus, 

Hence, 

R is a matrix 

transforma

tion and is therefore 
linear

. 

-y 

x 

Figure 3 . 5  

A 90° rotation 

umns of the matrix. 

For example, 

Observe that if we multiply a 

matrix by standard 
basis 

vectors, 

we obtain 

the col­

We can use this 

!Rm arises 

as a matrix 

observation to show that every linear 
transforma

tion. 

transformation from !Rn to 

2 1 6   Chapter 

ally, T = TA> where A is the m  X n matrix 

3 Matrices 
Theorem 3 . 3 1  Let T : !Rn � !Rm b e  a linear 
Proof Let e1, e2, ••. , en be the 
ofx). We also know that T(e1), T(e2), •.• , T(en) are (column) 
[T(e1): T(e2): • • ·: T(en)l be them  X n matrix with these 

standard basis vectors 
xnen (where 

x = x1e1 + x2e2 +· · · + 

in !Rn. We can write 

More specific

transforma

tion. 

Then 

vector 

in !Rn and let x be a 
the x/s are the components 
in !Rm. Let A = 
as its columns. 

vectors 
vectors 

Then T is a matrix transformation. 

T(x) =  T(x1e1 + X2e2 + . . .  + xnen) 

= x1 T(e1) + x2 T(e2) + · · · + 

Xn T(en) 

T(e,)] [ }] 

� Ax 

� [ T(e,) T(e,) : · · · 

as required. 

The matrix 

A in Theorem 3.31 is called 

the standard 

matrix of the linear 

trans­

formation 

T. 

Example 3 .5 8   Show that a rotation 

about the origin 

through an angle 

a linear 

transforma­

e defines 

tion from IR2 to IR2 and find its standard matrix. 
Solulion Let Re be the rotation. 
the fact that R0 is linear. 
Figure 
the entire 
diagonal 

Let u and v be vectors 
3.6(a) shows the parallelogram 

parallelogram 
of this parallelogram 
R0(u + v) = R0(u) + R0(v). (What happens 

is rotated through 
must be R0(u) + R0(v), again 

We will give 

the angle 

to establish 

argument 

a geometric 
in IR2• If they are not parallel, 
e, as shown in 

u + v. If we now apply 
3.6(b ). But the 

then 
R0, 

Figure 

by the parallelogram 

rule. 

ifu and v are parallel?) 

rule that determines 

I I I I 

Re(v)' 

,.......... Hence, 

y 

u+v r -
,.......... R8(cv) = cR8(v), as required. 

Similarly, 

3.7. But since 

in Figure 

Figure 3 . 6  

u 

(a) 

x 

and c < - 1.) 

y 

x 

(b) 

if we apply Re to v and cv, we obtain 

R0(v) and Re(cv), as shown 

the rotation 

does not affect lengths, we must then 

have 

(Draw diagrams 

for the 

cases 

0 < c < 1, - 1  < c < 0, 

Section 

3.6 Introduction 

Transforma
tions 

to Linear 

2 1 1  

y 

Re (cv) 

CV 

y 

'--_,-' cos 8 

(1, 0) 

Figure 3 . 1  

Figure 3 . 8  

R0(e1) 

We can find R

Therefore, 

Re is a linear 

its matrix by determining 
as Figure 3.8 shows, 

transforma

tion. 
its effect on the sta

According to Theorem 3.31, we can find 
e1 and e2 of IFR2• Now, 
ndard basis vectors 

but it is faster to observe that Re [ �] must be 

per-

0  sm () 

R0[1] = [c�s 0]. 
e [ �] similarly, 
ckwise) to R0[1] and so, by Example 
[cos () 
matrix of Re is . sm () 

-sin()]. 

cos() 

3.57, R8[0] = [- sin()] 

1  cos() 

pendicular 
(F. 3 9) 
1gure . .  

(counterclo
0 

Therefore, 

the standard 

y 

Figure 3 . 9  

Re(ez) 

The result of Example 
For example, suppose we wish to rotate 

used to compute the effect of any rota­
the point (2, - 1) through 
60° about the 
that a positive angle corresponds 

3.58 can now be 

(The convention is 

tion. 
origin. 

to a counterclo

ckwise 

2 1 8   Chapter 

3 Matrices 

y 

(2, - I) 

rotation, 
'\/3 /2, we compute 

while a negative 

angle is clockwis

e.) Since 

cos 60° = 1/2  and  sin 

60° = 

60 - 1  sin 60° 

R [ 2] = [cos60° 

cos 60° - 1  [ 1/2 - '\/3/2][ 2] 
- sin 60°] [ 2] 
(2'\/3 -1)/2) =  ( 1.87, 1.23), as shown in Figure 

[ (2 + '\/3 )/2] 

Thus, the image of the point (2, - 1) under this rotation 

is the point ((2 + '\/3)/2, 

'\/3/2  1/2 - 1  

(2 '\/3  - 1)/2 

3.10. 

Figure 3 . 1 0  

A 60° rotation 

Example 3 .5 9  

transforma

(a) Show that the transforma
a linear 
(b) More generally, 

tion P : IR2---+ IR2 that proj
if e is a line through the origin 
tion Pe : IR2---+ IR2 that projects a point onto e is a linear 

tion and find its standard matrix. 

in IR2, show that the transforma­
tion and find its 

transforma

ects a point onto the x-axis is 

standard matrix. 

Solution (a) As Figure 3.1 1  shows, 

P sends the point (x, y) to the point (x, O). 
Thus, 

It follows 

that P is a matrix transforma

tion (and hence a linear 

transformation) 

with 

y 

(x, y) T 

-+------+---+ x 
A projection 

Figure 3 . 1 1  

(x, 0) 

standard matrix [ � �] . 

line e have direction 

(b) Let the 
given by projd(v), the projection 
the formula 

vector 
ofv onto d, which you'll 

d and let v be an arbitrary vector. Then Pe is 
1 .2 has 

from Section 

recall 

Thus, to 

show that Pe is linear

, we proceed 

as follows: 

d 

d · d  

Pe(u + v) = 
d�d
(d ·  (u  + v)) 
=  ( d · u
d · v)d 
= -
+-
d (d · u  d · v) 
(d · u) (d • v) 

= d · d  

d · d   d · d  

d · d  

d  + 

d = Pe(u) + Pe(v) 

ly, Pe(cv) = cPe(v) 

Similar
transforma

tion. 

for any scalar c (Exercise 

52). Hence, 

Pe is a linear 

Section 

3.6 Introduction 

Transformations 

to Linear 
let d =  [ �:], then 

2 1 9  

To find the 

standard matrix of Pe, we apply Theorem 3 .31. If we 

and 

is 

dU(dl + di) 

Thus, the standard 

x-axis. Therefore, 

matrix of the projection 

d�  d1d2/(d? + di) 

d1d2] =  [ dU(df + dD 
d1 =  1 and d2 =  0, and we obtain 

d1d2/(di + di)] 
As a check, note that in part (a) we could take d =  e1 as a direction 
A =  [ � �], as before. 4 
S 0 T. Notice 
composite transformation S 0 T goes 
S 0 T :  !Rm� [RP). Figure 

New Linear 
Transformations 
from Old 
If T : !Rm � !Rn and S : !Rn � [RP are linear 
S to form the 
the codomain 

order for S 0  T to make sense, 

composition 
they are both !Rn) and the resulting 

of T to the codomain 
lly how this composition works. The formal definition 

then we may follow T by 
that, 

(in this case, 
from the domain 
shows schematica
tion of transformations 
responding 

of the two transformations, 

is taken directly from this 
figure and is the same as the cor­
functions: 

tions, 
denoted 

3.12 
of composi­

of S (in this case, 

definition 

transforma

vector 
for the 

in 

of T and the domain of S must match 

Of course, 
find that it is. We 
a linear 
transforma
we are assuming 
thing, 
tion [ 

transforma

of composition of ordinary 

that linear 
to show 

can demonst
tion (which we will do in Chapter 6), but, since 

(S o  T)(v) =  S(T(v)) 
we would like S 0 T to be a linear 
rate this by showing that S 0 T satisfies 
that S 0 T is a matrix 
�m 
�P 
= (S 0 T)(v) 
T 
•S(T(v)) 

�n 
S 
•�
T(v) 

transformations 

transformation T. 

of a linear 

v •�

it is enough 
T] for the standard matrix 

tion too, and happily 

we 
of 
for the time being 

the definition 

and matrix transformations 

are the same 
transformation. 

We will use the 
nota­

tion 
Figure 3 . 1 2  The composi
of transforma

tions 

2 2 0   Chapter 

3 Matrices 

Theorem 3 . 3 2  Let T: !Rm---+ !Rn and S : !Rn ---+ !RP 

be linear 

transforma
standar

tions. Then S 0 T: !Rm ---+ !RP 
[ S  0  T J  = [SJ [ TJ 
that A is p X n and B is n X m.) If v is a vector 

are related 

d matrices 

by 

Moreover, their 

is a linear 

transforma

tion. 

in !Rm, then we simply compute 

Proof Let [SJ = A  and [TJ = B. 
(S 0  T)(v) =  S(T(v)) =  S(Bv) = A(Bv) = (AB)v 
sense.) Thus, we see that the effect of S 0 T is to multiply 
tely that  S 0  T is a matrix (hence, 
[S 0 T J  = [SJ [TJ. 

dimensions 

that the 

of A and B 

it follows 

immedia

guarantee 

(Notice 

that the 
vectors 
linear) 

product AB makes 
by AB, from which 
transformation with 

this a great result? 

Say it in words

: "The matrix of the composite is the 

prod­

Isn't 
uct of 

the matrices:' 

What a lovely 

formula! 

......... (Notice here 

Example 3 . 6 0  

Consider 

the linear 

transformation T :  IR2 ---+ IR3 from Example 

by 

3.55, 

defined 

x� x2 ] 3x1 + 4X2 
r[::J = [ 2x1 

and the linear 

transformation S : IR3 ---+ IR4 defined 

by 

Solulion We see that the standar

d matrices 

are 

[ SJ 

[S o  TJ [ S J [ TJ 

so Theorem 3.32 gives [ ! _ � -� l and I T I [� -:l 
- 1  -m� -�i [ : -�] - 1  1 
[� 0 
[-1 -l l [:;] ['" + �, l 
-xi + X2 
6x1 + 3x2 

(Sa T)[:J 

It follows 

3x1 -7x2 

6 3 

that 

3 

(In Exercise 29, you will be asked to check this result 

2 2 1  

Transformations 

to Linear 

Section 
3.6 Introduction 
[;:] = r[::J = [ 2x1 

x� x2 ] 

by setting 

y3 
into the 
these 

3x, + 4x2 
of S, thereby 

definition 

values 

calculating 

and substi
dice<tly.) 

tuting 

(S 0 T) [ x,] 
.:+ 

Example 3 . 6 1  

that the 

lt in the x-axis. 

about the origin 

a point 90° counter­

Find the standard 
clockwise 

matrix of the transformation 

rotates 
and then reflects the resu

that first 

R and the 

in Examples 
3.57 and 
�] and 

Solution The rotation 
3.56, respecti
vely, 

reflection 
where we found their sta

F were discussed 
ndard matrices to 
its matrix 

be [ R ] = [ � -
[ F ]  = [ 1 O]. It follows 
composition F 0 R has for 
0 -1 [1  o][o -1]  [ o -1] 
0 -1 1 0  -1 0 
ii--""-- (Check that this result is correct 
the effect of F 0 R on the standard 
before F, but we write F 0 R. In this case, R ° F also makes sense. 
4 

basis vectors 
R is performed 
R o F  = F o R?) 

e1 and e2. Note the importance 

[Fa R ] = [F] [R ] = 

by considering 

order of the 

transforma

tions: 
Is 

of the 

= 

Transformations 
the effect of a 90° counterclockwi

Inverses of linear 
Consider 
a 90° clockwise 
changed. 
tive angle 

se rotation about the origin 

followed 

by 

these 

rotation 

. Clearly this leaves 

about the origin

transformations 
direction

Ifwe denote 
measure corresponds 

as (R90 ° R_90) (v) = v for every v in IR2. Note that, 
Thus, R90 ° R_90 (and R_90 ° R90 too) is a linear 

transformations 
for every v 

every point in IR2 un­
by R90 and R_90 (remember 
in this case, if 

(R_90 ° R90) (v) = v 

order, we get the same end result: 

transforma

to clockwise 

in the other 

we perform the 

), then we may express this 

that a 
nega­

in IR2• 

in IR2 unchanged. 

vector 
Such a transformation is called 
Generally, 
that I(v) = v for everyv in !Rn. (If it is important to keep 
space, 

we have one such transforma

we might write 

In for clarity.
So, with this notation, 
that are related to each other 

tions 

track of the 

dimension 

tion that leaves 

an identity transformation. 

every 
tion for every !Rn-namely, I :  !Rn� !Rn such 
of the 

we have R90 ° R_90 = I= R_90 ° R90. A pair of transforma­
if S 0 T = In and T 0 S = In-

from !Rn to !Rn. Then Sand T 

in this way are called 

transforma

transformations. 

inverse 

tions 

Defi n i t i o n  Let Sand T be linear 
are inverse 

transformations 

) 

3 Matrices 

2 2 2   Chapter 
�  tions, 

tely that if Sand T are inverse transforma-

S is the 

this definition 

Remark Since 
when this situation 
occurs, 

is symmetric 
inverse of 

with respect to S and T, we will say 
T and T is the 
inverse of S. Further­

In terms of matrices, 

that, 
more, we will say that S and T are invertible. 

then [SJ [TJ = [S 0 TJ = [JJ = I, where the last I 

must also have [T ][S J = [T 0 S J = [JJ = I. This sh

is the standard matrix 

of the identity 

transformation 

we see immedia

is the identity 
matrix. (Why 
matrix?) We 
the identity 
ows that [SJ and [TJ are inverse 
tion T is invertible, 
then its 

more: If a linear 

. It shows something 

J must be invertible, and 

matrices
standard matrix  [ T 
means that the inverse of T is also unique. Therefore, 
notation 
[TJ [T
We have just proved 

r-1 to refer to the inverse of 
the following 
theorem. 

matrix inverses 
we can unambiguou
sly use the 
the above equations 
as 
inverse matrix of [TJ. 

-1J =I= [T-1J [TJ, showing that the matrix 

T. Thus, we can rewrite 
of r-1 is the 

transforma
since 

are unique, this 

Theorem 3 . 3 3  

Let T: !Rn---+ !Rn be an invertible 
linear 
[ T J 

is an invertible 
matrix, 

and 

transformation. 

Then its standard 

matrix 

Remark Say this one in words 

too: "The matrix of the inverse is the inverse of 

the matrix:' Fabulous

! 

Example 3 .6 2  

Find the standard 

matrix of a 60° clockwise 

rotation 

in IR2• 
origin 
rotation about 
the matrix of a 60° counterclockwise 

about the 

be 

we computed 

Solulion Earlier 
the origin to 

- V3/2] 1/2 
- ( )-1 -[ 1/2 
- V3/2]-1 
1/2 [ 1/2 
lt--l'-- (Check the calculation 

Since 
apply Theorem 3.33 to obtain 

[R-6oJ -[  R6o J - V3/2 

of the matrix inverse. The fastest 

is the inverse of a 60° counterclo

a 60° clockwise 

rotation 

- V3/2 

cut from 
Theorem 3.8. Also, 
.) 
standard 
basis 

in IR2 by drawing 

a diagram

check that the resulting 

ckwise 

rotation, 

we can 

V3/2] 1/2 
4 

effect on the 

short­

way is to 

use the 2 X 2 

matrix has the 
right 

Example 3 . 6 3  Determine 

whether 
it is, find its inverse. 

projection 

onto the x-axis is an invertible 

transformation, 

and if 

Solulion The standard 
since 

its determina

nt is 0. Hence, 

matrix of this projection 

P is not 

invertible 

either

P is [ � 
. �], which is not invertible 

4 

3.6 Introduction 

Transforma
tions 

to Linear 

2 2 3  

Figure 3 . 1 3  

The 

"colla

this cannot 

many candida

some idea 

why P in Example 

3.63 is not invertible. 

Remark Figure 3.13 gives 

tes for the image of 

x-axis. For P to be invertible, 

when we have no way of knowing 

we would have to have 
(a, O) under such a hypothetica
with. However, 
there 
are 
l "inverse:' 
simply say that P-1 must send (a, 0) to (a, b), 
what b should 
be. 

Section 
to recover the point (a, b) we started 

we use? We cannot 
be a definition 

pses" IR2 onto the 
projection 
a way of "undoing" it, 
infinitely 
Which one should 
since 
(See Exercise 42.) 

T (a, b) 
I �(a, b') 
I (a, 0) 
I •(a, b'') 
Projections 
are not 
invertible 
�  cation: 
m  X n matrix 
�  We now prove the 

matrices 
restricted 
involved 
in an 
are about to give.) 

Associalivilv 
Theorem 3.3(a) in Section 3.2 
stated 
A(BC) = (AB)C. (If you didn't 

property for matrix multipli-
do so now. Even with all 
complexity 
"elementwise" proof, which should make you appreciate 
the proof 

the associativity 
try to prove it then, 

2  X 2, you will get some feeling 

Our approach 
A gives 

linear 
transformation 
correspondences 

T : !Rn ---+ !Rm has a corresp

TA : !Rn ---+ !Rm; convers
m  X n matrix 
main of both R 0 (S 0 T) and (R 0 S) 0 T-why?] .  To prove that R 0 (S 0 T) = (R 0 S) 0 T, 

A(BC) = (AB)C if and only if R 0 (S 0  T) = (R 0 S) a T 

onding 
[ T]. The two 
A, [TA] = A, and given T, TrTJ = T. 

Tn, and T =Tc. Then, by Theorem 3.32, 

transformation 

Let x be in the domain 

for the notational 

are inversely 

of T [and hence in the do­

identity. 

rise to a linear 

Let R =TA, S = 

is via linear 

transforma

related; 

that is, given 

to the proof 

ely, every 

latter 

we 

tions. We have seen that every 

it is enough to prove that they 
definition 
we have 

of composition, 

(Ra (S a T))(x) =  R((S 0  T)(x)) 

have the same effect on x. By repeated 

application 

of the 

= (R 0 S)(T(x)) = ((R 0 S) 0  T)(x) 

=  R(S(T(x))) 

(Carefully 

check how the 

definition 

of composi

tion has been used 

four 

�  as required. 

times.) 

This section 

has served as 

an introduction 

to linear 

and more general 

some additional 

transforma

look at these 
explorations 

tions. In Chap­
transforma
tions. 
of this important 

ter 6, we will take 
a more detailed 
The exercises that follow also contain 
concept. 

3 . 6  

I Exercises 
[ 3 - 1] 
IR2 ---+ IR3 be the 
1. Let TA : IR2 ---+ IR2 be the matrix transforma
sponding to A =  [� -:]. Find TA(u) and TA(v), 
sponding to A  = � ! . Find TA ( u) and 
where u = [�] and v = [ -�l  TA(v), where u = [�]and v = [ _�J. 

2. Let TA : 

tion corre­

matrix transforma

tion corre-

2 2 4   Chapter 
3 Matrices 
that the given transformation is a 
In Exercises 
3.55). 
using the definition (or the Remark 
linear 
followin

3-6, prove 
transformation, 
g Example 

20-25, find the standard matrix of the given 
transformation 
origin 

In Exercises 
linear 
from IR2 to IR2. 
20. Counterclo

ckwise rotation through 

120° about the 

the origin 

21. Clockwise 
22. Projection 
23. Projection 
24. Reflection 
25. Reflection 

rotation through 30° about 
y = 2x 
onto the line 
y = -x 
onto the line 
in the line y = x 
in the line y = -x 
26. Let e be a line through 

the origin 

in IR2, Pe the linear 
onto e, and Fe the 
in e. 
Fe is linear. 
the matrix 
ls of a parallelogram 

of Fe, 

that reflects 

transformation that projects a vector 
transformation 
a vector 
(a) Draw diagrams 
(b) Figure 

to show that 
a way to find 

3.14 suggests 
using 
the fact that the diagona
bisect each other. Prove 
and use this result 
to show that 
of Fe is 

that Fe(x) = 2Pe(x) - x, 
matrix 

the standard 

the direction vec

tor of 

between 
show that the matrix of Fe is 

(where 

(c) If the angle 
[cosW 

sinW 

e is d = [ �:]). 
positive x-axis is e, 
e and the 
sinW] - cosW 

to show that the 
In Exercises 
given transformation is not a linear transformation. 

7-10, give a counterexample 
7. r[; J [:2 J  8. r[; J 
10. r[; J [ lxl] IYI 
9. r[;J [x 7 J  [x + l] y - 1 
11-14, find the standard matrix of the linear 
In Exercises 
transformation in the given exercise. 
1 1 .  Exercise 3 
15-18, show that the given transforma-
13. Exercise 5 
In Exercises 
tion 
transformation. 
15. F reflects 
16. R rotates 
origin. 
a vector 
17. D stretches 

from IR2 to IR2 is linear by showing that it is a matrix 

12. Exercise 
14. Exercise 

in the y-axis. 
45° counterclo

a vector 
a vector 

by a factor of2 in the x-component 

ckwise 

about the 

4 
6 

and a factor of 3 in they-component. 

y 

y = x. 

18. P projects a vector 
19. The three 

onto the line 
of elementary 
matrices 

to five 
with one of the following 
forms: 

types 
of 2 X 2 

types 

matrices give rise 

[ � �] or [ � �] 
[� �] 
[ � �] or [� �] 

matrices 

elementary 

Each of these 
transformation from IR2 to IR2• Draw pictures 
the effect of each one on the 
at 
(0, O), (1, O), (0, 1), and (1, 1). 

unit square with vertices 

corresponds 

to a linear 

to illustrate 

Figure 3 . 1 4  

27 and 28, apply 
In Exercises 
part (b) or (c) of Exercise 
to find the standard matrix of the transformation. 
in the line y = 2x 
27. Reflection 

26 

formula 
the suggested 

28. Reflection 
29. Check the 
performing 

for S 0 T in Example 
in the line y = \/3x 
3.32 by finding the 
30-35, verify Theorem 
matrix of S 0 T (a) by direct substitution 
In Exercises 
and (b) by matrix 
of [S] [T]. 
multiplication 

direct substi

3.60, by 
tution. 

225 

26.) 

angle 

lines 

transforma

transforma

41. If() is the 

be invertible. 

can never 

between lines 

42. (a) If P is a projection, 

(b) The matrix of a projection 

then 
through 
the origin, 
in a line through 
the 

tion from IR2 to IR2 (or 
a 
vector 

Section 
3.6 Introduction 
Transforma
tions 
to Linear 
origin), then Fm ° Fe= R+w· (See Exercise 
e and m (through the 
then P 0 P = P. 
Fn ° Fm ° Fe is also a reflection 
43. If e, m, and n are three 
origin. 
44. Let T be a linear 
from IR3 to IR3). Prove that T maps a straight 
line to 
straight line or a point. [Hint: Use the 
form of 
the equation 
of a line.] 
45. Let T be a linear 
tion from IR2 to IR2 (or 
from IR3 to IR3). Prove that T maps parallel 
a pair of points, 
lines, 
parallel 
point. 
46-51, let ABCD be the square with vertices 
( - 1, 1), (1, 1), (1, - 1), and (- l, - 1). Use the results 
In Exercises 
44 and 45 to find and draw the image of ABCD 
in 
Exercises 
under the given transformation. 
46. T in Exercise 
3 
47. D in Exercise 
17 
48. P in Exercise 18 
49. The projection 
50. T in Exercise 
51. The transforma
52. Prove that Pe(cv) = cPe(v) for any scalar c 

lines to 
or a single 

53. Prove that T: !Rn ---+ !Rm is a linear 

transforma

a single line, 

tion in Exercise 37 

tion if and 

[Example 

in Exercise 22 

3.59(b)] .  

31 

only if 

36-39, find the standard matrix of the compos­
ckwise 
in the line y = x 
in the y-axis, followed 

In Exercises 
ite transformation from IR2 to IR2• 
36. Counterclo
reflection 
37. Reflection 

through 60°, followed 

by clockwise 

rotation 

by 

rotation 

through 30° 

38. Clockwise 

rotation 

by projec­
through 45°, followed 
by clockwise 
rotation 

39. Reflection 

wise rotation 

through 30°, followed 

tion onto the y-axis, followed 
through 45° 

in the line y = x, followed 
liney = -x 
40-43, use matrices to prove 
In Exercises 
from IR2 to IR2• 
ments about transformations 
angle(), then R" 0 R13 = Ra+f3· 
(about the 
a rotation 
40. If Re denotes 

by counterclock­
in the 
by reflection 
state­

the given 

54. Prove 

that (as noted at the beginning 

the range of a linear 
column 
space of 

transformation T : !Rn ---+ !Rm is the 

for all v1, v2 in !Rn and scalars c1, c2. 
its matrix [ T]. 
2 X 2 
55. If A is an invertible 
matrix, what does the 
Theorem of Invertible 
assert 
transformation TA in 

Fundamental 
about the 
light 
19? 

ofExercise 

corresponding 

Matrices 

linear 

origin) 

through 

the 

of this section) 

Vignette 

Robotics 

Remote 

In 1981, the U.S. Space Shuttle 
Shuttle 
has proved to be a vital 
yet precise 

Manipulator System 

Columbia blasted 
space 

tool in all subsequent 

and delicate handling 
their 

of its payloads 

(SRMS). This robotic 

(see Figure 
3.15). 

off equipped with a device 

called 

the 

arm, known as Canadarm, 
shuttle missions, 
strong, 

providing 

Canadarm has been used to place 
for repair, 

Notably, 

malfunctioning ones 
tle itself. 
Hubble Space Telescope. 
Since 1998, Canadarm 
of the Intern
assembly and operation 

and it has also performed 
arm was instrumenta
has played 
ational Space Statio

the robotic 

satellites into 
proper 
critical 

orbit and 
repairs 

to retrieve 
to the 
shut­
l in the successful repair 
of the 
role in the 

an important 

n. 

;;; <( z 

35 <( z 

Figure 3 . 1 5  Canadarm 

2 2 6  

A robotic 

arm consists 

of a series 

of links of fixed length 
in space, 

connected 

at joints where 

links) 

therefore rotate 

Each link can 
be translated 
parallel 

they can rotate. 
other 
rotations 
arm, we need to understand 
simplify 

or move by a combination 
design 
Before we can 
and translations 
how rotations 
that our arm is in IR2. 

we will assume 

or (through the effect of the 
of 

and transla
tions. 

a mathematical 

to itself, 

work in 

matters, 

model for a robotic 

(composition) 

composition. To 

R about the origin through 

an 
3.16(a)). If 

In Section 

angle e is a linear 

v =  : , then a 
[ ] 

transforma

3.6, we saw that the 

of a rotation 

sme cose 
translation along 

matrix 
tion with matrix . 
v is the 

(Figure 
transformation 

[cose - sine] 
T [;] =  [; : : ] 
T(x) =  x + v or, equivalently, 
y  T(x) = x + v 

(Figure 3.16(b)). 

y 

R(x) 

x 

(a) Rotation 

(b) Translation 

Figure 3 . 1 6  

problem

T(O) * 0. How­

because 

We can treat 

. We can represent 
the vector 

x in homogrn,om coor­

is not a linear 

rotations 

the translated 

dinates. Then the 

represents 

a trick that 

translation 

matrix multiplication 

transformation, 

will get us around this 

Unfortunately, 
ever, there is 

x � [;] "' th, mtoc [ �] in D;l'. This i&<oll,d "P""'nting 
0 o][x] [xcose  -y sin el 
[cose 
� ; =  xsine : ycose 
tion T 0 R 
ol [cose � =  si�e 
al [cose 
[i 0 
0 �] 

vector T(x) in homogeneous 
in homogeneous 
- sine 
cose 

the translation 
- sin e  
cose 
0 

sine 
0 
vector 
R followed by 

coordinates. The composi
T is now represented 
by the product 

the rotated 
the rotation 

represents 
that gives 

coordinates. 
too. The matrix 

to those 
frame) and examine how one link moves 
connected. 
the X;-axis aligned 

axes for the link A; be X; 
of A; is denoted 

with the link.  The length 

R(x) in homogeneous 

To model a robotic 

we let the coordinate 

[NotethatR oT *  To R.] 

To be specific, 

b sine 
1  0 

coordinates 

in relation 

to which it is directly 
and y;, with 

arm, we 

- sine 
cose 

1 
0 

by a;, and the angle 

give each link its own coordinate system 

(called a 

221 

multiplication 

bye;. The joint between A; and A;-1 is at 

to A; and (a;_1, O) relative 

between X; and X;-1 is denoted 
(0, O) relative 
system 
(Figure 3.17). This transformation is represented 
the matrix 

e i and then translated 

for A; has been  rotated  through 

along [a� I] 

in homogeneous 
coordinates by 

to A;_1, the coordinate 

to A;_1. Hence, 

relative 

the point 

To give a specific 
which A 1 is in 
45° from the previous 
link. 

links in 
rotated 
Figure 3.18(b) shows A3 in its initial 

example, 
its initia

consider 
l position and each of the other 

Figure 3.18(a). 

It shows an arm with three 

two links 

has been 

take the length 
We will 
of each link 
frame. The transforma
tion 

to be 2 units. 

Figure 3 . 1 1  

45 
0 

[cos45 
T3 =  sin
[cos45 
T2 =  sin

45 
0 

- 1/\/2 

l/v2 
0 

- sin45 
cos 45 
0 

2] [ l/v2 � =  1/:2 - 1/v2 

l/v2 
0 

- sin45 
cos 45 
0 

causes 
places 

a rotation 
A3 in its 

appropria

of 45° and then a translation 

te position relative 

by 2 units. As shown in 3.18(c), 
this 
transformation 

to A2's frame. Next, the 

to the previous 
result

. This places 

is applied 
tion relative 
(a rotation) 
transforma

to A1, as shown in Figure 3.18(d). Normally, a third 
would be applied 
tion because A1 stays 

previous 
in its initial 
Typically, we want to know the coordinates 

both A3 and A2 in their 
result, 
position. 

but in our case, 

of the end 

to the 

arm, given the length 
Following 

the above sequence of calcula

and angle 

parameters-this 
and referring to 

(the "hand") 

robotic 
is known as forward kinematics. 

Figure 3.18, we see that 

of the 

tions 

correct 
posi­
transforma
tion Ti 

Ti is the identity 

2 2 8  

Y1 

Y3 

(a) A three-link 

chain 

Y2 

YI 

(b) A 3 in its initial frame 

(c) T3 puts A3 

in Az's initial frame  (d) 

T2T3 puts A3 in A1's initial 

frame 

Figure 3 . 1 8  

we need to determine 
the arm's 

hand is 

at 

where the point (2, O) ends up after T3 and T2 are applied. 

Thus, 

- 1/ Vl  
1/ V2  
0 

which represents the point (2 + V2, 2 + Vl) in homogeneous 
checked from Figure 3.lS(a) that this is correct. 

coordinates. It is eas
ily 

The methods 

used in this example 
in IR3 there 

are more degrees 

generalize 

to robotic 

dimen­
of freedom and hence more variables. 

arms in three 

of homogeneous 

coordinates 

is also 

useful in other 

applications, 

notably 

although 

sions, 
The method 
computer 

graphics. 

2 2 9  

2 3 0   Chapter 

3 Matrices 

Applications 

a controlled 
survey 
of 200 people, 
of several 
the following 

The sample consists 

over a period 

Markov Chains 
A market research team is conducting 
toothpaste. 
erences in 
try two brands of toothpaste 
to the su
rvey, 
preferences. 
using 
while 30% 
using 
switch 
use it the following month, 
rized 
will think of them as probabilities. 

Brand A in any month, 
to Brand B; of those 

Of those 

the research team compiles 

70% continue 

Brand B in any month, 

to use it the following 

month, 
80% continue 

to 

in Figure 3.19, in which the percentages 

have been converted 

into decimals; 

we 

while 20% switch to 

Brand A. These findings 

are summa­

to determine 
people's pref­
each of whom is asked to 

months. Based on the responses 

statistics 

about toothpaste 

0.70�

0.30 

( 1 856-1 922) 

0.80 

0.20 

A. Markov 
Andrei 
tician 
was a Russian 
mathema
who 
at the 
studied 
and later 
taught 
He 
St. Petersburg. 
University of 
was interested 
, 
in number theory
analysis, 
and the 
of con­
theory 
tinued fractions, 
a recen
tly devel­
oped 
field 
that 
Markov 
applied 
of a (finite) 
number of states. 
to probability 
theory. Markov 
ing process 
At each step or point in time, 
the 
was also 
interested 
in poetry, and 
process 
can remain 
in its 
one of 
the uses to 
which 
he put 
moves 
present state 
the other 
to which the process 
Markov 
chains 
was the 
analysis 
of its doing so depend only on the pres
at the next step and the probability 
ent state 
and other 
of patterns 
in poems 
and not on the 
literary 
texts. 
s and are 
probabilitie
state i to state 
j is always 

past history 
assumed 
the sam
e). 

may be in any one of the states; at the next step, the 

These probabilities 
(that is, the probability 

of the process. 
to be constants 

Figure 3.19 is 
consisting 

Markov chain. It represents 
an evolv­

a simple example 

states. The state 

process 

or switch 

to one of 

of a finite 

Figure 3 . 1 9  

are called transition 

of moving from 

Example 3 .6 4  

Suppose that, 

toothpaste 

probabilities 

when the survey 
begins, 

survey described 
above, 

there are just two states-using Brand 
in 
Brand A and 

In the 
A and using Brand B-and the transition 
Figure 3.19. 
80 people are using Brand B. How many people will be using each 
later? 2 months 
Solution The number of Brand A users 
using Brand A (those 
loyal 
(those 

after 1 month will 
to Brand A) plus 20% of the Brand B users 

120 people are using 

be 70% of those 

brand 1 month 

who remain 

who switch 

from B to A): 

later? 

are those indicated 

initially 

Similar
who switch 

ly, the number of Brand B users 

to Brand B and those who 

after 1 month will be a combination 
of those 
continue 

to use it: 

0.70(120) + 0.20(80) =  100 
0.30(120) + 0.80(80) =  100 

Section 

3.7 Applications 

2 3 1  

We can summarize 
these 

matrix equation: 

two equations 

in a single 

[�:�� �:!�] [l!�] = [���] 

[120] [100] 

the vectors 
80 
are the numbers of Brand A and Brand B users, 

Xo =  and x1 =  . (Note 

P and label 
of each vector 

100 

ted by the subscript.) Thus, we have 

, 

Lets call the 
matrix 

components 

that the 
in that order, after the number of months indica
x1 = Px0. 
bution 
brand after 2 months have elapsed, we simply 
x1 instead 

of toothpaste 
users 

Extending 

the notation, 

let xk be the 
after k months. To determine 

vector 

whose components record 

the number of users of each 

the distri­

apply 

the same reasoning, 
with 

starting 

of x0. We obtain [0.70 

X2 = Px1 = 0.30 

0.20] [100] [ 90] 

0.80 100 - 1 10 
Brand A users 

from which we see that there 

are now 90 

and 1 10 Brand B users. 

The vectors 

xk in Example 

the state 
3.64 are called 

of the 
matrix. We have just 

Markov 
seen that a Markov 

chain, 
chain 

its transition 

vectors 

and the matrix P is called 
satisfies 

the relation 

xk+1 = Pxk fork = 0, 1, 2, . . .  
that we can compute 
other 
and its initial 
probabilities 

From this resu
lt it follows 
once we know x0 and P. In 
its transition 

words, a Markov 

state. 

state vector iterativel

an arbitrary 
chain is completely determine

y 
d by 

•  Suppose, 

Remarks 

to reflect the fact 

that, initi

is 60%-40%. 
Check by 

in Example 

3.64, we wanted 
but, rather, the 

to keep track 
relative 
numbers using 
or fractions 
by dividing 

of not the 
each brand. We 
by 200, the total 

actual numbers 
could con­
number of 

users 

of toothpaste 
vert the data into percentages 
users. 

Thus, we would start 

with 

direct 

calculation 
that PXo 

0.50 

with the 50-50 split 
components 

•  Observe how the 

that add up to 1, are called 
transition 

we computed 

matrix P. We can think 
rows as being labeled 

200 0.40 

Xo = [�s�o�] = [0.60] 
[0.50] 

ally, 
=  , which can then be taken 
such as these, 

the Brand A-Brand 

above). Vectors 

B split 

probability vectors. 

as x1 (in agreement 
with nonnega

tive 

probabilities 

are arranged 

within the transition 

with the present 
states 
and the 

columns as being labeled 
of the 
next states: 
with the 
Present 
A [0.70 
A 

0.20] 0.80 

B 
Next B 0.30 

stochast

have 

way. Note 

nature 

chains 

matrix. 

vectors; 

of Markov 

in another 

examine the 

that we can write 

We can realize 

and, in general, 

of Pare probability 

any square matrix with this 

This leads us to 

Note also that the columns 
property is called 

a stochastic 
the deterministic 

2 3 2   Chapter 
3 Matrices 
ic is derived 
The word 
from the Greek 
adjective 
stokhastikos, meaning 
"capable 
of 
aiming" 
(or guessi
ng). It 
has come 
to anything 
to be applied 
is 
that 
of probability 
governed 
by the laws 
in the 
sense 
that 
probability 
makes 
about 
predictions 
the likeli
hood 
of 
things 
happening. 
In probability 
xk = Pkx0 fork = 0, 1, 2, ... 
theory
processes
" form 
, "stochastic 
a generalization 
of Markov chains. 
4, we 
0.20] = [0.55 
0.20] [0.70 0.80 
p2 = [0.70 0.30 
0.80 0.45 0.30] 0.70 
0.30 
sum to 1. (You are asked to prove this 
in Figure 3.20 
-say, (P2)21 = 0.45. The tree diagram 
occur over 2 months, 
(or paths) of length 2 in the tree. 
A can end up using Brand B 2 months later 
to use A after 1 month and 
to B (with probability 0.7(0.3) = 0.21), 
. The sum of these 
probability 0.3(0.8) 
= 0.24)
of 0.45. Observe that these 
that (P2)n = 0.45 
from  state 1 

1 month and then stay with 
B (with 
probabilities 
probability 
exactly what we do when we compute (P2)n. 
(Brand A) to state 2 (Brand B) in two transi

Y A 0.49 
7A� B 0.21 '  
A � Y A 0.06 
B � B 0.24* 

What are we to make of the entries 
is another 
in Exerc
one of 
this entry 

correspond 
initially 
ways (marked * in the figure): The person 
then switch 

ise 14.) 

It follows 
is the reverse 

the probability 
of moving 
tions. (Note that the 
) The argument 

changes 
to the four branches 

of this matrix? 
its columns 
a transi

Someone 
in two different 

Could it be that P2 is also 

scripts 
ized to show that 

of what you might have guessed.

is using Brand 

its entries
came from. 

order of the sub­
can be 

powers of a transition 

of some kind? Consider 

calculations 

There are four possible 

stochastic 

represents 

to observe is that P2 

or the person 

clarifies 
where 

The first 
thing 

an overall 

that can 

matrix, since 

can continue 

tion matrix 

can switch 

general­

and these 

to B after 

matrix. 

gives 

state 

who 

are 

figure 3 . 2 0  

In Example 3.6

(Pk)ij is the probability 

of moving 

from state 

j to state i in k transi

tions. 

long run? Let's 
tions 

x -

in the 

as state 

places), we find 

vectors 

(rounding 

work with 
to three 

to the distribution 
vectors. 

happen 
probability 
decimal 

of toothpaste 
users 
our calcula­
Continuing 

In Example 3.6
4, what will 
[0.60] [0.50] [0.70 
0.20][0.50] = [0.45], 
50 0.55 
' 1 -0.50 ' 2 - 1 -0.30 
0 -0.40 
0.80 0.
0.20] [0.45] = [0.425] x = [0.412] x = [0.406] 
[0.70 
'  5 0.594 ' 
' 4 0.588 
55 0.575 
0.80 0.
X3  = Px2 = 0.30 
] [0.402
] [0.401] [0.400] [0.400
[0.403
] 
'X9 = 0.600 'Xio = 0.600 
X6 = 0.597 
'Xg = 0.599 
'X? = 0.598 

x -Px -

x -

Section 

3.7 Applications 

2 3 3  

state vectors 

on. It appears that the 

and so 
implying 

that eventually 40% 

Brand A and 60% 

of the toothpaste 
B. Indeed, 

will be using Brand 

is reached, 

it will never 

distribution 

change. 

approach (or converge to) the vector , 0.6 

[0.4] 

survey will be using 

users in the 
it is easy to check that, 

once this 

compute 

We simply 

[0.70 0.20] [0.4] = [0.4] 
0.30 
0.80 

0.6 0.6 

A state vector 

Chapter 4, we will prove that every 

Markov 

now, let's 
accept 
at all. 
iterations 

x with the property that Px = x is called a steady state 

vector. 

chain 

has a unique 

In 
steady state vector. For 
doing any 

without 

this as a fact and see how we can find such a vector 

to 

equations 

of linear 

which reduces 

by rewriting 

matrix I -P, so the augmented 

the matrix equation Px = x as Px = Ix, which can in 
turn be 

as (I -P)x = 0. Now this is just a homogeneous 
system 

We begin 
rewritten 
with coefficient 
we have 

[I -p I O J  = [1 -0.70 -0.30 

matrix is [I -P I  OJ. In Example 3.64, 
lo] 0.20 
-0.20 
-0.20 
I OJ [ 0.30 
0 --0.30 
0 
1 -0.80 
is x = [::],then x2 is a free variable and the 
1 = X1 + Xz = tt + t = �t 
and x1 = 5 = 0.4, 
x2 = t = 5 = 0.6 
2  [0.4] 
0.6 
3 
Therefore, 
the actual 
we must have x1 + x2 = 200, 
distribu
calculations 
above. 
iterative 
in this example 

so x =  , in agreement with our 
then 
tion, 

So, if our steady state 
solution is 

x to be a probability 

vector, then we must have 

from which it follows 

(If we require 

If we require 

parametric 

x to contain 

vector 

that x = [ 80] .) 120 
ts, as shown in Figure 3.21. 

a bell is rung and to 

places 

whenever 

compartmen

a rat in a cage with three 

it into the next compartment. 

A psychologist 
The rat has been trained to select a door at random 
move through 
(a) If the rat is initially 

in compartment 1, what is the proba
compartment 2 after 
P21 = P31 = 2, P12 = Pn = 3, P32 = p23 = 3, an P11 = P22 = p33 = O 
Solution Let P = [p;jJ be the 

transition 
2 d 

the bell has rung twice? three 

matrix for this Markov 

(b) In the long 

times? 

bility 

chain. 
Then 

run, what proportion of its time will the rat spend in each compartment? 

that it will be in 

I 

I 

Example 3 . 6 5  

2 3 4   Chapter 

3 Matrices 

�  (Why? Remember 

Figure 3 . 2 1  

that pij is the probability 

of moving 

from j to i.) Therefore, 

and the initia

l state 

vector 

is 

(a) After 

one ring of the bell, we have 

to three 

decimal 

(rounding 

places), we find 

Continuing 

[!] [0333-
[l l 3 
m11 
=  0.333 
2 3 
x2 = Px1 =  0 
[ l l [ 0222 l � =  0.389 
and  [l I 3 
!][!] 
2 3 
18  0.389 
that the rat is in compartment 2 is ! = 
fs =  0.389. 

the probability 
questions could 

that  the 
also be answered 

Therefore, 
0.333, 
and after 
and (P3)21.] 

x3 = Px2 =  0 

after two rings, 

the probability 

by computing 

[Note that these 

three rings, 

0.333 

(P2)n 

rat is in compartment  2  is 

3.7 Applications 

2 3 5  

(b) This question 
saw above, 

to solve 

vector 

is asking 

vector. As we 
the system 

x as a probability 

for the steady state 
x must be in the null space of 

Section 
I -P, so we proceed 
_l OJ [ 1 
-� OJ - 1  0 
-i 0 -----*  0 
ifx � [ :: l then x, � l idm and x, � l t, x, � l. Since 
vector, we need 1  = x1 + x2 + x3 = � t. Thus, t = i and 

1  0  0 

prnb· 

0  0 

0 

0 

Hence, 

ability 

x mu•t be ' 

us that, 

which tells 

i of its time in each of the other 

in the long run, the rat spends 

two compartments. 

� of its time in compartment 

1 and 

Economic Models 

linear 
We now revisit 
recast 
model. The system 

these 

the economic 

models that we first encountered 

models in terms 

of matrices

. Example 
2.33 illustrated 
we needed 
was 
to solve 

of equations 

in Section 
the Leontief 

2.4 and 
closed 

In matrix form, 

where 

this is the 

equation Ex = x, 

1/2] [x1] 
1/4  and  x = x2 

E =[��: ��! 1/2  1/3 
if E =  [ e;j], then e;j represents 

an exchange matrix 

(or percentage) 

the fraction 

vector 

X3 
x is called a price vector. 
and the 

1/4 

by industry 

i and X; is the 

In a closed economy, the sum of each column 

tive, 

E is a stochastic 

matrix and the problem of finding 

price charged 
by industry 
i for its 
the entries 
of E are 
a solution 
to the 

of E is 1. Since 

The matrix E is called 
In general, 
output 
output. 

that is consumed 

also nonnega
equation 

of industry j's 

Ex= x 

equation (I -E)x = 0. There will always be  infinitely 

problem of finding 
vector 

is precise
ly the same as the 
chain! Thus, to find a price 
homogeneous 
tions; 
is positive. 

we seek a solution where the prices 

x that satisfies 

Ex = x, we solve 

one price 

tive and at least 

the steady state 

are all nonnega

of a Markov 

the equivalent 

many solu­

vector 

(1) 

2 3 6   Chapter 

3 Matrices 

(2) 

d 

value 

to solve 
the 

of the 

x is the 

i's output 

vector, 
and d is 

The Leontief 

that is needed 

of industry 

production 

In matrix form, we have 

open model is more interesting. 

to produce one 
output, 

value (price) 
external demand 
for industry 

and d; is the 

worth of 
dollar's 
of industry i's 
dol­
i's output. Once again, 
tive entries 

the demand vector. 
the dollar 
industry j's 
lar value 
in finding 
is positive. We 

In Example 2.34, we needed 
system  x1 = 0.2x1 + 0.5x2 + O.lx3 + 10 
Xz= 0.4x1 + 0.2X2 + 0.2X3 
+ 10 
x3= O.lx1 + 0.3x2 + 0.3x3 + 30 
where  x = Cx + d or (I -C)x = 
c = 0.4 0.2 0.1 0.3 0.1] [x1] [10] 
[0.2 0.5 
0.2 , X = Xz ,  d = 10 
0.3 X3 30 
The matrix C is called the consumption matrix, 
In general, if C = 
[c;j] ,  x =  [x;J , and d =  [d;J , then c;j represents 
output, X; is the dollar 
a production vector x with nonnega
call such a vector x a feasible 
: [1/4 (a) C = 1/2 
1/3] 1/3 
1/2] 2/3 
(b) C= [��� 
OJ _  [1/4 1 1/2 1/3] 1/3 
[ 3/4 -1/2 -1/3] 2/3 
I -C = [� 
equation (I -C)x = d becomes [ 3/4 -1/2 -1/3] [X1] = [d1] 2/3 x2 dz 
and then to apply Theorem 3.7. We compute 
I -C is invertible 
[X1] = [ 3/4 X2 -1/2 -1/3]-1[d1] [2 l][d] 
2/3 d2 -3/2 9/4 d: 
of (I -C)-1 are nonnega
Since d1, d2, and all entries 

so are x1 and x2• 

the corresponding 
augmented 
it is instructi

In practice, 
we would 
a solution. However, 

whether 
consumption 
matrices

Determine 
following 

row reduce 
in this case, 

solution for any nonzero 

open model determined 

matrix to determine 

we are interested 

demand vector

is a solution to the 

that the coefficient 
matrix 

such that at least 

find a feasible 

Leontief 

ve to notice 

Solulion (a) 

one entry 

so the 

We have 

Thus, 

there 

tive, 

solution. 

we can 

by the 

. 

Example 3 . 6 6  

3.7 Applications 

2 3 1  

(b) In this case, 

I -C = - 1/2 

so that 

Section 
and (I -c)-l = [-4 -6 
[ 1/2 
-6] -6 
- 1/2] 2/3 
x = (I -C) - 1d = [-4 -6 -6]d -6 
B =  [b;j] ,  we will write 

4 
(For two m  X n 
tive if A  2 0 

A  2  B if aiJ 2 b;Jor all i and j. Similarly, 

this will not produce a feasible 

3.66, we have the following 

on. A matrix A is called 

definition. 

B, and so 

nonnega

vector 

solution for 

d. 

Since 
all entries 
any nonzero demand 

of (I - C)-1 are negative, 

Motivated by 
Example 

A = [aij] and 

matrices 
we may define A  > B, A  :::::: 
and positive 

if A  > 0.) 
and (I -c)-l 2 0. 

Definition 

A consumption 

matrix 

C is called 

productive 

if I -C is invertible 

We now give three results 

that give criteria 

for a consumption 

matrix to be 

productive

. 

Theorem 3 .3 4  

Let C be a consumption 
production vector 

x 2 0 such that 

matrix. 

Then C is pro

x > Cx. 

ductive 

if and only 

if there 
a 

exists 

Proof Assume 

that C is pro

ductive. 

Then I -C is invertible 

and (I - C) - 1  2 0. Let 

lently, x > Cx. 

a vector 

that there exists 

Conversely, assume 

Thus, x -Cx >  0 or, equiva­

x 2  0 such that x > Cx. Since 

real number ,\ with 0  < ,\ < 1 such that 

Then x = (I -c)-1j 2  0 and (I -C)x = j  >  0. 
C 2 0 and C * 0, we have x >  0 by Exercise 35. Furthermore, 
fore, as n ---+ oo, ,\"x ---+ 0 and hence C"x---+ 0. Since 
n ---+ oo. Now consider 
(I -C)(I + C + C2 + · · · + C"- 1) =I -C" 

�  By induction, it can be shown that 0 :::::: C"x < ,\"x for all n 2  0. (Write 
x > 0, we must have C"---+ 0 as 

Cx < ,\x. But then 
C2x = C(Cx) :::::: C(,\x) = ,\(Cx) < ,\(,\x) = ,\2x 

0  < ,\ < 1, ,\" approaches 

of this induction 

0 as n gets large. 

the matrix equation 

proof.) Since 

tails 

There­

there 

out the de­

a 

must exist 

2 3 8   Chapter 

3 Matrices 
As n � oo, en � 0, so we have 

(I - C) (I  +  C  +  C

2 + . . .  ) = I  -0 = I 
(I - c)- 1  = r + c  +  c2 + ... :::: o 

Therefore, 
I +  C  +  C

I - C is invertible, 
2 + . . .. Since 

infinite 
with its inverse given  by the 
tive, 

all the terms in this series are nonnega

matrix 
we also have 

series 

. 

Remarks 

Hence, 

C is productive

series I  +  C  +  C2 

•  The infinite 
•  Since 
•  For an alternative 

ric series 1 + x +  x2 + . . .. You may be familiar 
1 + x +  x2 + . . .  = 1/(1 -x). 
equality 
x > Cx means that there 
is producing more than it consumes. 

Cx represents the amounts 

approach to the first 

is some level 

the vector 

Exercise 42 

in Section 

4.6. 

+  . . . is the matrix analogue 
with the fact that, 

of the geomet­

for lxl < 1, 

consumed 

by each industry
in­
for which each industry 

, the 

of production 

part of the proof 

of Theorem 3.34, see 

corollar

v 3 . 3 5  

Let C be a consumption 
C is pro
. 

ductive

matrix. 

If the sum 

of each row of C is less than 1, then 

corolla

corollar

from 
y comes 
The word 
rium, which 
the Latin 
word 
to a garland 
given 
refers 
as a re­
ward. Thus, 
is a little 
a corollary 
reward 
that 
follows 
extra 
from a 
theorem. 

Proof If 

then Cx is a vector 
1, then the 

consisting 

of the row sums of C. If each row sum of C is less than 

condition 

x > Cx is satisfied. 
Hence, 

C is pro

ductive. 

Corollar

v 3 . 3 6  

Let C be a consumption 
C is pro

ductive. 

matrix. If 

the sum of each column 

of C is less than 1, then 

Proof If each column 
Hence, 

CT is productive, 

by Corollary 

3.35. 

Therefore, 

by Theorems 3.9(d) and 3.4, 

sum of C is less than 1, then each row sum 

of cT is less than 

1. 

that (I - c)- l  2: 0 too and, thus, 

It follows 
You are asked to give alternative 
Section 

7.2. 

ductive
3.35 and 3.36 in Exerc
proofs of Corollaries 

c is pro
. 

ise 52 of 

from the definition 
dollar 

It follows 
total 
j is the 
j's output-that is, industry 
industry 
such an industry 
Corollary 
a consumption 

is profitable. 
matrix is pro

value of all the inputs 

ductive 

of a consumption 
needed 

j's income 

matrix 
to produce 
its expenditur

exceeds 

if all industries 

are profitable

3.36 can therefore 

be rephrased 
to state 

es. We say that 
that 

one dollar's 
worth of 

that the sum of column 

. 

Biometrika 33 
P. H. 
Leslie, 
"On the Use of 
Matrices 
in Certain Population 
Mathematics;' 
pp. 183-212. 
(1945), 

Population 
G rowlh 
One of the most popular 
introduced 
by P. H. 
male portion 
females 
data about the average 
then able to determine 

birthrates and survival 
the growth 

models of population 

are divided into age 

of the population 

all of which span an equal 

of a population, 

Section 
in 1945. The Leslie model describes 

which is assumed 

probabilities 

over time. 

classes, 

Leslie 

growth 

is a matrix-based 
the growth 
fe­

model, first 
of the 

to have a maximum lifespan. The 

number of years. Using 

of each class, the model is 

3.7 Applications 

2 3 9  

Example 3 . 6 1  

d: 

females. 

these 

become 

a juvenile 

will be the 

will simply 

survival 

an average 

that have survived: 

for each of the 

surviving 

that have survive

The survival 

We can combine 

of four female beetles; 

Predict the 

beetle population 

the number of youths 

produces an average 

number of juveniles 

be the number of youths 

The number of juveniles 

Likewise, the number of adults 

the female VW beetles into three 
age 

The youths 
and each adult 

will be the number produced during 

each juvenile 
of three 

of a youth's 
to 
Suppose we begin 

A certain species 
for short), lives 

(that is, the probability 
rate for juveniles 

of German beetle, the Vollmar-Wasserman beetle (or VW beetle, 

do not lay eggs; 
produces 
rate for youths 

for at most 3 years. We divide 
classes of 1 year each: 
youths (0-1 year), juveniles (1-2 years), and adults (2-3 years). 
is 50% 
is 25%. 
is 0.5), and the 
with a population of 100 female VW beetles: 40 youths, 40 juveniles, 
and 20 adults. 
next 5 years. 
Solution After 1 year, 
that year:  40 x 4 + 20 x 3 = 220 
40 x 0.5 
= 20 
40 x 0.25 
= 10 
[�s 4 0 0.25 
vcrtornnd x, � [ 2:� -
tion after 1 year. We 
chains: xk+i = Lxk fork =  0, 1, 2, . . .  (although 
that xk = L kx0 for k = 0, 1, 2, . . .  , as for Markov 
3][220] [110] 0 20 110 0 10 5 3][110]  [455 l 
4 0 0.25 
x, � Lx, [�s 
4 0 0.25 
0 110 55 
x, � Lx, � [�s 
0 5 27.5 

°' Lxo �  x,, whm x, � [ ;n, 

is the distribu
same as for Markov 
is quite 
different
distribution 
chains, 

vectors. (It also follows 
will not use this 
fact here.) 

that we can iteratively 

into a single matrix equation 

but we 
We compute 

di'tcibution 

compute successi

populotion 

ve population 

structure 

of the equation 

the initilli 

is exactly 

). It follows 

see that the 

the 

the interpretation 

2 4 0   Chapter 

3 Matrices 

l 227.5 13.75 
3 l [455 l � ��.5 [302.5 
4 0 0.25 4 0 0.25 
3][302.5 l [951.2 l 
0 227.5 = 151.2 
0 13.75 56.88 
will 
the model predicts 
that 
after 5 years 
there 
Therefore, 
be approximately 
argue 
and 57 adults. 
(Note: You could 
151 juveniles, 
female 
951 young 
VW beetles, 
le, 28 
to the nearest 
rounded 
integer 
that we should 
have 
at each step-for examp
tions
the subsequent 
would 
3-which 
adults 
itera
have 
after step 
affected 
. We elected 
anyway 
approxim
not to do this, 
since the 
tions 
are only 
ations 
and it is much 
calcula
easier 
to use a calculator 
or CAS if you do not 
round 
as you go.) 
x L in Example 
a 
if we have 
a Leslie matrix. In general, 
3.67 
The matri
is called 
population 
be an n X n matrix with 
with 
n age classes 
of equal 
duration, 
L will 
the 
following 
structure: 
s, 0 0  0  0 
b, b2 b3  bn-1 bn 
0 S2 0  0  0 
L =  0 0 S3  0  0 
0 0 0 Sn-1 0 
b1, b2, ••• are the 
birth parameters (b; = the average 
Here, 
pro­
numbers of females 
s1, s2, ••• are the 
s (s; = the 
female in 
duced 
i) and 
class 
by each 
i survives 
probability 
that a female in class 
into class 
i + 1). 
al probabilitie
surviv
of our calcula
What 
are we to make 
the beetle population 
appears 
tions? 
Overall, 
although 
fluctua
to be increasing, 
such 
from 250 to 
there 
tions, 
are some 
as a decrease 
225 from year 
in the population 
shows the change 
in each 
of 
1 to year 2. Figure 
3.22 
age classes and clearly 
the three 
. 
shows 
the growth, with 
fluctua
tions
i:: 3000 
0 ·� :; 
Cl., 

§' 2000 

4000 

I Adults 

_. 

1000 

ol..:::::::::+::::::::i::::::::
4 

2 

0 

___,_� ::;..._."""'*��� =:::::::i:::::::::::+::::=:::+-

6 

Time 

(in years

) 

8 

10 

Figure 3 . 2 2  

Section 

3.7 Applications 

2 4 1  

0.9 

0.7 

0.8 

c  0.6 
0 -� :; 0.. 0.5 
0 0.. 4-< 0  0.4 I 
... c 0) (.)  I 
.... 0.3 
0) 0... 

0.2 

0.1 

iles 
Juven
Adults 

5 

20 

1 5  

Figure 3 . 2 3  

1 0 Time 
) 
(in years
the 
of plotting the 
If, instead 
in 
actual population, 
we plot 
relative population 
emerges. To do this, 
of 
class, a different 
each 
pattern 
to compute 
we need 
the fraction 
the population 
age class 
in each 
in each 
is, we need 
year; that 
to divide 
distribu­
each 
after 1 year, 
by the sum of its componen
tion 
vector 
ts. For example, 
we have 
[220] [0.88] 
-1-x1 = 
_ l_  20 = 0.08 
250 250 10 0.04 
88% of the population 
and 4% is 
8% is juveni
which tells 
consists 
us that 
of youths, 
les, 
3.23, 
time, 
we get a graph 
type 
of data over 
like 
adults. If we plot 
this 
in Figure 
the one 
clearly 
the proportion 
of the population 
that 
in each 
which 
shows 
class 
is approaching 
the steady 
example 
state 
a steady 
state. 
is 
vector 
out that 
in this 
It turns 
[0.72] 0.24 0.04 
72% of the population 
and 4% 
24% juveniles, 
That is, in the long 
run, 
be youths, 
will 
adults. (In other 
words
, the 
population 
is distributed 
among 
the three 
age classes in 
18:6:
1.) We will 
4. 
this ratio 
the ratio 
see how to determine 
exactly 
in Chapter 
it is 
to model the inter­
to be able 
tions 
situa
There 
in which 
important 
are many 
wish 
we might 
relationships 
set of objects. 
a finite 
For example, 
among 
to describe 
types of networks 
various 
cit­
(roads 
connecting 
towns, airline routes 
connecting 
ies, 
communication links 
conne
cting 
satelli
tes, 
etc.) or relationships 
groups 
among 
or individuals 
(friendshi
p relationships 
in a socie
ty, preda
in 
tor-pr
ey relationships 

Graphs and Digraphs 

2 4 2   Chapter 

3 Matrices 

c 

A 

D 

B 

, dominance 
Graphs are ideally 
etc.). 
relationships 
an ecosystem
in a sport, 
to 
suited 
networks 
and relationshi
such 
modeling 
ps, and it turns 
out that matrices 
are a useful 
tool 
in their 
study. 
set of points (called 
vertices) and a finite 
A graph consists 
of a finite 
set of 
of which 
edges, each 
connects 
arily 
We say that 
two (not 
necess
distinct) 
vertices. 
two vertices 
are 
adjacent if they 
of an edge. 
an 
Figure 
3.24 
shows 
are the endpoints 
example 
of the same graph drawn in two different ways. The graphs 
" 
are the "same
in the sense 
y the 
that identif
relationships 
the adjacency 
about are 
that all 
we care 
edges. 
We can record 
the essential 
and use matrix 
informa
tion 
about a graph in a matrix 
algebra 
us answer 
to help 
questions 
graph. This 
use­
is particul
arly 
certain 
about the 
very 
can handle 
tions 
computers 
since 
are large, 
ful if the graphs 
the calcula
quickly. 
Definition If G is a graph with 
then 
its 
n vertices, 
adjacency matrix is the 
A B 
D 
c 
n X n matrix 
by 
A(G)] defined 
A [or 
= { 1 if there 
.. 
'1  0 other
between 
vertices 
is an edge 
i and 
of the same 
Two repres
entations 
j 
wise 
graph 
a 
matrix. 
ated adjacency 
a graph and its associ
3.25 shows 
Figure 
(vertices is the 
The term 
comes 
plural) 
from the Latin 
verb 
vertere, which 
means "to turn:' 
In 
of graphs 
the context 
(and geom­
, a vertex 
etry)
is a corner-a 
point 
where 
an edge 
"turns" 
into 
a dif­
ferent 
edge. 

vertex 

Figure 3 . 2 4  

Vl 

V2 

A 

V4 

V3 

Figure 3 . 2 5  

0  0 

A =  1 0 

[ 1 tl 
A graph 
with 
adjacency 
matrix 
�  symmetric 
a 
that the 
Remark Observe 
adjacency 
matrix 
of a graph 
is necess
arily 
un­
(Why?) Notice 
matrix. 
also that a 
diagona
l entry 
a;; of A is zero 
more than one 
situations, 
at vertex 
there 
less 
i. In some 
is a loop 
a graph may have 
it may make 
In such 
sense 
edge 
between 
a pair 
of vertices. 
cases, 
to modify the 
definition of the 
adjacency matri
x so that 
the 
aij equals 
number of edges 
between 
vertices 
i and 
j. We define 
us to travel 
that allows 
of edges 
be a 
a path in a graph to 
sequence 
from one vertex 
y. The 
to another 
continuousl
length of a path is the 
number of edges 
it contains, 
and we will 
refer 
to a path with 
k edges 
as a k-path. For example, in the 
3.25, v1 v3v2v1 is a 3-path, 
and 
v4v1 v2v2v1 v3 is a 5-path. Notice 
that the 
graph of Figure 
and ends 
a path is called 
x); such 
is closed (it begins 
at the same verte
of these 
first 
a 
second 
uses 
v2 twice
circuit. The 
v1 and 
between 
the edge 
; a path that 
does 
not include 
the same 
edge 
more than 
once 
is called 
a simple path. 

3.7 Applications 
Section 
We can use the 
matrix 
us informa
to give 
of a graph's adjacency 
tion 
about 
ph. Consider 
of the 
in the gra
the square 
the paths of various lengths 
matrix 
adjacency 
in Figure 

powers 

3.25: 

2 4 3  

= l · l  + l · l  + l ·O +O · O  

(A2)23 = a2,al3 + a22a23 + a23a33 + a24a43 

What 
do the entries 
of A 2 repres
ent? Look 
at the (2, 3) entry
. From 
of 
the definition 
matrix multi
plica
that 
tion, 
we know 
way this 
expression 
The only 
can result 
in a nonzer
o number is if at least one of the 
products 
But 
up the sum is nonzero. 
a2kak3 that make 
if 
only 
a2kak3 is nonzer
o if and 
means that there is 
a2k and 
between 
v2 and 
an edge 
o, which 
ak3 are nonzer
both 
vk as 
well 
as an edge 
between 
v3. Thus, there will 
be a 2-path between 
2 and 
vertices 
vk and 
happens fork = 1 and fork = 2, so 
3 (via 
vertex 
k). In our examp
le, this 
= 2 
�  which 
see that 
2 and 
3. (Check to 
are two 2-paths between 
there 
us that 
tells 
vertices 
entries 
the remaining 
.) The argument 
of A 2 correctly 
2-paths in the graph
we 
give 
can be generalized 
have 
just 
given 
following 
whose proof 
result, 
we leave 
to yield the 
72. 
as Exercise 
(i,j) entry 
the 
jacency 
If A is the ad
to the 
of Ak is equal 
G, then 
matri
x of a graph 
number of k-paths between vertices 
i and 
3-paths are there 
How many 
3.25? 
between 
v1 and 
v2 in Figure 
(1, 2) entry 
Solution We need 
1 of A 2 and 
product of row 
is the dot 
of A3, which 
the 
2 of A. The calculation 
gives 
column 
=  3 · 1  + 2 · 1  + 
=  6 
can be easily checked. 
1 and 2, 
vertices 
are six 3-paths between 
so there 
which 
plications 
that can be modeled 
In many ap
by a graph, the vertices 
are ordered 
on the edges
a direction 
by some 
that imposes 
type of relation 
. For examp
le, 
directed 
edges 
to represent 
one-way routes 
in a graph 
might be used 
that models 
a transpo
rtation 
networ
k or preda
tor-prey relationships 
in a graph modeling 
an 
A graph with 
ecosystem. 
3.26 
a digraph. Figure 
directed 
edges 
is called 
shows an 
example. 
matrices 
An easy 
modification 
to the definition 
of adjacency 
allows 
us to 
use 
them 
with 
digra
phs. 

1 · 1  +  0 · 0 

(A3)12 

j. 

Example 3 . 6 8  

Figure 3 . 2 6  A digraph 

2 4 4   Chapter 

3 Matrices 

n X n matrix A [or 
Definition If G is a digra
the adjacency 
Thus, 

n vertices, 
ph with 
adjacency matrix is the 
then 
its 
by if there 
A(G)] defined 
i to vertex 
from vertex 
is an edge 
j 
otherwise 
matrix for the digra
ph in Figure 
3.26 is 

in general. 
symmetric 
matrix of a digraph 
ingly, 
Not surpris
the adjacency 
is not 
A k now contains 
no difficulty 
have 
(When would 
it be?) 
You should 
the 
seeing 
that 
numbers of directed k-paths between 
vertices, 
we insist 
that all edges 
along 
where 
a 
in the same directi
path flow 
gives 
example 
Exercise 
on. (See 
an applica­
72.) The next 
idea. 
tion 
of this 
Nadal, 
Safin) compete 
k, and 
(Djokovic, 
players 
Five 
tennis 
Federer, 
Roddic
in a 
round-robin 
tourna
in which 
each 
player 
plays 
every 
other 
player 
once. 
ment 
The 
digraph 
in Figure 
the resul
3.27 summarizes 
ts. A directed 
edge 
i to ver­
from vertex 
that player 
j. (A digraph 
i defeated 
tex j means 
in which 
player 
is exactly 
there 
one 
directed 
edge 
between 
a tournament
every 
is called 
pair of vertices 
The adjacency 
matrix 
in Figure 
3.27 is 
for the digraph 

.) 

Example 3 . 6 9  

D 

s 

F 

R 

N 

Figure 3 . 2 1  

A tournamen

t 

where 
the order 
of the vertices 
(and hence 
the rows and 
columns 
of A) is determined 
2 and column 
corresponds 
Federer 
etically. 
alphab
2, for example. 
to row 
Thus, 
results 
the five players, 
based 
Suppose we wish 
to rank 
of their 
on the 
matches. 
One 
way to do this 
might 
be to count 
the number of wins 
player. 
for each 
Obser
ve that 
the 
the sum of the entries 
number of wins 
each 
player 
had is 
just 
in the corresponding 
row; 
containing 
product 
all the 
row sums 
ntly, 
equivale
by the 
the vector 
is given 
Aj, where 

0 1 0 
0 0 

A =  1 0 0 1 0 
0 0  0 0 1 
0 0 1 0  0 

j  =  1 

1 
1 

1 
1 

Section 

3.7 Applications 

2 4 5  

we have 0 1 0 0 0 1 
In our case, 
3 3 2 1 1 
1 1 0 0 1 0 0 
Aj =  1 0 0 
0 0 0 
0 0 1 
: 
which 
produces 
the following 
ranking
First: 
Djokovic, Federer 
(tie) 
Second: 
Nadal 
Safin (tie) 
Roddick, 
Third: 
Are the players 
who tied 
ranking 
ovic 
that 
in this 
equally 
strong? Djok
might argue 
he deser
ves first 
Roddick 
same type 
place. 
would use the 
he defeated 
since 
Federer, 
that he has 
However, 
to break 
of argument 
Safin could 
argue 
the tie 
with 
Safin. 
two 
victories 
because he beat Nadal, 
two others
who defeated 
"indirect" 
; furthermore, 
that Roddick 
has only 
one indirect 
(over Safin, 
who then 
he might 
victory 
note 
Nadal). Since 
defeated 
all the others 
who defeated 
in the 
may not be a player 
there 
in a group of ties 
more useful. 
of indirect 
notion 
group, the 
seems 
wins 
Moreover, 
an indirect 
victory 
corresponds 
to a 2-path in the digra
ph, so we can use the square 
ma­
adjacency 
of the 
trix. 
To compute 
both 
for each 
and indirect 
wins 
player, 
the row sums 
we need 
wins 
are given by 
of the matrix A + A 2, which 
0 1 0  0 0 2  2 
0 0 1 1  1 0 1 1 1 
(A + A2)j =  1 0 0 1 0 + 0 1 0 1 2 
0 0 0 0 0 0 0 0 
0 0 0 0 1 0 0 0 
0 1 2 2 3  1 8 
0 2 2 2 1 7 
1 1 0 2 2 6 
0 0 1 0 
1 1 2 
0 1 0 1 3 
: Djokovic, 
we would rank the 
Thus, 
Nadal, 
Safin, 
players 
as follows
Federer, 
ately, 
this 
approach is not 
teed 
Unfortun
guaran
to break all ties. 
x1 and 
1. Compute 
2. What 
proportion 
of the state 1 
population 
will 
be in 
state 2 
after two steps? 
3. What 
of the state 
proportion 
2 population 
will 
be in 
state 
2 after two steps? 
stead
y state 
4. Find the 
. 
vector

Roddick. 

transition 

x2• 

ma-

3 . 1  

I Exercises 
Markov Chains 
1-4, let P = 
In Exercises 
trix for a Markov chain 
the initial 
state vector 

[0.5 0.3] 
0.5 0.7 [0.5] 
be 0.5 

tes. Let x0 = 

be the 

with two sta

for the population. 

ma-

population. 

Models 

in Archaeology 

tmn;;t;on 

for the 
x2. 

i,;x for n  Ma,kovcha;n 
the initial 
vector 
state 

2 4 6   Chapter 

In Emdm 5-8, let P � [ ! ! I ] be the 

3 Matrices 
of pifwn 
in the America
nut crops 
(pine) 
11. A study 
n 
southwest 
from 1940 to 1947 hypothesized 
that 
followed 
nut production 
[See 
a Markov 
chain. 
wUh th:"�al" Letx,, � [ :m be 
''A Computer Simulation 
D. H. Thomas, 
of 
Model 
Great Basin Shoshonea
n Subsistence 
and Settlement 
Patterns;' 
in D. L. 
Clarke, ed., 
(London: Methuen, 
1972).] 
that 
The data 
suggested 
bilities 
the proba
then 
was good, 
if one year's crop 
that 
x1 and 
5. Compute 
the following 
year's 
fair, 
crop would 
be good, 
or poor 
of the state 1 
will 
population 
6. What 
proportion 
be in 
were 
0.08, 
0.07, and 0.85, respecti
vely; 
if one year's 
state 1 
after two steps? 
crop 
was fair, 
that the follow-
then the 
probabili
ties 
proportion 
7. What 
will 
of the state 2 population 
be in 
were 
fair, 
be good, 
crop 
or poor 
ing year's 
would 
0.09, 
state 
3 after two steps? 
0.11, and 
vely; 
if one year's crop 
was poor, 
0.80, respecti
8. Find the 
steady state vector
. 
then 
that 
the probabilities 
crop 
year's 
the following 
or poor 
would 
be good, fair, 
were 
0.11, 0.05, and 0.84, 
er in a particular 
9. Suppose that the 
region 
weath
respecti
vely. 
Specific
ally, 
to a Markov 
ding 
behaves accor
chain. 
bility 
suppose that the 
proba
that tomorr
ow will 
be 
(a) Write down the 
transition 
Markov 
matrix for this 
a wet day is 0.662 if today is wet and 
0.250 if today 
chain. 
The probability 
is dry. 
that tomorrow 
will 
be a 
dry 
in 1940, find the 
(b) If the pifion 
nut crop 
was good 
day is 0.750 if today is dry and 
if today is wet. 
0.338 
proba
in the years 
crop 
bilities 
of a good 
1941 
of rainfall 
exercise 
[This 
is based on an actual 
study 
through 
1945. 
in Tel Aviv 
over 
a 27-year period. 
iel 
See 
K. R. Gabr
(c) In the long 
run, 
what 
will 
propor
tion 
of the 
crops 
''A Markov 
and 
J. Neumann, 
for Daily 
Chain Model 
fair, 
be good, 
and poor? 
Occurrence 
Rainfall 
at Tel Aviv;' 
the maze 
programmed 
to traverse 
been 
have 
12. Robots 
y, 88 (1962), 
y 
shown in Figure 
3.28 
and at each 
junction 
randoml
pp. 90-95.] 
way to go. 
choose 
which 
the transition 
for this 
(a) Write 
down 
matrix 
Markov 
chain
. (b) If Monda
bility 
what 
y is a dry day, 
is the proba
that 
Wednes
day will 
be wet? 
( c) In the long run, 
what 
will 
the distribution 
of wet 
and dry days 
be? 
of children 
on the 
10. Data have 
been 
accumulated 
heights 
relative 
to their 
parents. Suppose that the proba
bilities 
-height, 
a tall, medium
have 
will 
parent 
that a tall 
or 
the prob­
short child 
are 0.6, 
0.2, 
and 0.2, 
respecti
vely; 
abilities 
that 
a medium-
height 
parent 
have 
will 
a tall, 
medium-height, 
or shor
are 0.1, 0.7, and 0.2, 
t child 
re­
that a short parent 
and the probabili
vely; 
specti
ties 
will 
are 0.2, 
have 
a tall, 
medium
-height, 
or short child 
0.4, 
and 0.4, 
respecti
vely. 
(a) Construct 
the transition 
matrix for the Markov 
(a) Write 
down 
the transition 
matrix 
for this 
Markov 
chain 
that 
models 
this 
situa
tion. 
chain
. (b) What is 
junc-
15 robots 
(b) Suppose we start with 
at each 
the probability 
short person 
that a 
will 
the stead
. 
tion. Find 
y state 
distribution 
of robots
grandchild? 
have a tall 
that 
(Assume 
it takes 
each 
nt 
the same amou
robot 
( c) If 20% of the current 
50% is of 
population 
is tall, 
of time 
to travel 
between 
two adjacent 
junctions.) 
height, and 
30% is short, what 
medium 
the 
will 
13. Let j denote 
of ls. Prove 
a row vector 
consisting 
entirely 
be in three 
distribution 
generatio
ns? 
that a nonnega
tive 
matri
x Pis a stochastic 
matri
x if 
be tall, 
( d) What 
of 
propor
tion 
of the population 
will 
and only 
if jP = j. 
medium 
short in the long 
height, and 
run? 

of 
Meteorological 

Quarterly Journal 

the Royal 

Societ

Figure 3 . 2 8  

by 

ption 

j in a Markov 

31-34, a consum

In Exercises 
vector 

dare given. In each case, find a feasible production 

from state i to state j is given 

rage (or expected) 
number 
i to state 
that the following 
computation 
the jth row and the jth column 

Suppose we want to know the ave
it will take to go from state 
of steps 
chain. It can be shown 
answer
s this question: Delete 
of the transition 
expected number of steps 

241 [02 0.4 0.1 04] 
Section 
3.7 Applications 
[ 0 35 0.25 �35: 0.3 0.2 0.2 0.1 
14. (a) Show 
product of two 2 X 2 stochastic 
that the 
is also 
matrices 
a stochastic 
matrix. 
29. 0.15 0.55 
(b) Prove that 
the product 
of two 
n X n stochastic 
30. 0 0.4 0.5 0.3 
0.60 0.5 0 0.2 0.2 
0.45 0.30 
is also 
matrices 
a stochastic matrix. 
(c) If a 2 
is invertible, 
X 2 stochastic 
matrix P 
prove that 
P-1 is also 
a stochastic 
matrix. 
matrix C and a demand 
1/4] [l] 1/2 
(2). [1/2 31. c = 1/2 
vector x that satisfies Equation 
, d  = 3 0.4] d = [2] 0.2 , 1 
[0.1 
matrix P to get a new matrix Q. (Keep 
the rows and columns of Q labeled as they were in P.) The 
32. c = 0.3 
the sum of the entries in the column of (I -Q)-1 labeled i. 
y is a dry day, 
15. In Exercise 
9, if Monda
what 
is the 
0.2 
01] ['] 0.2 
33. c = [�5 
until 
a wet day? 
expected number of days 
, d = 2 0.5 4 01] [u] 0.2 
0.4 
number of genera­
is the expected 
16. In Exercise 
10, what 
0 0.4 
desce
a short person has a tall 
until 
tions 
ndant? 
CAS 34. C = 0 0.3 
[01 
is fair one year, 
17. In Exercise 
11, if the pifion 
nut crop 
what 
0.2 
, d = 3.5 0.3 2.0 
occurs? 
is the 
number 
of years until 
a good 
crop 
expected 
A 2 0. Suppose that 
0.2 
of the other 
junc­
12, starting 
from each 
18. In Exercise 
x in ll�r, x 2 0. Prove 
what 
tions, 
a 
is the expected 
number of moves until 
35. Let A be an n X n matrix, 
robot reaches 
4? 
junction 
that x 
some 
Ax< x for 
and x and 
D be n X n matrices 
36. Let A, B, C, and 
> 0. 
(a) If A 2 B 2 0 and C 2 D 2 0, then 
y 
in !Rn. Prove the 
vectors 
following 
inequalities: 
AC 2 BD 2 0. 
(b) If A > B and x 2 0, x * 0, then Ax 
[1/2 1/
4]  [1/3 2/3] 
19. I 
1 2 3/4  1/2 1 2 
[ 1/3 0 �] 
[ 0.4 0.7]  [ 0.1 0.6] 
1 1�3 l 0 0 2/3 
23. 1/3 3/2 1/3 -1/2 [1/2 24. 0 1/2 
0.6 0.4 0.9 0.4 
two age classes has a Leslie 
matri
37. A population 
with 
x 
L-0.6 
[ 2 �] . If the initia
vector 
is 
l population 
[ 03 0 02] [050 0.70 035] 
x0 = [ 1 �], compute 
x1, x2, and x3. 
25. 0.3 0.5 0.3 26. 0.25 0.30 
0.25 
L�H 1 � l · If the m;t;r
Leslie 
three 
age classes has a 
matrix 
38. A population 
with 
0.4 0.5 0.5 0.25 0 0.40 
u populahon 
0 0.5 
[0.2 0.3] 
is x0 = [ 1:]. compute 
rix is productive. [020 0.10 
010] 
x,. x,, ond x,. 
28. 0.30 
0.45 
0.15 
0.5 0.6 0.15 
0.50 
0.30 

vector 

Linear E c o n o m i c  
In Exercises 
exchange matrices. For those that are exchange matrices, 
find a nonnegative 

In Exercises 
tion mat

20. I 

that satisfies Equation 

27-30, determine 

19-26, determine 

whether 

vector 

of the matrices are 

the given 
p-

consum

which 

price 

(1). 

> Bx. 

21. 

22. 

27. 

P o p u l at i o n  G rowth 

M o d e l s  

x3. 

Rate Rate 

l 

Table 3 . 4  
Age Birth Survival 
(years) 

3 Matrices 
2 4 8   Chapter 
to about one calf per year 
their 
birth 
during 
middle 
matrix 
ses has a Leslie 
three 
with 
39. A population 
age clas
rate 
. The mortality 
calves 
years
high. 
is very 
for young 
[ 1  1  3 
L =  0. 7 0 0 . If the initial 
0 0.5 0 [100] x0 = 100 , compute 
vector 
population 
is 
x1, x2, and 
100 
has a 
Leslie 
40. A population 
four age classes 
with 
matrix 
L -_ [��.5 � � �i 
population 
0 0.3 0 . If the initial 
0.7 0  0 
o � rn l rnmpute 
x,. x,. Md x, 
voctodn
0-2 0.0 0.3 
2-4 0.4 0.7 
dura­
of 1 year's 
species 
41. A certain 
with 
two age classes 
4-6 1.8 0.9 
a survival 
tion has 
proba
of 80% from class 
bility 
1 to 
6-8 1.8 0.9 
2. Empirical 
class 
shows that, 
evidence 
on average, 
8-10 1.8 0.9 
to five females per 
each female gives 
birth 
year. 
Thus, 
10-12 
1.6 0.6 
L = [0 5] and 
L = [4  1] 
matrices 
Leslie 
two possible 
are 
0.6 0.0 
12-14 
I  0.8 0 2 0.8 0 
The numbers of woodland 
in 
caribou 
reported 
[ � � l compu
x0 = 
te x1, .•. , x10 in 
in 1990 are shown in 
Alberta 
Park in 
Jasper National 
with 
(a) Starting 
population 
Using 
caribou 
a CAS, 
3.5. 
predict the 
Table 
each 
case. 
project the 
for 1992 and 1994. Then 
population 
for the 
the relative 
(b) For each 
case, 
plot 
size 
of each 
age 
years 
2010 and 2020. 
What 
do you conclude
? (What 
class 
over 
do your 
3.23). What 
time 
(as in Figure 
does 
assumptions 
model 
could 
make, 
this 
and how 
it 
graphs 
suggest? 
matrix for the VW beetle is L = 
be improv
ed?) 
[ �.! L 2� l · sta,ting 
42. Suppose the Leslie 
AA acb;tmy 
Xo, detec­
with 
mine 
the behavior of this 
population. 
Park. 1990 
L = [ � � 2� l · Investigate 
the Leslie 
matri
43. Suppose 
x for the VW beetle is 
of varying 
the effect 
0-2 10 
0 0.5 0 the survival 
2-4 2 
s of the young 
bility 
proba
beetles. 
6-8  5 
4-6 8 
prima
rily 
in the western 
cAs 44. Woodland 
caribou are found 
rthwest. 
provinces of Canada 
and the American no
8-10 12 
is abou
The average 
of a female 
lifespan 
. 
t 14 years
10-12 0 
The birth and survival 
are 
rates 
for each 
age bracket 
12-14 
caribou 
in Table 
given 
cows 
3.4, 
which shows 
that 
do 
not give 
at all during 
their 
birth 
first 
2 years and give 
Source: 

Age 
(years) Number 

Population 
National 

Table 3 . 5  Woodland Caribou 

World Wildlife Fund Canada 

in Jasper 

Section 

3.7 Applications 

2 4 9  

53-56, determine 

In Exercises 
given 

digraph. 

the adjacenc

y matrix of the 

G r a p h s  a n d  D i g r a p h s  
In Exercises 
given 
45 VI 

45-48, determine the adjacenc
.------

graph. 

--. 

V2 

y matrix of the 

V4 

V3 

46. V1 

V2 

V4 

54.  v, 

V3 

V4 

V2 

47. 

55. 

56. VJ 

V2 

V5 

V4 

that has the 
given adja-

In Exercises 
cency matrix. 

49. [� 1 1 �] 50. [; 0 ll 

49-52, draw a graph 
0 0 
0 0 0 
0 0 
0 0 1 1 0 0 0 0 1 1 
0 0 0 1 1 0 0 0 
51. 1 0 0 0 
1 52. 0 0 0 1 1 
1 0 0 0 1 0 0 
0  0 0 1 0 0 

V4 

V3 

adja-

that has the given 

In Exercises 
cency matrix. 

57-60, draw a digraph 
0 0 
1 0 
0 

57. [� 1 0 ;] 58. [: 1 0 �: 

0 1 

0 0 

adjacency 
digraph 
matrix A for this 
ions. 
the following 
swer 
quest
Rodent 

and use it to an­

2 5 0   Chapter 

3 Matrices 

0  0 1 0 1 
1 0  0 1 0 

0 1 0 0 1 
0 0  0 0 

59. 0 0  0 0 1  60. 

In Exercises 
determine 
between 

1 0 1 0 0 
0  0 1 0 

powers of adjacenc

0  0 
0 1 0  0 
1 0  0  0 
y matrices to 
61-68, use 
the number of paths of the specified 
the given 
v2 
v2 
v3 
v2 

vertices. 

length 

Figure 3 . 3 0  

50, length 
61. Exercise 
2, v1 and 
52, length 
62. Exercise 
2, v1 and 
50, length 
63. Exercise 
3, v1 and 
52, length 
64. Exercise 
4, v2 and 
Fish Bird 
57, length 
65. Exercise 
2, v1 to v3 
57, length 
66. Exercise 
3, v4 to v1 
(a) Which 
species 
has the most 
direct 
of food? 
sources 
60, length 
67. Exercise 
3, v4 to v1 
How does 
A show 
this? 
60, length 
68. Exercise 
4, v1 to v4 
of food for the 
species 
is a direct 
(b) Which 
source 
A be the 
matrix of a graph 
69. Let 
adjacency 
G. 
most 
other 
species
A show 
? How does 
this? 
(a) If row 
i of A is all zeros, what 
does 
this 
imply 
(c) If a eats b
and beats 
c, we say that a has 
c as an 
about 
G? 
How can we use 
indirect 
source of food. 
A to de­
(b) If column 
this 
does 
j of A is all zeros, what 
imply 
has the most 
indirect 
termine which species 
food 
G? 
about 
species 
es? Which 
sourc
has the most 
and 
direct 
indirect 
food sources 
combined? 
matrix of a digraph 
70. Let 
A be the adjacency 
D. 
(d) Suppose that 
pollutants 
kill 
in this 
food 
the plants 
(a) If row 
i of A 2 is all zeros, what 
does 
this 
imply 
the effect 
to determine 
this 
web, and we want 
D? 
about 
on the ecosystem. Cons
will have 
change 
a 
truct 
j of A2 is all zeros, what 
(h) If column 
this 
does 
imply 
new adjacency 
matrix 
A* from 
the 
A by deleting 
about 
D? 
row and 
column 
corresponding 
to plants. Repeat 
ph of a tourna
71. Figure 
3.29 is the digra
ment with 
six 
which 
species 
parts (a) to (c) and determine 
are 
matrices, 
players, 
adjacency 
P1 to P6. Using 
rank 
the 
by the change. 
affected 
and least 
the most 
by determining 
players 
first 
wins 
by 
only 
and then 
(e) What 
will 
the long
-term effect 
be? 
of the pollution 
using 
the notion 
of combined 
indirect 
wins, 
wins and 
What matrix calcula
tions 
will 
show 
this? 
as in Example 
Whenever 
by e-mail. 
people 
73. Five 
are all connected 
a juicy piece 
one of 
them hears 
of gossip, he 
or she 
passes 
it along 
by e-mailing 
it to someone 
in the 
else 
group according 
to Table 
this 
models 
(a) Draw the digraph that 
"gossi
p 
rk" and find 
netwo
y matrix A. 
its adjacenc
Ann Carla, 
Ehaz 
Bert Carla, 
Dana 
Carla Ehaz 
Dana Ann, 
Carla 
Ehaz Bert 

p6 
repres
3.30 is a digraph 
72. Figure 
enting 
a food web in 
a small ecosystem. A directed 
a to b indi­
edge 
from 
a has 
cates that 
Construct 
of food. 
the 
b as a source 

Table 3 . 6  
Sender Recipients 

3.69. 

3.6. 

Figure 3 . 2 9  

2 5 1  

is called 

s can be subdi­

ven adjacen

with the gi

cy matrix is bipartite. 

in U and the other 

bipartite if its vertice

into two sets U and V such that every edge has one 

A graph 
vided 
endpoint 
endpoint 
1, v2, v3} 
the graph in Exercise 48 is bipartite 
with U = { v
and V = {v4, v5}. In Exercises 76-79, determine 
a 
whether 
graph 

matrix in Exercise 
matrix in Exercise 
matrix in Exercise 

Chapter 
Review 
to e-mail 
it takes a 
(b) Define a step as the time 
person 
in V For example, 
(Thus, 
everyone 
in one step, 
on his or her list. 
gossip gets 
from Ann to both 
Carla and 
Ehaz.) If 
will it 
how many steps 
Bert hears 
a rumor, 
take 
hear 
for everyone 
the rumor? 
What matrix 
else to 
calculation 
this? 
reveals 
(c) If Ann hears 
a rumor, 
how many steps 
will it 
take 
76. The adjacency 
49 
else to 
for everyone 
the rumor? 
hear 
What matrix 
77. The adjacency 
52 
calculation 
reveals 
this? 
78. The adjacency 
51 
(d) In general, 
adjacency 
if A is the 
matrix 
of a 
digra
ph, how can 
if vertex 
we tell 
to 
i is connected 
0 0 0 1 0 0 0 1 0 1 0 0 
h)? 
lengt
vertex 
j by a path (of some 
p network 
[The 
gossi
cent 
in this 
exercise 
is reminis
of the 
notion 
of separation" 
of "six 
degrees 
in the 
(found 
79. 0 0 1 0 1 1 
suggests 
play 
and film by that 
that 
name), which 
any 
1 0 1 0 0 
are connected 
path 
of acquaintances 
by a 
two people 
1 0 1 0 0 
Degrees 
"Six 
of 
whose length 
is at most 
6. The game 
frivolously 
Kevin 
Bacon" more 
are 
asser
ts that 
all actors 
80. (a) Prove 
that a graph is bipartite 
if and only 
if its 
connected 
to the actor 
Kevin 
Bacon 
a way.] 
in such 
vertices 
matrix 
labeled 
can be 
its ad
jacency 
so that 
A be the 
74. Let 
matrix of a graph 
adjacency 
can be partitioned 
as 
(a) By induc
n 2:: 1, the 
tion, 
prove that 
for all 
A =  [�--H�-] 
of n-paths 
entry of 
An is equal 
to the number 
i and 
vertices 
between 
statement 
(a) have 
and proof 
(b) How do the 
in part 
the result in part (a), 
(b) Using 
a bipartite 
prove that 
to be mod
if G is a digra
ph? 
ified 
of odd length
graph 
has no circuits 
. 
75. If A is the ad
jacency 
matri
x of a digraph 
G, what 
does 
(i, j) entry 
of AA T repres
the 
ent if i * j? 

(i,j) 

j. 

G. 

Chapter 
Review 
and concepts 
Kev Definitions 
172, 206 matrix, 
202 Fundamental 
Theor
198 Basis Theorem, 
basis, 
138 matrix addition, 
em of Invertible 
Matrices, 
140 
matrix, 139 
x (vector), 138 
column 
identity 
matri
180 
tion, 
matrix factoriza
a square 
inverse of 
of a matrix, 
column 
space 
matrix multipl
ication, 
141 
matrix powers, 149 
195 composition 
matrix, 163 
inverse of a linear 
oflinear 
of a matri
x, 140 
negative 
transformations, 
219 
transformation, 
221 
null 
197 
space of 
a matrix, 
respect 
coordinate 
vector 
with 
to a 
nullity 
of a matri
x, 204 
combination 
of matrices, 
154 
linear 
outer 
208 
basis, 
linear 
dependence/independence 
product, 
147 
diagonal 
(bloc
matrices 
partitioned 
k 
of matrices, 
157 
139 
matrix, 
dimension, 
203 
181 multiplica
213 
linear 
transformation, 
tion), 145, 148 
LU factoriza
tion, 
170 
permuta
187 
tion 
matrix, 
elementary 
matrix, 

X = A-1B. 

(I - A)-1 = I+ A + A2• 

2 5 2   Chapter 
3 Matrices 
standard matrix of a linear 
row matrix 
of matri
x algeb
properties 
ra, 154, 
(vector), 138 
row space 
transformation, 
216 
158, 159, 167 
195 
of a matrix, 
scalar matrix, 
subspace, 
192 
x, 204 
rank 
of a matri
139 
symmetric 
of a matrix, 
140 
scalar multiple 
Theorem, 
Rank 
205 
matrix, 
151 
span 
transp
repres
entations 
of matri
x 
of a set of matrices, 
156 
ose of a matrix, 151 
square 
zero 
141 
matrix, 
products, 
146-148 
matri
x, 139 
A = [ 1 ! ] as a prod-
Review Questions 
statements 
or false: 
true 
1. Mark 
each 
of the following 
the matrix 
10. If possible, 
express 
4 
matrices. 
uct of elementary 
(a) For any matrix 
AT A are defined. 
A, both 
AA T and 
that AB = 0 and 
(b) If A and B are matrices 
such 
A 3 = 0, show 
that 
matrix such that 
11. If A is a square 
B = 0. 
A * 0, then 
of A � [: -l :J 
(c) If A, B, and 
X are invertible matrices 
such 
that 
XA = B, then 
an LU faotmization 
12. Find 
(d) The inverse of an element
ary matrix is an elemen­
tary 
matrix. 
sp�<ofA � [� =� � � n 
(e) The transp
13. Find 
bases 
for the 
and null 
row space, 
column 
space, 
ose of an elementar
y matrix is an 
y matrix. 
elementar
(f) The product of two element
ary matrices 
is an 
elementar
y matrix. 
(g) If A is an m X n matrix, 
then 
space 
the null 
of A is 
. Do 
are row 
A and B
14. Suppose matrices 
equivalent
they 
of ll�r. 
a subspace 
the same 
have 
row space? Why 
or why not? 
Do A and 
in IR3 is a two-dimensiona
l subspace 
(h) Every 
plane 
the same 
B have 
column 
space? Why 
or why not? 
of IR3. 
explain why 
matrix, 
15. If A is an invertible 
A and 
AT must 
tion 
(i) The transforma
T: IR2 ---+ IR2 defined 
by 
have 
the same null 
if A is a nonin­
space. 
Is this 
true 
transformation. 
T(x) 
= -xis a linear 
square 
Explain. 
vertible 
matrix? 
transformation, 
(j) If T: IR4---+ IR5 is a linear 
then 
of T. 
16. If A is a square 
matrix whose rows 
add up 
to the zero 
there 
is a 4 
X 5 matrix 
= Ax for 
A such 
that T(x) 
be invertible. 
A cannot 
vector, explain why 
all x in the domain 
2-7,letA = [� �] andB = [� -� -�]. 
ly indepen
17. Let 
A be an m X n matrix with 
dent 
linear
columns. 
Explain 
why 
AT A must 
be an invertible 
matrix. 
Must 
AA T also be invertible? 
Explain. 
transforma
tion 
18. Find 
T: IR2 ---+ IR2 such 
a linear 
that 
2. A2B 3. A2B2 4. BTA-1B 
5. (BBT)-1 6. (BTB)-1 
[ _�J [�]. 
r[�] [�] and r
product expansion 
7. The outer 
of AAT 
the standard 
transforma
19. Find 
matrix 
of the linear 
tion 
A-1 = [ l//2 -l], find 
9. If A � [ � : = : ] ond X 
8. If A is a matri
that 
x such 
to a counte
rclockwise 
T: IR2 ---+ IR2 that corresponds 
A. -3 2 4 
of 45° ab
followed 
rotation 
out the origin 
by a projec­
tion onto 
the line 
is a matdx such 
thot 
20. Suppose that T 
: !Rn ---+ !Rn is a linear 
tion 
transforma
that T (v) * 0 but 
and suppose that vis a vector 
such 
T2 = T 0 T). Prove 
T2(v) = O (where 
that 
v and T(
v) 
AX= [-� -�], findX. 
are linearly 
indepen
dent. 

InExercises
Compute the 

if possible. 
ed matrices, 

indicat

y = -2x. 

3 -2 

Eigenval ues and 
Eigenvectors 

of 

Almost 

for what we call a 

every combination 

Finite Dimensional 

svstem on Graphs 

has been used in 
the literature 
proper 

4 . 0  Introduction:  A ovnamical 

CAS 
the adjectives proper, 
latent, 
characteristic, 
and secular, 
eigen 
produces 
x multiplication 
iterating 
er that 
We saw in the last 
inter­
often 
matri
chapt
with the nouns root, 
number 
resul
growth 
ts. Both Markov 
chains 
esting 
and the Leslie 
model of population 
exhibit 
and value, 
chapter 
is to 
stead
y states 
in certain 
situations
. One of the goals of this 
help 
you 
First we will 
such 
understand 
behavior. 
look 
at another iterative 
process, 
or dynami­
(In the problems 
, you will 
that 
matrices. 
cal system, that uses 
find it helpful 
follow
value. -Paul 
R. Halmos 
tions
to use a CAS or a calcula
matrix capabili
tor with 
ties 
the computa
to facilitate 
.) 
3.7). A complete 
Our example 
(see 
graphs 
Section 
involves 
graph is any graph in 
edition) 
Vector Spaces (2nd 
which 
every 
vertex 
is adjacent to every 
other vertex. 
n verti­
If a complete 
graph 
has 
Van Nostrand, 
1958, p. 102 
of K4• 
entation 
For example, 
it is denoted 
ces, 
by Kw 
Figure 
4.1 shows a repres
x in IR4 with 
tive 
Problem 1 Pick 
any vector 
and label 
entries 
the vertices 
nonnega
of K4 with 
ofx, so that 
the compon
ents 
v1 is labeled 
the 
with 
x1, and so on. Compute 
adjacency 
of K4 and relabel 
the vertices of the 
ing 
matrix A 
graph with the 
correspond
vectors 
nts of Ax. Try this 
compone
x and explain, 
for several 
of the graph, 
in terms 
can be determined 
how the new 
labels 
from the old labels. 
1 .  That is, for a given 
Problem 2 Now 
iterate the process 
in Problem 
of x, 
choice 
as descri
bed above 
and then 
relabel 
the vertices 
apply 
A again 
(and again, 
and again) 
components 
emerge
a pattern 
until 
vectors 
of the 
s. Since 
elves 
will 
thems
get quite 
large, 
scale them 
we will 
by dividing each vector 
by its 
largest component after 
each 
if a computa
itera
tion. 
tion 
Thus, 
results in the vector 

VJ 
-----

V2 

-

Figure 4 . 1  
K4 

we will 
it by 

replace 

.!.[�] [�·5 ] 4 1  0.25 

1  0.25 

2 5 3  

Figure 4 . 3  

Figure 4 . 2  

Petersen 

2 5 4   Chapter 

4 Eigenvalues 
and Eigenve
ctors 
will 
now 
vector 
onent of each 
ntees 
process 
guara
this 
that 
Note 
that the largest comp
place 
Ks. Use at least ten 
itera
tions 
for K4, then 
be 1 .  Do this 
K3 and 
and two-decimal-
What 
accuracy. 
appears 
to be happening? 
vector 
case, 
in each 
that, 
noticed 
have 
Problem 3 You should 
the labeling 
is 
of the com
a certain 
approaching 
vector 
y state 
label!). 
(a stead
Label the 
vertices 
plete 
graphs 
with 
this 
steady state 
vector 
and apply 
the adjacency matri
x A one more 
time 
(without 
scaling). What 
is the 
relationshi
and the old ones? 
p between 
the new 
labels 
the gene
is the 
Kw What 
a conjecture 
about 
Problem 4 Make 
ral case 
steady state 
happens if we label 
label? 
What 
Kn with 
the stead
y state 
the adja-
vector 
and apply 
cency 
matrix A without 
scaling? 
Problem 5 The 
graph is shown 
in Figure 
in 
4.2. Repeat the process 
graph. 
3 with this 
1 through 
Problems 
classes 
the process 
now explore 
We will 
some 
with 
other 
of graphs to see if they 
behave the same 
way. 
cycle Cn is the gra
ph with 
n vertices arranged 
in a cyclic 
The 
4.3. 
shown in Figure 
For example, 
fashion. 
graph 
Cs is the 
Cn for various 
cycles 
Problem 6 Repeat the process 
of Problems 
1 through 
3 with 
odd values 
of n and make 
a conjecture 
about the gene
ral case. 
Problem 1 Repeat Problem 
even values 
of n. What 
6 with 
happens? 
graph (see 
74-78 in Sec­
Exercises 
A bipartite 
graph is a complete 
into 
rtitioned 
tion 
3.7) if its vertices 
can be pa
sets 
U and V such 
in U 
every 
that 
vertex 
is adjacent 
then 
versa. If U and Veach have 
to every vertex 
n vertices, 
in V, and vice 
the graph 
is denoted 
by Kn,n-For example, 
K3,3 is the graph 
in Figure 
bipartite 
3 with 
of Problems 
Problem 8 Repeat the process 
complete 
1 through 
graphs Kn,n for various 
values 
happens? 
of n. What 
explain 
of this 
By the end 
chapter, 
you will 
be in a position to 
vations 
the obser
you 
in this 
made 
have 
duction. 
Intro
of 
3, we encountered 
of a steady state vector 
the notion 
In Chapter 
in the context 
. For a 
model of population 
two applica
tions
: Markov 
growth
chains 
and the Leslie 
transition 
P, a stead
y state 
Markov 
chain 
with 
x had the 
proper
ty that 
matrix 
vector 
Px = x; for a Leslie 
vector 
vector 
x satisfying 
y state 
was a population 
matrix L, a stead
Lx = rx, where r represented 
rate. For example, 
the stead
y state 
growth 
we saw that 
0.8 0.6  0.6 and [�5 4 
0.2] [0.4] = [0.4] 
[0.7 
0.3 
n adjective 
The Germa
eigen means 
ristic 
"own'' 
of' 
or "characte
chapter, 
In this 
we investigate 
phenomenon 
this 
ally. 
That 
ma­
more 
gener
is, for a square 
values and 
eigenvectors are charac­
trix 
A, we ask 
whether 
there 
x such 
a scalar 
exist 
nonzero 
vectors 
that 
Ax is just 
multiple 
teristic 
of a matrix 
in the 
sense 
that 
they 
contain 
important 
informa­
in linear 
and it is one 
of the most central problems 
is the 
of x. This 
the nature 
of the 
about 
tion 
other 
and in 
throughout mathematics 
It has applications 
algebra. 
well. 
many 
fields as 
matrix. 
The letter
,.\ (lamb
the 
da), 
Greek 
equivalent 
of the English 
for eigenvalues 
letter 
L, is used 
Defi n ition Let 
an eigenvalue of A if 
A is called 
A be an n X n matrix. A scalar 
also 
they were 
because at one time 
that Ax = Ax. Such 
a nonzer
there is 
x such 
o vector 
a vector 
an eigenvec­
xis called 
values. The prefix 
as latent 
known 
to A. 
tor of A corresponding 
eigen is pronounced 
"EYE-
gun:' 

Intro d u ction to Eigenvalues 

eigenvalue problem, 

a n d  Eigenvectors 

0 
0.25 

bipartite 

4.4. 

Figure 4 . 4  

Eigen­

Example 4 . 1  

Example 4 . 2  

2 5 5  

onding 

4.1 Introduction 
and Eigenve
ctors 
to Eigenvalues 
of A = [ � �] and find the 
vector 
corresp

Section 
x = [ �] is an eigen
Show 
that 
eigen
value. 
Solution We compute 
Ax = [ � �] [ � ] [:] = 4 [ � ] = 4x 
eigenvalue 
of A corresponding 
vector 
to the 
from which 
it follows 
that 
xis an eigen
4. 4 
all eigenve
determine 
1 �] and 
ctors 
corre-
of A = [4
that 5 is an eigenvalue 
Show 
eigenval
sponding 
to this 
ue. 
that Ax = Sx. But this 
a nonzero vector 
Solution We must 
x such 
that there is 
show 
equation is 
equivalent 
the null 
(A -SI)x = 0, so we need 
to compute 
to the equation 
space of 
the matrix 
A -51. We find that 
_ 
SJ= [I 2] _  [5  OJ = [-4 2] 
4 3 0 5 
Since 
x are clearly 
the Fundamental 
linear
matri
of this 
the columns 
ly dependent, 
Thus, 
is nonzero. 
that its null 
Matrices 
Theor
em of Invertible 
implies 
space 
Ax = 
so 5 is an eigenvalue 
Sx has a 
nontrivial 
solution, 
by 
of A. We find 
its eigenve
ctors 
space: 
computing 
the null 
[A- SIIO J=[-4 210]� [I -t10J 
vector 
Thus, 
if x = [:J is an eigen
to the eigenvalue 
corresponding 
5, it satisfies 
x1 -t x2 = 0, or x1 = t x2, so these 
eigenv
are of the form 
ectors 
of [ t] (or, equivale
That is, they 
multiples 
ntly, 
are the nonzero 
the nonzer
o multiples 
of [:Ji 1 
.+ 
,\ of an n X n matrix 
ing to an eigenvalue 
The set 
ctors 
of all eigenve
correspond
that 
this 
is just the set of nonzero vectors 
in the null 
space 
set of 
of A -,\I. It follows 
eigenv
together 
ectors, 
the zero 
with 
vector 
in !Rn, is the null 
space of 

A - AI. 

0  0  0 

4 -2 0 

4 - 2  

A 

A 

2 5 6   Chapter 

Example 4 . 3  

4 Eigenvalues 
and Eigenve
ctors 
A be an n x n matrix and let 
of A. The 
Defi n ition Let 
A be an eigenvalue 
ctors 
of all eigenve
collection 
with 
corresponding to 
the zero 
vector, is 
A, together 
called 
the 
eigenspace of A and is denoted 
by EA-
4.2, E5 =  { t[ �] } . 
Therefore, 
in Example 
that A =  6 is an eigenvalue 
of A = 
Show 
eigenspace. 
A - 6I. Row reduction 
space of 
Solulion As in 
4.2, we compute 
the null 
Example 
A - 6I = [-� -� -!] -----+ [� � -
�i 
produces 
2  2 -4  0  0 
0 
= 0, 
A - 6I is nonzero. 
we see that the 
space of 
6 is an eigenvalue 
Hence, 
null 
from which 
ctors 
correspond
ing to this 
of A, and the eigenve
satisfy x1 + x2 -2x3 
or x1 = - x2 + 2x3. It follows 
that 

eigenvalue 

Example 4 . 4  

In IR2, we can give 
a geometric 
of the notion 
The 
interpreta
tion 
of an eigenvector. 
Ax =  Ax says 
Thus, 
x are parallel. 
the vectors 
Ax and 
equation 
x is an eigenvector 
that 
a parallel 
if A transforms 
of A if and only 
vector 
tly, 
x into 
if and only 
[or, equivalen
if TA (x) is parallel 
to x, where 
TA is the matrix 
transforma
tion 
corresponding 
to A] . 
of A =  [ � O] geometrica
lly. 
Find 
the eigenve
ctors 
and eigenvalues 
(see 
F in the x-axis 
that A is the matrix of a reflection 
Solulion We recognize 
to thems
parallel 
that F maps 
3.56). The only 
vectors 
Example 
are vectors 
parallel 
elves 
of [ 0] ), which 
of [ �] ), w�ich are reversed 
(i.e., 
multiples 
to the y-axis 
- 1), and vectors 
(eigenvalue 
parallel 
multiples 
are sent to thems
to the x-axis (i.e., 
elves 
(eigenvalue 
A =  -1 and 
A =  1 are the eigenvalues 
4.5). Accordin
(see Fi
gly, 
gure 
of A, and the 
corresp
onding 
eigenspaces 
are 
E_1 =  span([�]) and E1 =  span([�]) 

1) 

- 1 

Section 

4.1 Intro

duction 

to Eigenvalues 

and Eigenve
ctors 

2 5 1  

y 

3 

- 3  

ctors 
Figure 4 . 5  The eigenve
of a reflection 

y 

4 

3 

2  3  4 

Figure 4 . 6  

is based 
on the 
The discussion 
"Eigenpicture
s: Pictur­
article 
26 (1996), 
ing the Eigenve
Problem" 
ctor 
by 
Schonef
Steven 
eld in 
pp. 316-319. 

Mathematics Journal 

The College 

Another 
way to think 
of eigenve
ctors 
geometrica
lly is to 
draw 
x and Ax head-to­
of A if and 
be an eigenvector 
tail. 
in a straight 
only 
x will 
if x and Ax are aligned 
Then 
xis an eigenve
line. In 
Figure 
ctor 
of A but 
4.6, 
y is not. 
so is any non­
ing to the eigenvalue 
of A correspond
If xis an eigen
vector 
A, then 
multiple 
zero 
of x. So, if we want 
to search 
for eigenve
ctors 
geometrica
lly, 
we need 
4.[\ �ny,]isplay 
= [ � �] of Example 
happens when 
shows what 
of A on unit vectors. 
Figure 
the effect 
only consider 
4.7(a) 
we transform 
unit 
vectors 
with 
the matrix A 
x =  1 / v'2 is an 
the results 
il, as in Figure 
4.6. 
We can see that the vector 
head-to-ta
eigenvector, 
that there 
notice 
but we also 
to be an eigenvector 
appears 
in the second 
[ - � j �]. 
. Indeed, 
quadrant
this 
is the 
case, 
and it turns out 
to be the vector 

2 5 8   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

y 

y 

(a) 

(b) 

Figure 4 . 1  

A =  [ 1 -1 
4.7(b), 
we see what 
happens when we use the 
matrix 
In Figure 
ctors 
There 
are no eigenve
at all! 
the correspond
to find eigenvectors 
We now know how 
once 
we have 
ing eigenval­
and we have 
ues, 
a geometric 
remai
interpretation 
ns: How 
but one question 
of them-
do we first 
find the eigenvalues 
of a given 
obser
matrix? The key is the 
vation 
that A is 
of A -AI is nontrivial. 
space 
an eigenvalue 
of A if and only 
if the null 
A =  [; �] is 
of a 2  X  2 matrix 
3.3 that the determi
nant 
Recall 
from Section 
det A =  ad -be, and A is invertible 
the expression 
if and 
if det A is nonzero. 
only 
guaran
a ma­
Furthermore, 
Matrices 
tal Theor
the Fundamen
that 
em oflnvertible 
tees 
null 
trix has a nontri
if 
vial 
space 
if and only 
if it is noninvertible-
only 
hence, 
if and 
its determ
inant 
is zero. 
Putting 
see that (for 
these 
together
, we 
2 X 2 matrices 
at 
facts 
if det(A - AI) = 0. This 
least) 
A is an eigenvalue 
of A if and 
fact characterizes 
only 
it to square 
soon 
eigenval
ues, 
For the 
of arbitrary size. 
matrices 
generalize 
and we will 
though, 
let's 
see how to use it with 
moment, 
2 X 2 matrices. 
[ � �] from Example 
A 
matrix 
eigenvalues 
ing eigenve
Find 
of the 
all of the 
ctors 
and correspond
4.1. 
Solulion The preceding 
find all solutions 
A of the 
that 
show 
remarks 
equa­
we must 
-AI) = 0. Since 
tion 
det(A 
det(A -AI) =  det 1 l ] =  (3  - A)(3  - A) -1 =  A2 - 6A  +  8 
[3  -A 
to solve the 
solutions 
quadratic 
we need 
A2 -6A + 8 = 0. The 
equation 
to this 
equa­
to be A = 4 and 
A =  2. These 
tion 
are easily found 
of A. 
the eigenvalues 

are therefore 

3  -A 

Example 4 . 5  

To find the 
null space of 

eigenve
A -4I. We find 

2 5 9  

- 1  0 

ctors 

0  0  0 

correspond

A  = 4, we compute the 

ing to the eigenvalue 

to Eigenvalues 

and Eigenve
ctors 

Section 
4.1 Introduction 
11 OJ-+ [1 - 11 OJ 
[-1 
[A -4I loJ =  l 
that x = [:J is an eigenve
E4 = { [::]} = { x2 [ �]} = 
[A -2I I 0 l = [ � �I � J ---+ [ � � I � J 

ctor corresponding to 
A  =  4 if and 

ing to A  =  2 if and only 

the eigenspace 

ctor correspond

if y1 + y2 = 0 or 

have 

from which it follows 
only if x1 - x2 = 0 or x1 = x2• Hence, 

span([�]). 

Similarly, 

for A  =  2, we 

so y = [;J is an eigenve
y1 = -Yi-Thus, the eigenspace E2 = { [ �2]} = {y2[-�]} =span([-�]). 

Figure 
multiplied 
eigenvector 
eigenve
ctors 
of themselves 

4.8 shows graphically how the 
by A: an eigenvector 
x in the eigenspace 
y in the eigenspace 
E2 is transformed 
of A are the only vectors 
when multiplied 
by A. 

into 2y. As 
in IR2 that are transformed 

eigenvectors 

into 4x, and an 
E4 is transformed 

Figure 4.7(a) shows, 
the 

of A are transformed 

when 

into scalar multiples 

Ay = 2y 

y 

- 1 

- 2  

- 3  

- 4  

- 4  - 3  - 2  - 1 

2  3  4 

Figure 4 . 8  How A transforms 

eigenve
ctors 

2 6 0   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

Example 4 . 6  

Example 4 . 1  

in Example 

equation 

4.5) need not have real roots; 

Remark You will recall that a polynomial equation with real coefficients 
(such as 
it may have complex 
ctors 
Thus, it is important to 

the quadratic 
roots. (See Appendix C.) It is also possible 
come from "ll_P' where p is prime. 
when the entries 
specify the setting 
to work in before 
of a matrix. However, unless 
otherwise 
entries 

specified, the 
to be real 

to compute eigenvalues 

of a matrix 
we intend 

eigenvalues of 

are real numbers will be assumed 

and eigenve

a matrix whose 

as well. 

we set out to compute the eigenvalues 

in 

except 

as above, 

exactly 

in Z3• (Check 

with this matrix.) 

4.5 as a matrix 

that the same answer 

would be obtained 

the matrix in Example 

by first reducing 

we work modulo 3. Hence, 

Find the eigenvalues 

A modulo 3 to obtain 

S o lulion The solution proceeds 

Interpret 
that field. 

over "11_3 and find its eigenvalues 

the quadratic equation A2 -6A +  8 = 0 becomes A2 + 2 = 0. This equation is the 
same as A2 = -2 = 1, giving A= 1 and A= -1 = 2 as the eigenvalues 
[ � �] and then working 
of A = [ � -�] (a) over IR and (b) over the complex numbers C. 
O = det (A -AI) = det 1 -1] = ,\2 + 1 -A 
the equation [-A 
are A = 
nant from 2 X 2 to 

i and A = -i. (See Appendix C.) 

we will extend 
n X n matrices, which in turn will allow 
matrices. (In fact, this isn't quite 
eigenvalues 
equation 
of a given 

(a) Over IR, 
(b) Over C, the solutions 

the notion 
us to find the 
at least 

of determi
eigenvalues 

true-but we will 
must satisfy.) 

so A has no real eigenval
ues. 

S o lulion We must solve 

next section, 

of arbitrary 

are no solutions, 

be able to find 

In the 

a polynomial 

matrix 

that the 

there 

square 

1-6, show that v is an eigenvector 

ponding eigenvalue. 

4 . 1  

In Exercises 
the corres

..  I Exercises 
1. A = [� �l v = [�] 
2.A = [� �lv=[-�] 
3.A = [-! �l v = [ _�] 

of A and find 4.A = [! =�l v = [�] 
-;].F[-:] -1] [-2] 1 , v = 1 
5.A = 1 0 [: 6.A = 2 
[1 0 
0  1 

• 

to Eigenvalues 

and Eigenve
ctors 

4.1 Intro
Section 
images Ax under the action of a 2 X 2 matrix A are drawn 

duction 
19-22, the unit vector

s x in IR2 and their 

o-tail, as in Figure 4.7. Estimate 
the eigenvect
e." 
pictur

s of A from each ''eigen

In Exercises 

ors and 

2 6 1  

head-t
eigenvalue
19. 

y 

7-12, show that A is an eigenvalue 

or corresponding to this eigenvalue. 

of A and 

5 , 

- 1  , 

In Exercises 
find one 

eigenvect

7.A = [� 2] A= 3 
8.A = [� 3],\=- 1  2 , 
9.A = [_� 4] A=  1 
IO.A =  [! -2] -7 , A =  - 6  H 0 :JA �-I 
12. A� [: 2 -I] � , A= 2 

11. A =   1 0 

13-18, find the eigenvalue

s and eigenvect
ors of 

20.  y 

onto the 
x-axis) 

in the y-axis) 

in the line y = x) 

In Exercises 
A geometrica
lly. 

13.A = [-� �] (reflection 
14.A = [� �] (reflection 
15.A = [� �] (projection 
16. A = [il �] (projection 
2direction 
origin �ith
17. A = [ � �] (stretching 
18. A = [ � -�] (counterclo

about the origin) 

and a factor of 3 vertically) 

the 

through 

onto the line 

vector [i]) 
by a factor of 2 horizontally 

ckwise rotation 

of 90° 

2 6 2   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

21. 

y 

- 2  

22. 

y 

29.A = [� �]  [ 0  1  + i] 

30.A = 1 -i 1 

31-34, find all of the eig

envalue
s of the ma­

In Exercises 
trix A over the indicated Zp. 

31.A = [� �] overZ3  32.A = [� 
33. A = [! �]over Zs 34. A = [: �]over Zs 
A =  [: �] 

35. (a) Show that the eigenvalues 
of the 2 X 2 

matrix 

solutions 
of the 

are the 
A2 -tr(A)A + det A = 0, where tr(A) is the trace 
of A. (See 

quadratic 

equation 

page 162.) 

(b) Show that the eigenvalues 

of the matrix A in 

part (a) are 

A =Ha + d ± V(a -d)2 + 4bc) 

( c) Show that the 
trace 

in part (a) are given 

and determinant 
by 

of the matrix 

A 

where A1 and A2 are the 

tr(A) = A1 + A2 and det A = A1 A2 
of A. 

the matrix A in Exercise 35. Give 

again 
on a, b, c, and d such that A has 

eigenvalues 

36. Consider 
conditions 
(a) two distinct 
(b) one real eigenvalue, and 
(c) no real eigenvalues. 

real eigenvalues, 

37. Show that the eigenvalues 

of the 

upper triangular 

A =  [� �] 

matrix 

eigenvect

s of the matrix A. Give bases for each of 
ponding eigenspaces. Illustrate 
the eigenspaces and 

In Exercises 
all of the eigenvalue
the corres
the effect of multiplying 

23-26, use the method of Example 
4.5 to find 
ors by A as in Figure 4.8. 
23.A = [� -�] 24.A = [! �] 
25. A = [� �] 26. A = [ _ � �] eigensp
27. A = [ _ � �]  [2 -3] 

27-30, find all of the eig
28. A = l  O 

In Exercises 
A over the complex numbers C. Give bases for each of the 
corresponding 

eigenspaces. 

envalue

over the 

,E:S7 

complex 

numbers. 

are A = a and A = 

d, and find the 

corresponding 

aces. 

,E:S7 38. Let a and b be real 
s of the matrix corresponding 
eigensp
aces of 

numbers. Find the eigenvalues 

and 

11 f Determi n ants 

Section 

4.2 Determi

nants 

2 6 3  

nts preceded 

Historically, 
determina
algebra 
is taught 
nants aro
se indepen
and the 
theory 
matrices 
were deemed 
tory of determina

nts is pres

today, with matrices 
of matrices 

dently 

fact in light 

matrices-a curious 

of the way linear 
determi­
of many practical 
problem
nts was well developed almost two centuries 
before 
of study in and of themselves. A snapshot of the 

before determinan
in the solution 

ts. Nevertheless, 

of determina

worthy 

his­

s, 

Recall that the determina

matrix A = [all 

ented at the end of this section. 
nt of the 2 X 2 
det A = alla22 - a12a21 
when  we 
r, we found that 

a21 

We first encountered 
inverse of a matrix. 

In particula

this expression 

determined 

ways to compute the 

nt of a matrix A is sometimes also denoted 

by IA I, so for the 2  X  2 

also write 

a21 a22 

The determina

matrix A = [a11 a12] we may 
·  .  . I all a12 I . 
nt of a 1 X 1 matrix 

tation. 
the notation 
be clear 

for the matrix 
from the context 

Warning This notation for the determina

a21 a22 
itself. 

We define the determina

which is intended. 

Do not confuse 

det A =  lal = a  

A = [a] to be 

nt is reminisce
It 1s easy to mistake  , the notat10n for determmant, for 

nt of absolute value 

. [all a12] 

no-
, 
a21 a22 

these. 

Fortunatel

y, it will usually 

(Note that we really 
absolute 
3 X 3 

matrix? 

If you ask your CAS 

value of a in this case.) How then should 

we define the determi

nant of a 

here: I a I does not denote 

the 

the answer 

will be equivalent 

where Li =  aei -afh -bdi + bfg + cdh -ceg. Observe that 

for the inverse of 

with notation 

have to be careful 

A =  [� � ;] 
g  h l 
bf-ce l cd -af 
to [ ei -fh ch -bi 
A-1 = � Jg - di ai - cg 
= a I � �I - b I � �I + c I � �
I 

= a(ei -fh) -b(di -Jg) + c(dh -eg) 

Li = aei -afh - bdi + bfg + cdh -ceg 

dh -eg bg -ah 

ae -bd 

2 6 4   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

and that each of the entries 
nant of a 2 X 2 
of the determina
determina

in the matrix portion 
submatrix of A. In fact, this is 
true, 
nt of a 3 X  3 
matrix 

b e  the determi­
of the 
is recursive in the sense 
in terms of determinants of 2 X 2 
. 

of A -i appears to 
and it is the basis 

matrix. 
is defined 

that the 
matrices

nt of a 3 X 3 

definition 

Definition  [all 

LetA =  a21 

The definition 

a13] a23 . Then the determinant 

of A is the scalar 

a31 a33 

( 1 )  

Notice 

that each of the 2 X 2 
the entry 

umn of A that contain 
the first summand 
row 1 and column 
deleting 
Equation 
( 1). If we denote 
i and column 

determina
the determina

nts is obtained 
nt is being 
nt of the 

by the determina

the row and col­
by. For example, 

by deleting 
multiplied 
submatrix 
by 
in 
row 

obtained 
alternate 

by deleting 

1. Notice 
by A;j the submatrix 

also that the plus and minus signs 
A obtained 

of a matrix 

j, then we may abbreviate 
Equation 
( 1) as 

is a11 multiplied 

det A = all det All - a12 det A12 + a13 det A13 

3 2: (- l)1+ja1j det A1j 
For any square matrix A, det A;j is called 

j=l 

the (i, j)-minor 
of A. 

Solution We compute 

det A = 51-� � I - ( -3) I � � I + 2 I � _ � I 

= 5(0  -(- 2)) +  3(3  - 4) +  2(- 1  - 0) 
= 5(2) +  3( - 1) +  2(- 1) = 5 

With a little 
in yom head. 

practice, 
Wciting 

you should 
out the secood 

find that you can easily work out 2 X 2 

determina

line in the above solution 

is then 

unnecess:+ 

nts 

method 

Another 
to the method 
columns 

for calculating 

the determina

for calculating 
the determina

matrix 
Copy the first two 
and take the products of the elements 
six 

nt of a 3  X  3 
matrix. 

nt of a 2  X  2 

of the matrix 

right 

on the 

of A to the 

is analogous 

Example 4 . 8  

Compute the determina

nt of 

Section 

4.2 Determi

nants 

2 6 5  

diagonals shown below. Attach 
diagonals and attach 
minus signs 

from the downward
to the products from the upward-sloping 
. 

-sloping 
diagonals

plus signs 

products 

to the 

(2) 

This method 

gives 

+ 

In Exercise 
tion ( 1) for a 3 X 3 

determinant. 

19, you are asked to check that this result 

agrees 

with that from Equa­

Example 4 . 9  

Calculate 

the determina

nt of the matrix in Example 4.8 using the 

shown in (2). 

method 

Solution We adjoin to A its first two columns  and 
products: 

compute the six indicated 

0  - 10  - 9  

-2 

Adding 
top gives 

the three 

products at the bottom and subtracting 
the three 

det A =  0  + (- 12) + (-2) - 0  -(- 10) -(-9) =  5 

products at the 

as before. 

nants for arbitrary 

square matrices

. 

in Example 

4.9 for larger 

matrices
. It is 

to define determi
of the method 

Warning We are about 
is no analogue 

However, 
there 
valid only for 3 X 3 
Delerminanls of n x  n Malrices 

matrices

. 

determina

nt of a 3  X  3 

The definition 
square matrices

of the 
. 

Defi D i l i O D  Let A =  [a;) be an n x n matrix, 

matrix 

minant of A is the scalar 

extends naturally 

to arbitrary 

where n 2: 2. Then the deter-

n 
_L (- l)1+ia11 det A11 

j�I 

(3) 

2 6 6   Chapter 

4 Eigenvalues 

and Eigenve
ctors 
the (i, j)-cofactor of A to be 

It is convenient to combine 

a minor with its plus o r  minus sign. 

To this end, 
we define 

With this notation, 

definition 

(3) becomes 

<let A =  2:a1jCij 

n 
j=l 

matrix when n =  2. 

Exercise 20 asks you to check that this definition 
determina

nt of a 2 X 2 

correctly gives 

the formula 

for the 

Definition 

( 4) is often referred 

fact that we get 

an amazing 
exactly 
even any column)! We summarize 
end of 
if we were to 

present it here). 

this section 

to as cofactor expansion 
the same result by expanding 
as a theorem but defer 
this fact 

along any row (or 
the proof 
until 
the 

along the first 

row. It is 

(since it is somewhat lengthy 

and would interrupt our discussion 

(4) 

Theorem 4 . 1  

The Laplace 

Expansion Theorem 

nt of an n X n matrix A =  [a;), where n 2: 2, can be computed 

as 

The determina

n 2:a;jCij 

j=l 
along the ith row) and also as 

(which is the cofactor expansion 

(5) 

(6) 

(the cofactor expansion 

along the jth column). 

Since Cij = ( -l)i+j <let A;j, each cofactor 

with the correct sign 
the sign is 

given by the 
remember 

+ or -is to 

is plus or minus the correspond
ing minor, 

term ( -1y+j. A quick way to determine 

that the signs form 

a "checkerbo

ard" pattern: 

whether 

=  2:aijcij 

n 
i=l 

+ 

+ 

+ 

+ 

+ 

+ 

+ 

+ 

Example 4 . 1 0  

Compute the determina

nt of the 

matrix 

Section 

4.2 Determi

nants 

2 6 1  

by (a) cofactor 
second 

column. 

expansion 

along the third 

row and (b) cofactor 

expansion 

along the 

Solution 
(a) We compute 

=21-� �1-(-1)1� �1+31� -�1 
8 + 3(3) 
= 2(-6) + 

= 5 
we have 

(b) In this case, 

(17 49-1827) 
Simon 
Pierre 
Laplace 
y, France, 
in Normand
was born 
to become 
and was 
expected 
a 
mathematical 
until his 
clergyman 
were 
talents 
noticed 
at school. 
=-(-3)1� �1+01� �1-(-1)1� �I 
He made 
many 
important 
contributions 
to calculus, 
=3(-1) +0+8 
probability, 
my. He was 
and astrono
of the young 
an examiner 
Napoleon 
Artillery 
Bonaparte 
at the Royal 
Corps and later, 
when 
was 
Napoleon 
in power, 
served briefly 
as Minister 
and then 
of the Interior 
Chancellor 
was granted 
of the Senate. 
Laplace 
of Count of the 
the title 
Empire 
in 1806 and recei
ved the 
of 
title 
Marquis de 
in 1817. 
Laplace 

to do fewer calculations 
that contained 
C22• It follows 
Theorem is most useful when the matrix contains 
os, since, 

that in part (b) of Example 4.10 we needed 

in part (a) because we were expanding 
namely, a22; therefore, 
we did not 
Expansion 
lots of zer
number of cofactors 

along a column 
need to compute 

we need to compute. 

Notice 

a row or column 
with 
the 

than 
a zero entry­

by choosing to expand along that row or column, we minimize 

that the Laplace 

= 5 

Example 4 . 1 1  

Compute the determina

nt of 

Solution First, notice 
fore expand along 

that column 3 has only one 

note that the + /-pattern 

this column. Next, 

nonzero entry; 

we should 

there­
a minus sign 

assigns 

2 6 8   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

Example 4 . 1 2  

Compute the determi

nant of 

to the entry a23 = 2 .  Thus, we have 

We  now continue 
(the third 
column 

det A  = a13C13 + a23C23 + a33C33 + a43C43 
= O(C13) + 2C23 + O(C33) + O(C43) 

2 - 3  1 
- 2  1 - 1  3 
-2  0 
along the third row of the determina

by  expanding 
would also be a good choice) 
to get 

det A  =  - 2 ( - 2 I = � � I -I � � I) 

- 2(- 2(- 8) - 5) 

nt above 

for the 3 X  3 

minor is not that 

of the original 
matrix 
but 

+I - pattern 

(Note that the 
that of a 3 X 3 

matrix 

in general.) 

- 2(1 1) = - 22 

The Laplace 

expansion is 

particular

ly useful when the 

matrix 

is (upper or lower) 

triangular. 

4 

Solution We expand along 

2 - 3  0 4 
0 3 2 5 7 
A =  0 0 1 6 0 
0 0  0 5 2 
0  0 0 0 - 1  

the first column 

to get 
3 2  5 7 
det A  =  2 0 1 6 0 
0 0 5 2 
0 0  0 - 1  

1  6 0 

det A  =  2 

· 3 0  5 2 
0  0 - 1  

(We have omitted all cofactors 
the first 

column 

again: 

corresponding 

to zero entries.) Now we expand along 

det A  = 2 

Continuing 

to expand along 

the first column, 

we complete 

the calculation: 

· 3 · 1 I � 21 = 2. 3. 1 . (5(- 1) - 2. 0) = 2. 3. 1. 5. (- 1) = - 30 
4 

- 1  

Section 

4.2 Determi

nants 

2 6 9  

Example 
product of 
We record 

4.12 should 
its diagonal 
the result 
as a theorem. 

convince 
entries. You are asked to give a proof 

you that the determinant of a triangular 

of this fact in Exerc
ise 2 1 .  

matrix is the 

Theorem 4 . 2  

The determina
diagonal. 
Specific

nt of a triangular 

matrix is the product of the entries on 

its main 

ally, 

if A = [a;) is an n X n triangular 

matrix, then 

<let A = a11a22 · • ·a"" 

Nole In general (that is, unless 

nt of a 3 X 3 

a determina
matrix 

form), computing 
the determina
cations, 
tions. 
and then n! - 1  additions 

For an n X n matrix, 

and then five additions 

the matrix 
nt by cofactor 

is triangular 
expansion 

has 6 = 3! summands, 
and subtractions 
are needed 

there will be n! summands, 
and subtract
T(n) = (n - l)n!  + n!  - 1 > n! 

The total 

ions. 

or has some other 
l 

specia

is not efficient. 
each requiring 

For example, 
two multipli­

to finish off the calcula­

each with n - 1 multiplica
tions, 
number of operations is thus 

Even the fastest 

of supercomputers 
expansion. 

matrix using cofactor 

cannot calcula

te the determina

nt of a mod­
to 

To illustrate

: Suppose we needed 

l camera.) To calculate the determina

nt. (Matrices 
such as those 

and 50! =  3  X 1064. If we had a computer 

much larger 
transmitted 
nt directly would require, 

than 50 X 50 are used to store 
over the Internet 

or taken by a 

per second, 
to finish 

the calcula

tions. To put this 

it would take 

approximately 

estimate the age of the universe to be at least 10 
billion 
determina

calculating 

in general, more 
that could perform 
3  X 1052 sec­
in perspective, con­
( 1010) 
a 50 X  50 
nt by 

images 

a 50 X 50 determina

erately 
large 
calculate 
the data from digital 
digita
than 50! operations, 
a trillion 
onds, 
sider 
years. Thus, on 
expansion 
cofactor 

(1012) operations 
or almost 1045 years, 
that astronomers 

even a very fast 
would take 

supercomputer, 
more than 
y, there are better methods
ally effective 
properties 

of determinan

means of finding 

1030 times 
-and we now 

determinan

ts. 

computation
some of the 

Fortunatel

turn to developing 

more 
ts. First, we 

need to look at 

the age of the universe! 

Properties of Determinants 
way to compute 
The most efficient 
not every elementary 
row operation 
The next theorem summarizes 
to use row reduction 

effectively. 

determina
leaves 

the determina
the main properties 

nts is to use row reduction. However, 

nt of a matrix unchanged. 

you need to understand 
in order 

Theorem 4 . 3  

Let A = [aij] be a square matrix. 

by interchanging 

two rows (columns) of A, then <let B = -<let A. 

(column), then <let A = 0. 

a. If A has a zero row 
b. IfB is obtained 
c. If A has two identical 
d. If B is obtained 
e. If A, B, and C are identical 
except 

by multiplying 

rows (columns), then <let A = 0. 

a row (column) of A by k, then <let B = k <let A. 
of 

that the ith row (column) 

of C is the sum 

the ith 

f. If B is obtained 

rows (columns) of A and B, then <let C = <let A + <let B. 
row (column) 

a multiple 

by adding 

of one 

of A to another 

row 

(column), then <let B = <let A. 

210  Chapter 

4 Eigenvalues 

and Eigenve
ctors 

es. We will prove the remaining 

the correspond

(a) and (f) are left as exercis

Proof We will prove (b) as Lemma 4.14 at the end of this section. 
properties 
terms of rows; 
(c) If A has two identica
<let B = detA. On the other 
so detA = 0. 
( d) Suppose row i of A is multiplied 
Since 
Cij of the elements 
the ith row of B gives 
expanding 
along 

ing proofs for columns 
obtain 

by k to produce B; that is, bij = ka;j for j = 1, . . .  , n. 
rows of A and Bare identica
l (why?), 

are analogous
the matrix 

by (b), detB = - detA. Therefore, 

B. Clearly, B = A, so 
detA = -detA, 

l rows, swap them to 

in the ith 

the cofactors 

hand, 

. 

The proofs of 
properties 
in 

detB = � b;jCij = � kaijcij = k � aijcij = k detA 

n 
j=l  j=l  j=l 

n 

n 

(e) As in (d), the cofactors 
identical. 
to obtain 

Cij of the elements 
Moreover, cij = a;j + b;j for j = 1, . . .  , n. 
detC = � c;jCij =�(au+ b;)Cu = � a;jCiJ + � buCu = detA  + detB 

in the  ith 
C are 
We expand along the ith 

n 
j=l  j=l 

n 
j=l  j=l 

row of C 

rows of A,  B,  and 

n 

n 

Notice 

form of 

f) are rela

the echelon 

that properties 

ted to elementary 

Since 
combine these properties 
Exploration: 
an n X n matrix uses on the order 
cofactor 
using 

(b), ( d), and ( 
a square matrix is necessarily 
with Theorem 2 to calcula
in Chapter 2, which shows that row reduction 
of n3 operations, 

of 
far fewer than the n ! needed 
for 
nts 
the computa

expansion.) The next examples 

tion of determina

upper triangular, 
we can 

illustrate 

Operations 

te determina

row operations. 

row reduction. 

Counting 

(See 

nts efficiently. 

Example 4 . 1 3  

Compute <let A if 

(a) A = [ 20 35 -31] 
- 4  -6 2 
(b) A� [i -� j il 

Solution 
(a) Using property (f) and then 

property (a), we have 

2 3 - 1   2 
<let A =  0 5 3  0 
-4 - 6  2  0 

3 - 1  
5 3 = O  
0  0 

R,+2R1 

Section 

4.2 Determi

nants 

211 

(b) We reduce 

A to echelon 

form as follows 

(there 

are other 

possible ways to do 

this): 

0 
3 
<let A =  
2 
5 

2 
0 
4 

3  0 - 3  

- 4  5 
- 3  6 R,<->R, 0  2  -4 
5  7  2  4  5 
5 - 1   - 3  

- 1  
- 3  
1 
0 
2 
0 
= - 3  
4 
0 
0 - 1  2 -9 

R3-2R1 
R4-SR1 
R,+4R, 1  0 - 1  2 
R4+2R2 Q  - 1  2 - 9  

- 1  
-4 
7 

=3 0 0 15 - 33 
0 0 0 - 13 

1 
2 
5 R1ttR4  Q 
3 = -(- 3) 
0 
0 

= - 3  

6  1 0 - 1  2 
5 Ri/3  0 2 -4 5 
7  2 4 5 7 
1  5 - 1  - 3  1 
0 - 1  2 
- 1  2 -9 

4 7  3 
2 -4 5 

=  3 . 1 . ( - 1) . 15 . ( - 13) = 585 

Remark By Theorem 

4.3, we can also use elementary 
column 

of computing 
operations. 
3 to column 
we used was faster, 

determinan
For example, 
1 to create 
but in other 

in the 
operations 
ts, and we can "mix and match'' elementary 
row and 
in Example 
have started 
by adding 
a leading 

1 in the upper left-hand corner. In fact, the 
operations may speed up 
examples 

4. l 3(a), 

we could 

column 

process 
column 
column 
method 
the calcula

tions. Keep this in mind when you work determina

nts by hand. 

oe1erminan1s of Elemen1arv Malrices 
Recall 
mentary 
the following 

on an identity 

3.3 that an elementary 

row operation 

theorem. 

from Section 

matrix. 

Setting 

matrix results 

from performing 

an ele­

A  = In in Theorem 4.3 yields 

Theorem 4 . 4  

Let E be an n X n elementary 
a. If E results from interchanging 
b.  If E 
c. If E results from adding 

two rows of In, then <let E = - 1. 
In by k, then <let E = k. 

results from multiplying 

one row of 

a multiple 

matrix. 

of one row of In to another 

row, then 

detE = 1 .  

<let In = 1, applying 

Proof Since 
(a), (b), and (c), respecti
vely, 

lemma is de
The word 
rived 
from 
lambanein, which 
verb 
the Greek 
"to grasp:' 
In mathematics, 
means 
a "helper theorem
a lemma is 
" 
that 
and use 
we "grasp hold of" 
another, 
to prove 
usually 
more 
important, 
theorem. 

forms the 
(b), (d), 
is straightforward 

elementary 
and (f) ofTheorem 4.3 succinctly 

Next, recall that multiplying 
a matrix 

corresponding 

and is left as Exercise 43. 

row operation 

of Theorem 4.4. 

following 

lemma, 

as the 

B by an elementary 

matrix on the left per­

on B. We can therefore rephrase 

the proof 

of which 

(b), (d), and (f) of Theorem 4.3 immediately 
gives 

212  Chapter 

and Eigenve
ctors 

4 Eigenvalues 
det(EB) =  (det E)(det B) 

Let B be an n X n matrix and let 

Lemm a  4 . 5  

matrix. 
Then 
E be an n X n elementary 

Theorem 4 . 6  

We can use 

tion of invertibility 

Lemma 4.5 to prove the 
in terms of determina

main theorem of this section: 
nts. 

a characteriza­

A square matrix A is invertible 
We will show first that det A  * 0 if and only if det R * 0. Let E1, E2, ••• , Er be the 

R be the reduced 

if and only if det A * 0. 

row echelon 

P r o o f  Let A be an n X n matrix 
elementary 
A to R. Then 

elementary 

correspond

ing to the 

matrices 

and let 

form of A. 

row operations that reduce 

applying 

Lemma 4.5, we obtain 

Taking 

determina

and repeatedly 

nts of both sides 
(detE) · · · 
By Theorem 4.4, the determina
conclude that det A * 0 if and only if det R * 0. 
Now suppose that A is invertible. 
Then, 

(detE2)(detE,)(detA) =  detR 
R =  In, so det R =  1  * 0. Hence, 

Matrices, 
then det R  * 0, so 
...--R must be I

R cannot contain 

n (why?), so A is invertible, 

Theorem oflnvertible 

Fundamental 

by the 
det A * 0 also. Conversely, if det A * 0, 
a zero row, by Theorem 4.3(a). It follows 
by the 

Fundamental 

Theorem again. 

that 

nts of all the elementary 

matrices 

are nonzero. 

We 

now try to determine 

Determinants and Matrix Operations 
Let's 
some of the basic 
det(kA), det(A + B), det(AB), det(A -i ), and det(A T) in terms of det A and det B . 
correct relationshi
p 
nts is given by the 
between scalar multiplication 
following 

4.3(d) does not say that det(kA) = k det A.  The 

between determina
nts and 
formulas for 

if any, exists 
ally, 

matrix operations. Specific

what relationship, 

we would like to find 

and determina

theorem. 

....,...  Theorem 

Theorem 4 . 1  If A is an n x n matrix, then 

det(kA) =  kn det A 

You are asked to give a proof 

of this theorem in Exercise 

44 . 

....,...  Unfortunatel

y, there is no simple 

formula for det(A + B), and  in 

general, 

det(A + B) * det A + det B. (Find two 2 X  2 matrices 
comes as a pleasant surprise 
matrix multipl
Indeed, 

to find 
we have the following 

out that determina

ication. 

that verify this.) It therefore 
nts are quite 

compatible 
with 

nice formula due to Cauchy. 

213 

Augustin 

Section 
nants 
4.2 Determi
engineering 
Paris 
and studied 
but switched 
Louis Cauchy ( 1 789-1 857) was born in 
and prolific mathematician, 
he published 
to mathematics 
A brilliant 
because of poor 
health. 
difficult 
His name 
on many 
700 papers, 
problems. 
can be found 
theorems 
over 
many 
on quite 
series, 
ra, and 
, algeb
theory
in differential 
equations, 
and definitions 
probability 
infinite 
X into 
rigor 
He is noted 
for introducing 
calculus, 
laying the 
foundation 
for the branch 
physics. 
into 
Politically 
known 
of mathematics 
conse
as analysis. 
rvative, 
Cauchy 
1830 
was a royalist, 
and in 
he followed 
returned 
Charles 
exile. He 
to France 
in 1838 
return to 
but did not 
his post 
at the Sorbonne 
until 
the university 
dropp
ed its 
t that 
faculty 
of 
requiremen
swear 
an oath 
loyalty 
to the new king. 

Theorem 4 . 8  

If A and B are n X n matrices, 

then 
det(AB) = (det A)(det B) 

Proof We consider 
two cases: 
then, 
If A is invertible, 

A invertible 

and A not 

invertible. 

by the Fundamental 

Theorem oflnvertible 
it can 

Matrices, 

be written 

as a product of elementary 
matrices-say, 

Then AB = E1E2 · · · EkB, so k applications 

of Lemma 

4.5 give 

A =  E,E2• · ·Ek 

Continuing 

to apply Lemma 4.5, we obtain 

det(AB) = det(E1E2 • • • 

Ek)det B = (det A)(det B) 

On  the 
in Section 
det(AB) = (det A)(det B), since 

other hand, if A is not invertible, 
3.3. Thus, by Theorem 

both sides 

are zero. 

then neither 

is AB, by Exercise 

47 

4.6, det A  =  0 and det(AB) 

= 0. Consequently, 

that 

Theorem 4.8 to A  = [22 �]and B  = [� �], we find 
[12 
AB= 16 �] 
det(AB) = 12 = 4 · 3 = (det A)(det B), as claim
4 

p between the determina

det B = 3, and 

det A = 4, 
assertions!) 

ed. 

and that 
(Check these 

The next theorem gives a 
matrix and the determina

nice relationshi
nt of its inverse. 

nt of an invertible 

Example 4 . 1 4  

Applying 

214  Chapter 

4 Eigenvalues 

and Eigenve
ctors 

Theorem 4 . 9  If A is invertible, 

then 

1 
det(A-1) = --detA 

Proof Since A is invertible, 

......... (<let A)(det A-1) = 1, by Theorem 4.8, and since 

AA -I = I, so <let (AA -I) = <let I  = 1. Hence, 
<let A  *  0 (why?), dividing 

by 

<let A yields 

the result. 

Example 4 . 1 5  Verify 

Theorem 4.9 for the matrix A ofExample 

4.14. 

Solulion We compute 

so 

detA-1 = (�)(�)-(-�)(-�) = %-t = � = de�A 

Remark The beauty of Theorem 4.9 is that sometimes 

we do not need to know 

a matrix 

is, but only that it exists, 

the matrix A in the last two exam

ples, 

nt 
or to know what its determina
once we know that <let A = 4 * 0, we 
and that <let A -l = � without 

actually 

that A is invertible 

what the inverse of 
is. For 
immedia
A -l. 
computing 

tely can deduce 

We now relate the 
AT are just the columns of 
<let A by exp

determinant of a matrix A to that of its transpose 
<let AT by expanding 

AT_ Since the 
along 
which the 
along its first column, 
do. Thus, we have the following 
result

rows of 
row is identical 
Laplace 

Theorem allows us to 

A, evaluating 

to evaluating 

Expansion 

anding 

the first 

. 

Theorem 4 . 1 0  

For any square matrix A, 

detA = detAT 

Introduction 

to the Analysis of Algebraic 

we derive 

systems and the inverse of 

Cramer's Rule and lhe Adioinl 
In this section, 
of linear 
a formula for describing 
variables 
entirely 
beyond 2 X 2 systems, 

(1704-1752) was 
Cramer 
Gabriel 
a Swiss 
mathematician. 
The rule 
that 
bears 
his name 
was published 
in 1750, in his 
treatise 
as 1730, however, 
As early 
special 
of the 
were 
cases 
formula 
known 
mathematicians, 
to other 
including 
Colin Maclaurin 
the Scotsman 
the greatest 
(1698-1746), perhaps 
Column i -1.-
who 
mathema
of the British 
ticians 
A;(b) = [ a1• · ·h· · ·anl 
were 
the "successors 
of Newton:' 

it is of great theoretical 
new notation 

trix A and a vector 
column 

in terms of determinan

b in !Rn, let A;(b) 

We will need some 

of A by b. That is, 

a matrix. 

Curves. 

two useful formulas relating 

determina

nts to the solution 

The first of these, 

Cramer's Rule, gives 

the solution of certain systems 

of n linear 

ts. While this result is of little 

practical 

equations 
in n 
use 

importance. 

for this resu
denote 
the matrix 

lt and its proof. For an n X n ma­

obtained 

by replacing 

the ith 

Theorem 1 . 1 1  

Cramer's 
Rule 

Section 

4.2 Determi

nants 

215 

Let A be an invertible 
n X n matrix 
Ax = b is given by 
solution x of the 

system 

X· = l  det(A;(b)) 

detA  for i = 1, . . .  , n 

and let b be a vector 

in !Rn. Then the unique 

e2, .•• , en-If Ax = b, then 

Proof The columns 

of the identity 

matrix 

I = In are the standard unit vectors 

en] = [Ae1 • • • Ax · · · 

Aen] 

e1, 

AI;(x) = A [ e1  x · · · 
an] = A;(b) 
b · · · 

= [ a1 · · · 

Therefore, 

by Theorem 4.8, 

(detA)(detI;(x)) = det(AI;(x)) = det(A;(h)) 

Now 

1 0  X1  0 0 
0 1  Xz  0 0 

detI;(x) = 0  0  X;  0 0 = X; 

0  0 Xn-1  1 0 
0  0  Xn  0 

as can 
result 

be seen by expanding 
follows 
by dividing 

along the ith row. 
nonzero, 
since 

Thus, (<let A)x; = det(A;(h)), and the 
A is invertible

by <let A (which is 

). 

Example 4 . 1 6  

Use Cramer's Rule to solve 

the system 

X1 + 2X2 = 2 

-x1 + 4x2 = 1 

Solution We compute 

detA = 1-� !I= 6, det(A1(b)) =I� !I= 6,  and 

det(A2(b)) = 1-� �I 

=3 

By Cramer's Rule, 

det(A1(b)) 6 

X1 = ---- = -= 1 
6 

detA 

det(A2(b)) 

and x2 = ---­detA 

3 
6 

2 

216  Chapter 

4 Eigenvalues 

and Eigenve
ctors 

s Rule is computa

because it involves 

tionally inefficient 
the calculation 

of many 
determinan
ts, using 

these 

for 

Gaussian elimination 

to 

just one of 
would be better spent using 

method, 

Remark As noted previou
small systems of 
linear 

sly, Cramer'
all but 
equations 
determinants. The effort expended to compute 
even the most efficient 
solve the system 
is a formula 
of this section 
hinted 
nts. This formula was 
proof 

which was given without 

determina
matrix, 
come full circle. 

The final result 

directly. 

for the 

of a matrix 

inverse 
for the 

in terms of 
of a 3  X  3 

inverse 

at by the formula 
at the beginning 

of this section. 

Thus, we have 

Let's 

discover the formula 

for ourselves. If A is an invertible 
AX = I. Solving 

n X n matrix, 
its 
for X one 

the equation 

matrix X that satisfies 

inverse is the (unique) 
column at a time, 

column 

of X. That is, 

let "i be the jth 
X· = 1 

Therefore, 

Axj = ej, and by 

Cramer's Rule, 

However, 

det(A/e)) 

detA 

ith column 

-J, 

a11 ai2 
az, a22 
<let (A;( e)) = ai, a12 
an, an2 
of A. 

JI 

JI 

= (- l)j+idet A =  C 

0  a1n 
0  a2n 
a Jn 

0  ann 

It follows 

which is the (j, i ) -cofactor 
that xij = ( 1/det A)Cji• so A 
In words
the determina
nt of A. 
The matrix 

, the inverse of A is the 

-i = X = ( 1/ <let A) [Cji] = ( 1/det A)[Cijf. 

transpose 

of the matrix 

of cofactors 

of A, divi
ded by 

the adjoint (or adjugate) of A and is denoted 

by adj A. 

The result we 

have 

is called 
just proved can be stated 

as follows. 

Theorem 4 . 1 2  

Example 4 . 11 

Section 

4.2 Determi

nants 

211 

adj A 

det A 

the inverse of 

Use the 

to compute 

cofactors 

adjoint method 

n X n matrix. 

Let A be an invertible 

Then 1 
A-1 =--
A =[�� -�i 
1 3 - 3  
Solution We compute det A = - 2 and the nine 
C11 = +I� 41 = -18 
- 3   C12 = -I� 41 = 10 
-11 
-11 C22 = +I� 
C13 = +I� 
�I = 4 
�I = -1 
C23 = - I� 
C31 = +I� -11 C32 = -I� -11 
C21 = -I� 
C33 = +I� 
�I= - 2  
= - 6  4 
4 = 10 
=[-�� -� ��1 
adjA  = [-1: �� -�Jr
10 - 6   - 2  4 -1 - 2  
- 2  -1 ��1 = [-� -t -:1 
[-18 
1 
1 
- 2   - 2  t 1 
detAadjA  = -2 l� 3 

The adjoint is the transpose of the matrix 

of cofactors-na

mely, 

Then 

A-1 = 

which is the same answer 

we obtained 

(with less work) in Example 

3.30. 

= - 2  

=  3 

- 3  

- 3  

-3 

Theorem 

Expansion 

lhe Laplace 

Proof of 
Unfortunately, 
proof 
several 
of a matrix is the same as cofactor 

there is no short, 

we give has the merit of being relatively 
steps, the first of which is to prove that cofactor 

expansion 

easy proof 

Expansion 

of the Laplace 
The 
straightforward. 
the first row 

Theorem. 
We break it down into 

expansion 
the first colu

along 

along 

mn. 

Lem m a  4 . 1 3  

Let A be an n X n matrix. 
Then 

2 1 8   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

on n. For n = 1, the result 

is trivial. 
Now 

for (n - 1) X (n - 1) matrices; 

this is our 

induction 

that the result 

Proof We prove this lemma by induction 
assume 
hypothesis. 
taining au are accounted 
containing 

Note that, by the definition 
for by the summand 

is true 

au. 

The ith summand 

on the right-hand side of Equation 

det A;1. Now we expand det A;1 along 

the first 

row: 

of cofactor (or 

minor), all of the terms con­
au Cu. We can therefore 
ignore 
(7) is a;1 C;1 = a;1 (- 1) i+l 

terms 

a12 a13  a11  aln 
a;-1,2 a;-1,3 a;-1,J  a;-1,n 
a;+1,2 a;+1,3 a;+1,J  a;+J,n 
an2 an3  an)  an,n 

The jth term in this expansion 
tion Akl,rs denotes 
the submatri
r ands. Combining these, 
side of Equation 
( 7) is 

of det A;1 is a11(- 1)1+J-l det A1;,11, where the nota­
rows k and l and columns 
x of A obtained 
a;1a11 on the right-hand 

we see that the term containing 

by deleting 

a;1 ( - l);+1a1/- l)1+J-I det A1u1 =  ( - l)i+J+1a;1a11 det A1;,11 

What is the term containing 

factor a11 occurs in the jth summand, 
hypothesis, 

we can expand det A1i along 

(7)? The 

aila11 on the left-hand side of Equation 

a11C1i = a1/ - 1)1+J det A11. By the induction 
umn: 
its first col
az1 az,J-1 az,J+1  a2n 
a31 a3,j-I a3,j+I  a3n 
a;1  a;,j-1 a;,J+l  a;n 
an!  an,j-1 an,)+!  ann 

The ith term in this expansion 
taining 

a;1a1i on the left-hand side of Equation 
(7) is 

of det A11 is ail (- l)(i-l)+l det A1;,11, so the 

term con­

a1J  a;1 

( - 1)1+J ( - 1)(;-i)+ i d  t A -( - l)i+J+i d  t A 
aila1J e 1;,11 
of Equation 

left-and right-hand sides 

that the 

e 1;,11 -

which establishes 

(7) are equival
ent. 

Next, 

we prove property (b) of Theorem 4.3. 

Lem m a  4 . 1 4  

Let A be an n X n matrix and let B be obtained 
(columns) of A. Then 

by interchanging 

any two rows 

det B = - detA 

Section 

4.2 Determi

nants 

219 

the proof 

Proof Once again, 
when n = 2, so assume 
that the resu
lt is true 
cent rows of A are intercha

that it is true 
for n X n matrices

is by induction 

on n. The result can be easily checked 
. We will prove 
when two adja­

for (n -1) X (n -1) matrices
. First, we prove that it holds 

nged-say, rows r and r + 1. 

By Lemma 4.13, we can evaluate 

umn. The ith term in this expansion is 
b;1 = a;1 and B;1 is an (n -1) X (n -1) submatri
two adjacent 

rows have been interchanged. 

expansion 

<let B by cofactor 
along its first col­
( -1)1 + ib;1 <let B;1. If i * r and i * r + 1, then 
to A;1 except 

x that is identical 

that 

all a12  aln 

a;1 a;2 

a;n 

ar+l,l ar+l,2  ar+l,n 
arn 
arl ar2 

anl an2  ann 

Thus, by 

the induction 

hypothesis, 
If i = r, then b;1 = ar+l,l and Bil = Ar+l,l· 
all a12 

aln 

<let B;1 = - <let A;1 if i * r and i * r + 1. 

Row i -+  ar+l,l ar+l,2 
arl ar2 

ar+l,n 
arn 

Therefore, 

the rth 

an] an2 
summand in <let 
B is 

ann 

Similar

( - l)r+Ibrl <let Br! = ( - l)r+Iar+I,I <let Ar+ I, I = -( - l)(r+I)+Iar+I,I <let Ar+ I, I 
in <let B is 
l)st summand 

ly, if i = r + 1, then b;1 = ar1, B;1 = Ar1, and the (r + 
(- l)(r+I)+Ibr+I,I detBr+I,I = (- l)'arl detArl = -(- l)r+larl detArl 
words, 

and (r + l)st terms in the first 
of the ( r +  1 
)st and rth 

column 
respecti
vely, 

cofactor 

terms, 

expansion 
in the first 
column 

of 

the rth 

In other 
<let B are the negatives 
cofactor 

expansion 
tuting 

<let B = ,L,.;  - 1  b;1 <let B;1 

all of these 

Substi

of <let A. 

we obtain 

Lemma 4.13 again, 

results into <let B and using 

n �( )i+l 
i=l 
n 
� (- l)i+lb;1 <let B;1 + (- l)r+lbrl <let Br1 + (- l)(r+J)+lbr+l,l <let Br+l,l 
i=l 
i;"r,r+l 
n 
-,L,.;  - 1 a;1 <let A;1 
� (- l)i+1a;1(- detA;1) -(- l)(r+l)+lar+l,l detAr+l,l -(- l)r+lar1 detAr1 
i=l 
n �( )i+l 
it=r,r+l 

i=l 
- detA 

2 8 0   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

for n X n matrices 

the result 
for arbitrary 

This proves 
see that it holds 
rows r ands, where r < s, can be swapped by performing 
adjacent rows (see Exercise 
changes 

of the determina

the sign 

row interch

67). Since 

of 
is odd and each one 
the number of interchanges 

2(s - r) - 1 interchanges 

nt, the net effect is a change of sign, 
that we exp

is analogous, 
except 

interchanges 

as desired. 

and along 

if adjacent rows are intercha

nged. To 
for example, 

anges, we need only note that, 

The proof for column 
1 .  

row 1 instead 

of along 

column 

Expansion 

Theorem. 

We can now 

prove the Laplace 

Proof of Theorem 4 . 1  Let B be the matrix obtained 
using i - 1 interchanges 
b1j = a;j and B1j = A;j for j = 1, . . .  , n. 

of adjacent rows. By Lemma 4.14, <let B = ( - 1) i-l detA. But 
<let B = a;-1,1 a;-1,j  ai-1,n 

a;1  a;j  ain 
all  a1j  aln 

by moving 

row i of A to the 
top, 

a;+1,1 a;+1,j  ai+l,n 
anl  anj  ann 

Thus, 

detA = (- l)i-1 detB = (- l)i-12:(- 1)1+jb1jdetB1j 

n 
j�I 

= (- 1);-i 2: (- 1)1+jaij <let Aij = 2: (- l)i+jaij <let Aij 

n 
j�I 

n 
j�I 

which gives the 

The proof 
use column 

formula for cofactor 
for column 
expansion 

expansion 
is sim
ilar, 
of row expansion 
expansion instead 

along row i. 
invoking 
(see Exercise 

68). 

Lemma 4.13 so that we can 

the history 

of this section, 
of determina
ants were first introduced, independen

nts predates 
that of 
tly, by Seki in 1683 
in's Treatise 
on Alge­
of Cramer's Rule up to the 4 X 4 
case. 

nts appeared in Maclaur

In 17 50, Cramer 

applying 

it to curve fitting, 

and in 1772, 

inan1s 

of oe1erm

A Brief Hislorv 
As noted at the beginning 
matrices. Indeed, 
determin
and Leibniz 
bra, which included 
himself proved 
Laplace 
gave a 

case of his rule, 

a treatment 

in 1693. In 1748, determina

the general 
proof 

child 
A self-taught 
prodigy, 
Seki Kowa 
Takakazu 
(1642-1708) 
from a family 
was descended 
of 
samurai 
In addition 
warriors. 
to disco
vering 
determ
inants, 
diophantine 
about 
he wrote 
and 
equations, 
magic 
squares, 
Bernoulli 
numbers (before 
Bernoulli) 
and quite 
likely 
made 
discoveries 
in calculus. 

Cauchy made the first use of determi
fact, was 
responsi
several 
cluding 
terminan
ts, the characteristic 
Determinants 
them, albeit in the context 
a multivariable 
calculus course. 
by Sylvester around 

The term determinant 

of his expansion 

was not coined 

polynomial, 

important 

ble for developing much of the early 

nants in 

the modern sense 
theory 

results that we have mentioned: 

in 1812. 
of determinan

Cauchy, in 
ts, in­
the product rule for de­
zable 
of a diagonali
when Jacobi 
popularized 

matrix. 

did not become widely 
known until 
of several variabl
types of determina

es, such as are encountered 
"Jacobia

nts were called 

of functions 

in 
ns" 

(These 

and the notion 
1841, 

1850, a term that is still used 

today.) 

until 

1801, when it was used by Gauss. 

theorem. 

2 8 1  

Section 
4.2 Determi
nants 
(1646-1716) was born in Leipzig 
and studied 
von Leibniz 
law, 
Gottfried 
Wilhelm 
and mathematics. 
known for 
best 
theology
, philosophy, 
(with 
He is probably 
developing 
and integral 
calculus. 
However, 
his 
endently) 
Newton, 
the main 
indep
ideas 
of differential 
impressi
ed the notion 
ve. He develop
tics 
of mathema
to other 
contributions 
are also 
branches 
Theorem 
of a determ
Expansion 
inant, 
of Crame
knew 
versions 
r's Rule 
Laplace 
before 
and the 
others 
were 
given 
credit for them, 
and laid the 
foundation 
for matrix 
theory 
he 
work 
through 
did on quadratic 
forms. 
Leibniz 
to develop 
was the 
first 
the binary 
system 
of arithmetic. 
also 
He believed 
in the importance 
notation 
and, 
of good 
the familiar 
along 
with 
notation 
for 
derivatives 
introduced 
and integrals, 
of a 
for the coefficients 
notation 
subsc
a form of 
ript 
the notation 
system 
linear 
that 
is essen
tially 
we use today. 

of determina

By the late 19th century, the theory 

monumental  five-volume 
20th century. While their history 
is fascinating, 

Muir's 

that entire 
books were devoted to it, including 
s in 1867 and Thomas 
Determinant
appeared in the early 
minants are 
of theoretical more than practical 
for solving 
inefficient 
of linear 
a system 
method 
have replaced any 
use of determina
nants are 
however, 
used, 
polynomial (as in Sections 

nts in the computa
an initial 

to give students 

4.1 and 4.3). 

nts had developed to the stage 
Dodgson's An Elementary Treatise 
work, which 
today deter­
Cramer's Rule is a hopeles
sly 
and numerical 
ues. Determi­

tion of eigenval

equations, 

interest. 

on 

methods 

understanding 

of the characteristic 

..  I Exercises 

4 . 2  

• 

Compute the 
expansion 

determ
along the 

inants in Exercises 
first row and along the 

1-6 using cofactor  -4 1 3 

cos e sine tane 
first column. 9. 2 - 2  4  10. 0 cose -sine 
0 sine cose 

-1 0 

1 0  3 

1. 5 

0  2 

1 - 1  0 

3. - 1  0 

0 1 -1 
2. 2 3 -2 

- 1  3  0 
1  1 0 

4. 1 0 

0  - 1  

0 

2 3 

2 3 
5. 2 3  1  6. 4 5 6 
7 8 9 

3  2 

Compute the 
expansion 

determ

inants in Exercises 

7-15 using cofactor 

along any row or column that seems 

5 2  2 

1 - 1  

7. - 1  2 

8. 2 0 1 

3  0  0  3 - 2  

13. 

- 1  0 3 
2  5 2 6 
0 
0 0 
4 2 1 
.  0 0  0 a 
convenient
0  0 c 
b 
0  e 
d  f 
g h  j 

15. 

a b 0 
11. 0 a b 
a 0 b 

0 a 0 

12. b c  d 

0 e 0 

14. 

2 0 3 - 1  
1  0 2 2 
0 - 1   4 
2 0  - 3 

4 Eigenvalues 

2 8 2   Chapter 
and Eigenve
ctors 
16-18, compute 
ed 3 X 3 determi­
4. 9. 

the indicat

Find the determinants in Exercises 

35-40, assuming that 

with the 

definition 

of a 

by induction 
would 

In Exercises 
nants using 
16. The determina
17. The determina
18. The determina
19. Verify 

the method of Example 
nt in Exercise 
nt in Exercise 
nt in Exercise 

6 
8 
1 1  
ted in (2) agrees 
with 

( 1) for a 3 X 3 determinant. 

that the method 

Equation 

indica

20. Verify 

2 X 2 determina
21. Prove Theorem 4.2. [Hint: A proof 

that definition 
nt when n = 2. 

be appropria

( 4) agrees 

te here.] 

22-25, evaluate the given deter
4.3 

minant 
and Theorem 

In Exercises 
elementary row and/or column operations 
to reduce the matrix to row 
22. The determina
23. The determina
24. The determina
25. The determina

form. 
1 
nt in Exercise 
nt in Exercise 
9 
13 
nt in Exercise 
nt in Exercise 
14 

echelon 

using 

26-34, use properties of determinant

s to 

inant 

by inspectio

n. Explain 

In Exercises 
evaluate the 
your reasoning. 

given 

determ

1  1  1 

26.  3 0 - 2  

2  2  2 

3 1 0 
27. 0 - 2  5 
0 0 4 

0 0 1 
28. 0 5 2 
3 - 1 4 

2 3 
30. 0 4 1 

6  4 

32. 

1 0  0 0 
0 0 1 0 
0  1 0  0 
0 0  0 1 

2 3 -4 
29. -3 -2 
- 1  5 2 

4 1 3 
31. -2 0 - 2  
5 4 1 

33. 

0 2 0 0 
- 3  0  0 0 
0 0  0 4 
0 0  0 

1  0 1 0 
0  0 1 

34. 1  1  0  0 

0  0 1 

a  b  c 
d e  f  = 
g  h 

4 

2a  2b 2c 

35. d  e  f 

g  h 
d  e  f 
37. a  b  c 

2a b/3 - c  
36. 2 d  e/3 -f 
2g h/3 - i  
a - c  b  c 
38. d - f  e  f 

g - i  h 

g  h 
2c  b  a 

39. 2f e  d 

2i  h  g 
b + 2h 
a +  2g 
40. 3d + 2g 
3e + 2h 

c + 2i 
3f + 2i 

g 

h 

47-52, assume that A and Bare n X n matrices 

42. Prove Theorem 4.3(f). 
41. Prove Theorem 4.3(a). 
43. Prove Lemma 4.5. 44. Prove Theorem 4.7. 

4.6 to find all values of 

45.A =  k + 1 
- 8  

45 and 46, use Theorem 

In Exercises 
k for which A is invert

ible. [1 -k 
k � 1: 
46.A � [� k �] 

2 
k 

s. 

In Exercises 
with det A = 3 and det B = -2. Find the indicated 
determinant
47. det(AB)  48. det(A2)  49. det(B-1A) 

In Exercises 
53. Prove 
54. If B is invertible, 
55. If 

50. det(2A)  51. det(3Br)  52. det(AA r) 
53-56, A and B are n X n matrices. 
prove that det(B-1AB) = det(A). 
t (that is, A 2 = A), find all possible 
nilpotent if Am = 0 for 
:' A nilpotent matrix is 

56. A square matrix A is called 

some m > 1. (The word nilpotent 
Latin nil, meaning "nothing;' 
"to have power

and potere, 
t 

meaning 
thus one tha

that det(AB) = det(BA). 

A is idempoten

values 

of det(A). 

comes from the 

Section 

4.2 Determi

nants 

2 8 3  

matrix-when 

where P and S are square matrices. 
said to 

be in block (upper) triangular 

form. Prove that 

Such a matrix is 

to some power.) Find all possible 

-that is, the zero 
of 

values 

57-60, use Cramer's Rule to solve the given 

"nothing" 

becomes 
raised 
det(A) if A is nilpotent. 
In Exercises 
linear 
system. 
=  1 
57. x + y 
x - y= 2  

58. 2x - y =  5 
x + 3y = - 1  

60. x + y - z 

=  1 

59.2x + y + 3z =  1 
=  1 
y +  z 
z =  1 

x + y +z = 2  
x - y  = 3 

61-64, use Theorem 

4.12 to compute the in­

of the coefficient 

In Exercises 
verse 
61. Exercise 57 
63. Exercise 
59 
65. If A is an invertible 

58 
60 
n X n matrix, 

62. Exercise 
64. Exercise 
show that adj A is 

matrix for the 

given exercise. 

and that 

also invertible 

(adj A)-1 =-- A  = adj (A-1) 

1 
detA 
66. If A is an n X n matrix, 
prove that 
det(adj A) = (det A)n-I 

67. Verify 

that if r < s, then rows r ands of a matrix can 
2(s - r) - 1 inter­

by performing 

be interchanged 
changes of 
adjacent 
rows. 

Laplace Ex

68. Prove that the 

pansion 
the jth column. 
69. Let A be a square matrix that can be 

column expansion 

along 

Theorem holds 

for 

partitioned 

as 

det A  = (det P)(det S) 

by induction 

on the number of rows 

Try a proof 

[Hint: 
of P.] 

70. (a) Give an example 

partitioned 

as 

to show that if A can be 

A =  [�--f{j 

where P, Q, R, and S are all square, 
necessarily 

true that 

then it is not 

det A  = (det P)(det S) -(det Q)(det R) 

(b) Assume 

that A 

is partitioned 
as in part (a) and 

that P is invertible. Let 

B  = [ ��-���-d-?-] 

Compute det (BA) using 
result to show that 

Exercise 

69 and use 

the 

detA  = detPdet(S -RP-1Q) 

Schur (1875-1941), 

the Schur com­

born in Belarus 

[The matrix S -RP-1Q is called 
plement of P in A, after Issai 
who was 
most of his 
but spent 
life in Germany. He is known mainly for his fun­
damenta
the representation 
theory of 
groups, but he also worked 
theory, 
analysis, and other 

l work on 

in number 

areas.] 

that A is partitioned 
as in 

part (a), that 

(c) Assume 

P is inverti

ble, and that PR = RP. Prove that 
detA  = det(PS - RQ) 

A  = [-�-f-�-] 

Writing Project Which Came First: 

The Matrix or the Determinant? 

over time? Who were some of the key mathematicians 

The way in which matrices 
determinan
history 
cally. There is 
the history 

to the way these 
nts at the end of 

today-matrices 
before 
topics 
Section 
and determinan

resemblance 
of determina

Write a report on 

and determina

ts-bears little 

of matrices 

ts. How did the nota­

nts are 

taught 

develop

a brief 

4.2. 

ed histori­

contribu
tions? 

(Philadelphia: 

used for each evolve 
tions 
and what were their 
involved 
Cajori, A Histor
1. Florian 
2. Howard Eves, An Introduction 
Saunders 
J. Katz, A Histor
3. Victor 
MA: Addison 
4. Eberhard Knobloch, 
ion Encyclopedia 
2013). 
(London: 

y of Mathematical Notations 
College 
y of Mathematics: An Introduction 
Wesley 
Determina
of the Histor

Longman, 
2008). 
nts, in Ivor Grattan
y and Philoso

to the Histor
Publishing, 

-Guinness, ed., Compan­
phy of the Mathematical Sciences 

y of Mathematics (Sixth Edition) 
1990). 
(Third Edition) 

(New York: Dover, 1993). 

(Reading, 

Routledge, 

Vignette 

L ewis C arroll's C ondens ation Method 

paper. 

In it, 
he called 

determinants, which 

he described 
"condensation:' 

Dodgson-better known by his pseudonym 

today and rendered 
the condensation 

Lewis Carroll­
his only mathematical research 
a "new and 
brief 
Although 
not 
for computing 
for evaluating 

In 1866, Charles 
published 
method" 
well known 
nants, 
computer algebra systems 
method 
2 X 2 

Defi n ition If A is an n x n matrix with n 2: 3, the interior of A, denoted 

many students 
only the ability 
to compute 
terminolo

by numerical 
useful for hand calculation. 

It requires 
the following 

obsolete 
is very 

find condensation 
to be their 

determinants. 

are not available, 

We require 

of choice. 

methods 

determi­

method 

(n - 2) X (n - 2) matrix obtained by 

deleting 

the first row, last 

int(A), is the 
row, first column, 

and last column 
of A. 

gy. 

When calculators or 

Charles 

Dodgson ( 1 832-

Lutwidge 

Adventures 

Alice's 

matrix 

for the 5 X 5 

tion method 

We will illustrate 

the condensa

A0 equal to the 6 X 6 

2 3 - 1  2 0 
2 3  1 -4 
A =  2 - 1  2 1 1 
3 1 -1 2 - 2 
-4  0  2 

known 
1 898) is much 
better 
by his 
Lewis 
pen name, 
Carroll, under 
he wrote 
which 
in Wonderland and Through the 
several 
Looking Glass. He also wrote 
books and collections 
mathematics 
oflogic 
puzzles. 
on the article 
This 
vignette 
is based 
Carrol
"Lewis 
l's Condensation 
for Evaluating 
Determ
Method 
inants" 
by Adrian 
Rice 
and Eve Torrence 
in 
Math Horizons, November 
2006, 
For further 
pp. 12-15. 
details 
of 
the condensation 
see 
method, 
David 
M. Bressoud, 
Ao 
MAA Spectrum 
Series (Cambridge 
Press, 
University 
1999). 

Begin by setting 
A1 = A. It is useful to imagine 
A0. We are going to 
we reach 
matrix 

Confirmations: 
Alternating Sign Matrix Conjecture, 

at the top-this 

vely smaller 

add successi

and smaller 

Proofs and 
The Story of the 

a 1  X  1 

layers 
will contain <let 
A. (Figure 4.9) 

matrix all of whose entries 

A0 as the base of a pyramid with 

Figure 4 . 9  

2 8 4  

are 1. Then, we set 
A1 centered 
on top of 
to the 

pyramid until 

2 

2 

7  1 

- 1  5 -4 

3 1  1 - 1  - 1  2  2 

nts of 

4 X 4 matrix A; whose 

entries 

are the determina

" A1 into a 
we "condense
Next, 
trices 
all 
2 X 2 subma
of A 1: 
I 2 � 1 1  3 -�1 1-� �I I� -�I 
I -�1 1-� � 1 1  3 � 1 1  � -�1 [-j 1 1  - 7  -:1 
A;= 2 
I 2 -11 1-1  21 1  2  11 11 -�I  1 - 1  6 
1-! � 1 1  1 -�1 1-� �I I� -�1 
1 Now we divide 
to get 
of int(A0) 
ing entry 
each entry 
correspond
of A; by the 
matrix 
A2. Since 
means A2 = A;. 
A0 is all 
1 s, this 
of A2 and then 
2 X 2 submatrices 
We repeat 
the procedure, 
constructing 
A� from the 
1), and so on. We 
entry 
each 
entry 
dividing 
of A� by the corresponding 
of int(A 
obtain: 
-27] - 29 , 26 
1-� 1�l I 
1� -;1 1-; -:1 
A�= 1-: _;1 1_; �I I � -!I 
-27] -29 , 13 
=���� l = [�� �� 
201 120 
A' 4 131 
-94 ] 350 , 
= [-42 
130 181 118 -291 
[ -42/7  -94/1 ] = [- 6  
-94] 70 
A� = [1-:6 -��I]= [ 8604] ,  
A5 = [ 8604/1 8 ]  = [478] 
be checked by other 
As can 
<let A = 
478. In general, 
methods, 
A, 
for an n X n matrix 
the condensa
tion 
method 
will 
produce 
a 1 X 1 matri
<let A. 
x An containing 
Clearly, 
of any of the A; s contains 
if the interior 
the method 
breaks 
a zero, 
down 
to const
then 
to divide 
ruct 
since 
we would 
by zero 
A;+ 
1. However, 
careful 
be trying 
use of elementary 
opera
column 
tions 
can be used 
to elimina
row and 
te the zeros so 
that we 
can proceed. 2 8 5  

-96/( - 1) 350/5  96 

- 271 - 29 

12 4  4 13 

26/2  12 4 

60/3 
36/2 

-4/ - 1  

18 18 

-96 

30 

Exploration 

G eometric Applications of D eterminants 

will reveal 

This exploration 
geometry. In particu
volume 
ideas 
certain 
being developed as a subject in its own right. 

some of the amazing 
lar, we will see that determina
be used to produce the 

of determinants to 
closely related to area and 
planes, and 
arose when the theory of determina

applications 
nts are 
equations 

formulas and can 
other curv

es. Most of these 

of lines, 

nts was 

The Cross Product in Chapter 1 that the cross 

product 

tion: 

from Explora

The Cross Product 
Recall 

of u  � [ ::J ond v  � [ :: ] 
;, thrndorn X v defined 
by [U2V3 -U3V2] 
U X V =  U3V1 -U1V3 

U1V2 -U2V1 

this cross 

Ifwe write 
where e1, e2, and e3 are the standard 
basis 
formula 

product as (u2v3 - u3v2)e1 -(u1v3 - u3v1)e2 + (u1v2 - u2v1)e3, 
then we see that the 

vectors, 

form of this 

is 

if we expand along the first column. (This 
since e1, e2, and e3 are vectors, 
not scalars; 
bering the somewhat awkward 
cross 
determina
Now let's 

nts to verify some of the properties 
some of the exercises from Chapter 1. 

revisit 

is not a proper determinant, 
however, 

of course, 

it gives 

a useful way of remem­
us use properties of 

product formula. It also lets 

of the cross 

product.) 

2 8 6  

nt version 

of the cross 

product to compute u X v. 

1. Use the determina

(a)u � [}�HJ (b)u � [-l� m 
(o) u � n l v � [ =: i (d) u � [ l � m 
2. Ifu � [:l � [::landw � [:lhowthat 
U • ( V X W) = det U2 U3 
[U1 
(b) u x 0 = 0 

the given property of the 
(a) v X  u = -(u X v) 
(c) u X  u = 0 
(e) u X (v + w) = u X v +  u X  w (f) u · (u X v) = 0 and v ·  (u X v) = 0 
(g) u · (v X  w) = (u X v) 

3. Use properties 
cross 

nts (and Problem 2 above, 

· w (the triple scalar 

(d) u X kv = k(u X v) 

of determina

if necessa

product. 

product 

identit

y) 

ry) to 

prove 

Area a n d  Vol u m e  
We can now 
matrices
determined 
by these 
Product in Chapter 1.) 

. Recall 

give a geometric 

that if u and v 
vectors 

interpretation 
are vectors 

of the determina
in IFR3, then the area 
is given by A = II u X v 11. (See Explora

nts of 2 X 2 and 3 X 3 
A of the parallelogram 

tion: 

The Cross 

determined 

by u and v is given by 

4. Let u = [ �:] and v = [ ::]. Show that the area A of the parallelogram 
w,;,, u and v"' [ �:] and [�:Ji 

[H;nt 

5 .  Derive the area formula in 
from the 

[Hint: Subtract 
areas 

guide. 
Where does the absolute 

Problem 4 geometric
ally, 
large rectangle 
sign come from in this case? 

value 

until the 

using Figure 4.10 as a 
parallelogram 

remains

.] 

2 8 1  

y 

Fioure 4 . 1 0  

v x w  h{ 

v 

Figure 4 . 1 1  

6. Find the area of the parallelogram 

determined 
by u and v. 

(a)u = [�],v = [-�] (b)u = [!lv = [�] 

Generalizing 
solid 
resembling 
site faces parallel 
base times its 

from Problems 
a "slanted" 
and congruent (Figure 4.1 1). Its volume 

4-6, consider 

a parallele

height. 

piped, a three-

l 
grams with oppo­
is given by the 

brick, whose six faces are all parallelo

area of its 

dimensiona

7. Prove that the volume 

value of the 

the absolute 

given by 
ash = llull cos e, where 
v, and w as its columns. [Hint: 
show that V = lu · (v X  w) I and apply the result 
expressed 

determina

angle 

ed determined 
3 X 3 matrix 

by u, v, and w is 
[ u  v w] with u, 
that the height 
between u and v X w. Use this fact to 
ofFroblem 2.] 

nt of the 
From Figure 
4.1 1  you can see 

V of the parallelepip
e is the 
V of the tetrahedron 
V = ilu ·(v X w)I 

by u, v, 

determined 

and w 

8. Show that the volume 

(Figure 4.12) is given by 

h can be 

is V = t (area of the 

[Hint: From geometry, we know that the 
volume of such a 
base) 

solid 

(height).] 
Now let's 

tions 

geometric 

view these 
Let A be a 2 X 2 

view. 
u and v. We will consider 
Let TA(P) denote 

from a transforma

interpreta
tional 
point of 
P be the parallelogram 
determined 
by the vectors 
TA on the area of P. 
transformation 
by TA(u) = Au and TA(v) =Av. 
determined 

the effect of the matrix 

matrix and let 

the parallelogram 
9. Prove that the area of TA(P) is given 
10. Let A be a 3 X 3 matrix and let P be the parallelepip
ed determined 

the parallelepip

by I det Al (area of P). 

vectors 
TA(v) = Av, and TA(w) = Aw. Prove that the volume 
(volume 

u, v, and w. Let TA(P) denote 
of P). 

by the 
by TA(u) =Au, 

of TA(P) is given by l det A I 

ed determined 

The preceding problems 

ing matrix transforma

the correspond
which the transformation acts. (Although 
ures, 
general and can 

the result 

is perfectly 

determina

that the 
tion does to the area or 

nt of a matrix captures 

what 
volume 
of figures 
upon 
only certain 
types of fig­
. We will 

not do so here.) 

we have considered 
be made rigorous

illustrate 

Figure 4 . 1 2  

2 8 8  

lines and Planes 
Suppose we are given two distinct 
unique 

line passing 

0 
Since the two given points are on this 
coordinates 
Thus, 

points (x1, y1) and (x2, y2) in the pla
points, and its equation is of the form 
ax+ by+ c = 
their 

through 

these 

line, 

ne. There is a 

satisfy this equation

. 

ax1 + by1 + c = 
ax2 + by2 + c = 

0 
0 
can be viewed 

The three 
equations 
ables a, b, and c. Since 
matrix 

together 
there is a nontrivial solution (i.e., 

of linear 
the line 

equations 
exists), the coefficient 

as a system 

in the vari­

cannot be invertible, 
quently, its determina
gives 

by the 
nt must be zero, 
the equation of the line. 

Fundamenta

l Theorem of Invertible 
Matrices
this determinant 

by Theorem 4.6. Expanding 

. Conse­

The equation 

of the line 

through the points (x1, y1) and (x2, y2) is given by 

1 1. Use the method 

described 

above to find the equation 

of the line through the 

given 

points. 

(a) (2, 3) and (-1, 0) (b) (1, 2) and (4, 3) 

12. Prove that the 

three 

points (xi, Yi), (x2, y2), and (x3, y3) are collinear 

(lie on 

the same line) 

if and only if 
Xi Yi 1 
Xz Y2 1  =  0 
X3 Y3 
of the plane through 

equation 

13. Show that the 

(x1, Y1, z1), (x2, y2, z2), and (x3, y3, z3) is given by 
x  y  z 
Xi Y1 Zi 
Xz Y2 Zz 
X3 Y3 Z3 

= O  

What happ
row reduction 

ens if the three 

is used to evaluate 

the determinant.] 

points are collinear? 

[Hint: Explain 

what happens when 

2 8 9  

the three 

noncollinear 

points 

14. Prove that the four 

points (x1, y1, z1), (x2, y2, z2), (x3, y3, z3), and (x4, y 4, z4) are 

coplanar 

(lie in 

the same plane) if and 
only if 

X1 Y1 Z1 
Xz Y2 Zz = O  
X3 Y3 Z3 
X4 Y4 Z4 

Curve Filling 
When data arising 
often 
plotted 
in the plane, 
and y. Ideally, we would 
like to find a function 
points. Sometimes 
are also possible in certain situations. 

from experimentation 
of interest 
it is 

to find a relationshi

take the form of points (x, y) that can be 
x 
7.3), but exact 

whose graph passes 

all we want is an approximation (see Section 

results 

through all of the 

p between the variables 

we may be able to find a parabola 

the points A( - 1, 10), B(O, 5), and C(3, 2). The equation of such a 

15. From Figure 4.13 it appears as though 
y = a + bx + cx2. By substi
equations 
of three 

passing through 
parabola is of the form 
equation, set 
up a system 
linear 
solving the system, use Theorem 4.6 to argue 
solve the system 

16. Use the method 

to find the equation 

in Figure 4.13. 
of Problem 15 to find the polynomials 

of the parabola 

that pass through 

sets of points. 

the following 

(a) A(l, - 1), B(2, 4), C(3, 3) (b)A(-1, -3),B(l, - 1), C(3, 1) 

tuting 

the given 

in the variables 

points into this 
a, b, and c. Without 
Then 

solution. 

that it must have a unique 

of degree 

at most 2 

17. Generalizing from 
Problems 

15 and 16, suppose a1, a2, and a3 are distinct 
ers. For any real numbers b1, b2, and b3, we want to show that there is 

real numb
unique 
points (a1, b1), (a2, b2), and (a3, b3). Do this by demonst
matrix of the associated 

rating 
has the determina

y = a + bx + cx2 passing through 

with equation 

quadratic 

of the form 

that the coefficient 

linear 

system 

the 

nt 

a 

a1 ai 
a2 a� = (a2 - a1) (a3 - a1)(a3 - az) 
a3 a� 

which is necessarily 

nonzero. 

(Why?) 

18.  Let a1, a2, a3, and a4 be distinct 

real numbers. Show that 

a1 ai ai 
az a� a3 2 = (a2 - a1)(a3 - a1)(a4 - a1)(a3 - a2)(a4 - a2)(a4 - a3) *  0 
a3 a� a� 
a4 az 4 a3 4 
For any real 
with equation y = a + bx + cx2 + dx3 passing through the four points (a1, b1), 

numbers b1, b2, b3, and b4, use this result 

cubic 
(a2, b2), (a3, b3), and (a4, b4). (Do not actually 
solve 

for a, b, c, and d.) 

to prove that there is 

a unique 

y 

�+--+--+--+--+--+---+--+--+---. x 
- 2  

2 

4 6 

Figure 4 . 1 3  

2 9 0  

19. Let a1, a2, ••• , an be n real numbers. Prove that 
a1 ai  n-1 a1 
l 5'i<j-:5n 
a1 a�  n-1 az 
a3 a�  n-1 a3  II (aj - a;) 
an az  n-1 
n  an 

where TI1:si<J:sn (a1 -a;) means the product of all terms of the form 
i < j and both i and j are between 1 and n. [The determina
(or its transpose) is called a Vandermonde determinant
mathematician 

(a1 -a;), where 
nt of a matrix of this form 
, named after the French 

A. T. Vandermonde 

(1735-1796).] 

Deduce that for any n points in the plane whose x-coordinates are all distinct, 
is a unique 

polynomial of degree 

n -1 whose graph passes 

through 

the given 

there 
points. 

2 9 1  

2 9 2   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

' 

Eigenvalues 

a n d  Eigenvectors 

ot n x  n M atrices 

Now that we have defined 
discussion 
that A is an eigenvalue 
is true if and 

of eigenvalues 

only if det(A -AI) = 0. To 

the determina
and eigenvectors 

nt of an n X n matrix, 
in a general context. 

our 
from Section 4.1 

we can continue 

Recall 

of A if and only if A -AI is noninvertible. 

By Theorem 4.6, this 

summarize

: 

The eigenvalues 

of a square matrix A are precis
det(A -AI) = 0 

ely the 

solutions 

A of the equation 

When we expand det(A -AI), we get a 

polynomial in A, called the characteristic 

equation 

polynomial of A. The 
of A. For 

det(A -AI) = 0 is called 

example, if A = [: �], its characteristic 
la - A  b I 

= (a - i\)(d - A) - be = A

e  d - i\ 

polynomial is 

det(A -AI) = 

2 - (a + d)i\ + (ad - be) 

the characteristic 

equation 

l Theorem of Algebra 

polynomial will be of degree 
n. According to the Fun­
(see Appendix D), a polynomial of degree 

If A is n X n, its characteristic 
damenta
or complex coefficients 
teristic 
polynomial, 
most n distinct 
and eigenve

has at most n distinct 
ues. 
eigenval
procedure we will follow (for now) to find the 
aces) 

n with real 
we see that an n X n matrix with real or complex entries 

summarize the 

roots. Applying 

of a matrix. 

this fact to the charac­

( eigensp

ctors 

Let's 

has at 

eigenvalues 

polynomial det(A -U) of A. 

envalues of A by solving 

Let A be an n X n matrix. 
1 .  Compute the characteristic 
2. Find the eig
for i\. 
3. For each eigenvalue 
the eigenspace 
ing to A. 
correspond
for each eige

A, find the 
EA, the nonzero 
nspace. 

4. Find a basis 

det (A -AI) = 0 
the characteristic 

equation 

null space of the matrix A  -i\I. This is 
vectors 

of which are the 

eigenvectors 

of A 

Example 4 . 1 8  

Find the 

eigenvalues 

and the corresponding 
of 

eigenspaces 

of n X n Matrices 
ctors 
and Eigenve

4.3 Eigenvalues 

Section 

2 9 3  

sly. The characteristic 
polynomial 

is 

det(A  -AI) = 

outlined 
- ;\  

Solution We follow the procedure 
previou
1 0 
0 - ;\  1 
2 - 5  4 - ;\ 
= - ;\  - 5  4 - ;\ 
= - ;\(;\2 - 4;\ + 5) -(- 2) 
=  -A3 + 4A2 -SA + 2 

I-A 1 I 10  1 I 

2 4 - ;\ 

-

we need to solve 

To find the eigenvalues, 

for A. The characteristic 
equation 
Theorem is helpful 
- (;\ - 1)2(;\ -2) = 0, which clearly 
has solutions ;\ = 1 and ;\ = 2. Since 
root, 
a multiple 

as -(A - 1)2(;\ -2). (The Factor 
them ;\1 = ;\2 = 1 and A3 = 2. 

polynomial factors 
see Appendix D.) Thus, the characteristic 

root and A = 2 is a simple 

the characteristic 

is 
;\ = 1 is 

equation 

let us label 

here; 

det(A  - AI)  = 0 

- 5  3 

0  0 

ctors 

eigenve

- 1  
- 5  

To find the 

correspond

null space of 

Row reduction 

in the eigenspace 

ing to ;\1 = ;\2 = 1, we find the 

A  -II=  - 1  
- 5  

;n odvan<ethot we mu" get ot lmt one 

4 � J n !] 
n 1 
produces n 1 0 �] [: 0 - 1  �] 
[A - IIO J=  - 1  1 -----+  1 - 1  
mo rnw. Why?) Thu', x � [ :: l' 
9--!>- (We knew 
x3 = t, we see that x1 = t and x2 = t, from which 
reduction: [- 2  
� �i ----+ [� � =i �i 
[A- 2I I OJ = � 1 
Sox � [::l ;, ;n the e;gmpoce 

A3 = 2, we find the 

E, ;fond only ;f x, � )x, ond x, � !x,. Sethng the 

x2 - x3 = 0. Setting 
it follows 

corresponding to 

2  0  0  0  0  0 

if x1 - x3 = 0 and 

null space of 

E1 if and only 

eigenvectors 

variable 

To find the 

the free 

by row 

- 2  
- 5  

A -21 

that 

free variable 

x3 = t, we have 

2 9 4   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

4 

where we have cleared 
common denomina

tor 4. (Why is this permissi

ble?) 

denominators 

in the basis by multiplying 
through 

by the least 

Remark Notice 

that in Example 
if we count multiplicities, 

4.18, A is a 3 X 3 

A has exactly 

once). This is what the Fundamental 

multiplicity of an eigenvalue 
Thus, A = 1 has algebraic 

eigenvalues. 
However, 
twice and A = 2 
Let us define the algebraic 
equation. 
of the characteristic 
algebraic 
1. 
Next notice 

multiplicity 
dim E1 = dim E2 = 1 .  Let us 
words, 
A to be dim E;"' the dimension 
4.4, a comparison 
Section 
of these 

matrix 

but has only two distinct 
three 

eigenvalues 

Theorem of Algebra guarantees. 

(A = 1 
to be its multiplicity 
as a root 
multiplicity 
2 and A = 2 
has 
of just one vector. In other 

that each eigenspace 

has a basis consisting 
define the geometric 

multiplicity of an eigenvalue 

of its correspond

ing eigenspace. 
of multiplicity 

is important. 

As you will see in 

two notions 

Example 4 . 1 9  

Find the eigenvalues 

and the corresponding 

eigenspaces 

of 

A=[-� � -�] 1  0 - 1  
- 3  = -A 1- 1  - A 

-A 

- 1  - A 

1 

0 

1 

0 

Solulion The characteristic 

equation is 

- 1  - A 

0 = det(A -AI) = 3 
1 

Hence, 
braic 

the eigenvalues 

are A1 = A2 = 0 and A3 = -2. Thus, the eigenvalue 
alge­
multiplicity 

1. 
-2 has algebraic 

2 and the eigenvalue 

0 has 

multiplicity 
For A1 = A2 = 0, we compute 
0 
0 

[A -OJ IO J  [ A  I O J 

0 -� �i �  [� � 
-1 OJ 0  0 
x  � [::l in E0 "ti'fi" x, � x,. Thecefoce, 

0  0 

0  0 

- 1  0 

&om which it follow' 

th"t '"'  eigenwdm 

both x2 and x3 are free. Setting x2 = s and x3 = t, we have 

For A3 = -2, 

[A -(-2)IIO J [A+ 2IIO J  

of n X n Matrices 
ctors 
and Eigenve

4.3 Eigenvalues 

Section 

2 9 5  

so x3 = t is free and x1 = -x3 = - t and x2 = 3x3 = 3t. Consequently, 

It follows 

that ,\1 = ,\2 = 0 has geometric 
1. (Note that the algebraic 

multiplicity 

2 and ,\3 = -2 has geometric 
the geometric 

multiplicity 

multiplicity 

equals 

multiplicity 
for each eigenval

ue.) 

In some situations, 

the eigenvalues 

of a matrix are very 

triangular 
matrix, then so is A -AI, and Theorem 
the product of the diagonal 
a triangular 
matrix is 

entries. 

This implies tha

4.2 says that det(A -AI) is just 
t the characteristic 

equation of 

0 
(all -.\)(a22 - A)· · · 
from which it follows 
immedia
An = an,,. We summarize 

(ann - A) = 
eigenvalues 

as a theorem and illustrate 

tely that the 

this result 

easy to find. If A is a 

4 
are A1 = a11, A2 = a22, •.• , 

it with an exam

ple. 

Theorem 4 . 1 5  

The eigenvalues 

of a triangular 

matrix are the entries 

on its main diagonal. 

Example 4 . 2 0  

The eigenvalues 

of 

are ,\1 = 2, ,\2 = 1, ,\3 = 3, and ,\4 = -2, by Theorem 
polynomial 

is just (2 -,\)(1 -,\)(3 -,\)(-2 - A). 

4.15. Indeed, 

the characteristic 

Note that 

diagonal 

matrices 

are a special 

case of Theorem 4.15. 

In fact, a diagona

l 

matrix is 

both upper and lower triangular. 

Eigenvalues 
capture 

Once we know the eigenvalues 
doing any more work. The next theorem is one 

much important information about the behavior of a matrix. 
without 
in this regard. 

we can deduce 

of the most important 

a great many things 

of a matrix, 

Theorem 4 . 1 6  

only if 0 is not an eigenvalue 

of A. 

A square matrix A is invertible 

if and 

Proof Let A be a square matrix. 
<let A  * 0. But <let 
root of the characteristic 

A  *  0 is equivalent 
to <let (A 
of A (i.e., 

By Theorem 4.6, A is invertible 
if and only if 
-OJ)  * 0, which says that 0 is not a 

0 is not an eigenvalue 

equation 

of A). 

We can now extend 

the Fundamental 

Theorem 

of Invertible 

Matrices 

to include 

results 

we have proved in this cha

pter. 

2 9 6   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

Theorem 4 . 1 1  

The Fundamental 

Theorem of Invertible 

Matrices: Version 3 

The following 

statements 

are equival
ent: 

for every 

b in IJ�r. 

solution 
solution. 
has only the trivial 
form of A is In-
ary matrices. 

row echelon 

an n X n matrix. 

Let A be 
a. A is invertible. 
b. Ax = b has a unique 
c. Ax = 0 
d.  The reduced 
e.  A is a product of element
f. rank(A) = n 
g. nullit
y(A) = 0 
h. The column 
i. The column 
j. The column 
k. The row vectors 

1. The row vectors 

ly independent. 

of A are 
linear

of A form a basis 

of A span IJ�r. 

vectors 
vectors 
vectors 
of A are line
arly independent. 
of A span !Rn. 
basis for !Rn. 
of A form a 

for IJ�r. 

m. The row vectors 
n. detA  *  0 
o.  0 is not an eigenvalue of A. 

Proof The equival
Theorem 4.16. 

ence (a)� (n) is 

Theorem 

4.6, and we just proved (a)� (o) in 

There are nice formulas 

for the eigenvalues 

of the powers and inverses of a matrix. 

Theorem 4 . 1 8  

with eigenvalue 

A and corresponding eigenvector x. 

Let A be a square matrix 
a. For any positive integer 

eigenvector x. 
b. If A is invertible, 
c.  If A 

is inverti

n, An is an eigenvalue of An with corresponding 
eigenvector 

of A -i with corresponding 

x. 
n, An is an eigenvalue of An with corre­

then 1 /A is an eigenvalue 

ble, then for any integer 

sponding eigenvector x. 

Proof We are given that Ax = Ax. 
(a) We proceed 
Assume 

on n. For n = 1, the result is just what has been given. 

A kx = A kx. We must now prove the result 

by induction 
is true for n = k. That is, assume that, 

for n = k + 1 .  But 

for some positive integer k, 

the result 

by the induction 

hypothesis

Ak+1x = A(Akx) = A(Akx) 
. Using property ( d) ofTheorem 3.3, 

we have 

A(Akx) = Ak(Ax) = Ak(Ax) = Ak+Ix 

Thus, Ak+lx =  Ak+1x, as required. 
n 2: 1. 
(b) You are asked to prove this property in Exercise 
(c) You are asked to prove this property in Exercise 

13. 
14. 

By induction, the result 

The next example 

shows one application 
of this theorem. 

is true 

for all integers 

Example 4 . 2 1  

2 9 1  

Compute 2 

of n X n Matrices 
4.3 Eigenvalues 
Section 
and Eigenve
ctors 
1]10[5
1]. 
[o 1
=  [ � l then what we want to find is A 10 x. T�e 
=  [ � �] and x 
of A are A1 =  - 1  and A2 = 2, with correspond
and v2 = [�]. That is, 
�  (Check this.) Since 

v1 =  [ _ 1] 

Solution Let A 
eigenvalues 

ing eigenve

ctors 

x =  3v1 + 2v2. 

bination 

Therefore, 
using 

of v1 and v2. Indeed, 

as is easily checked, 

{v1, v2} forms a basis for IR2 (why?), we can write x 
as a linear 
com­
Theorem 4.18(a), we have 

A 10x =  A 10(3v1 + 2v2) =  3(A 10v1) + 2(A 10v2) 
=  3(A:0)v1 + 2(A�0)v2 
=  3(- 1)10[ l] + 2(210)[1] =  [ 3 + 211] = [2051] 

- 1   2  - 3  + 212  4093 
in fact, there 
than computing 

A 10 first; 

a lot easier 

are no matrix 

This is cert
multiplications 
at all! 

ainly 

When it can be 

the method 

of Example 

4.21 is quite 

general. 

We summarize 

it as the following 

which you are asked to prove in Exercise 42. 

used, 
theorem, 

Theorem 4 . 1 9  

Suppose the n X n matrix A has eigenvectors 
eigenvalues 
combination 

A1, A2, . . .  , Am. If x is a vector 
of these 

eigenvectors-say, 

v1, v2, •.. , vm with corresponding 

in !Rn that can be expressed 

as a linear 

then, 

for any integer 

k, 

Warning The catch here is the "if" in the second sentence. 

There is 

were a basis 

that such a linear 

no guarantee 
would be if there 
this possibility 
in the next section. 
have the following theorem, 
eigenvalues 
are linearly independent. 

combination 
of !Rn consisting 
of eigenve
As a step in 
ctors 
which states 

is possible. 
The best possible 
ctors 
that direction, 

we 
correspond

that eigenve

further 

however, 
ing to distinct 

of A; we will explore 

absolutely 
situation 

Let A be an n X n matrix 
responding 

eigenvectors 

v1, v2, ..• , vm. Then v1, v2, ..• , vm are linear

and let A 1, A2, . . .  , Am be distinct 

eigenvalues 

of A with cor­

ly independent. 

Theorem 4 . 2 0  

2 9 8   Chapter 

4 Eigenvalues 
and Eigenve
ctors 
If v1, v2, •.• , v m are linearly dependent, 
are scalars c1, c2, •.• , ck such that 

is indirect. 
Proof The proof 
and show that this assumption 
leads 

ible as a linear 
that can be so 
there 

combination 
expressed. 

of the previous 

We will assume 

to a contrad

In other 

that v1, v2, •.. , vm are linear

ly dependent 

iction. 
of these 

words, v1, v2, •.. , vk are linearly independent, 

must be express­
V; 
but 

Let vk+ 1 be the first of the vectors 

then one 
ones. 

vectors 

both sides 
Multiplying 
A;V; for each 
i, we have 

of Equation 

( 1) by A from the left and using 

the fact that Av; = 

Ak+iVk+i = Avk+1 = A(c1v1 + c2v2 + · · · 
+ ckvk) 
= C1AV1 + C2AV2 + . . .  + ckAvk 
= C1A1V1 + C2A2V2 + . . .  + ck,\kvk 

Now we multiply 

both sides 

ofEquation 

(1) by Ak+l to get 

(1) 

(2) 

(3) 

When we subtract 

Equation 

(2), we obtain 

(3) from Equation 

of v1, v2, •.• , vk implies 

0 = c1 (,\1 - Ak+1)v1 + c2(A2 - Ak+1)v2 + · · · 
independence 
c1 (,\1 - Ak+1) = c2(A2 - Ak+1) = · · · 

that 
= ck(,\k - Ak+1) = 0 

+ ck(,\k - Ak+1)vk 

The linear 

Since the eigenvalues 
i = 1, . . .  , k, are all nonzero. 

Hence, 

A; are all distinct, 

the terms in parentheses 

(A; - ,\k+1), 
= ck= 0. This implies 

c1 = c2 = · · · 
+ ckvk = Ov1 + Ov2 + · · · 
+ Ovk = 0 

that 

vk+1 = c1v1 + c2v2 + · · · 
the eigenve

which is  impossible, since 
contradiction, 
dependent is false. It follows 

ctor Vk+ 1 cannot 
be zero.  Thus, 
that v1, v2, . . .  , vm are linearly 
which means that our assumption 
linear

ly independent. 

thatv1, v2, . . .  , vm must be 

we have a 

4 . 3  

s of A, ( c) a basis 

polynomial 
for each eigenspace 

of A, (b) the eigenvalue
of A, and ( d) the algebraic and geometric multiplicit
each eigenvalue. 

..  I Exercises 
In Exercises 1-12, compute (a) the characteristic 
1. A =  [ 1 3] -2 6 
2. A = [ 2 l] - 1  0 
3.A � [� -� :J 
5.A � [-� -: :J 
4.A � [� : �] 
6.A � [� -� :J 

y of 

of n X n Matrices 
ctors 
and Eigenve

4.3 Eigenvalues 

Section 

2 9 9  

13. Prove Theorem 
14. Prove Theorem 

4.18(b). 
4.18(c). 

[Hint: 

Combine the proofs of 

s 

to eigenvalue

In Exercises 

parts (a) and (b) and see the fourth Remark following 
Theorem 3.9 (page 169).] 

15 and 16, A is a 2 X 2 matrix with eigenvec­

15. Find A 10x. 
16. Find A kx. What happens 
In Exercises 

tors v1 = [ _ �] and v2 = [ �] corresponding 
A1 = t and A2 = 2, respectively, and x = [ �]. 
v, � [H v, � [Jandv, � [ :J wm,pond;ngto 
17 and 18, A is a 3 X 3 matrix with eigenvect
Fm 17. FindA20x. 

s A1 = -L A2 = t, and A3 = 1, respectively, and 

eigenvalue

ask becomes 

(i.e., 

large 

ors 

k ---+ oo)? 

18. Find A kx. What happens 
19. (a) Show that, 

the same 
same eigenval
ues. 

ask becomes 
k ---+ oo)? 
square matrix 

large 
(i.e., 
A, AT and A have 

for any 
characteristic 

polynomial and hence the 

of a 2 X 2 matrix A for which AT 
ent matrix (that is, Am = 0 for some 

eigensp

aces. 

(b) Give an example 

and A have different 

20. Let A be a nilpot

of A. 

A =  O andA = 1 

m > 1). Show that A = 0 
21. Let A be an idempotent 
matrix 

is the only eigenvalue 
is, A 2 = A). Show 
that 
aretheonlypossibleeigenvalues
ctor of A with corresponding eigen 

22. If v is an eigenve

-
c is a scalar, show that 

value A and 
ctor 
v is an eigenve
of A -cI with corresponding eigenvalue 

(that 

ofA. 

and eigenspaces 

A -c. 
of 

23. (a) Find the eigenvalues 
A = [� �] 
24. Let A and B be n X n matrices 
(a) Give an example 

and eigenspaces 

(b) Using Theo

IL, respect

values 

ively. 

rem 4.18 and Exercise 

22, find the 
of A -i, A -2I, and A + 2I. 
A and 

with eigenvalues 

eigen­

an eigenvalue 
of A + B. 
(b) Give an example 

to show that A + IL need not 
to show that AIL need not 

be an 

be 

eigenvalue of AB. 

(c) Suppose A and IL correspond 
to the 

same eigen­

in this case, 

vector 
x. Show that, 
value of A 

-
of AB. 
+ B and AIL is an eigenvalue 
matrices, 
do they 
have the same eigenval
prove 
ues? Either 
le. 

necessarily 
that they do or give a counterexamp

25. If A and B are two row equivalent 

A + IL is an eigen 

Let p(x) be the polynomial 

The companion matrix of p(x) is then X n matrix 
-an-I -an-2  -a1 - ao 
1  0 

0  0 

C(p) =  0 
0 
0  0 

0  0  0 
0 

(4) 

26. Find the companion 

then find the characteristic 

matrix 

of p(x) = x2 -7x + 12 and 
polynomial of C( p). 
matrix of p(x) = x3 + 3x2 -

27. Find the companion 

4x + 12 and then find the 
ofC(p). 

28. (a) Show that the companion 

matrix 

C(p) of p(x) = 

characteristic 

polynomial 

x2 + ax + b has characteristic 
A2 + aA + 

polynomial 

b. 

ing to A. 

polynomial 

matrix 

29. (a) 

(b) Show that if A is an eigenvalue 

(b) Show that if A is an eigenvalue of the companion 

of C( p) correspond
Show that the companion 
C(p) of p (x) = 
x3 + ax2 + bx + c has characteristic 
- (A3 + aA2 + bA + c). 
of the companion 

matrix C(p) in part (a), then [ �] is an eigenvector 
C(p) in part (a), then [ �2 l is an eigenvector 
n 2 2, the companion 
32. (a) Use mathematical 
an_1xn-I + · · · 
polynomial 
along 
introduce 

with eigen­
- 2, 1, and 3. [Hint: Use Exercise 
29.] 
to prove that, 
for 
C(p) of p(x) = xn + 
tic 

matrix 
of C(p) corresponding to 
with eigen­

( - 1) np (A). [Hint: Expand 
the polynomial q(x) = (p(x) -a0)/x.] 

2 X 2 matrix 
3 X 3 matrix 

+ a1x + a0 has characteris

30. Construct a nontriangular 

31. Construct a nontriangular 

the last column. You may find it helpful 

2 and 5. [Hint: Use Exercise 28.] 

by cofactors 

induction 

values 

matrix 

values 

A. 1 

to 

3 0 0   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

(b) Show that if A is an eigenvalue of the companion 

(4), then an eigenvec

tor 

matrix C(p) in Equation 
by 
correspond
A is given 
ing to 
An-I 
,tn-2 

square matrix, 

we can define a square matrix p(A) by 

If p(x) = xn + an_1xn-! + · · · + a1x + a0 and A is a 
p(A) = An+ an_1An-i + · · · + a1A + a0I 
then cA (A) = 0 (in words, 
page 2). Cayley 

An important 
theorem in 
if cA (,\) is the characteristic 
teristic 
Cayley 
Theorem, named after Arthur 
pictured below, and Sir William 
Rowan Hamilton (see 

advanced linear 
polynomial of the matrix A, 
every matrix satisfies its charac­

1858. Hamilton 

). This is the celebrated 

Cayley-Hamilton 
( 1821-1895), 

proved this th

eorem in 

equation

algebra 

says that 

discovered it, independently, in his work on quaternio
generalization of the complex numbers. 

ns, a 

That is, find the 

characteristic 

the Cayley-Hamilton 

Theorem for A = 2 
3
polynomial c A ( ,\) of A 
Theorem for 

Hamilton 

[1 -

1]. 

33. Verify 

c j 
and showthatcA(A) = 0. 
A� [� � :J 

the Cayley-

34. Verify 

The Cayley-Ha
milton Theorem 
powers and inverses of matrices. 
matrix with characteristic 
then A 2 + aA + 

bl = 0, so 

2 X 2 
can be used to 
calculate 
if A is a 
For example, 
polynomial cA(,\) = ,\2 + a,\ +  b, 

A2 = - aA - bl 

42. Prove Theorem 4.19. 

and  A3 = AA2 = A(-aA -bl) 

= -aA2 - bA 
= -a(-aA -bl) -bA 
= (a2 - b )A + ab I 
It is easy to see 
that by continu
express 
A(A +al) = - bl, so 

tion of I and A. From A 2 + aA + bl = 0, we also obt

of A as a linear 

ing in this fashion 

any positive power 

we can 

ain 

combina-

1  a 
A-1 = --A --I 
b  b 

Cayley­

provided b * 0. 
35. For the matrix A in Exerc
ise 33, use the 
Theorem to compute A 2, A 3, and A 4 by 
combination 
each as a linear 
of I and A. 
34, use the Cayley­
pute A 3 and A 4 by express­

Hamilton 
expressing 
36. For the matrix 
Hamilton 
ing each as 
matrix 

of I, A, and A 2. 
combination 
33, use the 
Theorem to compute A -i and A -2 by 
combination 
of I and A. 
38. For the matrix A in Exercise 34, use the Cayley­

Hamilton 
expressing each as 

Theorem to com
a linear 

37. For the 

A in Exercise 

A in Exercise 

a linear 

Cayley­

then the character­

set of eigenvalues 

Hamilton 
expressing each as 

Theorem to compute 

a linear 

2. 
combination 
if the square matrix A can be 
as 

of I, A, and A 
partitioned 

A -i and A -2 by 

polynomial of A is cA(,\) = cp(A)c5(,\). [Hint: 

Use 

4.2.] 

in Section 

39. Show that 

40. Let A1, A2, . . .  , An be a complete 

where P and S are square matrices, 
istic 
Exercise 69 

A =  [-ij-i-;-] 
of the n X n matrix 
• • ·An and 
tr(A) = A1 + A2 + · · · + An 
det(A -AI) = ( - l)n(,\ - ,\1)(,\ - A2) • • • (,\ - An) 
41. Let A and B be n X n matrices. 

Find the constant term and the 
the left and right sides 

[Hint: The characteristic 
polynomial of A 

etitions 
det(A) = A1 A2 

included) 

of this equation.] 

coefficient of An-! on 

(rep­
A. Prove that 

as 

factors 

Prove that 

the sum of all 
eigenval­
of A + B is the sum of all the 
duct of all 
of AB is the product of all the eigenval­
with 

the eigenvalues 
ues of A and B individua
the eigenvalues 
ues of A and B individuall
Exercise 24.) 

lly. Prove that 
the pro

this exercise 

y. (Compare 

Section 

4.4 Similarity 

and Diagonalization 

3 0 1  

Writing Proiect The History 

of Eigenvalues 

in which they origina

lly arose. Who were some of the 

with these 

problems? How did the terminology for 

Like much oflinear 
correspond 
tems of differential 
equations 

algebra, the way the topic 

of eigenvalues 
is taught 

today does not 

to its historical 

development. Eigenvalues 

arose out of problems 

in sys­

before the concept 

of a matrix was even formulated. 

Write a report on 

the historical 

development of eigenval

ues. Describe 

the types 

of mathematical problems 
key mathematicians 
involved 
eigenvalues 
change over 
1 .  Thomas 
Hawkins, 

time? 

Cauchy and the Spectral Theory 

of Matrices, 

2. Victor 

ematica 2 (1975), pp. 1-29. 
J. Katz, A Histor
ing, MA: Addison 
Kline, 

3. Morris 

Oxford University 
Press, 
1972). 

y of Mathematics: An Introduction 

(Third Edition) 

Wesley Longman, 
2008). 

Mathematical 

Thought from Ancient to Modern Times (Oxford: 

Historia 
Math­
(Read­

Similarilv and D i a u o nalizalion 

ently 

triangular 

eigenvalues 

a given square matrix to a triangular 
ues. Of course, 
triangular 

As you saw in the last section, 
that their 
are transpar
relate 
had exactly 
verting 
tunately, this process 
we consider 
respect 

a square matrix into 

the same eigenval

a different 

does not preser

to eigenval
ues. 

and diagonal 
displa

matrices 
are nice in the sense 
would be pleasant if we could 
one in such a way that they 

yed. It 

or diagonal 
we alread

y know one procedure for con 

form-namely, Gaussian 

tion. 

Unfor­

-
elimina
matrix. 

sort of transformation of a matrix that does behave well with 

ve the eigenvalues 

of the 

In this section, 

P such that P-1 AP = B. If A is similar to 

Remarks 

Similar 

Malrices 

n X n matrix 

is an invertible 

there 
we write A � B. 

Defi n ition Let A and B be n x n matrices. We say that A i s  similar to B if 
•  If A � B, 
•  Similarity 
•  The matrix 

equal 
in the definition. 
B � A. (In fact, this is true, 
that A � B implies 
but it does not follow immedia
P depends 
A and B. To see this, 
trices 
for any invertible 
matrix 

Just as a :::; b does not necessarily 

we can write, 
is a relation on square matrices 
integers. Note that there 

is a direction 
as we will prove 

tely from the definition
on A and B. It is not unique 

-1 or AP= PB. 
that "less 
t 
(or order) implici

imply b :::; a, we should not assume 

pair of similar 
I, in which case I �  I, since p-i IP = I 

simply 
P. 

.) 
for a given 

take A = B = 

to" is a relation 

that A = PBP

in the same sense 

equivale

on the 

ntly, 

than or 

B, 

ma­

in the next theorem, 

Example 4 . 2 2  

4 Eigenvalues 

3 0 2   Chapter 
and Eigenve
ctors 
0J. Then A � B ,  since 
1 2J and B  = [ 1 
LetA  = [0
1 - - 1   - 1  - 1 -lJ[ 1 OJ 
[� -�J [� -lJ [ 3 lJ [1 
[ 1 -lJ 
4 

1  1 . (Note that it is not necessary to compute 

Thus, 
See the first 

Remark before Example 

AP= PB with P = 

- 1   -2 

1 -2 - 1  

4.22.) 

P-1. 

- 1  

Theorem 4 . 2 1  

C be n X n matrices

Let A, B, and 
. 
a.  A � A  
b. If A �  B, then B � A. 
c.  If A � B and B � 

C, then A � 

C. 

Theorem 4 . 2 2  

Proof (a) This property follows 
then P-1AP =  B for some invertible 
(b) If A  �  B, 
Remark on the previous 
have Q-1BQ = (P-1)-1BP-1 = PBP-1 = A. Therefore, 
(c) You are asked to prove property (c) in Exercise 30. 

that r1AI = A. 
from the fact 
in the first 
matrix P. As noted 
Q = P-1, we 
to PBP-1 = A. Setting 
B � A. 
by definition, 

equivalent 

page, this is 

Remark Any relation sa

an equivalence 
objects that are related 
We are about to see that this is true 

relation. Equivalence 
via an equival

properties 

relations arise 
ence relation usually 
of similar 

of Theorem 4.21 is called 
share 
matrices. 

in mathematics, 
and 
important properties. 

frequently 

tisfying the three 

A � B. Then 

if B is invertible. 

Let A and B be n X n matrices with 
a. det A = det B 
b.  A is invertible 
if and only 
c. A and B have the same rank. 
d. A and B have the same characteristic 
e. A and B have the same eigenvalues. 
f. Am� Bm for all integers 
g. If A is inverti

m. 
then P-1AP = B for some invertible 

the remaining 
matrix P. 
nts of both sides, 

ble, then Am � Bm for all integers 

m 2::: 0. 

Proof We prove (a) and (d) and leave 

determina

(a) Taking 

polynomial. 

we have 

det B = det(P-1AP) = (det P-1)(det A)(det P) 

= (-1-)(det A)(det P) = det A 

det P 

(d) The characteristic 
det (B -AI) = det (p-i AP -AI) 

polynomial of B is 

= det(P-1AP -AP-1IP) 

properties 
as exercis

es. If A �  B, 

Section 

4.4 Similarity 

and Diagonalization 

3 0 3  

det(P-1AP - P-1(AI)P) 
det(P-1(A  -AI)P) = det(A  -AI) 

as in (a). Thus, det(B -i\I) = det(A -i\I); that is, 

the 

with the last step 
following 
characteristic 
Remark Two matrices 

polynomials of B and A are the same. 

may have properties 
For example, 

(a) through ( e) (and more) in common 

A  = [ � �] and B = [ � �] both have de­

have characteristic 

not be similar. 

and yet still 
termina
and eigenvalues 
I * B for any 

nt 1 and rank 2, are 

invertible, and 
to B, 
i\1 =  i\2 = 1 .  But A is not similar 

invertible 

matrix P. 

polynomial ( 1  -i\)2 
P-1AP =  P-1IP = 

since 

Theorem 4.22 is most useful in showing 

properties 

that two matrices 
(a) through (e) fails. 

are not similar

, since 

A and B cannot be similar 

if any of 

Example 4 . 2 3  

(a) A= [� �]and B= [� �]arenot similar,sincedet A=- 3butdet B= 3. 
(b) A  = [ 1 3] and B = [ 1  1] are not similar, since 

3 - 1  
that of B is i\2 -4. (Check this.) Note that A and B do 
mial of A is i\2 -3i\ - 4 while 
have the same determina

nt and rank, however. 

2  2 

the characteristic 

polyno-

Diagonali
zation 
The best possible 
situation 
As you are about to see, whether 
eigenvalues 
of the 

Defi n ition An n x n matrix 

and eigenve

ctors 

D such that A is similar 
that P-1AP = D. 

is when a square matrix is similar 
lizable 

to a diagonal 
matrix. 
ely related to the 

is diagona

is clos

a matrix 
matrix. 

to D-that is, if there 

A is diagonalizable if there is a diagonal 
matrix 
P such 

is an invertible 

n X n matrix 

Example 4 . 2 4  

A =  [ l 3] is diagona
P-1AP = D, as 

2  2 

lizable 

since, 

1 - 2  
can be easily checked. (Actually, 
it is faster to check the equivalent 

if P = [ l 3] and D = [4  0], then 
P-1.)  -+ 

0 - 1  

statement 

AP= PD, since 

it does not require finding 

Example 

4.24 begs the question 

of where matrices 

P and D 

came from. Observe 

of its characteristic 

entries 

diagonal 

that the 
roots 
of matrix P is less obvious, 
from the eigenve

ctors 

4 and - 1  of D are the eigenvalues 
of A, since 
polynomial, 
but, as we are about to demonstrate, 

which we found in Example 

its entries 

they are the 

4.23(b ). The origin 

of A. Theorem 

4.23 makes this connection 

precis

are obtained 
e. 

3 0 4   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

Theorem 4 . 2 3  

More precis

dent eigenvectors
ely, there 

Then A is diagonaliza
. 
exist 

Let A be an n X n matrix. 
only if A has n linearly 
indepen
D such 
diagonal 
that P-1AP = D if and only if the columns of P are n linearly independent eigen­
vectors 
to the eigenvectors 

of D are the eigenvalues 

an invertible 

matrix P and a 

of A and the diagona

ble if and 

in P in the same order. 

l entries 

matrix 

of A correspond

ing 

Pz 

Pz 

-1AP =  Dor, 

to the diagonal 

matrix D via P
l 

is similar 
y, AP = PD. Let the columns 

A[p1  ... Pn l  [pl 

Proof Suppose first that A 
equivalentl
entries 

of D be A1, A2, •.. , An. Then  [A, 0  ]J 

of P be p1, p2, ..• , Pn and let the diagona
p.] ! Az 
or  [Ap1 Apz ... Apn]  = [A1P1 Azpz  AnPn] 
Ap1 =  A1P1, Apz =  Azpz, · · ·, Apn =  AnPn 
A1, A2, .•• , An, respecti
Ap1 =  A1P1, Apz =  Azpz, · · ·, Apn =  AnPn 

of A whose correspond
Since 
of D in the same order. 
by the Fundamental 
independent, 
Theorem 
ctors 
ly indepen

(2) 
where the right-hand side is just the column-row representation of the product PD. 
Equating 

p1, p2, .•. , Pn with cor­

which proves 
eigenvalues 
umns are linearly 

ing 
its col­
oflnvertible 
Matrices. 

that the column 
are the diagonal 

Conversely, if A has n linear

of P are eigenvectors 

vectors 
entries 

eigenvalues 

dent eigenve

responding 

columns, we have 

vely, 

then 

(1) 

0 

P is invertible, 

Equation 

which is equivalent 

(2) above, 
AP= PD. Since the 

This implies 
if we take P to be the n X n matrix with columns 
becomes 
P are linear
Theorem oflnvertible 
that P is invertible, 
is diagona

to Equation (1). 
Consequently, 
p1, p2, . . .  , Pn' then Equation 
(1) 
ly independent, 
the Fundamen
tal 
so P-1AP = D; that is, A 

columns of 

lizable. 

Matrices 

implies 

Example 4 . 2 5  

If possible, find 

a matrix P that diagonalizes 

Solulion We studied 
eigenvalues 

A1 = A2 = 1 and A3 = 2. The eigenspaces 

this matrix 

4.18, where we 

discovered that it has 
bases: 

have the following 

in Example 

A � [� -� :J 
Fod,  �A,� 1,E, hashasis [:J 
Fod, � 2, E, has basis [ n 

Section 

4.4 Similarity 

and Diagonalization 

3 0 5  

all other 
be three 

eigenve
Since 
cannot 
linearly independent 
not diagonalizable. 

ctors 

are just multiples 

of one of 

these 

two basis vectors, 

eigenvectors

. By Theorem 4.23, 

therefore, 

there 
A is 

4 

Example 4 . 2 6  

If possible, 

find a matrix P that 

Solution This is the matrix of Example 
4.19. There, 
A are A1 = A2 = 0 and ,\3 = - 2, with the following 

eigenvalues 

of 

diagonalizes 

A � [-� � =il 
p, � m ond p, � m 
Fod, � A, � 0, E0 h" b"i' 
Fod, � - 2,E_,h" b"i'P; � [-:J 

we found that the 
eigensp
aces: 
bases for the 

It is straightforward 
if we take 

to check that these 

three 

vectors 

are linearly independent. Thus, 

then P is invertible. 

Furthermore, 

as can be easily checked. 
equivalent 

equation 
AP= PD.) 

(If you are checking by hand, it is much easier 

to check the 

•  When there 

Remarks 

in any order. However, the 
eigenvectors 
order 

corresponding 

as their 

are enough 

eigenvectors, 
eigenvalues 

into the columns 

of P 
of D in the same 
if we had chosen 

will come up on the diagonal 

in P. For example, 

they can be placed 

then we would have found 

3 0 6   Chapter 

4 Eigenvalues 

and Eigenve
ctors 
•  In Example 

4.26, 

Theorem 4 . 2 4  

E0• We also knew that the 

ctors p1, p2, and p3 
{p1, p2} was 

We knew that 

you were asked to check that the eigenve

But we could not 
dent. The next 
ved when the bases 

of 

for the 

ly indepen

it was a basis 

independence 

dent. Was it necessary to check this? 
since 

were linear
linearly independent, 
sets { p1, p3} and { p2, p3} were linearly independent, 
conclude from this information that {p1, p2, p3} was linear
theorem, 
is preser
different 

guarantees that linear 
are combined. 

however, 
eigenspaces 

eigenspace 
by Theorem 4.20. 
ly indepen

a basis for the eigenspace 
of basis vectors 

of A. If B; is 
arly independent. 

Let A be an n X n matrix and let A1, A2, ••• , Ak be distinct 
E;v then B =  B1 U B2U · · · UBk (i.e., 
Proof Let Bi =  {v;1, v;2, •.• , v;n) for i =  1, . . .  , k. We have to show that 
(C1 1V1 1  + . .  '+ C1n,V1n) + (C21V21 + . ' . + C2n2V2n) + . ' . + (ck!Vkl + . . .  + Ckn,Vkn) =  0 

is linearly indepen
is the zero vector-say, 

dent. Suppose some nontrivial 

for all of the eigensp

tion of these 

combina

linear 

is line

aces) 

eigenvalues 

vectors 

the total collection 

Denoting 

the sums in parentheses 

by x1, x2, . . .  xk, we can write 

Equation 
(3) as 

x1 + x2 + · · · + xk =  0 

(3) 

(4) 

either 

�  Now each x, is in EA, (why?) and so 
the eigenvalues 
ly independent, 

A; are distinct, 
by Theorem 4.20. 

is an eigenve
if any of the factors 
Yet Equation 

is 0. But, since 
they are linear
dence 
trivial; 

relationship; 
that is, all of its coefficients 

B is linear
There is one case in which diagonalizability 

this is a contradict

ctor corresponding to A; or 

X; is an eigenvector, 
( 4) is a linear 
depen­

ion. We conclude that Equation 
(3) must be 
are zero. Hence, 
ly indepen
dent. 

is automatic: an n X n matrix with 

n distinct 

eigenval
ues. 

Theorem 4 . 2 5  If A is an n X n matrix 

with n distinct 

eigenval

ues, then A is diagonalizable. 

Proof Let v1, v2, . . .  , vn be eigenve
�  of A. (Why could there not be 
v1, v2, . . .  , vn are linearly independent, 

ctors 

so, by Theorem 

more than n such eigenvecto

4.23, 

rs?) By Theorem 4.20, 
A is diagonalizable. 

corresponding to then distinct 

eigenvalues 

Example 4 . 2 1  The matrix 

A1 =  2, A2 =  5, and A3 =  - 1, by Theorem 

matrix, 

for a 3 X 3 
P such that p-1AP is diagonal, 

eigenvalues 

has eigenvalues 
distinct 
ally require 
eigensp

a matrix 
as in Example 

4.19 and Example 

aces, 

are three 

Since 

4.15. 

we must still 

these 
(If we actu­
A is diagonalizable, 
by Theorem 4.25. 
for the 
bases 
compute 

4.26 above.)  4 

Section 

4.4 Similarity 

and Diagonalization 

3 0 1  

Lem m a  4 . 2 6  

Theorem 4 . 21 

4.26. 

can be diago­

Example 

We first prove a 

than or equal 

or not a matrix 

of this section is an 

introduced 

conditions 

The final theorem 

in terms of the two notions 

able matrices 
Example 
nalized, 
lemma that holds whether 

4.18. It gives 
even when it 

precise 
has fewer than n eigenvalues, as in 

important 
result 
of multiplicity 
that were 
under which an n X n matrix 
is diagonalizable. 

that characterizes 
diagonaliz­
following 

to its algebraic 

multiplicity 

of each eigenvalue 

the geometric 
multiplic

If A is an n X n matrix, then 
Proof Suppose A1 is an eigenvalue 
let EA, have basis B1 = {v1, v2, ..• , vp}. Let Q be any invertible 
v1, v2, .•. , vp as its 
Q = (Y1 · · · Yp Yp+l . . . YnJ 
matrix, Q = [U i VJ 

of A with geometric 

dim EA,= p. Specific
n X n matrix having 

multiplicity 

first p columns-say, 

or, as a partitioned 

p; that is, 

ally, 

is less 

ity. 

Let 

where C is p X n. 

Since 

also have 

the columns 

of U are eigenvectors 

ing to A1, AU = A1 U. We 

correspond

By Exercise 69 

from which we obtain CU= IP, CV= 0, DU= 0, and DV = In-p· Therefore, 
Q-1AQ = [�]A [U :VJ = [-�!.l:!!._l-�!.l:Y] = [�_1-�_f!_�-�!.l:YJ = [�-1-�!'.L��-�-] 
lD ' DAU!DAV LA1DU:DAV 0 !DAV 
det(Q-1AQ -AI) = (A1 -A)P det(DA V -AI) 
of Q-1AQ, 
( Q-1AQ -AI) is the characteristic 
of A, by Theorem 4.22(d
of A1 is at least p, its geometric 

But det 
as the characteristic 
that the algebraic 

(5) 
which is the same 

). Thus, Equation 
multiplic

multiplicity 

(5) implies 

4.2, it follows 

polynomial 

polynomial 

in Section 

that 

ity. 

The Diagonalization 
Theorem 

statements 

Let A be an n X n matrix 
a. A is diagonalizable. 

whose distinct 
are equivalent: 

eigenvalues are A 1, A2, ... , Ak. The following 
of A (as in Theorem 4.24) contains 

of the eigenspaces 

b. The union B of the bases 
c. The algebraic 

n vectors. 

its geometric 

multiplici

multiplici

eigenvalue 

ty of each 

equals 

ty. 

3 0 8   Chapter 

n. 

4 Eigenvalues 
and Eigenve
ctors 
n linearly 
dent 
Proof (a)=? (b) If A is diagona
indepen
then 
it has 
lizable, 
c­
eigenve
�  then 
correspond 
tors, by 
Theor
eigenvectors 
A;, 
If n; of these 
to the eigenvalue 
em 4.23. 
B; contains 
that these 
. (We 
alread
y know 
n; vectors 
are linearly 
at least 
n; vectors
from being a basis 
for EA; is that 
thing 
indep
the only 
that might prevent them 
endent; 
. But, 
at least 
they 
might not span 
it.) Thus, B contains 
by Theor
n vectors
em 4.24, 
n vectors. 
set in !Rn; hence, 
it contains 
exactly 
B is a linearly 
indep
endent 
of A; bed; = 
(b) =? (c) Let the geometric 
dim 
multiplicity 
EA; and let the algebraic 
d; :s m; for i = 1, ... 
of A; be m;. By Lemma 
multiplicity 
that 
, k. Now assume 
4.26, 
property (b) holds. 
Then 
we also 
have 
multi
+  mk = n, since 
But 
of the 
the sum of the algebraic 
plicities 
polynomia
of the characteristic 
of A is just 
eigenvalues 
the degree 
l of A-namely, 
m1  +  m2 + · · · 
that d1 + d2 + · · · 
+ dk = m1 + m2 + · · · 
It follows 
that 
+ mk, which implies 
(6) 
that m; -d; 2: 0 
for i = 1, ... 
we know 
Using 
again, 
, k, from which 
we 
Lemma 
4.26 
m; = d; for 
can deduce 
i = 1, ... 
that each 
summand 
in Equation 
is zero; 
that 
is, 
( 6) 
d; are 
m; and the geometric 
( c) =? (a) If the algebraic 
multiplicity 
multiplicity 
B has d1 + d2 + · · · + dk = m1 + m2 + · · · 
for each 
equal 
eigenvalue 
A; of A, then 
Thus, 
by Theor
independent, 
+ mk = n vectors, 
em 4.24. 
are 
which 
are linearly 
these 
n linearly 
indepe
ndent 
eigenv
ectors 
of A, and A 
is diagonalizable, 
by Theor
em 4.23. 
[� � �i from 
Example 
eigenval
matrix A = 
ues, 
4.18 has two distinct 
(a) The 
2 -5 4 
A1 = A2 = 1 and A3 = 2. 
the eigenvalue 
Since 
A1 = A2 = 1 has 
multiplicity 
algebraic 
multiplicity 
1, A is not 
2 but geometric 
diagonalizable, 
tion 
by the Diagonaliza
Theo­
A=[-� � -�] fromExample
rem. (See 
also 
4.25.) 
Example 
4.19alsohas
eigen-
twodistinct
(b) The matrix 
l 0 -1 
mul­
and geometric 
-2. The eigenvalue 
0 has 
values, 
A1 = A2 = 0 and A3 = 
algebraic 
tiplici
ty 2, and the eigenvalue 
-2 has algeb
raic 
and geometric 
multiplicity 
1. Thus, 
this 
matri
Theorem. 
(This 
with 
agrees 
our 
by the Diagonalization 
x is diagonalizable, 
findings 
4.26.) 
in Example 
this 
to the computa­
of diagonalization 
with 
We conclude 
section 
an application 
tion 
of the powers 
of a matrix. 
A 10 if A = [� �]. 
Compute 
Solution In Example 
that 
4.21, we found 
this 
A1 = -1 
matrix has eigenvalues 
v1 = [ _ �] and 
and A
2 = 2, with 
corresponding 
eigenve
ctors 

v2 = [ �]. It follows 

, k. 

Example 4 . 2 8  

Example 4 . 2 9  

3 0 9  

Section 
4.4 Similarity 
and Diagonalization 
that A is diagonaliza
P-1AP = D, where lJ [-1 OJ 
section) 
ems in this 
ble and 
(from any one of a number of theor
2 and D = 
0 2 
em 4.22(f), An = PDnP-1 for all 
A = PDP-1 and, 
for A, we have 
Solving 
by Theor
n 2: 1. Since [(-01r OJ 2n 
we have An= PDnp-1 = [_� �J [(-01r �nJ[ _� �rl 
[_� �J [ ( -Ol)n 0 J [I 2n i -iJ 
2(-1)•: + 2•+• Hr'+ 2· l r-1r:+2·+• 
2(-1)" + 2" 
[ 
for A 10, this 
Since 
we were only 
is more than we 
asked 
needed. 
n = 10 to find 
set 
2(-1)�+210 
(-1)11 + 210
AlO = 2(-1)11 + 211 3 3 342 341 
(-!)': + 2" � [682 683] 
[
] 

But now we can simply 

4 . 4  

In Exercises 

I Exercises 
1-4, show that A and Bare not similar matrices. 
5-7, a diagonalization of the matrix A is given 
in the form P-1AP = D. List the eigenvalues of A and bases 
1. A = [! �l B = [� �J 
2A= B =  [ 2 lJ [ 3 -lJ 
5. [ 2 -1 -�J[� -�J[� �J = [� �J 
. -4 6 , - 5  7 [: 1 !J.B�[-: 0 
l I 1 �m 1 J 
6 
-2 1  0 
-1 0 1 0 -1 
3.A = 2 4 
3 -3 1 
6. [! 
�] 1 �] 1 0 
0 3 
� [: 0 
4.A � [: 2 -:JB� [: -1 
-�l 0 0 

In Exercises 
for the 

corresponding eigenspaces. 

3 1 0  

ible matrix P and a diagonal 

Chapter 

4 Eigenvalues 

and Eigenve
ctors 

In Exercises 
8-15, determ
invert
and, if so, find an 

8 -� 2 3  0 1 
4 3  3 - 1  
I I 3 �m 0 _:] 
-3 -3 3 0 -�l -2 0 
7. [-! 
� [� 
ine whether A is diagonalizable 
matrix D such that P- 1AP = D. [� �]  [-3 �] 
8.A = 9. A= - 1  
[� �] [: 0 il 
IO.A= 3 1 1 .  A= 
0 
[� 0 :J H 2 il 
12.A = 2 13. A= 0 
0  1 ll 0 0 ;] 15. A� l� 0  0 j] 
3 2  2 0 
14.A = 0 3  0 -2 
0 0 0  0 
[-4 :r [-� �r 
17. -3 
18. [ 4 -�r6 19. [� �r 
20. [� n [i 1 �r" 
21.  - 1  0 - 1  
22. [: 0 J 23. [� ff 
-2 
0 
values ofkfor which A is 
diagonalizable. 24.A = [� �] 25. A= [� �] 
[� �]  [i 0 �] 
26.A = 27. A= 1 0 

In Exercises 
compute the indicated power 
16. 

In general, it is 
lar. However, if two 
task becomes easier

difficult to show that two matrices are simi­
similar matrices are diagonalizable, the 
. In Exercises 
same 
matrix P such that 

Theor
30. Prove 
em 4.2l(c). 
31. Prove 
Theor
em 4.22(b). 
em 4.22(c). 
32. Prove 
Theor
33. Prove 
Theor
em 4.22(e). 
34. Prove 
Theor
). 
em 4.22(f
35. Prove 
Theor
em 4.22(g). 
matrices, 
are invertible 
36. If A and B 
show 
that AB and 
BA are simila
r. 
matrices, 
37. Prove 
that if A and B
are similar 
then 
. [Hint: Find a way to 
tr(A) 
45 
use Exercise 
= tr(B)
from Section 
3.2.] 
38-41, show that A and 
B are similar by showing that they are similar to the 
P-1AP = B. 
38.A = [� 
-�lB = [� �] -3],B=[-l l] -2 -6 4 
39.A = [: 
1 :JB� [: -2 0 0 2] [-3 
2 -Si 2 - 1  2 -4 -2 -�l 5 4 
40.A = [: 
- 1  � ,B = : 0 
41. A= [: 
to BT. 
AT is similar 
42. Prove 
that if A is similar 
to B, then 
so is AT. 
43. Prove that 
if A is diagonalizable, 
an invertible 
44. Let A be 
matrix. 
Prove that 
if A is diago­
so is A - i. 
nalizable, 
45. Prove 
that if A is a diagonaliza
ble matri
x with 
only 
one 
A, then 
A is of the 
eigenvalue 
form A = AI. (Such a 
x is called 
a scalar 
matri
with 
each 
46. Let A and B be 
n distinct 
n X n matrices, 
eigenval
eigen 
ues. 
Prove 
-
A and B have 
the same 
that 
vectors if 
and only 
if AB = BA. 
47. Let A and B be 
matrices
alge­
. Prove that the 
similar 
braic 
multiplicities 
of the eigenvalues 
of A and B are 
the same. 

method of Example 
4.29 to 
of the matrix. 

diagonal matrix. Then find an invertible 

24-29,find all (real) 

In Exercises 

16-23, use the 

- 1  

matrix.) 

1 

3 1 1  

v1, v2, v3 in IR6 such that 

4.5 Iterative 
Section 
puting Eigenvalues 
Methods 
for Com
(a) Prove that 
ble to find three line
arly 
it is 
not possi
Prove 
A and B be similar 
matrices. 
that the geo­
48. Let 
plicities 
of the eigenvalues 
of A and B are 
metric 
multi
vectors 
independent 
that, 
if B = P-1AP, then 
the same. 
every 
[Hint: Show 
Av1 = v1, Av2 = v2, and 
vector 
eigen
eigen­
p-1v for some 
of Bis of the form 
lizable, 
(b) If A is diagona
what are the 
of 
dimensions 
vector 
v of A.] 
the eigenspaces 
E_1, E1, and 
52.LetA=[: �]. 
lizable matrix such 
49. Prove that 
if A is a diagona
that 
of A is either 
eigenvalue 
every 
0 or 1, then 
A is idem­
potent 
(that is, 
(a) Prove that 
lizable 
A is diagona
if (a - d)2 + 
Am = 0 for some 
50. Let 
A be a 
nilpotent 
matrix (that is, 
matrix.  4bc > 0 and is 
not diagonalizable 
if (a - d)2 + 
that if A is diagonaliza
ble, then 
m > 1). Prove 
A must 
be the zero 
(b) Find 
two examples 
te that 
to demonstra
if 
(a - d)2 + 4bc = 0, then 
A may or 
may not be 
characteristic 
51. Suppose 
that A is a 6 X 6 matrix with 
diagona
lizab
le. 
cA(,\) = (1 + ,\)(1 - ,\)2(2 - ,\)3• 
polynomial 

Av3 = v3. 

A2 = A). 

4bc < 0. 

E2? 

Iterative 

Methods tor C o m p uting Eigenvalues 

solvable by radicals; 

At this 
point, the only 
method 
we have 
for computing 
the eigenvalues 
of a matrix is 
to solve 
the characteristic 
equation. 
However, 
problems 
are several 
with 
this 
there 
is that 
in all but 
that render 
method 
small examples. 
it impractical 
The first 
problem 
is a very 
of a determinant, 
it depends 
on the computation 
which 
time-co
nsuming 
process 
for large 
matrices
. The second 
problem 
equation 
is 
is that the characteristic 
a polynomia
l equation, 
and there 
are no formulas 
for solving 
polynomial equations 
In 1824, the Norwegian math­
nomials 
than 
higher 
of degree 
of degrees 
lved 
2, 3, and 
4 can be so
4 (poly
the 
using 
Henrik Abel 
Niels 
ematician 
to approximate eigenvalues 
quadratic 
formula 
and its analogu
es). Thus, 
we are forced 
that 
proved 
(1802-1829) 
a general 
in most 
problems
practical 
the roots 
. Unfortunately, 
for approximating 
of a 
methods 
fifth-degree 
(quintic) 
polynomial 
polynomial 
are quite 
tive 
sensi
to roundoff 
error 
ble. 
and are therefore 
unrelia
equation is not 
polynomial altogether 
a different 
and take 
Instead, 
we bypass 
the characteristic 
that 
is, there 
is no 
formula 
for its 
in terms 
of its coefficien
roots 
ts 
to find 
an eigenve
approach, 
approximating 
ctor 
first 
ctor 
and then using this 
eigenve
that 
uses 
only 
the operations 
of 
we will 
ing eigen
the correspond
value. 
In this 
section, 
on 
explore 
several 
varia
tions 
addition, 
subtraction, 
multipli­
one such 
method 
on a simple 
that is based 
iterative 
technique. 
cation, 
division, 
and taking 
nth 
written 
roots. 
In a paper 
in 1830 
posthumously 
and published 
in 
mathematician 
1846, the French 
has a dominant 
to an 
The power method 
n X n matrix that 
applies 
Evariste 
(1811-1832) gave 
Galois 
an eigenvalue 
eigenval
all of the other 
that is larger 
that is, 
value than 
in absolute 
ues. 
a more 
complete 
that 
estab­
theory 
4 = 1 -4 1  > l -31 2 1 3 1  2 I l l. On the 
-4 is the 
For example, 
if a matrix 
has eigenvalues 
- 4, -3, 1, and 
3, then 
nt 
domina
under 
conditions 
lished 
which 
an 
other 
hand, 
a matrix with 
eigenvalue, 
since 
arbitrary 
polynomial 
can 
equation 
- 4, -3, 3, and 
eigenvalues 
4 has no domina
ue. 
nt eigenval
be solve
Galois's 
radicals. 
work 
d by 
was instrum
ental 
in establishing 
a sequence 
iteratively 
The power method 
proceeds 
of scalars 
to produce 
con­
that 
called 
of algebra 
the branch 
verges 
to ,\1 and a sequence 
that converges 
ec­
to the corresp
onding 
eigenv
of vectors 
to polynomial 
theory; his approach 
eigenvector. For simplici
tor 
v1, the 
that the matrix 
ty, we will 
assume 
A is 
equations 
is now known 
as Galois 
diagonalizable. 
The following 
for the power method. 
theor
em is the basis 

eigenvalue A1-

The Power Method 

dominant 

group 

theory. 

3 1 2   Chapter 

1 

4.21, 

Xo = C1V1 + C2V2 + . . .  + CnVn 

there 

4 Eigenvalues 
and Eigenve
ctors 
Theorem 4 . 2 8  Let 
domina
A1. Then 
nt eigenvalue 
lizable matrix 
A be an n X n diagona
with 
by 
vectors 
exists 
a nonzer
x0 such 
that 
xk defined 
o vector 
the sequence of 
x1 = AXo, x2 = Ax1, x3 = Ax2, . . .  , xk = Axk_1, ••• 
ctor 
a domina
approaches 
nt eigenve
of A. 
been 
of A have 
that the eigenvalues 
Proof We may assume 
labeled 
so that 
I A1 I > I Az I 2: I A3 I 2: · · · 
2: I A" I 
_.  indepen
Let 
v1, v2, ... 
, vn are linearly 
v1, v2, . . .  , v" be the 
corresponding 
eigen
vectors
. Since 
basis 
), they form a 
dent 
for !Rn. Conseq
x0 as a 
(why?
uently, we can write 
of these 
linear 
eigenv
ectors
-say, 
combination 
Nowx1 = Ax0, x2 = Ax1 = A(Ax0) = A2x0, x3 = Ax2 = A(A2x0) = A3x0, and, 
gener
ally, 
xk = A kXo for k 2: 
As we saw in Example 
AkXo = C1A�v1 + C2A;v2 + . . .  + cnA�vn 
+ c"(��yv")  (1) 
= Ai(c1v1 + c2(�:)\2 + · · · 
used 
where 
we have 
the fact that A1 * 0. 
nt eigenvalue 
each 
means 
A 1 is the domina
that 
The fact that 
of the fractions 
than 
1 in abs
olute 
value. Thus, 
A2/ A1, A3/ A1, . . .  , An/ A1, is less 
oo. It follows 
that xk = AkXo � A�c1v1 ask� oo 
all go to zero as 
k � 
a nonzero multiple 
ofv1 (that is, an 
Now, 
A1 * 0 and 
v1 * 0, xk is approaching 
since 
is the required 
condition 
eigenvector 
ing to A1) provided c1 * 0. (This 
correspond
compon
on the initial 
vector 
x0: It must 
a nonzero 
of the 
have 
ent 
c1 in the direction 
domina
ctor 
nt eigenve
of A = [� �] using 
nt eigenve
Approximate 
the domina
the method 
ctor 
of 
em 4.28. 
Theor
x0 = [ �] as the 
take 
Solution We will 
initial 
vector
. Then 
x1 = Ax0 = [� �] [�] [�] 
x2 = Ax1 = [� �] [�] [�] 
We continue 
fashion 
to obtain 

the values 
ofxk in Table 

in this 

v 1.) 

(2) 

4.1. 

Example 4 . 3 0  

Section 

4.5 Iterative 
Methods 
for Com

puting Eigenvalues 

3 1 3  

Table 4 . 1  

k  0  1 xk [�] [�] rk 0.50 lk 1.00 

3 [:J 0.83 1.67 
2 [�] 1.50 3.00 
4  5  6 [��] [��] [!�] 
1.02 
1.10 
2.20 
2.05 

0.95 
1.91 

8 [!:] [171] 170 
0.99 1.01 
1.98 2.01 

7 

y 

7 

6 

5 

4 

3 

2 

o 0-/-t-----t------1

---t-----t---r---r--x 

2  3  4  5  6  7 

Xo 

Figure 4 . 1 4  

�  for the 
Figure 
4.14 shows what 
is happening 
geometric
ally. 
We know that the 
eigenspace 
There­
dimension 
have 
ector 
nt eigenv
domina
ise 46.) 
1. (Why? See Exerc
will 
fore, 
few iterates 
IR2. The first 
through 
it is a line 
the origin in 
xk are shown along 
with 
on the 
the directions 
they 
determine. 
It appears 
as though 
the iterates 
are converging 
is [ �]. To confirm that 
ctor 
nt eigenve
this 
vector 
line 
is the domina
whose direction 
we seek, we need 
only 
obser
ve that 
the ratio 
second 
rk of the first 
component 
to the 
of xk gets very 
close to 1 as k increas
in the body of Table 
es. The second 
line 
4.1 gives 
1. We deduce 
these 
values, 
and you can see clearly 
that 
approaching 
that rk is indeed 
a 
of A is [ �]. 
domina
nt eigen
vector 
Once 
we have 
found 
a domina
nt eigenvector, 
how can we find the 
ing 
correspond
is to 
One approach 
value? 
domina
obser
a 
ve that if an xk is approximately 
nt eigen
A1, then 
domina
nt eigen
vector 
dominant eigenvalue 
of A for the 
that the ratio 
lk of the 
It follows 
first comp
onent of xk+ 1 to that 
approach 
of xk will 
A 1 
you can see that they 
4.1 gives the 
ask increas
es. Table 
values 
of lk, and 
ach­
are appro
ing 2, which 
is the domina
nt eigenval
ue. 

3 1 4   Chapter 

4 Eigenvalues 
and Eigenve
ctors 
ents 
of Example 
ack to the method 
There 
is a drawb
4.30: The compon
of the iter­
and can 
cause 
large 
xk get very 
ates 
very 
quickly 
significa
nt roundoff 
errors
. To avoid 
drawback, 
this 
we can multiply 
by some 
iterate 
scalar that reduces 
the magni­
each 
scalar multiples 
converge 
Since 
tude of 
its components. 
to a 
xk will still 
of the iterates 
is acceptable. 
domina
nt eigenvector, 
this 
approach 
ways 
There are various 
to accom­
plish 
it. One is to norma
lize each 
xk by dividing 
to make each 
it by 
II xdl (i.e., 
iterate 
a unit vector). An easier 
method-
we will 
and the one 
xk by 
use-is 
each 
to divide 
the largest 
value, 
so that 
absolute 
the maximum 
the component with 
compone
nt is 
method 
now 1. This 
if mk denotes 
the component of xk with 
is called 
scaling. Thus, 
the maximum 
absolute 
value, we will replace 
xk by Yk = (1 / mk)xk. 
We illustrate 
this 
approach with 
the calculations 
from Example 
4.30. For 
x0, there 
since 
is nothing 
m0 = 1. Hence, 
to do, 
but now we scale with 
x1 = [ �] as before, 
compute 
We then 
m 1 = 2 to get 
tions 
Now the calcula
change. 
get 
and scale to 
Y2 = C�5)[�·5] = [�.67] 
few calculations 
in Table 
The next 
are summarized 
4.2. 
to [ �], a 
You can now see clearly 
that the sequence 
Yk is converging 
of vectors 
domina
Moreover, 
nt eigenvector. 
mk converges 
of scalars 
the sequence 
to the corre­
sponding 
,\1 = 2. 
domina
nt eigenvalue 

We take 

Table 4 . 2  

3 

1 

2 

k 

0 

xk  [�] [�] 
Yk [�] [�·5] 
mk  1 2 

[ �·5] [�·67] [ 1.83] 1.67 [�·91] [ 1.95] 1.91 [�·98] [ 1.99] 1.98 
[ �.67] [ �.83] [�.91] [�·95] [ �.98] [�·99] [ �.99] 
1.5 2  1.83  2  1.95 2  1.99 

7 

8 

5 

6 

4 

The Power Method 

Example 4 . 3 1  

3 1 5  

,\1. 

eigen­

nt eigenvalue 

4.5 Iterative 
Section 
puting Eigenvalues 
Methods 
for Com
below. 
the 
called 
This 
method, 
power method, is summarized 
Let 
a correspond
with 
n X n matrix 
lizable 
ing dominant 
A be a diagona
value 
1. Let x0 = y0 be any initial 
vector 
component 
is 1. 
in !Rn whose largest 
2. Repeat 
steps 
1, 2, ... : 
fork = 
the following 
(a) Compute 
xk = AYk-1· (b) Let m
absolute value. 
xk with the 
k be the component of 
largest 
(c) Setyk= (l/mk)xk. 
For most 
,\1 and Yk con-
to the dominant eigenvalue 
k converges 
choices 
of x0, m
vector. 4 
to a dominant eigen
verges 
and a 
the domina
to approximate 
power method 
Use the 
domin
ant 
of [-� 5 -6l 
eigen
vector 
A= 12 - 12 
- 2  - 2  10 
Solution Taking 
as our initial 
vector 
4.3. [ 0.50l 
we compute 
the entries 
in Table 
1 and the scalars 
mk are 
vectors 
You can see that the 
Yk are approaching 
-0.50 
suggests 
16. This 
approaching 
respecti
vector 
that they are, 
vely, 
a domina
nt eigen
and 
of A. 
the dominant 
eigenvalue 
• If the initial 
of the domina
ent in 
vector 
the direction 
x0 has 
compon
nt 
a zero 
if c = 0 in the proof 
eigenvector 
v1 (i.e., 
of Theor
em 4.28), then the 
power method 
[:l [ =:l [ - 9 33 l [ 8 62l [ 8 12 l [ 803 l [ 8.01-[ 800] 
xk  - 19.33 17.31  16.25 16.05 16.01  16.00 
1 1.67 -9.00 - 8.20 -8.04 -8.01 -8.00 
Yk [:l [ =��;l [ �48l [ �sol [ �sol [ �sol [ �sol [ �sol 
-0.50 
1 -0.60 -0.52 
-0.50 
-0.50 
mk 1  6  - 19.33 17.31 16.25 16.05 16.01 16.00 

-0.50 

Table 4 . 3  

k  0  I 

Remarks 

1 

7 

4 

3 

2 

5 

6 

3 1 6   Chapter 

4 Eigenvalues 

Strutt 

( 1 842-1 9 19), 

William 

John 
was a British 
Rayleigh, 
Baron 
contribu­
who made 
physicist 
major 
of acoustics 
to the 
tions 
fields 
and 
optics. In 1871, 
he gave 
the first 
of why 
the sky is 
correct 
explanation 
1895, he disco
blue, 
and in 
vered 
the 
inert gas 
very 
argon, 
for which 
disco
he recei
ved the 
Nobel 
Prize 
in 1904. 
Rayleigh 
t of the Royal 
was presiden
from 1905 to 1908 and 
Society 
chancellor 
of Cambridge 
became 
in 1908. He used 
University 
Rayleigh 
quotients 
in an 1873 paper 
on vibrating 
in his 
systems 
and later 
book 

The Theory of Sound. 

) 

(2) 

21-24.

and Eigenve
ctors 
it is quite 
that 
However, 
nt eigenvector. 
to a domina
will 
not converge 
likely 
the 
during 
error 
point roundoff 
itera
the su
bsequent 
calculation of 
tes, 
will 
an xk 
produce 
at some 
power method 
in the direction 
a nonzero 
with 
component 
of v1. The 
start 
will 
then 
to 
• The power method 
actually 
one instance 
to a multiple 
converge 
of v1. (This is 
where 
roundoff 
errors 
help!) 
is a repeated domina
still 
works 
there 
nt eigenvalue, 
when 
the matrix is not 
or even 
when 
diagonalizable, 
in conditions
under 
certa
. Details may 
• For some 
be found 
in most 
modern 
textb
ooks on numerica
l analys
Exercises 
is. (See 
converges 
to a domina
rapidly 
the power method 
matrices 
nt eigen
­
, while 
vector
look 
for others 
the convergence 
may be quite 
slow. 
A careful 
at the proof 
l,\2/,\11 2: l,\3/,\112': · · · 2: l,\n/,\11, ifl,\2/,\11 is 
why. 
of Theorem 
4.28 reveals 
Since 
(,\2/ ,\1) k, .•• , (,\n/ ,\1) k will all 
approach 
zero 
rapid
ly. Equation 
close to zero, 
then 
k = Akx0 will 
approach 
,\ic1v1 rapidly too. 
then 
that x
shows 
consider 
are 
As an illustration, 
Example 
4.31. The eigenvalues 
16, 4, and 
2, so 
,\2/ ,\1 = 4/16 = 0.25. Since 
0.257 = 0.00006, by the seventh 
we should have 
iteration 
• There 
close to four-
decimal-place 
we saw. 
accuracy. 
This 
is exactly 
what 
nt eigenvalue 
way to 
te the 
is an alternative 
estima
,\1 of a ma­
domina
trix A in conjunction 
with 
ve that if Ax = ,\1x, then 
the power method. 
First, 
obser
(Ax) · x (,\1x) · x  A
1 (x · x) 
x·x = x·x = x·x =Ai 
a Rayleigh quotient. As we compute 
((Ax) · x) / (x · x) 
R (x) = 
The expression 
is called 
ve Rayleigh 
,\1. In fact, for 
quotients 
the iterates 
xk, the successi
R (xk) should 
approach 
as fast as the scaling 
matrices, 
the Rayleigh 
method 
is about 
twice 
symmetric 
quotient 
ises 
Exerc
(See 
method. 
factor 
17-20.) 
of a matrix, but 
the 
us approximate 
The power method 
can help 
dominant eigenvalue 
ately, 
what 
should we do if we want 
the other 
eigenval
ues? Fortun
there 
are several 
varia
tions 
that can be applied. 
of the power method 
that, if
The 
shifted power method uses 
vation 
the obser
,\ is an eigenvalue 
of A, 
a (Exercise 
eigenvalue 
,\ -a is an 
then 
22 in Section 
of A -al for any scalar 
of A, the 
eigenvalues 
nt eigenvalue 
of A -,\if will 
Thus, 
if ,\1 is the domina
be 0, 
A2 -A1, ,\3 -,\1, ... , An -,\1. We can then 
apply 
to compu
the po
wer method 
te 
,\2. Repeating 
this 
process 
will allow us to 
,\2 -,\1, and from this 
we can find 
value 
compute 
all of the eigenvalue
s. 
of the matrix A 
eigenvalue 
to compute 
Use the shifted 
the second 
power method 
= 
[� �] from Example 
,\1 = 2. To find ,\2, we apply 
Solulion In Example 
the power 
that 
4.30, we found 
method 
to A -21 = [ -� _ �] 
0 = [ �], but other 
We take x
tions 
calcula
also 
will 
choices 
work. The 
are summarized 
in Table 

The Shilled Power Melhod  and 

lhe Inverse Power Melhod 

4.4. 

4.30. 

4.3). 

Example 4 . 3 2  

Section 

4.5 Iterative 
Methods 
for Com

puting Eigenvalues 

3 1 1  

Table 4 . 4  

0 

2 

k 

3 

1 

4 

inverse 

- 3  -3 

reciprocal 

of x0 has 

xk  [�] [-�] [ -�·5] [ -�·5] [ _�·5] 
Yk [�] [-�.5] [-�.5] [-�.5] [-�.5] 
mk  1  2 - 3  
of A:._+ 
Our choice 
only 
produced 
-3 after 
two itera
the eigenvalue 
tions. 
There­
fore, 
A2 -A1 = -3, so A2 = A1 - 3 = 2 -3 = - 1  is the 
second 
eigenvalue 
Recall from property (b) of Theorem 
with 
eigenvalue 
4.18 that if A is invertible 
to A -l, its 
A -l has eigenvalue 
if we apply 
1 /A. Therefore, 
A, then 
the power method 
will 
domina
nt eigenvalue 
eigenvalue 
of the smallest (in magnitude) 
be the 
of A. To use 
power method, we follow 
as in the power 
the same steps 
this 
method, 
except 
2(a) we compute 
that in step 
xk = A-1 Yk-l· (In practice, 
we don't 
actually 
we solve the 
compute 
instead, 
equivalent 
A -i explicitly; 
equation 
Gaussian 
for xk using 
elimina
tion. 
turns 
out to be faster
.) 
This 
x A = 
the second 
eigenvalue 
to compute 
inverse power method 
Use the 
of the matri
[ � �] from Example 
as in Example 
Solution We start, 
Xo = y0 = [ �]. To solve 
4.30, with 
we use 
row reduct
ion: 
x1 = [�],so y1 = [�]·Then 
Thus, 
we get 
x2 from 
[ 1  1 I o J [ 1 o I 0.5 J 
[A I Yi] = 2 0  1 
x2 = [ 0·5], and, 
y2 = [ 1]. Continuing, 
Hence, 
we get 
by scaling, 
we get the 
values 
mk are converging 
to - 1. Thus, the 
shown in Table 
the values 
4.5, where 
of A is the reciprocal 
of - 1  (which is also 
eigenvalue 
smallest 
- 1). This 
agrees 
our previous finding 
in Example 

with 

Axk = Yk-i 

4.32. 

4.30. 

0  1 -0.5 

Ax2 = y1: 

Ax1 = y0, 

-0.5 

- 1  

� 

Example 4 . 3 3  

3 1 8   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

Table 4 . 5  

k 

3 

5 

4 

0  1 

2 [�] [�] [-�:�] 
[�] [�] [ _�] O.S 

[-0.S] [ o.s ] [ o.s] [ o.s ] [ o.s ] [ o.s ] [ o.s ] 
l.S -0.83 -1.1 -0.9S -1.02 -0.99 -1.01 
[-�.33] [-�.6] [-�.4S] [-�.S2] [-�.49] [-�.Sl] [-�.SO] 
l.S -0.83 -1.1 -0.9S -1.02 -0.99 -1.01 

9 

6 

8 

7 

Example 4 . 3 4  

shifted inverse 

T h e  Shilled Inverse Power Method 

the two 
is one that 
of the power method 
of the variants 
The most 
combines 
versatile 
mentioned. 
just 
It can be used 
to find an approximation 
for any eigenvalue, 
provided 
to that eigenvalue. 
we have 
a close approximation 
if a scalar 
words, 
a is given, 
In other 
the 
power method will 
find the 
to a. 
,\ of A that is closest 
eigenvalue 
A -al is invertible 
if a is not 
an 
a * ,\, then 
of A and 
If ,\ is an eigenvalue 
of A and 1 / (,\ -a) is an eigenvalue 
eigenvalue 
of (A -aI)-1. (See 
4S.) If a 
Exercise 
of (A -aI)-1. In fact, 
1/(,\ -a) will 
is close to,\, then 
be a 
nt eigenvalue 
domina
if 
1 / ( ,\ -a) will 
a is very close to ,\, then 
tude 
than the next 
be much bigger in magni
so (as noted 
eigenvalue, 
Remark following 
in the third 
4.31) the conver­
Example 
rapid. 
gence 
will 
be very 
-6] -12 10 
A =[-� -2 
inverse power method 
the eigenvalue 
to approximate 
Use the shifted 
of 
s 12 -2 
is closest 
that 
to S. 
Solulion Shifting, 
we have 
A -SI= [ =: � -��sl -2 -2 
with 
the inverse power method 
Now we apply 
We solve 

(A -SI)x1 = y0 for x1: [-s 
-6 -12 s :]--� [: � � =�::�] 0 1 -0.39 
s 7 -2 
[A -SI I y0] = -4 -2 

Section 

4.5 Iterative 
Methods 
for Com

puting Eigenvalues 

3 1 9  

Table 4 . 6  
k  0 

7 

1 

6 

5 

2 

4 

3 

[:J [-061] 
xk  -0.88 
-0.39 
[:] [ 069-
Yk  1.00 0.45 
mk  -0.88 

[-047: [-049] [-050] [-050] [-050] 
[-041: -0.69 
-0.89 -0.95 -0.98 -0.99 -1.00 
-0.44  -0.48 -0.49 -0.50 -0.50 
-0.35 
[053] [ 051 l [050] [ 050] [050-
[059] 1.00 0.51 -0.69 
1.00 1.00 1.00 1.00 1.00 
0.50 0.50 0.50 0.50 0.50 
-0.89 -0.95 -0.98 -0.99 -1.00 
This 
gives 
l [0.69] 
[-0.61] 
m1 = -0.88, and 1 [-0.61 
= 1 
, -0.39 
---0.88 
X1 = -0.88 
0.88 -0.39 0.45 
to obtain 
we deduce 
4.6, 
the values 
fashion 
in this 
We continue 
in Table 
from which 
that the eigenvalue 
of A closest 
to 5 is approximately 
5 + 1/ m7 = 5 + 1/ ( -1) = 4, 
in fact, 
which, 
is exact. 4 
The power method 
approach to the compu
and its variants 
repres
ta­
ent only one 
5, we will 
ues. 
In Chapter 
tion 
of eigenval
discuss another method based 
on the 
of a matrix. 
factoriza
you can consult 
topic, 
treatment of this 
tion 
For a 
more 
complete 
on numerical 
almost 
any textbook 
methods. 
�  Gerschgor
tions 
on the power method 
varia
several 
discussed 
we have 
section, 
In this 
for ap­
the eigenvalues 
proximating 
and the 
of a matrix. 
All of these 
methods 
are iterative, 
theorem 
We owe this 
to the 
speed 
with which 
they 
converge 
vector
we had 
depends 
on the 
choice 
of initial 
. If only 
mathematician 
Russian 
matri
of a given 
of the 
some 
eigenvalues 
tion" about the location 
informa
"inside 
x, 
hgorin 
(1901-1933), who 
S. Gersc
a judicious 
choice 
of the initial 
make 
then 
we could 
vector 
up the 
and perhaps 
speed 
receive 
it in 1931. It did not 
stated 
conve
rgence 
of the iterative 
process. 
attention 
until 
much 
1949, when 
it 
Fortun
is a way to estima
ately, 
there 
te the lo
cation 
of the eigenvalues 
of any 
by Olga 
was resurrected 
Taussky­
eigenvalues 
Gerschgorin's Disk Theorem states 
matrix. 
of a (real or complex) 
that the 
she published 
Todd 
in the 
in a note 
in the complex 
of n circular disks 
the union 
x all lie inside 
n X n 
matri
plane. 
andletr;denote 
nX nmatrix,
Definition LetA = [a;j]bea(realorcomplex)
the sum of the absolute 
values 
of the off-diagona
l entries 
in the ith row of A; that 
is, r; = 2: lau·I The 
chgorin disk is the circular 
disk 
D; in the complex 
j1'i 
r;. That is, 
a;; and radius 
plane 
with 
center 

Mathematica/ 

i n 's Theorem 

ith  Gers

QR 

American 

Monthly. 

3 2 0   Chapter 

4 Eigenvalues 
and Eigenve
ctors 
Empire 
Austro-Hungarian 
in the 
Taussky-Todd 
(1906-1995) was born 
Olga 
in Olmiitz 
in number theory 
rate 
Czech 
in the 
Olmuac 
(now 
Republic)
ved her docto
. She recei
from the 
in 1930. During World War 
University 
of Vienna 
II, she worked 
for the National 
Physical 
she investigated 
of flutter 
the problem 
tory 
Labora
in the wings 
where 
in London, 
of super­
the problem 
involved 
Although 
sonic 
aircr
aft. 
differential 
equations, 
the stability 
of an aircr
aft 
y-Todd remembered 
depended 
on the eigenvalues 
of a related 
Taussk
matrix. 
Gersc
hgorin's 
Theorem 
from her gradua
te studies 
able 
in Vienna 
and was 
to use it to simplify 
the otherwise 
laborious 
computations needed 
to determine 
the eigenvalues relevant 
to the flutter problem. 
United 
States 
to the 
moved 
in 1947, and ten 
she became the 
Taussky-Todd 
first 
years later 
woman appointed 
to the California 
Institute 
of Technolo
over 
gy. In her career
, she produced 
ental 
200 publications 
and received 
numerous 
awards. 
She was instrum
development 
of 
in the 
the branch 
of mathema
tics 
now known 
ry. 
as matrix theo

Example 4 . 3 5  

the Gersc

(a)A=[� -�J (b)A = [� -�] 
Sketch 
disks 
hgorin 
and the eigenvalues 
for the following 
matrices
: 
Solulion (a) The 
are centered 
disks 
two Gerschgorin 
at 2 and -3 with 
1and2, 
radii 
of A is ,\2 + ,\ -8, so the eigenvalues 
The characteristic 
polynomial 
vely. 
respecti
are 
,\ = (-1 ± v12 -4(-8))/2 =  2.37, 
-3.37 
Figure 
two Gerschgorin 
4.15 shows that the eigenvalues 
within the 
are contained 
disks
. (b) The two Gerschgorin 
disks 
are centered 
radii 
at 1 and 3 with 
I-3 I = 3 and 2, 
of A is,\ 2 -4,\ + 9, so the eigenvalues are 
polynomial 
The characteristic 
respecti
vely. 
± iVs =  2 + 2.23i
A = (4 ± V(-4)2 -4(9))/2 = 2 
, 2 -2.23i 
disks
of the eigenvalues 
4.16 plots 
Figure 
. 
relative 
the location 
Im 

to the Gerschgorin 

4 

- 4  

Figure 4 . 1 5  

4.5 Iterative 
Methods 
for Com

puting Eigenvalues 

3 2 1  

Section 
Im 

4 

- 4  

Figure 4 . 1 6  

Theorem 4 . 2 9  

Disk Theorem 

Gerschgorin's 

within its 

4.35 suggests, 
the eigenvalues 
As Example 
of a matrix are 
contained 
Gerschgorin 
that this is 
disks
. The next 
verifies 
theorem 
so. 
Let 
A be an n X n (real or complex) 
Then 
matrix. 
every 
eigenvalue 
of A is contained 
within 
disk. 
a Gerschgorin 
_....  the entry 
Proof Let 
eigenve
corresponding 
of A with 
ctor 
x. Let 
x; be 
A be an eigenvalue 
(Why?) Then 
nonzero. 
value-a
absolute 
the largest 
nd hence 
of x with 
Ax = Ax, the ith row of which 
is 
n 2: aijxJ = Ax; 
1�1 
we have 

Rearranging, 

of absolute 
value 
(see 

values 
and using 
properties 
X; * 0. Taking 
because 
absolute 
Appendix 
C), we obtain 
2:a;1x1 I fi1aijxJI 2: la;1x11 2: laullx1I 
lx;I :S  lx;I  lx;I 
J*i  J*i  :S 2: la;1I 
I A - a;;I = J*i 
lx11 :S lx;I forj 
J*i 
because 
* i. 
that the eigenvalue 
establishes 
A is contained 
This 
disk 
the Gerschgorin 
centered 
at a;; with 
radius 

within 

r;. 

= r; 

X; 

3 2 2   Chapter 

Remarks 

4 Eigenvalues 
and Eigenve
ctors 
• There is 
a corresponding 
version 
of the preceding theorem 
for Gersc
hgorin 
• It can be shown that 
disks 
whose radii 
are the sum of the 
off-diagonal 
column of A. 
entries 
in the ith 
disjoint from the other 
if k of the Gerschgorin 
disks are 
union 
of these 
k eigenvalues 
then 
disks, 
within the 
are contained 
k disks
exactly 
. In 
from the other disks
particul
ar, if a single 
disk 
exactly 
, then 
it must 
contain 
is disjoint 
•  Note 
4.35(a) illustrates 
one eigenvalue 
this. 
of the matrix. 
Example 
in a Gerschgorin 
that is, 
0 is not contained 
that in Example 
disk; 
4.35(a), 
tion, 
an eigenvalue 
0 is not 
of A. Hence, 
computa
we can deduce 
without 
any further 
that the matrix A is invertible 
is particularly 
by Theor
obser
vation 
use­
em 4.16. This 
ful when 
applied to 
larger 
matrices, because 
hgorin 
disks 
can be determined 
the Gersc
from the entries 
matrix. 
directly 
of the 
x A = [ � 6 �l · Gersc
Consider 
the matri
-
hgorin's 
Theor
em tells us 
that the eigen
2 0 8 
radii 
of A are contained 
disks 
values 
centered 
at 2, 6, and 8 with 
within 
three 
1, 1, 
disk 
vely. 
is disjoint 
and 2, respecti
See Figure 
4. l 7(a). 
Because the first 
from the 
other 
two, 
it must 
contain 
exactly 
one eigen
value, 
by the second 
Remark af
ter Theo­
if it has 
of A has real 
polynomial 
the characteristic 
rem 4.29. Because 
coefficients, 
in conjugate 
of A), they 
roots 
complex 
(i.e., 
eigenvalues 
must 
occur 
Ap­
pairs. 
(See 
there is a unique real 
eigen
pendix D.) Hence 
between 
value 
union 
1 and 3, and the 
two disks 
contains 
two (possi
of the other 
bly complex) eigen
values 
whose real 
parts 
lie between 
5 and 10. 
em 4.29 tells 
us that the same 
Theor
Remar
hand, 
On the other 
k after 
the first 
three 
of A are contained 
eigenvalues 
�, 1, 
in disks 
centered 
at 2, 6, and 8 with 
radii
and 
t, respecti
vely. 
See Figure 
disks 
are mutually 
disjoint, 
con­
4.l 7(b ). These 
so each 
tains 
a single 
(and hence 
real) 
eigenval
these 
resul
ue. Combining 
ts, we deduce 
that 
one in each 
eigenval
real 
A has three 
5]. 
[1, 3], [5, 7], and [7.5, 8.
of the intervals 
ues, 
(Compute 
eigenvalues 
the actual 
of A to verif
y this.) 

Example 4 . 3 6  

Im 

4 

Im 

4 

- 4  

(a) 

- 4  

(b) 

Figure 4 . 1 1  

Section 

4.5 Iterative 
Methods 
for Com

puting Eigenvalues 

3 2 3  

4 . 5  

4.30. 

Example 

these data to approximate a dominant 

along with an iterate 
eigenvector 

In Exercises 
1-4, a matrix A is given 
x5, produced 
as in 
(a)  Use 
whose 
eigenvalue. (Use three-
(b) Compare 
the actual 

. '� 1 Exercises 
first component is 1 and a corresponding dominant 
[l 2] [ 4443] 
1. A = 5 4 ' Xs = 11109 
4] [ 7811] 
-1 ,Xs = -3904 
[_� [� 
l] [144] 1 'Xs = 89 
4 A =   x  = [1.5 0.5] [ 60.62
5] 
.  2.0 3.0 ' 5 239.500 

your approximate eigenvalue 

dominant eigenvalue. 

decimal-place 

accuracy.) 

3.A = 

2.A = 

in part (a) with 

11.A = [� �lXo = [�lk = 6 
-0.5 'Xo = 0 'k = 6 
1.5] [l] 
[3.5 12.A = 1.5 
4 15 -4 
13.A = [49
8 
: n��uk�6 

method to approxi­

15 and 16, use the 

power 
In Exercises 
mate the 
of A to 
and eigenvector 
eigenvalue 
dominant 
any initial 
two-decimal
vector you 
4.31 in mind!) 
like (but keep the first Remark after Example 
and apply 
place 

the method until the digit 

-place accuracy. Choose 

of the iterates 

in the second 

decimal 

stops 

gh 

7.A = 

5.A = 

6.A = 

power 

ponding 

In Exercises 
5-8, a matrix A is given 
using the 
xk> produced 
(a) Approximate the dominant 
by computing the corres
you have approximated an eigenvalue 
of A by comparing 
Ayk with mkYk· 

on page 316. In Exer-

for the matrix A in the given 

and an eigenvector 

gh quotients are described 

method, as in 
eigenvalue 
mk and Yk· (b) Verify that 

along with an iterate 
Example 
4.31. 
and eigenvector 

quotient method ap­
more rapidly than the 
method, compute the successive 
Raylei

Raylei
cises 17-20, to see how the Rayleigh 
proximates the dominant 
eigenvalue 
ordinary power 
exercise. 

-3] [-3.667] 
10 ,xs= 11.001 
[_� 
-�l X10 = [�:!��] � �1,Xg = [1�:���1 
[� [-� 
0 4 10.000 
2 1 -1 -2
1 [ 3.4151 
-3 , X10 = 2.914 
1 -1.207 
l�l Xo = [�l k = 5 
accuracy. [14 
-�l Xo = [�l k = 6 

2 60 
-61 -2 12 
changing. [ 12
16.A = -6 6 
quotients R(x;) for i = 1, ... , k 
19. Exercise 13 18. Exercise 12 
17. Exercise 11 
23. A � [ � � +• � m 
24.A � [� � n� � m 

are not diagonaliz­
The matrices in Exercises 
able or do not have a dominant eigenvalue 
the power 
x0, 
performing eight 
eigenvalue

21.A = [� �],x0 = [�] 22. A= [ _� �],x0 = [�] 

20. Exercise 14 

method anyway with the 
iterations in 
each case. 
s and eigenvectors and explain 

method to approximat
e 
and eigenvector 
given 
of A. Use the 
number of iterations 
k, and 

given initial 
Compute the exact 
what is happening. 

(or both). Apply 
vector 

21-24 either 

In Exercises 9-
the power 
the dominant eigenvalue 
initial 
specified 
vector 
three-decima

x0, the 
l-place 

9. A =  5 [-6 10. A= 8 

14, use 

3 2 4   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

In Exercises 
25-28, the power 
the dominant eigenvalue 
the given 
initial 
and eigenvect

ors and explain 

method does not conver
ge to 
or. Verify this, using 

and eigenvect

42. p(x) = x2 - x -3, a = 2 
43. p(x) = x3 -2x2 + 1, a = 0 
44. p(x) = x3 -5x2 + x + l, a = 5 

9 

s 

10 

2i 

4 
0 

-2i 

A -al 

- l+ i 0 

disks 

power 

matrix. 

for the given 

vector 

the inverse 

33-36, apply 

what is happening. 

2  - 2  
0 

1  -2i 2i -5 -Si 

x0• Compute the exact eigenvalue

of the matrix A in the 
vector 

method to 
x0, k iterations
, 

In Exercises 
approximate, for the 
genvalue 
x0, k iterations
vector 

the shifted power 
29-32, apply 
In Exercises 
eigenvalue 
second 
approximate the 
initial 
given exercise. Use the given 
accuracy. 
l-place 
and three-decima

A be an eigenvalue 
45. Let 
corresponding 
of A with 
eigenve
ctor 
x. If 
a is not 
of 
an eigenvalue 
a * A and 
A, show 
that 
1 /(A - a) is an eigenvalue 
of (A -al)-1 
with 
ing eigenve
ctor 
correspond
x. (Why must 
be invertibl
e?) 
that the ei­
46. If A has a domina
nt eigenvalue 
A1, prove 
genspace 
EA, is one-dimensional. 
47-50, draw the Gerschgorin 
1 + �i 

25. A=[=� �lXo=[�] 
26. A = [ _ � � l Xo = [ �] 
27.A _ n ; _n� _  [:J 
47. u 1 !] 48. [� - i  
2&A -[: �: �]·� -[:]  � In Exercises 
[ 4 � 3i 
49. 1 + i - i  5 + 6i 2; l 
13 30. Exercise 
29. Exercise 
2 
31. Exercise 
32. Exercise 
14 
4 ! 4 
50. [; I 0 �] 
I 6 6 0 I 8 
is strictly diagonally dominant if the 
matrix 
51. A square 
34. Exercise 
33. Exercise 
value of 
absolute 
each 
diagonal entry 
is greater than 
7. � -[ _:J k - 5 
the sum of the absolute 
values 
of the remaining 
entries 
35. Exmi" 
in that 
hgorin's 
2.5.) Use Gersc
Section 
row. 
Disk 
(See 
diagon
Theor
em to prove 
nt 
that a strictly 
ally 
domina
36. Exercise 
matri
x must be 
k 
invertible. 
Remar
[Hint: See the third 
after Theor
em 4.29.] 
37-40, use the shifted inverse power 
11 A 11 denote 
52. If A is an n X n matrix, let 
the maximum of 
In Exercises 
method 
values 
of the absolute 
the sums 
of A; that 
is, 
of the rows 
llAll = 1�'!,� c� laiJI). (See 
to approximate, for the matrix A in the 
given 
exercise, 
37. Exercise 
7.2.) Prove 
that 
Section 
closest 
eigenvalue 
12, a = 0 
9, a = 0 
39. Exercise 
if,\ is an eigenvalue 
of A, then 
4.3 demonstrates 
7, a = 5 
13, a = -2 
matrix A 
53. Let 
of a stochastic 
A be an eigenvalue 
::; 1 .  [Hint: Apply 
(see 
Section 
3.7). Prove that 
52 to Ar.] 
Exercise 
of A = [� � ! 0 
0  0 � �1 are 
3 2 � 7 
that the eigenvalues 
54. Prove 
of these 
each 
all real, 
within 
eigenvalues 
and locate 
a 
line. 
closed interval 
on the real 

that every poly­
Exercise 
nomial is (plus or minus) the characteristic 
polynomial of 
matrix. Therefore, the ro
its own companion 
ots of a poly­
s of C(p). Hence, we can use 
nomial pare the 
the methods of this section 
to approximate the ro
ots of any 
41-44, apply 
polynomial when exact results 
In 
method to 
Exercises 
the companion 
matrix C ( p) of p to approximate the root of 
p closest 
to a to three 
41. p(x) = x2 + 2x -2, a = 0 

method to 
the ei­
matrix A in the given 
exercise, 
that is smallest in magnitude. Use the given 
initial 
-place 
, and three-decimal
accuracy. 
10 

38. Exercise 
40. Exercise 

are not readily available. 

the shifted inverse power 

32 in Section 

eigenvalue

decimal places. 

I A I 
::; II A II. 

I A I 

the 

to a. 

14 

9 

' 

Theorem 4 . 3 0  

Section 

4.6 Applications 

and the 
Perron-
Froben

ius Theorem 

3 2 5  

4.3, 

Applications a n d  the Perron-Frobenius Theorem 

pYf = (jPf = f 

explore 
of eigenvalues 
section, 
we will 
several 
. 
applications 
and eigen
In this 
vectors
chapters
some 
applications 
We begin by revisiting 
from previous 
. 
vations 
obser
3.7 introduced 
and made several 
Markov 
Markov Chains Section 
chains 
about the tran­
sition 
(stochastic) 
we obser
matrices 
associa
them. In particu
if 
lar, 
ted with 
ved that 
transition 
P is the 
P has a stead
y state 
chain, then 
vector 
matrix of a Markov 
x. That 
is, there is 
a vector 
x such 
that Px = x. This 
to saying 
P has 
1 as an 
that 
is equivalent 
eigen
value. 
We are now in a position 
to prove 
fact. 
this 
n X n transition 
1 is an eigenvalue 
chain, then 
matrix of 
If Pis the 
a Markov 
of P. 
Proof Recall 
that 
transition 
hence, 
matri
x is stochastic; 
each 
every 
of its columns 
consisting 
sums 
jP = j. (See 
to 1. Therefore, 
of n ls, then 
if j is a row vector 
Exer­
3.7.) Taking 
cise 
13 in Section 
transposes, 
we have 
1. By 
corresp
onding 
of pT with 
implies 
which 
eigenvalue 
ector 
that jr is an eigenv
P and 
Exercise 
19 in Section 
ues, 
the same eigenval
so 1 is also 
an eigen­
pT have 
of P. In fact, 
value 
IAI ::=::: 1 and the eigenvalue 
more 
transition 
For most 
much 
A sat­
every eigenvalue 
matrices, 
is true. 
isfies 
1 is dominant
; that is, if A * 1, then 
IAI < 1. We need 
the following 
two defini
tions
: A matrix is called 
positive if all of its entries 
are posi­
tive, 
and a square 
matrix 
is called 
regular if some 
power of it is positive. 
For example, 
B2 = 
A = [� �] is positive 
but B = [� �] is not. 
since 
However, 
Bis regular, 
[ 1 ! �] is positive. 
a. IAI ::=::: 1 b. If 
eigenvalue 
Let 
P be an n X n transition 
matri
x with 
and 
Pis regular 
A * 1, then 
em is 
to use the fact that 
to proving 
this 
theor
em 4.30, the trick 
Proof As in 
Theor
has the 
same 
as P. 
eigenvalues 
::=::: lxkl = m for i = 1, 2, . . .  , n. Comparing 
xk be the 
nt of x 
compone
of pT corresp
to A and let 
(a) Let 
x be an eigenve
ctor 
onding 
m. Then 
the largest 
with 
absolute 
value 
the kth components 
of the 
PTx = Ax, we have 
equation 
PlkX1 + P2kX2 + · · · 
+ PnkXn = Axk 

IAI < 1. 

lx;I 

A. 

pT 

Theorem 4 . 3 1  

3 2 6   Chapter 

+ Pnkm 

the rows 

values, 
we obtain 

of pT are the columns 
of P.) Taking 
absolute 

4 Eigenvalues 
and Eigenve
ctors 
er that 
(Rememb
I Alm =  IAI lxkl = IAxkl = lplkx1 + P2kX2 + · · · 
+ Pnkxnl 
:S lplkxl I + IP2kX2 I + · · · 
+ IPnkXn I 
= P1klx,I + P2klx2I + · · · 
+ Pnklxnl  (1) 
= (p,k + P2k + · · · 
:S Plkm + P2km + · · · 
+ Pnk)m = m 
The first 
inequality 
Triangle 
follows 
from the 
Inequality 
in IR, and the last 
equality 
of pT sum to 1. Thus, 
comes 
the rows 
from the fact that 
IAlm :s m. After 
dividing 
by 
IAI :s 1, as desired. 
m, we have 
A = 1. First, 
implica
we show 
(b) We will prove 
the equivalent 
tion: 
If IAI = 1, then 
PT) is a positive 
P (and therefore 
matrix. 
is true 
when 
that it 
If IAI = 1, then 
all of the 
( 1) are 
, 
in Equations 
inequalities 
actually 
equali
ties. 
In particular
Equivale
ntly, 
m - lx;I 2: 0 for i = 1, 2, ... , n. 
Pis positive,p;k > 0 for i = 1, 2, ... , n. Also, 
since 
Now, 
= m for i = 1, 2, ... , n. Furthermore, 
Therefore, 
each 
summand 
in Equation 
be zero, 
and this 
if 
(2) must 
can happen 
only 
we get equality 
in 
in the Triangle Inequality 
if all of the 
IR if and only 
; in other 
or all are negative
positive 
summands are 
words, 
This 
the p;
kX; 's all have 
the same sign. 
implies 
that 
of n ls, as in 
case, 
in either 
em 4.30. Thus, 
j is a row vector 
where 
Theor
the eigenspace 
of pT correspond
ing to A is EA = span(jr). 
jT = pT { = A{, and, 
But, 
using 
the proof 
of Theor
em 4.30, we see that 
compar­
P is positive. 
ing components, 
A = 1. This 
we find that 
the case 
where 
handles 
�  also 
pk+I must 
power of Pis positive-say, Pk. It follows 
If Pis regular, 
then 
some 
that 
A k+ 1 are eigenvalues 
k+ 1, respecti
A k and 
of pk and p
be positive. (Why?) Since 
vely, 
Ak(A - 1) = 0, 
just proved 
by Theor
em 4.18, we have 
that Ak = Ak+l = 1. Therefore, 
IAI = 1. 
A = 0 is impossi
that A = 1, since 
which implies 
ble if 
We can now 
some 
behavior of Markov 
that 
explain 
of the 
chains 
we observed 
in 
Chapter 
3. In Example 
3.64, we saw that 
for the transition matrix 
p = [0.7  0.2] 0.3  0.8 
[0.6] 
xk converge 
to the vector 
vectors 
Xo =  , the state 
and initial 
state 
vector 
[0.4]  0.4 
that 
Px = x). We are going 
, a steady state 
for P (i.e., 
vector 
to prove 
for regular 

lx;I 

0.6 

x = 

(2) 

Example 4 . 3 1  

3 2 1  

Section 
4.6 Applications 
and the 
Perron-
Froben
ius Theorem 
more. 
we will 
Markov 
happens. Indeed, 
prove 
chains, 
always 
this 
much 
Recall that 
xk satisfy xk = Pkx0. Let's 
the state vectors 
investigate 
what 
happens 
to the powers 
large. 
as P becomes 
pk 
0.2] 0.8 has characteristic 
[0.7 
matrix P 
The transition 
equation 
= 0.3 
0.2 I 2 = A - l.5A + 0.5 = (A - l)(A -0.5) 
10.7 - A 
0 = det(P -AI) = 0.3 
0.8 - A 
A1 = 1 and 
A2 = 0.5. (Note 
ems 4.30 
so its eigenvalues 
that, 
are 
thanks 
to Theor
and 
1 would 
be an eigenvalue and 
that 
advance 
4.31, we knew in 
the other 
eigenvalue 
1 in abs
olute 
would 
be less 
than 
value. 
However, 
we still 
needed 
to compute 
A2.) The 
aces 
eigensp
are 
E1 = span( [ �]) and E0.5 = span( [ _ �]) 
Q-1PQ = [ 1 O ] = D. From 
Q = [23 1], we know that 
So, taking 
-1 
0 0.5 
in Example 
4.29 
in Section 
we have 
used 
4.4, 
pk= QDkQ-i = [2 l][lk 0 ][2 3 -1 0 (0.5)k 3 1 ]-! -1 
ask� oo, (0.5)k � 0, so 
Now, 
and pk � [ � _ �] [ � �] [ � 1]-l 
= [0.4 0.4
-1  0.6 0.6 
] 
that the columns 
(Observe 
of this 
"limi
a steady 
each is 
t matrix" are identical and 
Xo = [a] be any initial 
a + b = 1). 
proba
(i.e., 
bility 
state 
vector 
for P.) Now let 
vector 
Then b 
k ·-u  0.6 0.4] [a] = [0.4a 
+ 0.4b] = [0.4] 
X = p ·v� � k [0.4 
0.6 b 0.6a 
+ 0.6b 0.6 
x, will 
we saw in Example 
Not only 
does 
this 
what 
tells 
explain 
3.64, it also 
us that the state 
[0.4] 
of x,!4 
vedoc x � 0.6 foe any choke 
veoto;s 
oonmge 
to the steod
y stote 
about Example 
special 
nothing 
There is 
theorem 
shows 
4.37. 
The next 
that this 
with 
always occurs 
of behavior 
type 
matrice
transition 
regular 
s. Before we can present 
the theorem, 
we need 
the following 
lemma. 
If P is diagonalizable, 
matrix. 
Let P be a regular 
n X n transition 
then 
nt 
the domina
A1 = 1 has algebraic 
1. 
eigenvalue 
multiplicity 

the method 

Lemm a  4 . 3 2  

Finite 

Theorem 4 . 3 3  

3 2 8   Chapter 

Markov Chains by J. G. 
See 
and J. L. Snell 
Kemeny 
(New 
York: 
Springer-V
erlag, 
1976). 

and Eigenve
4 Eigenvalues 
ctors 
proof 
pT are the same. From the 
of P and 
Proof The eigenvalues 
of Theor
em 4.31 (b ), 
of Pr. Since 
1 as an 
ble, 
multiplicity 
A1 = 1 has geometric 
Pis diagonaliza
eigenvalue 
so is Pr, by 
A1 = 1 has algebraic 
4.4. 
Therefore, 
Exercise 
41 in Section 
the eigenvalue 
em. 
Theor
multiplicity 
1, by the Diagonalization 
oo, pk approaches 
n X n transition 
n X n matrix L wh
Let P be a 
an 
regular 
matrix. 
Then as 
k ---+ 
ose columns 
x. 
equal to 
the same 
each 
are identical, 
vector 
This vector 
xis a stead
y state 
bility 
vector 
for P. 
proba
consider 
only 
y the proof
Proof To simplif
where P is diagonaliz­
the case 
, we will 
however, 
is true, 
without 
mption. 
able. The theorem 
this assu
Pas Q-1PQ =Dor, equivale
-1, where 
We diagonalize 
P = QDQ
ntly, 
0 
oo, A7 approaches 
, n. It follows 
satisfies 
is 1 or 
4.30 
Theorems 
From 
that each 
and 4.31, we know 
A; either 
eigenvalue 
that Dk ap­
IA;I < 1. Hence, 
ask---+ 
1or0 for i = 1, ... 
proaches 
l entries 
diagona
of whose 
a diagona
l matrix-say, D*-each 
is 1or0. Thus, 
pk= QDkQ-1 approaches 
*Q-1. We write 
L = QD
lim 
pk = L 
ve that 
Obser
with 
liberties 
some 
We are taking 
pk = lim 
ppk = lim 
pk+i = L 
PL = P lim 
of a limit. 
Neverth
the notion 
eless, 
be intuitively 
these 
should 
steps 
ing to A1 = 1. To see 
of P correspond
of Lis an eigen
Therefore, 
each 
vector 
column 
clear
. Rigorous 
proofs follow from 
n 1 s, 
matri
(i.e., 
that each 
of these 
columns 
is a probability vector 
L is a stochastic 
x), we 
oflimits, 
the properties 
you 
which 
encountered 
may have 
in a calcu­
only 
obser
need 
ve that, if 
j is the row vector 
with 
then 
lus course. 
Rather than 
get side­
jL = j lim 
pk = lim jPk = lim j = j 
tracked 
with 
a discussion 
of matrix 
limits, 
omit the 
we will 
proof
s. 
pk is a stochastic 
since 
matrix, 
13 in Section 
ise 14 in Section 
by Exerc
3.7. Exercise 
3.7 
now implies 
that L is stochastic. 
We need 
only 
show 
that the columns 
of L are identical. 
of L is 
The ith column 
ith standard 
e; is the 
where 
just Le;, 
be eigenve
basis 
vector
. Let v1, v2, ... , vn 
of 
ctors 
P forming 
!Rn, with 
v1 correspond
ing to A1 = 1. Write 
a basis of 
c1, c2, .•. , cw Then, 
for scalars 
em 4.19, 
by Theor
oo, for j -=F 1. It follows 
Aj -=F 1 for j -=F 1, so, by Theor
4.32, 
By Lemma 
em 4.3l(b), 
AJ ---+ 0 as k ---+ 
that 
Le; = lim 
Pke; = c1v1 

j -=F 1. Hence, 

IAjl < 1 for 

k---+oc· 

k---+'X! 

k---+'X! 

k---+oc, 

k---+oc· 

3 2 9  

sition 

long range tran­

g state 

were to continue 
startin

4.6 Applications 
Section 
and the 
Perron-
Froben
ius Theorem 
onding 
corresp
vector 
column 
words, 
i of Lis an eigen
to A1 = 1. But we have 
In other 
the columns 
of L are proba
bility 
x of 
vectors, 
so Le; 
is the 
unique multiple
shown that 
components sum to 1. Since 
that 
this 
is true 
for each column 
of L, it implies 
v1 whose 
all of the columns 
vector 
of L are identica
l, each 
x. 
to this 
equal 
matrix, 
Remark Since 
L is a stochastic 
we can interpret 
it as the 
matrix of the Markov 
bility 
the proba
That is, Lij represents 
chain. 
of being in 
started 
i, having 
state 
indefinitely. The 
from state j, if the transitions 
says 
fact that the columns 
that 
the 
of L are identical 
does not matter, as 
the next 
example 
illustrate
s. 
Recall the rat in a box from 
Example 
matri
x was 
3.65. The transi
tion 
y state 
We determined 
probability 
that the stead
was 
vector 
[� � �i -[0.250 0.250 
Hence, 
of P approach 
the powers 
L = 8  � 8  -
�  �  � 0.375 0.375 
from which 
we can see that 
the rat will 
lly spend 
25% of its time 
in compart­
ment 1 and 
3 7 .5% of its time 
in each 
of the 
other 
two compartmen
ts. 
Markov 
chains 
of regular 
our discussion 
We conclude 
that the steady 
by pr
oving 
x is indepen
is easily adapted 
of the initial 
dent 
vector 
state 
to cover 
state. 
The proof 
constan
the case 
of sta
te vectors 
whose 
components sum to an arbitrary 
t-say, s. In 
the exercises, 
you are asked 
to prove 
some other 
properties 
chains. 
of regular 
Markov 
tion 
x the steady state probability 
P be a regular 
Let 
n X n transi
vector 
matri
x, with 
Theor
probability 
for P, as in 
x0, the sequence 
vector 
em 4.33. Then, 
for any initial 
of iterates 
x. 
xk approaches 
Proof Let 

0.250] 0.375 

eventua

0.375 0.375 

0.375 

Example 4 . 3 8  

Theorem 4 . 3 4  

3 3 0   Chapter 

k-toc-

k-'>GO 

k-tGO 

k--+oo 

Pooulalion 
G rowlh 

4 Eigenvalues 
and Eigenve
ctors 
x1 + x2 + · · · + Xn = 1 .  Since 
xk = Pkx0, we must 
lim Pkx0 = x. Now, 
that 
show 
where 
is L = [x x · · · x] and lim pk= L. 
matrix 
transition 
em 4.33, the long 
by Theor
range 
( lim pk)x0 = Lx0 
Therefore, 
lim pkx0 = 
= X1X + X2X + ' ' ' 
+ XnX 
+ Xn)x = X 
= (xl + Xz + · · · 
We return 
growth, which 
explored 
model of population 
to the Leslie 
we first 
in 
r � [�s L �] 
Section 
matri
3.67 
3.7. In Example 
in that 
section, 
we saw that for the Leslie 
x 
a multiple 
of the vector 
of the population 
iterates 
began 
to approach 
vectors 
words
, the three 
In other 
up in the 
ended 
population 
of this 
age classes 
eventually 
once this 
ratio 
1 8 :  6 :  1 . Moreover, 
state 
is reached, 
it is stable, since 
the ra
tios 
for the 
year are 
following 
by 
given 
IF [�5 4 0 0.25  l.5x 
and the compon
ents are 
still 
27: 9 :  1.5 = 
in the ratio 
1 .  Obser
ve that 
1.5 repre­
y state. 
it has reached its 
sents the 
population 
rate of this 
stead
when 
xis an eigenve
recognize 
We can now 
that 
ctor 
to the eigen­
of L corresponding 
growth 
rate 
value 
,\ = 1.5. Thus, the steady state 
of L, and an 
is a positive eigenvalue 
corresponding 
to this eigenvalue 
the 
eigen
vector 
represents 
relative sizes 
of the age 
can compute these 
the steady state 
classes when 
tly, 
direc
has been reached. We 
with­
as we did before. 
to iterate 
out having 
between 
and the correspond
ing ratios 
Find 
the steady state growth 
the age clas
ses 
rate 
above. 
for the Leslie 
matrix L 
ctors 
and corresponding 
eigenve
eigenvalues 
of 
Solution We need 
to find all positive 
of L is 
L. The characteristic 
polynomial 
det(L -AI) = 0.5 -A 0 0 0.25 - ,\  
- ,\3 + 2 ,\  + 0.375 

growth 

- ,\   4  3 

1 8 : 6 :  

Example 4 . 3 9  

3 3 1  

to A � 1.5, it "ti'fi" 

Section 
4.6 Applications 
and the 
Perron-
Froben
ius Theorem 
solve 
-,\3 + 2,\ + 0.375 = 0 or, equivale
so we must 
ntly, 
8,\3 -16,\ - 3 = 0. Factor­
we have (2,\ - 3)(4,\2 + 6,\ + 1) = 0 
ing, 
(- 3  + Vs)/4 =  -0.19 
roots 
has only the 
the second 
D.) Since 
(See 
Appendix 
factor 
(- 3  - Vs)/4 =  -1.31, the only 
is A =�= 1.5. 
of this 
positive 
and 
root 
equation 
The correspond
ing eigenve
ctors 
are in 
the null 
space of 
L -l .5I, which 
by 
we find 
row reduct
ion: [-1.5 
[L -1.5IIOJ = �-5 4 -1.5 0.25 3 0 -1.5 
0 1 0 
-18 OJ -6 0 0 0 
if x � [ :: ] i"n eigenvecto
x, � I Bx, 
c '°""ponding 
Thu,, 
and 
x2 = 6x3• That is, 
the stead
Hence, 
growth 
is 1.5, and when 
y state 
rate 
this 
rate 
has been reached, 
the age 
18: 6 :  1, as we saw before. 
classes are in 
the ratio 
for the steady state 
one candidate 
In Example 
4.39, there was only 
rate: the 
growth 
eigenvalue 
more than 
done 
of L. But what would 
unique positive 
we have 
if L had had 
that 
also apparently 
or none? 
We were 
one positive 
eigenvalue 
was a 
there 
fortunate 
correspond
ing eigenve
ents 
were 
positive, which 
allowed 
us 
ctor 
all of whose compon
to relate 
these 
the size 
components to 
of the 
population. 
We can prove 
that this 
situ­
matrix has exactly 
ental; 
that 
ation is not accid
eigenvalue 
one positive 
is, 
every Leslie 
positive 
componen
and a corresponding 
eigenve
ctor 
ts. 
with 
Recall that the form of a Leslie 
matrix is 
51 0 0 0  0 
0 52 0 0 0 
0 0 53  0 0 
0 0 0 Sn-I 0 
sj repres
ent survival 
Since 
the entries 
are all 
probabilities, 
we will 
assume 
that they 
also 
rapidly 
would 
(otherwise, 
nonzero 
that 
will 
the population 
die out). We 
assume 
one of 
b; is nonzero 
at least 
the birth parameters 
(otherwise, there 
would 
be no births 
again, 
would 
die out). With 
and, 
the population 
we can 
these 
standing 
assumptions, 
we made 
above 
now prove 
the asser
tion 
as a theorem. 
has a unique 
Leslie 
Every 
eigen­
and a corresponding 
eigenvalue 
matrix 
positive 
vector 
with 
positive 
components. 

b1 b2 b3  bn-1 bn 

L =  

(3) 

Theorem 4 . 3 5  

3 3 2   Chapter 

4 Eigenvalues 

and Eigenve
ctors 
Proof Let L be as in 

Equation 

(3). The characteristic 

polynomial 

of Lis 

= (- l)nj(A) 

the 
of L are therefore 
16.) The eigenvalues 
this 
to prove 
(You 
are asked 
in Exercise 
b; is positive 
the 
parameters 
of f(A). Since 
roots 
at least 
one of the birth 
and all of 
sign 
once. 
the coefficients 
survival 
proba
bilities 
sj are positive, 
of f(A) change 
exactly 
one positive 
f(A) has exactly 
By Descartes's 
Rule 
(Appendix D), therefore, 
of Signs 
Let us call 
root. 
it A 1. By direct 
an eigenve
correspond
that 
ctor 
calcula
tion, 
we can check 
ing to A1 is 
S1S2 I Af 

s1/ A1 

S1S2S3/Ai 

two consecu

all of the components 
to prove 
are asked 
this 
(You 
in Exercise 
18.) Clearly, 
of x1 are 
positive. 
l requiremen
t that 
With 
more is true. 
In fact, 
the additiona
tive birth 
ue positive 
parameters 
b; and 
b;+i are positive, 
it turns 
out that the uniq
eigenvalue 
A1 of Lis dominant; that is, every 
other 
(real or complex) 
A of L satisfies 
eigenvalue 
IAI < A1. (It is beyond the scope 
proof 
but a partial 
this 
book to prove 
of this 
is 
result, 
familiar 
ise 
outlined 
of com
plex 
in Exerc
27 for readers 
who are 
the algebra 
num­
with 
bers.) This 
explains 
why we get convergence 
to a stead
y state 
vector 
we iterate 
when 
the population 
vectors: 
It is just 
the power method 
for us! 
working 
and Leslie 
Markov 
two applica
In the previous 
matrices, 
chains 
tions, 
we saw that 
the 
eigenvalue 
of interest 
was positive 
and dominant. 
Moreover, 
there 
was a correspond­
ing eigenve
ctor 
with 
positive 
componen
ts. It turns out 
em 
theor
that a remarkable 
guarante
will 
be the case 
for a large 
class 
of 
of matrices, 
including many 
es that this 
theor
we have been 
The first version 
considering. 
of this 
the ones 
em is for positive 
Oskar Perron 
(1880-1975) 
was a 
matrices
. First, 
mathematician 
who did 
German 
and notation. 
Let's 
as 
we need 
some 
terminology 
to a vector 
agree 
to refer 
fields 
work 
in many 
of mathemat­
ics, 
including 
analysis, 
differential 
positive if all of its components 
are positive. For two 
m X n matrices 
A = [aii] and 
equations, 
geome
try, 
and 
algebra, 
i and 
j. (Similar 
A 2: B if aii 2: bij for all 
definitions 
will 
apply 
B = [bii], we will 
write 
number theory. Perron
's Theorem 
Thus, 
for A > B, A :s B, and so 
a positive 
on.) 
x satisfies 
vector 
x > 0. Let us define 
in 1907 in a paper 
was published 
IAI = [ la;jll to be the matri
x of the absolute 
values 
of the 
entries 
of A. 
on continued 
fractions. 

The Perro n-Frobenius 

Theorem 

Theorem 4 . 3 6  

3 3 3  

IAI :::; A1. 

Section 
4.6 Applications 
ius Theorem 
Froben
Perron-
and the 
Perron's Theorem Let 
A 1 with 
the following 
n X n matrix. Then 
A be a positive 
eigenvalue 
A has a real 
properties: 
a. A1 > 0 
b. A1 has a 
ing positive 
eigen
correspond
vector. 
value 
eigen
c. If A is any other 
of A, then 
two statements 
Intuit
ively, 
we can see why the first 
should 
be true. 
Consider 
the case 
A. The corresponding 
matri
matrix 
of a 2 X  2 positive 
tion 
maps the first 
x transforma
itself
into 
of the plane 
quadrant 
are positive. If we re­
properly 
, since 
all components 
peatedly 
necess
allow 
A to act on the 
images we get, they 
arily 
converge 
toward 
some 
ray in the first 
(Figure 
quadrant 
4.18). A direction 
vector 
for this 
ray will 
be a positive 
vector 
A1), since 
of itself 
some 
be mapped into 
x, which 
(say, 
must 
positive multiple 
x, Ax 2 Ax for some 
A1 both 
, Ax= A1x, with 
words
leaves 
the ray fixed. In other 
positive. 
x and 
A(kx) 2 A(kx) for all 
Proof For some 
nonzer
o vectors 
scalar 
happens, 
A. When 
this 
then 
k > O; thus, 
we need only 
x. In 
consider 
unit vectors 
the set of all unit 
vectors 
7, we will 
Chapter 
in !Rn (the 
see that 
A maps 
that Ax 2 Ax. (See 
vectors 
over 
on this 
into a 
"generalized 
ellipsoid:' 
the nonnega
tive 
unit 
So, as x ranges 
sphere, 
there 
will 
be a maximum 
value 
of A such 
Figure 
Denote this 
number by A1 and the 
ing unit 
correspond
by x1. 
vector 

unit sphere) 
4.19.) 

A 

y 

y 

y 

y 

Figure 4 . 1 8  

y 

Figure 4 . 1 9  

3 3 4   Chapter 

3.7 

(4) 
40.) 

4 Eigenvalues 
and Eigenve
ctors 
> A1x1, 
A again, 
and, 
we 
then Ax1 
Ax1 = A1x1. 
that 
We now show 
If not, 
applying 
obtain 
40 and Section 
Exercise 
since 
is preserved, 
the inequality 
where 
A is positive. (See 
Ay > A1y, so 
vector 
Exerc
ise 
36.) But then 
y = (
that satisfies 
l/llAx1 ll)Ax1 is a unit 
there 
will 
be some 
A2 > A1 such 
that Ay 2: A2y. This 
contradicts 
the fact that 
A1 was 
with 
the maximum 
uently, 
be the case 
it must 
rty. 
value 
prope
this 
Conseq
that 
Ax1 = 
A1 is an eigenvalue 
A1x1; that is, 
of A. 
Now A is positive 
and x1 is 
positive, 
so A1x1 = Ax1 > 0. This means 
A1 > 0 
that 
and x1 > 0, which 
completes 
the proof 
of (a) and (b). 
of A with 
(real or complex) 
suppose A is any other 
To prove 
eigenvalue 
(c), 
cor­
z. Then 
responding 
Az = Az, and, 
eigenvector 
taking 
s, we have 
absolute 
value
Exercise 
Inequality. (See 
follows 
where the 
middle 
inequality 
from the Triangle 
of lzl is also 
Since 
lzl > 0, the unit 
vector 
positive 
and satisfies 
u in the direction 
Au 2: IAlu. 
part of this 
proof
, we 
By the maximality
ofAJrom the first 
must 
have 
IAI::::: A1. 
so IAI 
It turns 
out that A1 is dominant, 
more is true. 
In fact, 
< A1 for any eigenvalue 
multiplicity 
raic, 
A i= A1. It is also the case 
that A1 has algeb
and hence 
geometric, 
We will 
facts. 
not prove 
these 
Perron
's Theor
from positive 
to certain 
nonnega
matri­
tive 
em can be generalized 
l condition 
requires 
Frobenius 
did so in 1912. The result 
ces. 
on the matrix. 
a technica
reducible if, subject to 
matri
A square 
x A is called 
tion 
and 
some 
permuta
of the rows 
the same 
tion 
of the col
umns, 
A can be written 
form as 
in block 
permuta
[� �] 
tion 
some 
A is reducible 
if there is 
D are square. 
Equivalently, 
where 
B and 
permuta
matrix P such 
that 
PAPT = [� �] 
the matrix 
(See 
page 187.) For example, 
A= 1 2 7 3 0 
rows 
1 and 
since 
interchanging 
�----�-J __ � ____ ?. ____ ?. 
0  0 i 2  1  3 
0  0!6  2  1 
0  0!1  7  2 

is reducible, 
1 and 

columns 
3 produces 

3 and then 

2 0  0 3 
4 2 1 5  5 

7 2:1  3  0 

0  0 7 2 

6 0 0 2 

1. 

Section 

4.6 Applications 

and the 
Perron-
Froben

ius Theorem 

3 3 5  

(This is just PAPT, where 

0  0 1 0  0 
0 1 0  0  0 

P =  1 0  0 0  0 

0  0  0 0 
0 0  0 0 

Theorem 4 . 31  The Perron-Frobenius Theorem 

A1 

�  Check this!) A square 
A k > 0 for some 
is called 
is not reducible 
matri
irreducible. If 
x A that 
k, then 
A is called 
regular 
primitive. For example, 
every 
tive 
Markov 
chain 
has a primi
__..  irred
transition 
matrix, by definiti
on. It is not hard 
primitive 
to show 
that every 
x is 
matri
ucible. 
(Do you see why? 
the contra
Try showing 
positive 
of this.) 
eigenvalue 
Then 
n X n matrix. 
tive 
ucible 
nonnega
Let 
A be an irred
A has a real 
with 
: 
the following 
properties
a. A1> 0  b. A1 has a correspond
IAI ::=::: A1. If A is primi
eigen
ing positive 
vector. 
then this 
tive, 
of A, then 
eigenvalue 
c. If A is any other 
IAI = ,\ 1, then 
is strict. 
inequality 
A is a (complex) 
root 
d. If,\ is an eigen
value 
of A such 
of the 
that 
,\ n - ,\� = 0. 
equation 
ity 
multiplic
e. A1 has algebraic 
of the Perron-Frobenius 
Theor
can find a proof 
em in many 
The interested 
reader 
is. The eigenvalue 
tive 
texts 
on nonnega
matrices 
or matrix analys
A1 is often called the 
a corresponding 
necess
Perron root of A, and 
ctor 
probability eigenve
arily 
(which is 
(Cambridge, 
the 
unique) 
is called 
Perron eigenvector of A. 
University 
13, 21, ... , 
Fibonacci numbers are the numbers in the sequence 
The 
where, 
each 
after the first 
two terms, 
new term 
the two terms 
is obtained 
by summing 
sequence 
number by fn, then this 
prece
ding 
is com­
it. If 
the nth Fibonacci 
we denote 
by the equations 
Jo = 0, f1 = 1, and, 
pletely defined 
for n 2: 2, 
This 
return 
We will 
relation. 
recurrence 
of a linear 
is an example 
last 
to the 
equation 
consider 
Fibonacci 
numbers, 
but first 
we will 
linear 
recurrence 
relations 
somewhat 
more generally. 

Jn = fn-1 + fn-2 

2, 3, 5, 8, 

0, 1, 1, 

Recurrence 

Relalions 

linear 

1. 

Matrix 

See 
Analysis by R. A. Horn 
and C.R. Johnson 
Cambridge 
England: 
Press, 
1985). 

3 3 6   Chapter 

initial 

Fibonacci 

recurrence 

relation of 

4 Eigenvalues 
and Eigenve
ctors 
, 
Fibonacci
Leonardo of Pisa 
left, 
0), pictured 
(1170-125
known by 
his nickname, 
is better 
He wrote a 
many 
of which 
of Bonaccio:' 
means "son 
which 
number of important 
books, 
have 
appears 
including 
survived, 
Liber abaci and 
sequence 
Liber quadratorum. The Fibonacci 
"A certain 
in Liber abaci: 
in a place 
man put a pair of rabbits 
to a problem 
as the solution 
surrounded 
can be produced 
from that 
How many 
pairs of rabbits 
on all sides 
by a 
wall. 
month each 
in a year 
pair 
a new pair which 
from the 
if it is supposed 
that 
pair begets 
every 
second month on becomes 
productive
?" The name 
numbers was given 
to the terms 
Edouard 
of this 
sequence 
by the French 
mathematician 
Lucas 
(1842-1891). 
of numbers that is defined 
(xn) = (x0, x1, x2, . . .  ) be a sequence 
Defi n ition Let 
as follows
: 
a0, a1, . . .  , ak-l are scalars. 
1. x0 = a0, x1 = a1, . . .  , xk-l = ak_1, where 
c1, c2, .•. , ck are 
+ ckxn-k' where 
2. For all 
n 2: k, Xn = c1xn-l + c2xn_2 + · · · 
scalars. 
order k. The 
a linear 
in (2) is called 
If ck * 0, the equation 
conditions of the recurrence. 
in ( 1) are referred 
to as the 
equations 
of order 
numbers satisfy a linear 
recurrence 
the Fibonacci 
Thus, 
relation 
• If, in order 
relation, 
the nth term 
to define 
in a recurrence 
we require 
the 
• The number of initial 
(n -k)th 
term 
before it, then 
but no term 
relation 
k. 
the recurrence 
has order 
• It is not necess
is the order 
of the recurrence 
relation. 
conditions 
ary that the first 
be called 
term 
x0. We could 
of the sequence 
• It is possi
start 
at x1 or anywhere 
else. 
linear 
recurrence 
even 
ble to have 
more general 
relations 
by allowing 
the coefficients 
C; to be functions rather 
than 
scalars 
an ex
isola
tra, 
ted 
and by allowing 
be the recurrence 
An example would 
be a function. 
which 
coefficient, 
may also 
such 
We will 
not consider 
recurrences here. 
x1 = 1, x2 = 5 and the 
Consider 
the sequence 
(xn) defined 
conditions 
by the initial 
xn = 5xn-l -6xn-l for n 2: 2. Write 
of this 
five terms 
out the first 
recurrence relation 
sequence. 
We use the 
recurrence 
Solulion We are given 
the first 
two terms. 
relation 
to calcu­
three 
late 
the next 
terms. We have 
1, 5 ,  19, 65, 2 1 1, .... 
begins 
so the sequence 

X3 = 5X2 - 6X1 = 5 • 5 - 6 • 1 = 19 
X4 = 5X3 - 6X2 = 5 . 19 - 6 . 5 = 65 
X5 = 5X4 - 6X3 = 5 · 65 -6 · 19 = 2 1 1  

2. 

Remarks 

Example 4 . 4 0  

3 3 1  

4.40. 

Section 
and the 
Perron-
Froben
ius Theorem 
4.6 Applications 
Exam­
of the sequence in 
interested 
were 
ly, if we 
Clear
in, say, the lOOth term 
the approach 
rather 
tedious, 
since 
ple 
4.40, then 
used 
have 
there would be 
we would 
of n. We refer 
98 times. 
It would be nice 
if we could find 
the recurrence 
to apply 
relation 
an explicit 
to finding 
such 
for Xn as a function 
formula 
as solving the re­
a formula 
illustrate 
currence 
relation. 
We will 
the process 
the sequence 
with 
from Example 
To begin, 
we rewrite 
the recurrence 
relation 
as a matrix equation. 
Let 
A = [� -�J 
xn = [x::J for n 2: 2. Thus, 
and introduce 
vectors 
x2 = [::J [�l X3 = [::J = 
[ 1 � l x4 = [:: J = [ �: l and so on. Now obser
for n 2: 2, we have 
ve that, 
AXn-i = [5 -6J [Xn-IJ = [5Xn-I -6xn-2J = [ Xn J = Xn 
1 0 Xn-2 Xn-1 Xn-1 
ntered 
we encou
Notice 
that this 
is the same 
type 
of equation 
with 
and 
Markov 
we can write 
those 
Leslie 
cases, 
. As in 
matrices
of Example 
We now use the technique 
4.29 to com
pute 
of A. 
the powers 
The characteristic 
of A is A2 -5,\ + 6 = 0 
equation 
we find 
that the 
from which 
that the 
eigenvalues 
are ,\1 = 3 and ,\2 = 2. (Notice 
form 
as Xn -5xn-I + 6xn-z = 0, it is apparent 
that of the recurrence 
If we write 
characteristic 
of the 
follows 
relation. 
equation 
the 
recurrence 
are exactly 
that the 
coefficients 
eigensp
aces 
the same!) The corresp
are 
onding 
E3 = span( [ �]) and E2 = span( [ �]) 
P-1AP = D = [� �J. ThenA = PDP-1 and 
P = [� �J, we knowthat
Setting
[3 2J [3k OJ [3 2J-1 
Ak = PDkp-1 = 
1  1 0 2k 1  1 
[3 2J[3k OJ[ 1 -2J 
1  1 0 2k - 1  3 
-2(3k+I) + 3(2k+1)J -2(3k) + 3(2k) 
It now follows 

chains 

that 

3c1 + 2c2 =  1 
9c1 + 4c2 = 5 

5 = X2 = C132 + C222 = 9c1 + 4c2 

1  = x1 = c131 + c221 = 3c1 + 2c2 

3 3 8   Chapter 

and Eigenve
ctors 
4 Eigenvalues 
xn = 3n -2n. (To check 
we could 
our work, 
off the solution 
�  from which 
we read 
plug 
that we 
the same terms 
this formula 
y that 
inn = 1, 2, . . .  , 5 to verif
calculated 
gives 
recurrence 
relation. 
using the 
Try it!) 
of powers 
combination 
ues. 
Obser
of the eigenval
ve that Xn is a linear 
This is 
nec­
as the eigenvalues 
as long 
essarily 
are distinct 
the case 
[as Theorem 
4.38(a) will 
make 
ourselves 
some 
we can save 
Once 
explici
t]. Using 
this 
observation, 
work. 
we have 
tely 
computed 
,\1 = 3 and ,\2 = 2, we can immedia
write 
the eigenvalues 
we have 
Using the initial 
condi
be determined. 
c1 and 
where 
tions, 
c2 are to 
= 1 and 
when n 
system 
= 2. We now solve the 
when n 
xn = 3n -2n, as before. 
c2 to obtain 
c2 = - 1. Thus, 
c1 = 1 and 
for c1 and 
its use to find an 
use in practice. 
This 
is the method 
we will 
We now illustrate 
numbers. 
explicit 
formula 
for the Fibonacci 
Solve the Fibonacci 
ef0 = O,f1 = 1, andfn = fn-l + fn-2 for n 2: 2. 
recurrenc
as fn -fn-l -fn-2 = 0, we see that 
the charac
­
teris
Solulion Writing 
the recurrence 
tic equation 
eigenvalues 
is ,\ 2 -,\ - 1 = 0, so the 
are 
A1 = ----
A2 =---
2  and 1  -Vs 
has 
relation 
to the recurrence 
solution 
that the 
from the 
discussion 
It follows 
above 
the form 
Binet 
made 
(1786-1856) 
Jacques 
contributions 
, 
to matrix 
theory
c1 and 
scalars 
for some 
number 
theory
and as­
, physics, 
c2. Using 
the initial 
conditions, 
we find 
trono
my. He discovered 
the rule 
0 = fo = C1A� + C2A� = C1 + C2 
multiplication 
in 1812. 
for matrix 
Binet
's formula 
for the Fibonacci 
numbers 
is actually 
due to Euler, 
and 
who published 
it in 1765; how­
ever, 
it was forgotten 
until 
Binet 
in 1843. 
published 
his version 
X abdicated 
Solving 
c1 =  1 /Vs and 
for c1 and 
c2, we obtain 
number is - 1/Vs. Hence, 
an explicit 
Like 
Cauchy, Binet 
was a royalist, 
for the 
formula 
nth Fibonacci 
his university 
and he lost 
posi­
_ l (1 + vs)n -
_ l (1  - vs)n 
tion when 
Charles 
in 
1830. He recei
honors 
ved many 
his elec­
for his work, 
including 
(5) 4 
tion, 
in 1843, to the 
des 
Academie 
Jn Vs  2 
Sciences. 

Vs  2 

Example 4 . 4 1  

1  +Vs 

c2 

= 

2 

Theorem 4 . 3 8  

3 3 9  

irra­

Section 
4.6 Applications 
and the 
Perron-
Froben
ius Theorem 
formula, 
(5) is a remarkable 
because it is defined 
Formula 
in terms 
of the 
tional number Vs yet the Fibonacci numbers are all intege
rs! Try plugging 
in a few 
for n to see how the 
Vs terms 
cancel 
out to leave the integer 
values 
values 
fn-Formula 
as Binet's formula. 
(5) is known 
linear 
just outlined 
for any second 
works 
order 
The method 
we have 
recur­
rence 
relation 
whose associ
ated eigenvalues 
When 
are all distinct. 
there is 
a repeated 
eigenvalue, 
be modified, 
we 
lization 
method 
the techniq
ue must 
since 
the diagona
. 
both situations
theor
may no longer 
work. The next 
used 
em summarizes 
recurrence 
Let X
n = axn-l + bxn-z be a 
that is satisfied 
relation 
sequence 
by a 
(xn). 
,\2 -
Let ,\1 and ,\2 be the 
equation 
iated 
of the assoc
eigenvalues 
characteristic 
a,\ -b = 0. 
a. If A1 * A2, then 
Xn = c1A7 + c2A� for some 
c1 and c2. 
scalars 
Xn = c1An + c2n,\n for some 
c1 and c
2• 
b. If A
scalars 
1 = A2 =A, then 
determined 
the initia
case, 
c1 and c2 can be 
In either 
l condi
tions. 
using 
Proof (a) Generalizing 
above, 
the recurrence 
our discussion 
we can write 
as xn = 
Axn_ 1, where 
[ Xn ] x = n Xn-1 and A= [� �] 
diagonalized. 
distinct 
eigenval
Since 
A has 
The rest 
ues, 
of the details 
are left 
it can be 
for Exercise 
showthatxn = c1,\n + c2n,\n satisfies 
53. (b) We will 
Xn = axn-l + 
relation 
bxn-z or, equivalently, 
if ,\2 -a,\ -b = 0. Since 
( 6) yields 
into 
substi
tution 
Equation 
Xn -axn-I -bxn-2 = (c1An + c2n,\n) -a(c1An-I + c2(n -l),\n-l) 
-b(c1An-z + c2(n -2),\n-2) 
= C1(An -a,\n-1 -b,\n-2) + C2(n,\n - a(n -l)An-1 
- b(n -2),\ n-Z) 
= c1,\n-2(,\2 -a,\ -b) + c2n,\n-2(,\2 -a,\ -b) + c2An-2(a,\ + 2b) 
= c1An-2(0) + c2n,\n-2(0) + c2,\n-2(a,\ 
+ 2b) 
= c2A n-2(a,\ + 2b) 
of,\ 2 -a,\ -b = 0, we must 
a2 + 4b = 0 and ,\ = 
But, 
root 
have 
,\ is a double 
since 
2/2 + 2b = 
a/2, using 
formula. 
the quadratic 
Consequentl
-4b/2 + 
so 
2b = 0, 

the recurrence 

y, a,\+ 2b = a

(6) 

3 4 0   Chapter 

4 Eigenvalues 

Example 4 . 4 2  

Theorem 4 . 3 9  

and Eigenve
ctors 
(a) or (b) there 
l conditions 
are 
the initia
Suppose 
in either 
x0 = rand x1 = s. Then, 
is a unique 
solution for c1 and 
Exercise 
c2. (See 
54.) 
Solve the recurrence 
x0 = 1, x1 = 6, and Xn = 6xn-l -9xn-l for n 2 2. 
relation 
has 
equation 
is A 2 -6,\ + 9 = 0, which 
Solulion The characteristic 
A = 3 as a 
Xn = c13n + c2n3n = (c1 + c2n)3n. 
double 
root. 
em 4.38(
By Theor
b), we must 
have 
c2 = 1, so 
6 = x1 = (c1 + c2)3, we find that 
1 = x0 = c1 and 
Since 
can be 
The techniques 
outlined 
order recurrence 
to higher 
extended 
em 4.38 
in Theor
. We state, 
of, the gene
relations
without pro
ral result. 
Let Xn = am-iXn-l + am-lXn-l + · · · + a0Xn-m be a recurrence 
relation 
of order 
(xn). Suppose the associa
m that is satisfied 
by a 
sequence 
ted characteristic 
polynomial 
as (,\ - A1)m1(A - A2)m2 • • • (,\ - ,\k)m', where 
m1 + m2 + · · · + mk = m. 
factors 
X _  ( \ n + \ n + 2 \ n + . . .  +  m1 -I \ n) + . . .  
Then 
xn has the form 
+ ckm,nm,-IAJ:) 
In calcu
lus, you learn 
that 
if x = x(t) is a differentiable 
function 
a differ­
satisfying 
entia
l equation 
x' = kx, where 
nt, then 
solution 
the general 
of the form 
k is a consta
x(O) = x0 is specified, 
If an initial cond
C is a constant. 
is x = Cekt, where 
then, 
ition 
solution, 
t = 0 in the general 
by substi
tuting 
we find that 
the unique 
C = x0• Hence, 
x = xoekt 
to the differential 
equation 
solution 
the initial 
condition 
is 
satisfies 
that 
oft-say, x1, x2, ••• , Xn-that satisfy a 
Suppose 
functions 
n differentiable 
we have 
x{ = a11x1 + a12x2 + · · · 
[x{(t) l x'(t) = x��t) , 
system 
this 
in matrix form 
as x' = Ax, where 
x�(t) and 
us find the 
Now we can use matrix 
to help 
solution. 

We can write 

methods 

n -C111t1 C12n1t1 C13n fl1  C1m1n fl J  

+ (ck1AJ: + ck2nAJ: + ck3n2AJ: + · · · 

x� = az1X1 + az2X2 + · · · 

+ a1nxn 
+ aznXn 

Differenlial 

svs1ems of Linear 

system of differential 

Equalions 

equations 

Example 4 . 4 3  

3 4 1  

2 

x; = 5X

X1 = C1e21 
Xz = C2est 

the coefficient 

Section 
4.6 Applications 
and the 
Perron-
Froben
ius Theorem 
sys­
the following 
to solve 
Suppose we want 
a useful observation. 
First, 
we make 
: 
equations
tem of differential 
X{ = 2X1 
can be solved 
Each 
equation 
separately, 
as above, 
to give 
in matrix 
C2 are consta
Notice 
that, 
nts. 
where 
C1 and 
form, 
our equation 
x' = Ax has 
matrix 
a diagonal coefficient 
A = [� �] 
2 and 
in the exp
5 occur 
and the eigenvalues 
onentials e21 and 
e51 of the solution. This 
start 
that, 
suggests 
system, 
we should 
for an arbitrary 
by diagonalizing 
matrix, if 
possible. 
: 
Solve the following 
of differential 
system 
equations
x is A = [ 1 2], and we find 
3 2 [2] [-1] 
coefficient 
Solution Here the 
matri
that the 
eigenvalues 
v2 = 1 , 
,\2 = -1, with 
,\1 = 4 and 
are 
eigenve
corresp
onding 
ctors 
v1 = 3 and 
and the 
A is diagonalizable, 
the job is 
Therefore, 
x P that does 
respect
matri
ively. 
that P =  [v1 v2] = [� -�] 
We know 
P-1AP = [4 OJ = D 
0 -1 
these 
Let 
x = Py (so that x' = Py') and substi
tute 
results 
y, 
Py' = APy or, equivalentl
x' = Ax to get 
This 
is just the system 
whose general solution is 

original 

equation 

into the 

y' = P-1APy = Dy 

Y1 = C1e41 

Y2 = C2e-t or 

3 4 2   Chapter 

4 Eigenvalues 
ctors 
and Eigenve
compute 
x, we just 
To find 
x = Py = [� 
values 
that these 
x2 = 3C1e41 + C2e-t. (Check 
so x1 = 2C1e4t - C2e-t and 
satisfy the 
.) 
given 
system
in Example 
the so
that we could 
Remark Observe 
4.43 as 
lution 
also 
express 
I 3  2 l  1 1  2  2 
1[2] + C e-1[- 1] = C e4
easily to n X n systems where 
the coefficient 
This 
techniq
matrix is 
ue generalizes 
diagonalizable. 
the 
summarizes 
is left as an exercise, 
theorem, 
The next 
whose proof 
tion. 
situa
A be an n X n diagonalizable 
P = [ v1 v2 • • • v"] be such that 
and let 
matrix 
Let 

x = C e4

1v  + C e

-1v 

Theorem 4 . 4 0  

0 

x' = Ax is 
Then 
the general solution 
to the system 
x = C1eA1tv1 + C2eAirv2 + · · · + C"eA"1V" 
model in which 
The next 
involves a 
example 
live 
biological 
two species 
in the 
that the growth 
rate 
to assume 
species 
It is reaso
ystem. 
same ecos
nable 
of each 
de­
there 
both populations
pends on the 
sizes of 
. (Of course, 
govern 
are other 
factors that 
by ignoring 
growth, but we will 
our model simple 
keep 
these.) 
If x1 (t) and 
x2 (t) denote 
the sizes 
of the two populations 
at time 
t, then 
x{(t) and 
t. Our model is of the form 
at time 
rates 
x�(t) are their 
of growth 
d depend 
c, and 
the coefficients 
where 
on the conditions. 
ecosystem 
and compete 
els inhabit the same 
Raccoons 
and squirr
with 
for 
each other 
el populations 
food, 
water, 
Let the raccoon 
and squirr
and space. 
at time 
t years be 
given 
by r (t) and 
of squirrels, 
vely. 
s(t), respecti
In the absence 
the raccoon 
growth 
rate 
is r'(t) = 2.5r(t), but when 
els are present, 
squirr
the competition 
slows 
the rac­
to r'(t) = 2.5r(t) - s(t). The squirrel 
rate 
coon 
is similarly 
population 
affected 
growth 
of raccoons, 
the growth rate 
by the raccoons. 
In the absence 
squirrel population 
of the 
when they 
is s'(t) = 2.5s(t), and the population 
growth 
rate 
for squirrels 
are sharing 
is s'(t) = -0.25r(t) + 2.Ss(t). Suppose that initially 
the ecosystem 
with 
raccoons 

x{(t) = ax1(t) + bx2(t) 
x;(t) = cx1 (t) + dx2(t) 

a, b, 

Example 4 . 4 4  

3 4 3  

v1 = 

x = x(t) = 

x(t) = C e3tv  + C e

Section 
4.6 Applications 
and the 
Perron-
Froben
ius Theorem 
60 squirrels 
. Determine 
what 
60 raccoons 
and 
there 
are 
ecosystem
in the 
to 
happens 
these 
two popula
tions
. 
x' = Ax, where 
Solution Our system is 
A - -0.25 -I.OJ 2.5 
[r(t)] 
s(t)  [ 2.5 and 
ctors 
eigenve
A1 = 3 and 
The eigenvalues 
corresponding 
A2 = 2, with 
of A are 
[ -�] and 
v2 = [ �]. By Theor
em 4.40, the general solution 
is 
to our system 
2tv = C e3r[-2] + C e2r[2] 
1  1 2  2 1 1  2 1 
is x( 0) = [: � � �] = [ :�], so, 
setting 
t = 0 in Equa­
. The initia
l population 
vector 
tion 
(7), we have 
C2 = 45. Hence, 
equation, 
Solving 
this 
C1 = 15 and 
we find 
x(t) = 15e3r[-�] + 45e2t[�] 
we find 
s(t) = 15e31 + 45e21. Figure 
r(t) = - 30e31 + 90e21 and 
from which 
4.20 shows 
�  dies 
the graphs 
of these 
two functions, 
and you can see clearly 
population 
that the raccoon 
out after 
more than 
1 year. 
a little 
out?) 
(Can you determine 
exactly when it dies 
.+ 
is a source of food for 
in which 
We now consider 
one species 
a similar example, 
called 
-prey model. Once 
a predator
a model is 
. Such 
the other
again, 
our model will 
oversimplified 
be drastic
ally 
its main 
s. 
in order 
to illustrate 
feature

(7) 

Figure 4 . 2 0  Raccoon 

and squirrel 

populations 

3 4 4   Chapter 

(8) 

(9) 

x' 

- 1  0 

x'(t) = y(t) 
-x(t) 
y'(t) = 

) -12 
r'(t) = w(t
- r(t) + 10 
w '(t) = 

4 Eigenvalues 
and Eigenve
ctors 
. The robins 
eat the worms, 
and worms 
Example 4 . 4 5  Robins 
cohabit an ecosystem
which 
are their 
at time 
robin and 
source 
only 
of food. The 
worm 
populations 
by 
t years 
are denoted 
r(t) and 
the growth 
governing 
ively, 
w(t), respect
of the two popula­
and the equations 
tions 
are 
the behavior 
occupy the ecosystem, determine 
6 robins 
and 
If initially 
20 worms 
of 
the two populations 
time. 
over 
example 
Solulion The first 
is the presence 
thing 
of the 
we notice 
about this 
extra 
ts, 
constan
- 12 and 
10, in the two equations
. Fortunately, 
rid of them 
we can get 
Ifwe let 
with 
a simple 
change 
bles. 
of varia
r(t) = x(t) + 10 and 
w(t) = y(t) + 12, then 
r'(t) = x'(t) and 
w'(t) = y'(t). Substituting 
into 
Equations 
(8), we have 
=  Ax, where 
to work 
which 
the form 
(9) have 
with. Equations 
is easier 
[ O  1 ] . Our new initial 
conditions 
are 
x(O) = r(O) -10 = 6 -10 = -4 and 
so x(O) = [-:]. 
Proceed
ing as in the last 
example, 
we find the 
eigenvalues 
and eigenve
ctors 
of A. 
is A2 + 1, which 
The characteristic 
polynomial 
has no real 
roots
we do? 
should 
. What 
no choice but 
A1 = i and A2 = - i. The 
We have 
to use the complex 
roots, which are 
complex-n
amely, v1 = [�]and v2 = [ -�l By 
corresponding 
eigenve
are also 
ctors 
Theor
em 4.40, our solution has the form 
x(O) = [-:],we get 
From 
whose so
lution is 
C1 = -2 -4i and C2 = - 2  + 4i. So the solution to 
(9) is 
system 
What 
are we to make 
of this solu
Robins 
and worms 
world­
tion? 
inhabit a real 
yet our 
Euler's 
we apply 
sly proceeding, 
numbers! Fearles
solution involves 
complex 
formula 
t + i sin 
e;1 = cos 

y(O) = w(O) -12 = 20 -12 = 8 

A  = 

t 

Section 

4.6 Applications 

ius Theorem 

and the 
Perron-
Froben
IN.Si\NC.T. 
T\<?sERS AAE 
n. 
�n·tum

3 4 5  

CALV I N  A N D  HOBBES© 1988 Watterson. Reprinted with permission 

o f  U NIVERSAL 

PRESS SYND I CATE. A l l  rights reserved 

-i sin 

-t) + i sin ( 
t. Substi
tuting, 
we have 

-t) = cos t 
e -it = cos ( 
(Appendix C) to get 
x(t) = (-2 -4i)(cost + isint)l�J + (-2 + 4i)(cost -isint)� �J 
[(-2cost + 4sint) + i(-4cost -2sint)] 
(4cost + 2sint) + i(-2cost + 4sint) 
+ l(-2cost + 4sint) + i(4cost + 2sint)J 
_ ( 4 cos t + 2 sin t
) 
) + i (2 cos t 
-4 sin t
] 
= [-4 cos t + 
8 sin t
8 cos t + 4 sin t 
y(t) = 8 cost+ 4 sin 
gives 
t. Putting 
This 
x(t) = -4 cost+ 8 sin t and 
in 
l varia
bles, 
we conclude 
of our origina
that 
terms 
8 sin t + 10 
and r(t) = x(t) + 10 = -4 cost + 
w(t) = y(t) + 12 = 8 cost + 4 
sin t + 12 

everything 

25 

20 

1 5 

10 

5 

0  2  4  6  8  1
Figure 4 . 2 1  

0 12 14  1 6 

Robin 

and worm 

populations 

15 

Figure 4 . 2 2  

30 

3 4 6   Chapter 

4 Eigenvalues 

and Eigenve
ctors 
all! The graphs 
4.21 show 
w(t) in Figure 
So our solution is 
after 
real 
of r(t) and 
that 
increas
As the robin 
oscillate 
the two populations 
lly. 
periodica
population 
es, the 
to decrease, 
worm 
population 
starts 
only 
food source 
but as the robins' 
diminishes, 
as well. As 
the preda
to decline 
their 
tors 
disappear, the worm 
numbers start 
popula­
to recover. 
population, 
tion 
begins 
As its food supply 
es, so does 
increas
the robin 
oscillation 
and the cycle 
repeats itself. 
This 
is typical 
of examples 
in which 
the eigen­
values 
are complex. 
Plotting 
4.22, clearly 
axes, as in 
on separate 
and time 
robin
Figure 
s, worms, 
of the two populations. 
reveals 
the cyclic 
nature 
at what 
from a different 
we have 
by looking 
We conclude 
done 
this 
section 
point 
of view. 
If x = x(t) is a differentia
ble function 
oft, then 
the genera
the 
l solution of 
ordinary 
differential 
equation 
x' = ax is x = cea1, where 
c is a scalar. 
of 
The systems 
linear 
equations 
we have 
differential 
been 
x' = Ax, so if 
consi
dering 
form 
have the 
thinking, 
without 
plowed 
we simply 
that the 
to deduce 
we might be tempted 
ahead 
. But what on 
earth could 
solution would 
be x = ceA1, where 
c is a vector
this 
mean? 
to the power of a matrix. This 
On the righ
t-hand side, 
the 
number e raised 
we have 
appears to 
be nonsense, 
see that there is 
a way to 
make sense 
yet you will 
of it. 
Let's start 
by considering 
the expression 
lus, you learn 
that the func­
eA. In calcu
tion 
ex has a power series expansion 
ex = 1 + x + -+ -+ . . .  2! 3! 
number x. By analogy
for every 
rges 
that conve
, let us define 
real 
2! 3! 
of A, and it can be shown that 
The right-hand 
side 
is just 
defined 
of powers 
in terms 
matri
eA is a matri
it conve
rges 
for any real 
x A. So now 
the 
x, called 
exponential of A. 
it is easy. 
eA or eA1? For diagonal 
compute 
But how can we 
matrices, 
eDt for D = [4  0]. 0 - 1  
Compute 
Solution From the 
definition, 
we have 
Dt 
-- + -- + · · · 
0 ] I [(4t)3 
2! 3! 
e = I + Dt + 
(Dt)2 (Dt)3 
(- t)2 + 31  0 
0 ] + _i,[(4t)2 
)2 + t,C4t)3 + · · ·  o  ] 
[ e;t 
[1  + (4t) + tC4t
0
e�t] 
The matrix 
zable
is also nice 
. 
exponential 

if A is diagonali

1 + C-t) + tiC-t)2 + t,C-t)3 + · · · 

eA = I + A + - + - + · · · 

- t  2·  0 

Az A3 

xz x3 

Example 4 . 4 6  

Example 4 . 4 1  

Theorem 4 . 4 1  

3 4 1  

1 

Section 
and the 
Perron-
Froben
4.6 Applications 
ius Theorem 
eAforA = [� �]. 
Compute
Solution In Example 
of A to be ,\1 = 4 and ,\2 = - 1, 
the eigenvalues 
4.43, 
we found 
2 = [ -�], respecti
vely. 
Hence, 
v1 = [ �] and v
ctors 
with 
correspond
ing eigenve
with 
P= [vi V2] = [2 -l],wehaveP-1AP=D= [4 0].sinceA=PDP-1,we 
3 1 
0 - 1  
Ak = PDkP-1, so 
have 
+ A + - + - + · · · 2! 3! 1 
A1 A3 
eA = I 
= PIP-1 + PDP-1 + -PD2P-1 + -PD3P-1 + · · · 
( Dz D3 ) 
2! 3! 
+ D + -+ -+ · · · p- i  2! 3! 
= P I 
0 ][2 - 1]-I e-1 3 1 2e4 -2e-1] 3e4 + 2e-1 
_!_[2e4 + 3e-1 
5 3e4 -3e-1 
ingly 
bold (and seem
We are now in a position 
at 
guess 
that our 
to show 
foolish) 
solution of x' = Ax was not so 
an "exponential" 
far off after 
all! 
an n X n diagona
,\1, ,\2, •.. , Aw Then 
the 
lizable matrix 
Let A be 
with 
eigenvalues 
to the 
general 
solution 
x' = Ax is x = eA1c, where 
c is an arbitrary 
consta
nt 
system 
vector
. If an initial 
c = 
condition 
x(O) is specified, then 
A = PDP-1, and, 
Proof Let P diagonalize 
as in Example 
A. Then 
-1c. Now, 
to check 
we need 
Hence, 
by x = Pe01P
that x' = Ax is satisfied 
nt except 
for e01, so 
is consta
(10) 

4.47, 

x(O). 

everything 

If 

3 4 8   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

then [I 0 

0  ).,] 

Taking 
we have 

derivatives, 

eDt =  eA2t 

0 

0 

0 

0 

0  0  -(eA,t) dt 

d 

0 
AzeA,t 

�n·, 
0  JJ �r·' 
� [! 0 
(10), we obtain J,] 

0 
eA,t 

An 0 

A1 

0 

0 

Linear 

Algebra 

see 
For example, 
by S. H. 
Friedber
g, A. J. Insel, 
and 
L. E. Spence 
od Cliffs, 
(Englewo
1979). 
NJ: Prentice-Hall, 

Jordan 

-1PeDtp-1c = (PDP

x' = PDeDtp-1c = PDP

x(O) = eA·Oc = e0c = le= c 

-1)(PeDtp-1)c = AeAtc = Ax 

this resu
tuting 
Substi
lt into 
Equation 
as required. 
( t) = eAt c, then 
statemen
easily from the fact that 
The last 
t follows 
if x = x 
since 
e0 = I. (Why?) 
will 
but we 
even 
em 4.41 is true 
In fact, 
Theor
if A is not diagonalizable, 
not prove 
onentials 
this. 
Computa
tion 
of matrix exp
for nondia
gonaliza
ble matrices requires the 
form of a matrix, 
a topic 
that 
in more advan
ced linear 
may be found 
algebra 
texts. 
illustrate 
has served to 
tics 
short digression 
the power of mathema
y, this 
Ideall
to 
of creative 
generalize 
and the value 
thinking
turn 
out to be very 
. Matrix 
exponentials 
important 
tools 
in many 
applications 
of linear 
algebra, 
etical 
both theor
and applied. 
system
at dynamical 
looking 
it-by 
as we began 
this 
We conclude 
s. Markov 
chapter 
growth 
chains 
of discrete 
are examples 
model of population 
and the Leslie 
dynamical systems. Each can be described 
by a matrix equation 
of the form 

normal 

Discrele Linear 

ovnamical 

svs1ems 

linear 

3 4 9  

X1 = AXo 
x2 = Ax1 
x3 = Ax2 

Section 
4.6 Applications 
and the 
Perron-
Froben
ius Theorem 
the vector 
k and 
xk records 
at "time" 
of the system 
the state 
where 
A is a square matrix. 
As we have seen, 
the long-term 
behavior 
of these 
systems 
is related 
to the eigenval­
of the 
matrix 
exploits 
A. The power method 
the iterative 
ues and eigenvectors 
nature 
eigenvalues 
and the 
and eigenv
systems 
to approximate 
dynamical 
of such 
ectors, 
Perron -Frobenius 
Theorem 
lized 
informa
tion 
gives 
specia
about 
the long-term 
behavior 
ete linear 
of a discr
dynamical 
system 
coefficien
whose 
t matrix 
A is nonnega
tive. 
When 
X 2 matrix, 
of a dynamica
we can describe 
the evolution 
l system 
A is a 2 
XcJ, we have: 
geometrica
lly. 
of equations. 
collection 
an infinite 
xk+I = Axk is really 
The equation 
Beginning 
with 
an initia
l vector 
The set {Xo, x1, x2, .•. } is called 
a trajectory of the system. (For graphical 
purposes, 
we 
will 
y each vector 
can plot 
that we 
in a trajectory 
it as a point.) 
identif
its head so 
with 
Note 
that xk = A kx0. 
[0.5 0 ] Let 
xk+ 1 = Axk, plot the first five 
system 
A =  . For the dynamical 
points 
0 0.8 in the trajectories 
with 
: 
the following 
initial vectors
Xo = [Os] 
(a) 
[2.5] [1.25] 
Solution (a) We compute 
x1 = AXo = 0 ,  x2 = Ax1 = 0 ,  x3 = Ax2 = 
[0.625] [0.3125] 
0 , X4 = Ax3 = 0 . These 
4.23, 
are plotted 
points are 
and the 
to highli
ght the trajectory. Similar calculations 
connected 
produce the trajectories 
(d) in Figure 
marked 
4.23. 
(b), (c), and 

in Figure 

Example 4 . 4 8  

y 

X3 X2 Xj 
X4  2 
- 2  (b) 

- 4  

- 2  

Xo 
(a) 
x 

6 

4 

Fioure 4 . 2 3  

3 5 0   Chapter 

4 Eigenvalues 

and Eigenve
ctors 
an attractor 
is called 
to 0. The origin 
converges 
In Example 
4.48, every 
trajectory 
4.19. The matrix A 
We can understand 
why this 
case. 
in this 
is so from Theorem 
in 
�  0.8, respecti
[ �] and [ �] correspond
ing to its eigenvalues 
4.48 has eigenve
Example 
0.5 and 
ctors 
vely. 
initial 
dingly, 
this.) Accor
(Check 
for any 
vector 

we have 

Example 4 . 4 9  

. 

-0.15 0.65 

Because both 
(0.5)k and 
xk approaches 
0 for any 
ask gets large, 
(0.8)k approach zero 
Theor
choice 
we know from 
em 4.28 that because 0.8 is the domi­
ofx0• In addition, 
t of Xo corresp
eigenve
eigenvalue 
of the corresponding 
approach 
of A, xk will 
ctor 
a multiple 
nant 
to [ �]). In other 
as c2 * 0 (the 
[ �] as long 
coefficien
onding 
words
, 
those 
the x-axis 
all trajectories 
except 
that 
c2 = O) will 
approach the 
(where 
begin on 
4.23 shows. 
y-axis, as Figure 
the behavior of the dynamical 
ing to the 
Discuss 
xk+i = Axk correspond
system 
[ 0.65 -0.15] 
xA = 
matri
[ �] 
correspon
Solution The eigenvalues 
of A are 
ding 
eigenvectors 
0.5 and 0.8 with 
�  and[-�],respecti
Xo = c,[�] + c2[-�l 
foraninitial
vector
vely.(Checkthis.)Hence
we have 
xk = A Xo = c1 0.5 1 + c2 0.8 1 
k [-1] 
k ( )k [ 1]  ( )
of Xo· If 
choice 
an attractor, 
origin is 
again the 
0 for any 
Once 
because xk approaches 
Xo where 
the origin 
will 
approach the line 
c2 * 0, the trajectory 
with 
direction 
vector 
through 
[ -�]. Several 
4.24. The vectors 
trajectories 
are shown in Figure 
such 
[ �], and the correspond­
direction 
through 
with 
vector 
the origin 
are on the line 
this line 
into 
the origin. 
ing trajectory 
case 
in this 
follows 

c2 = 0 

Section 

4.6 Applications 

and the 
Perron-
Froben

ius Theorem 

3 5 1  

y 

Figure 4 . 2 4  

Example 4 . 5 0  

systems 
xk+ 1 = Axk correspond
ing to the 

the behavior 
dynamical 
of the 
Discuss 
following 
matrices
: 
A = [�.5 �-SJ 
A = [� �] (b) 
(a) 
[ �] 
ctors 
Solution (a) The eigenvalues 
of A are 5 and 
eigenve
3 with 
corresponding 
Xo  =  c1 [ �] + c2 [ -�], we have 
and [ -�], respect
vector 
for an initial 
ively. 
Hence 
3k_ Hence, 
As k becomes 
large, 
so do both 5k and 
xk tends 
away 
from the 
origin. 
c1 -=fa 0 will 
[ �], all trajec­
nvector 
of 5 has corresponding eige
nt eigenvalue 
the domina
Because 
end up in the first or the third 
. Trajec­
tories 
eventually 
quadrant
for which 
is [ -l]. 
y  = -x whose direction 
c1 = 0 start and stay 
tories 
on the line 
vector 
with 
4.25(a). 1 
See Figure 
correspond
0.5 with 
ing eigenve
ctors 
the eigenvalues 
(b) In this 
example, 
are 1.5 and 
[ �] and [ -�], respecti
vely. 
Hence, 

3 5 2   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

y 

20 

y 

- 20 

(a) 

Figure 4 . 2 5  

c1 -=fa 0, then 
xk = c2(0.5)k[ -�] ---+ [ �] ask---+ cxi. But if 
If c1 = 0, then 
4.25(b) 4 
y � x. See Egme 
and such 
osymptotkally 
trnjeoto;ies 
apprnooh the line 
increas
the origin 
In Example 
4.50(a), all points that start 
become 
out near 
ingly 
large 
a repeller. In 
in magni
tude 
because 
l,tl > 1 for both eigenvalues
; 0 is called 
point because the origin 
attracts 
points in 
some 
Example 
4.50(b ), 0 is called 
a saddle 
case, 
directions 
and repels 
other 
points in 
ons. 
In this 
directi
l-\11 < 1 and 
l-\21 > 1. 
example 
can happen 
The next 
of a real 
the eigenvalues 
shows what 
when 
2 X 2 
matri
x are comple
x (and hence 
of one another). 
conjugates 
Xo = [ !] for the 
Plot the 
xk+1 = Axk 
systems 
dynamical 
with 
beginning 
trajectory 
correspond
matrices: 
ing to the following 
(a) A= [0.5 0.5 -0.5] 0.5 (b) A= [0.2 
0.6 -1.2] 1.4 
Solulion The trajectories 
vely. 
a) and (b 
4.26( 
are shown in Figure 
), respecti
that 
Note 
into the 
spiraling 
to follow 
(a) is a trajectory 
origin, 
whereas 
(b) appears 
an ellipt
ical 
orbit
. 

Example 4 . 5 1  

Section 

4.6 Applications 

and the 
Perron-
Froben

ius Theorem 

3 5 3  

y 

y 

- 4  

Figure 4 . 2 6  

Theorem 4 . 4 2  

Im a+ bi 
b 

r 
e 

Figure 4 . 2 1  

(a) 

of the trajectory 

-10  (b) 
The following 
behavior 
theorem 
explains 
the spiral 
in 
4.Sl(a). 
Example 
[a -bJ Let A = 
b  a . The 
are not 
,\ = a ± 
of A are 
eigenvalues 
bi, and if a and b 
A= [a -bJ = [r OJ [c�s() 
as 
both 
A can be factored 
zero, then 
b  a 0 r sm() -sin() J cos() 
r = IAI = Va2 + b2 and() is the principal 
where 
of a + bi. 
argument 
of A are 
Proof The eigenvalues 
A= t(2a ± �)  = t(2a ± 2\/b2\/=l) =a± lbli =a± bi 
A = [a -bJ = r[a/r b  a b/r -b/rJ = [r OJ [c�s() 
4.27 displays 
a + bi, r, and(). It follows 
35(b) in Section 
by Exercise 
that 
4.1. Figure 
a/r 0 r sm() -sin() J cos() 
[: �b J -=F 0, 
Remark Geometrica
Theor
lly, 
em 4.42 
implies 
that when A = 
S = [ r OJ with 
T(x) = Ax is the composition 
transforma
of a rotation 
tion 
the linear 
R = 
[ c�s () -sin() J through 
fac-
by a 
followed 
scaling 
the angle () 
sm () cos() 
0 r 
tor r 
(Figure 
4.28). 
4.Sl(a), 
In Example 
the eigenvalues 
,\ = 0.5 ± O.Si so 
are 
r = IAI = \./2/2 =  0.707 < 1, and hence 
toward 
all spiral inward 
the tra
jectories 
[a -bJ 
2 X 2 matri
theor
The next 
in general, 
em shows that, 
when 
a real 
x has com
plex 
b  a . For a 
eigenval
vector 
it is similar to a matrix of the form 
complex 
ues, 
x= [:J [::�:J = [:J + [�} 

0. 

3 5 4   Chapter 

4 Eigenvalues 

y 

and Eigenve
ctors 
Scaling /Ax = SRx 
/ 

Rx 

Rotation 

Theorem 4 . 4 3  

Figure 4 . 2 8  

b i= 0) 

A rotation 
followe
d by a scaling 
x, of x to be 
we define 
the real 
part, Re x, and the imaginary part, Im 
Rex=[:]  [:::] and Im
x = [�]  [:::] 
Let 
eigenvalue 
2 X 2 matrix with 
a complex 
A be a real 
A = a - bi (where 
eigen
x. Then 
vector 
and corresponding 
the matrix P = [Re x Im x] is invertible 
and 
Proof Let x = u + vi so that Re
x = u and Im x = v. From 
Au + Avi = Ax = Ax = (a -bi)(u + vi) 
we obtain 
real and 
Equating 
parts, 
imaginary 
Au= au+ bv and 
Now 
P = [ulvJ, so 
[a - b]  [a 
- b] a = [au+ bvl- bu + av]= [AulAv] = A [ulvl 
P b a = [ulvl b 
that u and 
to show 
ly indep
v are linear
it is enough 
that P is invertible, 
To show 
en­
not linear
dent. 
would 
then it 
v were 
If u and 
ly independent, 
follow 
that v = ku for 
u nor 
o complex) 
some 
(nonzer
scalar 
k, because neither 
v is 0. Thus, 
x = u + vi = u + kui = ( 1  + 
A is real, 
Now, 
because 
that 
Ax = Ax implies 
Ax = Ax = Ax = Ax = Ax 
so x = u -
corresponding 
vi is an eigenvector 
But x = (1 + ki)u = (1 -ki)u 

eigenvalue 

to the other 

Ax = Ax, we 

have 

A = a + bi. 

=au+ avi - bui + bv 

= (au+ bv) +(- bu + av)i 

Av= - bu + av 

=AP 

ki)u 

3 5 5  

Section 
4.6 Applications 
and the 
Perron-
Froben
ius Theorem 
the eigenve
nonzer
x of A are both 
. Hence, 
vector
u is a real 
because 
ctors 
x and 
o 
are multiples 
multiples 
of u and therefore 
of one another
. This is 
impossi
ble because 
eigenvectors 
correspond
ing to distinct 
be linearly 
by 
eigenvalues 
must 
ndent 
indepe
is valid over 
the com
theorem 
(This 
Theor
as the real 
ers as well 
em 4.20. 
plex numb
numbers.) This 
contradiction 
implies 
that u and 
v are linearly 
P is 
indepen
dent 
and hence 
invertible. 
It now follows that 
0.6 -1.2] 1.4 are 0.8 ±: 0.6i. 
4.5l(b). 
of 
The eigenvalues 
Theor
serves to explain Example 
em 4.43 
A= [0.2 
eigenve
0.8 -0.6i, a corresp
ctor 
For ,.\ = 
is 
onding 
-0.6] 0.8 , we 
have  -1] [0.8 and C = 
em 4.43, 
that for P = 
From 
Theor
it follows 
0 0.6 
A = PCP-1 and P
-1AP = C 
For the given 
e. Let 
perform 
dynamical 
system 
xk+ 1 = Axk, we 
variabl
a change of 
Then 
so 
�  NowChasthe
±: 0.6i
l = 1.Thus,thedynami­
asA(why?
sameeigenvalues
)andl0.8 
Yk+i = Cyk simply 
cal system 
rotates 
the points in 
every 
trajectory 
in a circle about 
by Theor
the origin 
em 4.42. 
4.5l(b), 
To determine 
we it­
a trajectory 
system 
of the dynamical 
in Example 
T(x) = Ax = PCP-1x. The transforma
eratively apply 
transforma
the linear 
tion 
tion 
can be though
t of as the com
position 
of a change 
of variable 
(x toy), followed 
by the 
rotation determined 
by C, followed 
by the reverse 
change 
of variable (y back to 
x). We 
this 
in 
quadratic 
to graphing 
encounter 
in the application 
again 
will 
idea 
equations 
Section 
6.3. 
as "change 
Section 
5.5 and, 
of bas
more gener
ally, 
is" in 
74 of 
In Exercise 
in Example 
an ellip
se, 
Section 
you will 
show 
that 
the trajectory 
4.51 (b) is indeed 
5.5, 
a ±: bi, 
as it appears 
to be from Figure 
4.26(b). 
then: 
To summarize 
,.\ = 
x A has complex 
eigenvalues
2 X 2 matri
If a real 
inward 
then 
the trajectories 
of the dynamical 
if 1,.\1 < 1 
system 
xk+I = Axk spiral 
(O is a spiral attract
or), spiral 
outward 
if 1,.\1 > 1 (O is a spiral repeller)
on a 
, and lie 
closed orbit if 1,.\1 = 1 
(O is an orbital center)

. 

Vignette 

Ranking Sports Teams and S earching 
the I nternet 

In any competitive 
sports league, 
rward 
it is not necess
arily 
a straightfo
to 
process 
rank 
the players 
or teams. Counting 
wins 
bility 
alone overlooks the 
and losses 
possi
against 
number 
one team 
a large 
that 
weak 
may accumulate 
teams, while 
of victories 
another 
team 
against 
strong 
fewer 
victo
ries 
but all of them 
teams. Which 
may have 
is better? 
of these 
How should 
never 
we compare 
two teams 
that 
play one 
teams 
another? 
Should points scored be taken 
into 
account? 
Points against? 
has become 
and sports teams 
of athletes 
xities, 
te these 
the ranking 
Despi
comple
a commonplace 
and much-
anticipated 
feature 
in the media. 
For example, 
there are 
various 
annual 
of U.S. 
rankings 
college 
football and basketball 
teams, and golfers 
and 
tennis 
also 
players are 
ranked 
ally. 
There 
internation
are many copyrighted 
schemes 
rankings, 
such 
to produce 
used 
into 
some 
but we can gain 
insight 
how to approach 
chapter. 
the ideas 
the problem 
by using 
from this 
play 
To establish 
one 
the basic 
idea, let's 
players 
revisit Example 
3.69. Five tennis 
another 
robin 
in a round-
tournament. 
in the form of 
Wins and 
are recorded 
losses 
aiJ = 1 if player 
in which 
a digraph 
i to j indicates 
that 
from 
a directed 
player 
player 
edge 
i defeats 
aiJ = 0 other
has 
The corresponding 
adjacenc
y matrix 
A therefore 
i defeats 
player 
j and has 
wise. 
0 1 0 
2  0 0 1 
5 
A =  1 0 0 1 0 
1 
0 0 0 0 
0 0 1 0 0 
4  3 
like 
We would 
a way that 
player 
a ranking 
r; with 
to associate 
i in such 
r; > r1 indicates 
i is ranked 
highly 
than player 
more 
j. For this 
that player 

j. 

3 5 6  

r;'s be proba
require 
bilities 
that the 
purpose, 
0 :s r; :s 1 for all 
(that is, 
let's 
i, and 
organize 
r1 + r2 + r3 + r4 + r5 =  1) and then 
in a ranking vector 
the rankings 

Furthermore, let's 
insist 
that 
player 
i's ranking 
should 
sum of 
be proportiona
l to the 
the rankings 
1 defeated 
player 
defeated 
of the 
players 
players 
i. For example, 
by player 
5, so we want 
2, 4, and 
a is the 
consta
where 
nt of proportionality. 
Writing 
out similar 
equations 
for the other 
players 
produces 
the following 
system: 

+ r4 + r5) 
r1 = a(r2 
r2 = a(r3 + r4 + rs) 
r3 = a(r1 + r4) 

'1  0 1 0  '1 
r2  0  0 1  r2 

in matrix 

we can write 
this 
system 
that 
Observe 
form as 
f3 = ll'  1 0  0 1 0 f3 or 
Equivale
we see that the 
ntly, 
ranking 
vector 
, 
r must 
satisfy Ar = -r. In other 
words
ctor 
r is an eigenve
ing to the matrix 
correspond
Furthermore, 
Frobenius 
tive 
matri
A is a primi
tive 
nonnega
x, so the 
Perron-
Theo­
rem guara
ntees that there 
r. In this 
examp
is a unique ranking 
vector 
le, the ranking 
vector 
turns 
out to be 

f4  0 0  0 0 1 f4 
f5 0 0 1 0  0 f5 

r = aAr 

1 
a 

A! 

0.29 
0.27 
r =  0.22 
0.08 
0.14 
1, 2, 3, 5, 
4. 

in the order 
the players 
so we would 
rank 
account many 
to take into 
of the com­
By modifying 
the matri
x A, it is possible 
ph. However, 
this 
has 
example 
plexit
ies mentioned in 
the opening 
paragra
simple 
of ranking 
teams
. 3 5 1  
served to indicate one 
useful 
approach to 
the problem 

unordered. 

such 
as 
to understand 
how an Internet 
search engine 
can be used 
The same idea 
used 
to return 
the results 
of a search 
search engines 
. Older 
Google works
was often 
among 
scrolling 
irrelevant 
Much 
sites would 
Useful 
often be buried 
ones. 
search re­
for. By contrast, 
Google returns 
looking 
you were 
needed 
to uncover what 
a method 
likely 
for ranking 
ce. Thus, 
websites 
sults 
ordered according 
to their 
relevan
is needed. 
Instead 
of teams 
playing 
one another
an­
, we now have 
websites linking 
to one 
now an edge 
use a digraph to 
model the situation, only 
again 
other
from 
. We can once 
i to j indicates 
j. So whereas 
that website 
i links 
to (or refers 
to) website 
for the sports 
ph, incoming 
team 
digra
directed 
are bad (they 
indica
te losse
edges 
s), for the Internet 
digra
ph, incoming 
edges 
are good 
directed 
(they 
te links 
sites). In 
indica
from other 
we want 
setting, 
this 
rank­
l to sum of the 
the ranking 
of website 
i to be proportiona
ings 
of all the websites 
to i. 
that link 
Using 
ent just 
the digra
ph on 
page 356 to repres
five websites, 
we have 
to use the 
want 
le. It is easy 
for examp
to see that we now 
transpose of the adjacency 
ph. Therefore, 
matrix 
of the digra
the ranking 
vector 
r must 
satisfy AT r = -rand will 
thus be the 
Perron 
eigenv
ector 
of Ar. In this 
example, 
we obtain 
0.14 
AT= 0 1 0  0 1 and r= 0.22 
turns 
up these 
so a search that 
five sites would 
in the 
order 
list them 
and computes 
here 
of the method 
the rank­
a variant 
described 
Google actually 
uses 
via an iterative method 
ing vector 
very 
method 
(Section 
similar 
to the power 

0  0  0  0 0.08 

1  1 0  0 
1 0  0 

r4 = a(r1 + r2 + r3) 

0.27 
0.29 

0  0 1 0  0 

5, 4, 3, 1, 2. 

1 
a 

4.5). 

3 5 8  

3 5 9  

4 . 6  

7.P = 

S1S2· · ·sn-1 

7-9, P is the 

1-6 are regular? 

transition 

Find the long range 

matrices in Exercises 

matrix of a regular 
transition 

In Exercises 
Markov chain. 

Section 

Markov C h a i n s  
Which of the stochastic 

and the 
Perron-
Froben

ius Theorem 

4.6 Applications 

17. If all of the survival 

rates 
s; are nonzero, 
let 
1 0  0 0 
0 S1 0  0 
P =  0 0 S1S2  0 
0 0  0 
to find the 
Compute 
charac
P-1LP and use it 
­
teris
to Exercise 
tic polynomial of L. [Hint: Refer 
32 in 
4.3.] 
Section 
to A1 is 
an eigenve
ding 
ctor 
18. Verify 
that 
of L correspon

I Exercises 
1. [ � �]  2. [ � !] 
3. [i �]  4. [! 0 0 1 
�] 
[OJ 0 �'] ["' 
�] 0 0 
6. �.5 
5. 0.5 1 
0.4 0 0.5 
matrix L of P. [l l [ 
8.P = l 2 I 6 
[ 02 0.3 04] 
[i iJ  3 
9. p = 0.6 0.1 0.4 
0.2 0.6 0.2 
the steady state proba
10. Prove that 
bility 
of a 
vector 
chain 
Markov 
regular 
[Hint: Use Theo­
is unique. 
or Theorem 
4.34.] 
rem 4.33 
11.L = [O 0.5 
39 in Section 
19. Exercise 
3.7 
40 in Section 
20. Exercise 
3.7 
21. Exercise 
44 in Section 
3.7 
13. L � [�s �] 7 0 0.5 �] 12. L=[1 0.5 
14.L � [i 5 0 2 3 �] 
suffered 
species 
from commercial 
GAs 22. Many 
of seal have 
skin, 
hunting. 
been 
They have 
killed for their 
blubber, 
and meat. The fur trade, 
lar, 
in particu
reduced 
some 
populations 
seal 
the 
to the point of extinction. 
Today, 
are decline 
seal populations 
greatest 
ts to 
of fish 
threa
eigenvalue 
matrix 
has a 
15. If a Leslie 
positive 
unique 
pollution, 
of 
ance 
stocks due to overfishing, 
disturb
is the significance 
if A1 > 1? 
what 
for the population 
habitat, 
in marine 
entanglement 
and culling 
debris, 
by fishery 
owners. 
Some seals have been declared 
endangered 
are carefully 
; other 
species
species 
characteristic 
l of the Leslie 
16. Verify 
polynomia
that the 
managed. 
4. 7 gives 
the birth 
and survival 
rates 
Table 
(3) is 
matrix Lin Equation 
fur seal, divided into 
2-year age 
for the northern 
cL(A) = ( - l)"(A" - b1A"-1 - b2s1A"-2 - b3s1s2A"-3 
data 
classes. 
[The 
J. R. 
are based 
on A. E. York and 
Harvest 
Hartley, 
Following 
"Pup Production 
of Female 
Fur Seals;' 
Northern 
[Hint: Use mathematical 
induction 
and 
and exp
pp. 84-90
, 38 (1981), 
.] 
det(L -Al) along 
the last 
column.] 

with 
17 above 
Exercise 
32 in 
Exercise 
ercise 
46 in Section 
4.4.] 

[Hint: Combine 
Section 
4.3 and Ex

P o p ulation  Growth 
In Exercises 
corres

the population 
exercise. 
distribution 

positive eigenvalue 
and a 
of the Leslie matrix L. 

Then use Exercise 18 to help find the corresponding 

11-14, calcul
ponding positive 

19-21, compute the steady state growth 
rate of 

with the Leslie matrix L from the given 

ate the 
eigenvector 

GAs In Exercises 

S1/ A1 
S1S2/ Ai 
S1S2S3/ Ai 

of the age classes. 

- bns1s2 · · ·Sn-I) 

Canadian Journal 

Aquatic Science

of Fisheries and 

-· · · 

A1 <l?A1= 1? 

A1, 

3 6 0   Chapter 

4 Eigenvalues 

and Eigenve
ctors 

zero population 

r = 1 if and only 
if ,\1 = 1. (This 
repre­
that 
(b) Show 
sents 
growth.) [Hint: Let 
- + - + -- + . . .  + ----
g(,\ = 
) bl bzS1 b3S1S2  bnS1S2· . .  Sn-1 
,\ ,\2 ,\3 ,\" 
value 
of L if and only if 
Show 
that ,\ is an eigen
g(,\) 
= 1.] 
(c) Assuming 
that there 
is a unique 
eigen­
positive 
value 
,\1, show 
that r < 1 if and only if the popu­
if the 
and 
lation 
is decreasing 
r > 1 if and only 
population 
is increasing. 

Table 4 . 1  
Age (years) Birth 

Rate Survival 

Rate 

e that allows a 

(represented by a population 
so that the population 

policy is a procedur

x) to be harvested 

A sustainable harvesting 
certain fraction 
of a population 
distribution 
vector 
returns to x after one time interval 
of one age class). 
is the length 
age class 
that is harvested, 
then we can express 
ing procedure 
ically as follows: 
mathemat
time interval 
a population 
vector x, 
after one 
harvesting removes hLx, leavin
g 

of each 
the harvest­
If we start 
with 
we have Lx; 

(where a time interval 

If h is the fraction 

0-2 
0.00  0.91 
2-4 
0.02  0.88 
4-6 
0.85 
0.70 
6-8 
1.53 
0.80 
1.67 
8-10 
0.74 
10-12  1.65 
0.67 
12-14  1.56 
0.59 
14-16 
1.45 
0.49 
16-18 
1.22 
0.38 
0.91  0.27 
18-20 
20-22  0.70 
0.17 
22-24  0.22  0.15 
24-26  0.00  0.00 

Sustainabilit
y requires that 

Lx - hLx = (1 - h)Lx 
(1 - h)Lx = x 
positive 
24. If,\ 1 is the unique 
eigenvalue 
of a Leslie 
matrix L and 
h is the 
sustaina
ble harvest 
ratio, 
prove 
cAs 25. (a) Find 
thath = 1 -1/,\1. 
ble harvest 
the sustaina
for the wood­
ratio 
L for these 
(a) Construct 
matrix 
the Leslie 
data 
and 
in Exercise 
land 
44 in Section 
caribou 
compute the 
eigen
positive 
value 
and a corre­
(b) Using 
the data 
in Exercise 
44 in Section 
sponding 
vector. 
eigen
positive 
the caribou 
to your 
herd 
reduce 
answer 
according 
be in 
of seals 
long 
percentage 
(b) In the 
run, 
what 
will 
to part (a). Verify 
to its 
that the population 
returns 
the growth rate be? 
each 
age class 
and what 
will 
after one 
origina
l level 
val. 
time inter
26. Find 
ble harvest 
seal in 
the sustaina
ratio 
for the 
23 shows that the long-
harvest 
have 
22. (Conservationists 
Exercise 
had to 
seal populations 
when 
overfishing 
the 
has reduced 
available food 
to the point where 
the seals are 
supply 
is defined 
23. The 
as 
rate of a population 
in danger 
of starvation.) 
eigen­
positive 
with 
matrix 
Leslie 
L be a 
� 27. Let 
a unique 
r = b1 + b2s1 + b3s1s2 + · · · 
value 
,\1. Show 
(real or com­
that if,\ is any other 
b; are the birth 
rates 
sj are the 
where 
the 
and the 
of L, then 
plex) 
eigenvalue 
1,\1 :s ,\1. [Hint: Write
,\= 
survival 
for the population. 
rates 
() + i sin()) and substi
r(cos 
tute 
it into the 
equation 
why 
(a) Explain 
as the 
average 
r can be interpreted 
(b) ofExercise 
ime.  g(,\) = 1, as in part 
23. Use De Moivre's 
born to a single 
number 
of daughters 
female over 
part of both sides
Theorem 
and then 
take 
. The 
the real 
her lifet
Triangle 
Inequality 
should prove 
useful.] 

Exercise 
tion can be determine
matrix. 

d directly from the entries 

+ bns1s2· · · 

3.7. 
3.7, 

net reproduction 

of its Leslie 

run behavior 

of a popula­

sn-l 

3 6 1  

c 

form 

[Hint: 

block 

rrespond­

e value 

of a matrix A =  [ a

that a nonnegative 

c­
c­
of its rows and 

Section 

The P e r r o n - F r o b e n i u s  
T h e o r e m  
In Exercises 
ing Perron eigenvector 
of A. 

n X n matrix is irredu
to determine whether the matrix A is irredu

It can be shown 
ible if and onl
this criterion 
ible. If A is reducible, find a permutation 
columns that puts A into the 

28-31, find the Perron root and the co
28.A = [21 01] 
30.A � [: � i] 29.A = [21 03] 
31. A � [: 0 � l 
y if (I + A) n-t > 0. In Exercises 
32-35, use 

and the 
Perron-
Froben
4.6 Applications 
ius Theorem 
that if A is primitive, 
other 
then the 
(b) Show 
eigen­
ute value. 
are all less 
thank in absol
values 
Adapt 
Theorem 4.31.] 
in Section 
lts of your 
39. Explain 
the resu
exploration 
4.0 
4.5. 
in light 
of Exercises 
36-38 and Section 
ij] is 
defined to be the matrix IAI  = [ laul]. 
In Exercise 40, the absolut
x a vector 
40. Let 
A and 
B be n X n matrices, 
in IJ�r, and 
Prove the 
a scalar. 
matri
following 
x inequalities: 
(a) lcAI = lcl IAI 
(c) IAxl :s IAI lxl 
IS re UC! e 
(b) IA + Bl ::; IAI + IBI 
(d) IABI ::; IAI IBI 
if a12 = 0 or a21 = 0. a,2] · d 'bl 
2 X 2 matrix 
41. Prove that a 
A = [a" 
if and only 
(I -A)-1 2 0. Let 
az1 
az2 
nonnega
42. Let 
A be a 
matrix such 
tive, irred
ucible 
that I -A is invertible 
and 
v1 be the Perron 
and Perron eigenve
ctor 
root 
A1 and 
33. A � [ ! � � � l 0 0 35.A = 1 0 0 
of A. 
(a) Prove that 
0 < A1 < 1. [Hint: Apply Exe
rcise 
4.18(b).] 
in Section 
4.3 and Theorem 
v1 > Av,. 
from (a) that 
(b) Deduce 
0 0 0 
0 
0 0 0 1 0 
0 
0 1 0 0 
1 0 0 0 0 
0 0 
0 0 1 
34.A = 1 0 0 0 0 
1 0 0 
43-46, write out the first six terms of the 
1 0 
1 0 0 
0 1 1 
43. Xo = 1, Xn = 2xn-I for n 2 1 
36. (a) If A is the adjacency 
44. a, = 128, an = an_,/2 for n 2 2 
x of a graph 
matri
G, show 
that A is irreducible 
if and 
only 
if G is connected. 
45.y0 = O,y1 = l,yn = Yn-I -Yn-z forn 2 2 
(A graph is connected if there is 
a path between 
46. bo = 1, b, = 1, bn = 2bn-I + bn-2 for n 2 2 
of vertice
pair 
every 
s.) 
(b) Which 
of the graphs 
4.0 have 
in Section 
an 
irred
adjacency 
matrix? 
ucible 
Which 
have 
a 
adjacency 
matrix? 
primitive 
47-52, solve the re
G be a bipartite 
graph with 
adjacency 
matri
x A. 
37. Let 
47. Xo = 0, X1 = 5, Xn = 3Xn-I + 4Xn-2 for n 2 2 
(a) Show 
A is not primitive
that 
. 
48. Xo = 0, X1 = 1, Xn = 4Xn-I -3Xn-2 for n 2 2 
value 
if A is an eigen
(b) Show 
that 
of A, so is -A. 
[Hint: Use Exercise 
3.7 and partition 
80 in Section 
an eigenvec
tor for A so that 
it is compa
with 
tible 
find 
this 
partitioning to 
partitioning 
of A. Use this 
tor for -A.] 
an eigenvec
meet 
38. A graph 
is called 
k-regular if k edges 
ver-
at each 
graph. EV 52. The recurrence 
relation 
your 
in Exercise 
45. Show that 
tex. Let 
G beak-regular 
solution 
agrees 
the answer 
with 
to Exercise 
45. 
(a) Show 
that 
y matrix 
the adjacenc
A of G 
Xn = axn-i + bxn-2 has 
value. [Hint: Adapt 
has A 
= k as an eigen
53. Complete 
em 4.38(a) by showing 
the proof 
of Theor
Theorem 
4.30.] 
relation 
that 
if the recurrence 

L i n e a r  R e c u r r e n c e  
In Exercises 
sequence 
initial 

In Exercises 
given initial 

recurrence relation with the gi

defined by the 

conditions. 

conditions. 

currence relation with the 

ven 

R e l a t i o n s  

22 

(b) 

A1 - Az 

3 6 2   Chapter 
4 Eigenvalues 
and Eigenve
ctors 
1.......-_..... 
,\1 * ,\2, then the 
� .......... 
will 
solution 
distinct 
eigenvalues 
be 
form 
of the 
v � 
__.v" 
_...... v 
the method 
of Example 
in 
4.40 works 
that 
[Hint: Show 
l.] 
genera
of initia
54. (a) Show 
that for any choice 
l conditions 
x0 = rand x1 = s, the scalars 
c1 and c2 can be 
es ,\1 and ,\2 are distinct 
in Theor
as stated 
em 4.38(a
found, 
) and (b). 
(b) If the 
eigenvalu
and the 
Figure 4 . 2 9  The area 
is 64 
square 
of the square 
but the 
units, 
initial 
are 
conditions 
Xo  = 0, x1 = 1, show 
that 
units
recta
did the 
! Where 
is 65 square 
ngle's area 
Xn = ( l )(,\7 -,\�). 
does 
[Hint: What 
from? 
extra 
come 
square 
this 
f,, = fn-I + fn-2 has the 
Fibonacci 
have 
to do with the 
seque
nce?] 
56. You have 
a supply 
of three 
kinds 
of tiles
: two kinds 
recurrence 
55. The Fibonacci 
of 1 X 2 tiles 
as shown in 
of 1 X 1 tile, 
and one kind 
matri
iated 
assoc
x equation 
xn = AXn_1, where 
Xn = I fn ] and A = [ l l] 
Figure 
4.30. 
D CJ  
Ltn-1  1 0 
(a) Withf0 = 0 andf1 = 1, use mathematical induc­
tion 
to prove that 
A" = [Jn+I fn ] Jn fn-1 
n 2 1. 
Figure 4 . 3 0  Let 
tn be the number of different 
ways 
to cover a 
with 
1 X n rectangle 
tiles
these 
. For example, 
for all 
that t3 = 5. 
4.31 shows 
Figure 
(b) Using 
part 
(a), 
prove that 
n 2 1. [This 
(a) Find 
2 = (-1)" 
�  (Does 
t0 make any sense? 
If so, what 
is it?) 
for all 
is called 
order 
(b) Set up a second 
fort"' 
relation 
recurrence 
Giovanni 
after the astronomer 
Domenico 
Cas­
(c) Using 
t1 and t
2 as the initia
l conditio
ns, solve the 
sini 
(1625-1712). Cassini 
was born 
in Italy 
but, 
in part 
recurrence 
relation 
(b). Check 
your 
answer 
on the invit
ation 
of Louis 
XIV, 
in 1669 
moved 
(a). 
in part 
against the 
data 
director 
of the Paris 
to France, 
where he 
became 
vatory. 
a French 
He became 
Obser
citizen 
and 
adopted the 
French 
version 
of his name: 
Jean­
Dominique Cassini. Mathematics 
was one of his 
y. Cassini's 
many 
interests 
other 
than 
astronom
was published 
sub­
in 1680 in a paper 
Identity 
of Sciences 
to the Royal 
mitted 
in Paris.] 
Academy 
(c) An 8 X 8 checker
board 
can be dissected 
as shown 
in Figure 
reass
embled 
4.29(a) and the 
pieces 
4.29(b). 
in Figure 
to form the 5 X 13 rectangle 
\ \ ' 
v ........  \ 
� v 
rectangle 
a 1 X 3 
to tile 
Figure 4 . 3 1  The five ways 
\ 
57. You have 
with 
of 1 X 2 dominoes 
a supply 
which 
\ ' 
a 2 
to cover 
. Let 
X n rectangle
dn be the number of 
different 
ways 
to cover 
the rectangle. 
For example, 
Figure 
4.32 shows that 
d3 = 3. 

t1, • • •  , t5• 

fn+ ifn-1 -fn

Cassini's Identit

(a) 

y, 

3 6 3  

b. 

+ I.Sy 

Section 

ius Theorem 

4.6 Applications 
and the 
Perron-
Froben
y' = X -2y + Z, 
x(O) = 2 
64. x' = x +  3z, 
y(O) = 3 
z(O) = 4 
Em / l �  
z' = 3x +  z, 
of bacteria, 
two strains 
65. A scientist places 
X and 
Y, in 
a petri dish. 
there 
Initi
are 400 
ally, 
ofX and 500 ofY. 
The two bacteria 
for food and space 
compete 
but do 
[8 BJ ITIJ 
not feed on each 
other
. If x = 
y = y(t) are 
x(t) and 
t days, 
at time 
the numbers of the strains 
the growth 
rates 
are given 
of the two populations 
by the system 
x' = l.2x -0.2y 
y' = -0.2x 
(a) Determine 
to these 
what 
happens 
two popu­
Figure 4 . 3 2  The three 
with 
a 2 X 3 
rectangle 
ways 
to cover 
1 X 2 
dominoes 
by solving 
lations 
the system 
of differential 
d1, .•• , d5. 
ions. 
equat
(a) Find 
(b) Explore 
populations 
of changing 
the initial 
the effect 
is it?) 
If so, what 
..-... (Does 
d0 make any sense? 
ibe what 
byletting
x(O) = a and y(O) = 
hap­
b. Descr
order recurrence 
(b) Set up 
a second 
for dw 
relation 
pens 
in terms 
of a and 
to the populations 
(c) Using 
d1 and 
ns, solve the 
d2 as the initia
l conditio
Y, live 
X and 
66. Two species, 
in a symbiotic relationship. 
recurrence 
answer 
relation 
in part (b). Check your 
can survive 
species 
is, neither 
That 
and each 
on its own 
in part (a). 
against 
the data 
on the other 
depends 
Initi
for its survival. 
ally, 
there 
v1 and 
4.41, find eigenve
58. In Example 
ctors 
v2 corre-
15 ofX and 10 ofY. If x = 
are 
y = y(t) are the 
x(t) and 
at time 
sizes of the populations 
t months, 
the growth 
y fo:mula 
A1 = and A
2 = . With 
sponding to 
by the system 
are given 
two populations 
rates 
of the 
2 4.5. That is, 
xk = I fk ] , verif
(2) in Section
x' = -0.Sx + 0.4y 
y' = 0.4x -0.2y 
Ltk-1 show 
for some 
c1, 
scalar 
that, 
what 
these 
happens to 
Determine 
two populations. 
on species Y. The 
. xk hm k = C1V1 
67 and 68, species 
k->oo A1 
are represented by x = x(t) and 
y = y(t). The growth 
by the system of differential equations x' = Ax + b, where 
x = [;] 
conditions x(O). (First 
and y = v + b convert 
such that the substitutions x = 
67.A = [_� l]  [-30] [20] 
1] b -[ OJ x o -[10] 
1 ,b= -10 ,x(O)= 30 
[-1 - 1  
- 1 ' -40 ' ( ) -30 
68.A = 
and 
function 
fferentiable 
69. Let x = 
x(t) be a 
twice-di
consider 
the 
x" + ax' + bx= 0 (1 1) 
(a) Show that 
y = x' and 
of variables 
the change 
z = x allows 
Equation 
( 1 1) to be written 
as a sys­
differential 
tem of two linear 
equations 
in y and 

llili_ 
59-64, find the general solution 
59. x' = x + 3y, x(O) = 0 
+ 2y, y(O) = 5 
y' = 2x 
60. x' = 2x -y, x(O) = 1 
y' = -x + 2y, y(O) = 1 
61. x; = X1 + Xz, X1(0) = 1 x� = X1 -Xz, Xz(O) = 0 
62. Yi = Y1 -Yz, Y1(0) = 1 
63. X1 = y -Z, 
Y� = Y1 + Yz, Yz(O) = 1 
x(O) = y(O) = 0 
y' = X 
z(O) = - 1  
z' = x 

and b is a constant vector
for the given 
show that there are 
constants a and b 
u + a 

In Exercises 
system of differential 
solution 
all functions to be functions 
oft.) 

to the 
given 
equations. 
specific 
Then find the 
(Consider 
conditions. 

. Determine 
what happens 
A and b and initial 

In Exercises 
sizes 

order differential equation 

ent one with no constant 

the system into an equival

+ Z, 

to the two populations 

that satisfies the initial 

rate of each population 

of the populations 

is governed 

terms.) 

svstems of L i n e a r  Differential 

second 

X preys 

EV 

+ y, 

1 +Vs 

1 -Vs 

E q u a t i o n s  

z. 

1 

- 1  

a0• 

4.41. 

81. A =  

82.A = 

0.5 0.5 

60 
64 

59 
63 

jector

the general 

1.2 3.6 

y for the 

Sketch 

69 to find 

s of the tra

using Theorem 

85-88, the given 

equations 

86.A = -0.5 0 

71 and 72, use Exercise 

ral attractor, spiral repeller, or orbital 

73-76, solve the system of differential 

svstems 
77-84, consider 
the dynamical system 

Discrete 
linear D v n a m i c a l  
In Exercises 
xk+1 = Axk. 

In Exercises 
solution 
of the given 
equation. 
71. x" -5x' + 6x = 0 72. x" + 4x' + 3x = 0 
In Exercises 
in the given exercise 

83.A = -0.2 0.8 
In Exercises 
matrix is of the form 
A = b  a . In each case, A can 
product of a scaling matrix and a rota
the scalin
first four point

x(n) + an_1x(n-I) + · · · + a1x' + ao = 0 
p(,\) = ,\" + an_1,\n-I + · · · + a1A + 

3 6 4   Chapter 
4 Eigenvalues 
and Eigenve
ctors 
that 
(b) Show 
the characteristic 
of the 
equation 
79.A = [_� -�] 80.A = [-� -�J 
,\2 + a,\ + b = 0. 
system in part (a) is 
[ 1.5 -�] [ 0.1 0.9] 
70. Show 
is a change of variables 
that there 
that converts 
the nth order differential equation 
[ 0.2 0.4] 84. A =  [O - 1.5] 
equations 
of n linear 
into a 
system 
differential 
whose 
companion 
coefficient 
matri
matrix C( p) of the 
x is the 
[a - b] 
polynomial
of x. See 
[The 
notation 
x(k) denotes 
the kth derivative 
Exercises 
26-32 in Section 
of a 
4.3 for the definition 
companion 
matrix.] 
be factored as the 
g factor rand the angle e of rotation. 
tion matrix. 
Find 
the 
xk + 1 = Axk with x0 = [ �] and classify the origin 
dynamical system 
center. [ 0 0.5] 
as a spi­
85.A = -�] 
[ 1
74. Exercise 
87.A = [ 1 V3] 
76. Exercise 
-V3  1  88.A = [-V3 ;2 - 1/2 ] 
trix C of the form C = b  a such that A = PCP -i. 
[ a - b] 
system xk + 1 = Axk with Xo = [ �] and classif
-0.2] 0.3 90.A = [ _� �] 
[0.1 
91.A = [� -�] 92.A = [o -1] 1 V3 
' 

73. Exercise 
75. Exercise 
(a) Compute and plot Xo' x1, x2, x3 for x0 = [ 1
1]. 
1 ]. 
(b) Compute and plot Xo' x1, x2, x3 for Xo = [0
77. A = [2 l] 78. A = [0.5 -0.5] 
Chapter Review 
and Concepts 
Kev Definitions 
disk, 
Gersc
hgorin 
diagonaliza
ble matrix, 
x, 276 
adjoint of a matri
multiplicity 
of an 
Gersc
algebraic 
hgorin's 
Disk Theorem, 321 
eigenvalue, 
eigenvalue, 
Laplace 
254 eigenvector, 
Expansion 
Theorem, 
(and its 
power method 
ace, 
254 eigensp
294 characteristic 
equation, 
characteristic 
polynomial, 
ts), 311-319 
varian
Fundamental 
em of Invertible 
Theor
expansion, 
cofactor 
of determinan
properties 
ts, 
Matrices, 
296 geometric 
multiplicity 
of an 
Cramer's Rule, 
determinant, 
eigenvalue, 
269-274 similar 
matrices, 

(c) Using eigenvalue
origin as 
an attractor, repeller, saddle point, or none of these. 
( d) Sketch several 

s and eigenvectors, 
typical 

1/2  -V3 ;2 
matrix P anda ma-

a spiral attractor, spiral repeller, or orbital 

dynamical 
origin 

y for the 
y the 

274-275 
263-265 

89-92,find an invertible 

the first six points of the tra

In Exercises 

89.A = 0.1 

classify the 

ies of the system. 

Sketch 

trajector

0  0.5 

jector

0  3 

319 

301 

256 

292 

303 

292 

266 

266 

294 

center. 

as 

Chapter 
Review 

3 6 5  

5 

Review Questions 
9. LetA = [-: -: -�]. 
statements 
true 
or false: 
1. Mark 
each 
of the following 
(a) For all sq
matrices 
A, det(-A) = -<let A. 
uare 
= 
then 
det(AB) 
(b) If A and Bare n X n matrices, 
0 0 -2 
<let (BA). 
(a) Find 
A. 
the characteristic 
polynomia
l of 
(c) If A and Bare n X n matrices 
whose columns 
of A. 
all of the eigenvalues 
(b) Find 
same but in different 
are the 
orders, 
then 
(c) Find 
of the eigenspaces 
a basis 
for each 
of A. 
<let B = -<let A. 
(d) Determine 
whether 
If A is 
A is diagonalizable. 
-i) = <let AT. 
( d) If A is invertible, 
then 
det(A 
not diagona
go­
lizable, 
explain 
why not. 
If A is dia
(e) If 0 is the only 
of a 
square 
A, 
eigenvalue 
matrix 
and a diago­
find an invertible 
matrix P 
nalizable, 
matrix. 
then 
A is the zero 
P-1AP = D. 
nal matrix 
D such that 
(f) Two eigenvectors 
correspond
ing to the same 
10. If A is a 3 
ble matri
X 3 diagonaliza
eigenvalues 
x with 
ly dependent. 
be linear
must 
eigenvalue 
4, find <let A. 
-2, 3, and 
(g) If an n X n matrix has 
n distinct 
eigen
values, then 
it must 
lizab
be diagona
le. 
with 
X 2 matrix 
A1 = t, A2 = - 1, [�], Vz = [-�l 
11. If A is a 2 
eigenvalues 
(h) If an n X n matrix is diagonalizable, 
then 
it must 
v1 = 
and correspond
ing eigenvectors 
have 
eigenvalu
n distinct 
es. 
tors. 
eigenvec
(i) Similar 
the same 
matrices 
have 
findA-s[�J. 
n X n matrices 
with 
(j) If A and B are two 
the same 
A is similar to 
form, then 
B. 
reduced row echelon 
12. If A is a diagona
all of its eigenvalues 
matri
lizable 
x and 
that An approaches 
satisfy I A I < 1, prove 
the zero 
ma­
[ 1 3 
7  9 �]. 1 1  
trix as n gets 
large. 
2. LetA = 3 
13-15, determine, with reasons, whether A is 
(a) Compute 
similar to B. If A � B, give an invertible 
matrix P such that 
any 
expansion 
<let A by cofactor 
along 
P-1 AP= B. 
row or column. 
13.A = [! �],B = [� �] 
(h) Compute 
A to triangular 
reducing 
<let A by first 
form. 
3. If d e f = 3, find 
14.A = [� �],B = [� �] 
15.A � [� i :J.B � [� � �] 
4. Let A and B be 4 X 4 matrices 
with 
<let A = 2 
and 
<let B = -�. Find 
matrix C: 
<let C for the indicated 
(a) C=(AB)-1 (b)C=A2B(3AT) 
[ � �]. Find 
5. If A is a skew-symmetric 
n X n matrix and 
n is odd, 
all values 
ich: 
of k for wh
16. Let A = 
prove 
that <let A = 0. 
- 1  2 
(a) A has eigenvalu
es 3 and 
1 1 k = 0. 2 4  k2 
all values 
6. Find 
of k for which 
ity 2. 
multiplic
algebraic 
with 
(b) A has 
an eigenvalue 
(c) A has no real 
values. 
eigen
are the possible 
of A? 
17. If A 3 = A, what 
eigenvalues 
7 and 8, show that x is an eigenvector 
of A and 
18. If a square matrix 
A 
rows, why must 
A has two equal 
ues? 
have 
0 as one of its eigenval
s.FHJA�[�: -60 -45] 15 -32 
7. x = [�],A= [! �] 
ctor 
19. If xis an eigenve
A = 3, show 
of A with 
eigenvalue 
of A 2 -SA + 2I. What 
that 
xis also 
an eigenve
ctor 
is 
the correspond
ing eigenval
ue? 
P-1AP = Band xis an eigen­
20. If A is similar 
to B with 
that P-1x is an eigenv
ector 
vector of 
A, show 
of B. 

3a  2b - 4c  c . 

a  b  c 3d 2e -4f f 

ponding eigenvalue. 

g  h  3g 2h  -4i 

In Questions 
find the corres

In Questions 

18 

- 1. 

-40 

O rthogonal ity 

. . .  that sprightly Scot of Scots, 
that runs a-horseback 
perpendicular-

up a hill 

Douglas, 

-William 
Shakespeare 
Henry IV, Part I Act II, Scene 

IV 

5 . 0  Intro d u ction :  Shadows o n  a Wall 

In this 
chapter, 
extend 
we will 
the notion 
of orthogonal 
projection 
that we 
encoun­
3. Until 
again 
1 and then 
in Chapter 
tered 
now, 
first 
in Chapter 
we have discussed 
subspace 
ntly, 
a single 
only 
projection 
vector 
onto 
(or, equivale
the one-dimensional 
for­
spanned 
by that vector)
we will 
. In this 
section, 
see if we can find 
the analogous 
for example, 
mulas 
for projection 
in IR3. Figure 
onto 
a plane 
5.1 shows 
what 
happens, 
occurs 
process 
parallel light 
create a shadow 
rays 
when 
A similar 
on a wall. 
a 
when 
object is 
dimensional 
three-
displayed 
on a two-dimensiona
l screen, 
such 
as a com­
puter 
monitor. Later in this 
we will 
chapter, 
ideas 
consider 
these 
lity. 
in full genera
To begin, 
let's 
look 
at what 
take 
another 
about 
we alread
projections. 
y know 
In 
in IR2, the standard 
that, 
Section 
onto the 
3.6, we showed 
line 
matrix of a projection 
vector 
direction 
through 
the origin with 
d = [ �:] is 
d,d2] = [ dl!(dl + di) 
d,d2!(dl + di)] 
of the vector 
the projection 
Hence, 
v onto 
is just Pv. 
this 
line 
form 
in the equivalent 
that P can be written 
Problem 1 Show 
[ cos2 () P= cos() sin() cos() sin()] sin2 () 
�  (What does() repres
ent here?) 
in the form 
P r o b l e m  2 Show 
that P can also 
be written 
u is a unit 
P = 
uu T, where 
of d. 
vector 
in the direction 
Problem 3 Using 
2, find P and then find 
Problem 
of v = [ _!] 
the projection 
vectors
lines 
onto the 
the following 
unit 
: 
with 
direction 
(a) " = [ �:�J (b) " = [fl (c) " = [ -n 
uuT, show 
Pis symmetric) 
the form P = 
Problem 4 Using 
pT = P (i.e., 
that (a) 
and (b) P2 = P (i.e., 
t). 
Pis idempoten

di  d,d2!(df + di) 

di!(df + di) 

Figure 5 . 1  

Shado

ws on a wall 

are projections 

3 6 6  

Section 
5.0 Intro
Shado
duction: 
ws on a Wall 3 6 1  
if P is a 2 X 2 projection 
onto 
why, 
Problem 5 Explain 
matrix, the line 
it 
which 
vectors 
is the column 
space 
of P. 
projects 
move into 
IR3 and consider 
onto 
projections 
Now we will 
through 
the 
planes 
. 
We will 
origin. 
explore 
several 
approaches
n and ifv is a vector 
Figure 
5.2 shows one way to proceed. 
through 
in IR3 with 
the origin 
If <!J' is a plane 
v -c n = p for some 
c. 
normal vector 
in IR3, then 
that 
in <!J' such 
p = projgp (v) is a vector 
scalar 
n 
v -en 

Figure 5 . 2  Projection 
onto 
a plane 
the fact that n is orthogonal 
v -c n = p for c to find an ex
n. 
to every 
Problem 6 Using 
in <!/', solve 
vector 
of v and 
for p in terms 
pression 
of 
the projection 
Problem 1 Use the method 
of Problem 
6 to find 
(a) x + y + z = 0 (b) x -2z = 0 (c) 2x - 3y + z = 0 
onto 
the planes 
with 
: 
equations
the following 
onto a 
the projection 
of a vector 
of finding 
to the problem 
approach 
Another 
of v onto 
ose the projection 
by Figure 
plane 
is suggested 
5.3. We can decomp
<!J' into 
the direction 
the 
sum of its projections 
onto 
for <!/'. This 
vectors 
works 
only 
if the 
vectors 
are orthogona
direction 
u2 be direction 
u1 and 
rs. Accordingl
l unit 
y, let 
vecto
for <!J' with 
the property that 
vectors 
llu1 ll = llu2ll = 1 and 

u1 • u2 = 0 

IV I I I I '""" I U2 I 
.L.:....---::::� p = p I + P2 

Figure 5 . 3  

3 6 8   Chapter 
5 Orthogon

ality 

(1) 

(1). 

1/\/6  - 1/V2 

2, the projections 
u2 are 
of v onto 
By Problem 
u1 and 
p2 =  u2ufv 
p1 =  u1ufv and 
projection 
of v onto 
p1 + p2 gives the 
that 
vely. 
respecti
To show 
to show 
!JP, we need 
that v -(p1 + p2) is orthogonal to 
v -(p1 + p2) is 
!JP. It is enough 
to show 
that 
orthogonal to 
both u1 and u2. (Why?) 
u2 • (v -(p1 + p2)) = 0. [Hint: 
u1 • (v -(p1 + p2)) = 0 and 
that 
Problem 8 Show 
xTy = x · y, together 
dot product, 
form of the 
Use the alternative 
with 
the fact that u1 
u2 are or
vectors
and 
thogonal 
unit 
.] 
It follows 
it that 
preceding 
the matri
from Problem 
8 and the comments 
x of the 
the subspace 
!JP of IR3 spanned 
unit 
by orthogonal 
onto 
projection 
vectors 
u1 and 
u2 is 
Problem 9 Repeat Problem 
by Equation 
the formula 
7, using 
for P given 
u1 and 
v and use 
Use the 
same 
u2, as indica
ted below. 
(First, 
verify 
that u1 and 
u2 are 
orthogonal 
plane.) 
vectors 
in the given 
unit 
Uz = [ l/� ] 
u1 = [-�j�] and 
(a) x + y + z = 0 with 
u1 = [21:5] and 
(b) x -2z = 0 with 
u2 = [�] 
0 [ l/v'3] [ 2/\/6] 
(c) 2x -3y + z = 0 with 
u1 = -l/v'3 and 
Problem 10 Show 
matrix 
given 
by Equation 
that a projection 
( 1) satisfies 
proper­
ties 
(a) and (b) of Problem 
P of a projection 
onto 
that the matrix 
Problem 11 Show 
a plane 
in IR3 can be 
as 
expressed 
3 X 2 matrix 
(1) is an outer 
ion.] 
Equation 
that 
for some 
A. [Hint: Show 
product 
expans
Problem 1 2  Show 
that if Pis the matrix of a projection 
onto 
a plane 
in IR3, then 
= 2. In this 
rank(P) 
look 
of orthogonality 
chapter, 
we will 
pro­
at the concepts 
and orthogonal 
jection 
see that the ideas 
introduced 
in greater detail. 
We will 
in this 
section 
can be 
applications 
and that they 
many 
. 
generalized 
have 
important 
O rth o g o n alilV in IR n 
the notion 
in !Rn from 
lity 
section, 
we will 
In this 
of orthogona
generalize 
of vectors 
see that two properties 
so, we will 
. In doing 
to sets of vectors
two vectors 
make the 
standard 
basis 
to work 
{ e1, e2, . . .  , en} of !Rn easy 
with: First, 
in 
any two distinct 
vectors 

u2 =  1/\/6 

p = AAT 

l/v'3 

-1/\/6 

4. 

l/Vs 

• 

z 

Example 5 . 1  

Sels of Veclors 

O rthogonal and Orthonormal 

in lffi11  3 6 9  
Section 
5.1 Orthogo
nality 
are orthogonal. 
the set 
vector 
. These 
in the set 
Second, 
vector
each 
is a unit 
two prop­
the notion 
erties lead us to 
bases 
and orthonormal 
bases-
that 
concepts 
of orthogonal 
be able 
we will 
apply 
of applica
tions. 
to fruitfully 
to a variety 
{v1, v2, ••. , vd in !Rn is called 
an orthogonal set if 
of vectors 
Definilion A set 
of distinct 
all pairs 
vectors 
in the set 
l-tha
are orthogona
t is, if 
i * j for i,j = 1, 2, . . .  , k 
vi· vj = 0 whenever 
{e1, e2, •.• , en} of !Rn is an orthogonal 
set, 
as is 
The stand
ard basis 
any subset 
of it. As 
other 
example 
are many 
tes, 
possi
the first 
illustra
there 
bilities. 
set in IR3 if 
that 
Show 
{v1, v2, v3} is an orthogonal 
from this 
This 
Solution We must 
that every 
pair 
show 
of vectors 
is true, 
since 
Geome
trica
lly, 
the vectors 
in Example 
5.1 are mutually 
, as 4 
perpendicular
5.4 shows
Figure 
. 
Figure 5 . 4  An orthogonal 
set of vectors 
One of the main 
advantages 
of working 
is that 
sets of vectors 
with 
orthogonal 
arily 
em 5.1 shows. 
as Theor
ent, 
linear
they 
ly independ
are necess
If {v1, v2, .•• , vd is an orthogona
these 
in !Rn, then 
vectors 
l set of 
nonzer
o vectors 
are linear
ly indepen
dent. 
Proof If c1, .•. , ck are scalars 
that c1v1 + · · · + ckvk = 0, then 
such 
ntly, 
or, equivale
all of the dot 
set, 
Since 
products 
{v1, v2, . . .  , vk} is an orthogonal 
except 
are zero, 
V; ·vi. Thus, 
to 
( 1) reduces 

in Equation 

Equation 

V1 • Vz = 2(0) +  1(1) + (- 1)(1) = 0 
Vz • V3 = 0(1) +  1(- 1) + (1)(1) = 0 
V1 • V3 = 2(1) +  1(- 1) + (- 1)(1) = 0 

+ ci(vi · vi) + · · · 

c1(v1 · vi) + · · · 

+ ckvk) · vi = 0 · V; = 0 

(c1v1 + · · · 

+ ck(vk · vi) = 0 

(1) 
(1) 

Theorem 5 . 1  

y 

x 

c;(vi · v;) = 0 

set is orthogonal. 

310  Chapter 
5 Orthogon

ality 

V; • V; * 0 because 
V; * 0 by hypothesis. 
Now, 
have 
So we must 
C;  = 0. The fact that 
this 
is true 
for all 
i = 1, . . .  , k implies 
that {v1, v2, . . .  , vk} is a linearly 
indepen
dent 
set. 
it 
is orthogonal, 
em 5.1, we know that 
to Theor
Remark Thanks 
if a set of vectors 
dent. For example, 
indepen
linearly 
is automatically 
we can immedia
t 
tely 
deduce tha
approach 
vectors 
the three 
in Example 
5.1 are linear
ly indepen
dent. Contrast 
this 
linear 
direct
independence 
their 
to establish 
needed 
with 
the work 
ly! 
is 
W of !Rn is a basis of W that 
Definition An orthogonal basis for a subspace 
l set. 
an orthogona
The vectors 
5.1 are orthogonal 
any three 
independent. 
linearly 
and, 
from Example 
hence, 
Since 
for IR3. 4 
for IR3, by the Fundamen
vectors 
in IR3 form a basis 
linearly 
ndent 
tal Theorem 
indepe
oflnvertible 
{v1, v2, v3} is an orthogonal 
Matrices, 
basis 
it follows that 
5.2, suppose only the 
Remark In Example 
orthogonal 
v1 and 
v2 were 
vectors 
asked 
and you were 
given 
{v1, v2, v3} an orthogonal 
v3 to make 
to find a third 
vector 
basis 
for IR3. One way to do 
er that in 
this is to 
rememb
IR3, the cross 
product of two 
of them. (See 
The Cross 
Product 
vectors 
to each 
Explora
tion: 
v2 is orthogonal 
v1 and 
in Chapter 
we may take 
1.) Hence 
vector 
Note that 
is a multiple 
v3 in Example 
the resulting 
of the vector 
5.2, as it must 
be. 
by 
for the subspace 
Find 
an orthogonal basis 
W of IR3 given 
5.3 gives 
Solution Section 
proced
a general 
ure for problems 
of this 
sort. 
For now, 
find the 
we will 
orthogonal 
basis by brute 
force. 
The subspace 
Wis a plane 
through 
x = y - 2z, so W consists 
the origin 
equation 
in IR3. From the 
of the 
plane, 
we have 
of vectors 
of the 
form 

Example 5 . 2  

Example 5 . 3  

3 1 1  

Section 

in IR" 
5.1 Orthogona
lity 
v � [-�] "e a b";' 
foe W, but they 
Me nol mthogo­
It follow' 
that u � [ �] and 
nal. It 
one of 
l to either 
to find another 
in W that 
o vector 
suffices 
is orthogona
nonzer
these. 
w � [ �] ;, a vedm 
to u. Then 
;n W that ;, mthogonal 
Suppo'e 
x -y + 2z � 0, 
u · w = 0, we also 
W. Since 
since 
w is in the 
plane 
x + y = 0. Solving 
the linear 
have 
system 
�  we find 
x -y + 2z = 0 
y = z. (Check this
.) Thus, 
that x = -z and 
w of the form 
any nonzer

o vector 

x + y = 0 

Theorem 5 . 2  

w � [-:J-It ;""Y to ehe<k that {u, w} ;, an 
will 
do. To b"pedfie, 
we wold toke 
basis 
an orthogonal 
orthogonal 
set in Wand, hence, 
dim W = 2. 
for W, since 
advantage 
with 
Another 
of working 
an orthogonal 
basis 
of 
is that the coordinates 
to such 
a basis are easy to 
respect 
with 
a vector 
a formula 
there is 
compute. 
Indeed, 
theor
for these 
coordinates, as the following 
em establishes
. 
subspace 
basis 
{v1, v2, . . .  , vk} be an orthogonal 
Let 
w be any 
W of !Rn and let 
for a 
scalars 
unique 
in W. Then the 
vector 
that 
c1, . . .  , ck such 
by W 'V; 
are given 
C; = -- for i = 1, ... 
{v1, v2, . . .  , vk} is a basis 
Proof Since 
w = c1v1 + · · · +  ckvk (from 
c1, . . .  , ck such 
that 
the dot product of this 
for C;, we take 
formula 
vj · V; = 0 for j 
since 
result
desired 
. 

scalars 
there 
for W, we know that 
are unique 
the 
Theorem 
3.29). To establish 
with 
linear 
combination 
V; to obtain 
by V; • V;, we obtain 
• V; * 0. Dividing 
the 

*  i. Since 

= c1(v1 · v;) + · · · 
= C;(V;' v;) 

+ ckvk) • V; 
w · V; = (c1v1 + · · · 

+ ck(vk • v;) 
+ c;(v; • v;) + · · · 

V; * 0, V; 

, k 

V;' V; 

to the orthogona

l basis 

B = { v1, v2, v3} 

ality 

312  Chapter 
5 Orthogon
Example 5 . 4  Find 

5.2. 3 

of w = [ �] with 
the coordinates 
respect
5.1 and 
of Examples 
Solulion Using 
Theor
em 5.2, we compute 
]  V1 • V1 4 +  1  +  1 
c  =--
=-
C2 = --
= -
C3 = --
Thus, 
�  (Check this.) With 
introduced 
equation 

the notation 
in Section 

3  5 
w· V2 0 + 2 + 
V2" V2 0 +  1 + 1  2 
w· V3 1 -2 + 3 2 
= V3 • V3 1  +  1  +  1 

as 

w· V1 2 + 2 
6 

-3  1 

3 

= 

= 

3.5, we can also write the 

above 

Compare 
the proced
ure in Example 
the work 
5.4 with 
required 
to find these 
bases. 
to appreciate 
coordinates dire
of orthogonal 
ctly 
and you should start 
the value 
the other prope
of this 
at the beginning 
As noted 
section, 
rty of the standard 
basis 
. Combining 
d basis vector 
each 
in !Rn is that 
standar
vector
this 
is a unit 
property 
with 
the following 
orthogon
ality, we 
have 
defini
tion. 
orthonormal set if it is an 
Definition A set of vectors 
in !Rn is an 
orthogonal 
. An orthonormal basis for a subspace W of !Rn is a basis 
set of 
unit 
vectors
of W 
mal set. 
that is an orthonor
q; · qj = 0 for 
i -=fa j and 
Remark If S = { q1, . . .  , qd is an orthonormal 
vectors, 
then 
set of 
to q; · q; = 1. 
q; is a unit 
vector 
each 
llq;ll = 1. The fact that 
is equivalent 
can summarize 
t that S is orthono
It follows 
that we 
the statemen
rmal 
as 
{o if i * j 
q; . qj = 1 if i = j 
[ l/v'3] 
qi= - l/v'3  [ l/v'6] 
that S = {q1, q2} is an orthonor
Show 
mal set in IR3 if 
and 

q2 = 2/v'6 
l/v'6 

l/v'3 

Example 5 . 5  

in lffi"  313 
5.1 Orthogona
lity 

Section 
Solution We check 

that 

q1 • q2 = 1/\/18 - 2/\/18 + 1/\/18 = 0 
qi. qi = 1/3 + 1/3 + 1/3 = 1 
q2· q2 = 1/6 + 4/6 + 1/6 = 1 

Example 5 . 6  

l set, 
an orthogona
If we have 
we can easily obtain 
We 
an orthonorma
each vector
lize 
simply 
norma
. 
an orthonormal 
Construct 
basis for IR3 from the 
in Example 
vectors 
we alread
Solution Since 
y know 
that v1, v2, and 
we nor­
v3 are an orthogona
l basis, 
malize them 
to get 

5.1. 

l set from it: 

Theorem 5 . 3  

[ l l [ l/V3] 

3 

1  l/V3 

q3=11;1r3 = � - 1  = - 110 
for IR3. 
{q1, q2, q3} is an orthonor
Then 
mal basis 
it is linearly 
orthogonal, 
any orthonormal 
is, in particu
Since 
set of vectors 
lar, 
in -
em 5.1. If we have 
depend
by Theor
ent, 
an orthonormal 
basis, 
Theorem 
5.2 becomes 
. 
even 
simpler
w be 
{q1, q2, . . .  , qk} be an orthonormal 
basis for a 
Let 
subspace 
W of IR" and let 
in W. Then 
any vector 
is unique. 
repres
and this 
entation 
that q; · q; = 1 for i = 1, . . .  , k. 
em 5.2 and use the fact 
Proof Apply 
Theor
Matrices 
l set arise 
orthonorma
whose columns 
tions, 
form an 
as 
which 
have 
several 
you will 
see in Section 
5.5. Such 
matrices 
we 
now examine. 

frequently 
attractive 

in applica
properties, 

w =  (w· q1)q1 + (w· qz)qz + · · · 

+ (w· qk)qk 

Orthogonal 

Malrices 

314  Chapter 
5 Orthogon

ality 

(2) 

Theorem 5 . 4  

QTQ = In" 

only 
if 

QT). Since the 

l set if and 

The columns 
of an m X n matrix Q form an orthonorma
Proof We need 
that 
to show 
T  {Q (Q Q)ij = 1 ifi * j ifi = j 
of Q (and, hence, 
the ith row of 
(i,j) 
Let 
q; denote 
the ith column 
of Q, it 
entry 
of QT Q is the dot product 
of the ith row of QT and the )th column 
that 
follows 
(QTQ);j = q;·qj 
of matrix multi
by the definition 
tion. 
plica
Now the columns 
only 
l set if and 
Q form an orthonorma
if 
{o if i * J 
q; . qi = 1 if i = j 
if and only if 
by Equation 
which, 
(2), holds 
T _  {Q (Q Q)ij -1 
ifi * j ifi = j 
completes 
the proof
This 
. 
Orthogonal matrix is an unfortu
­
em 5.4 is a square matrix, 
gy. "Ortho­
bit of terminolo
nate 
l name. 
specia
If the matri
x Q in Theor
it has a 
matrix" 
normal 
be a 
clearly 
would 
Definition Ann x n matrix Q whose columns 
but it is not standard. 
better 
term, 
Moreover, 
for a 
there 
is no term 
mal set is 
form an orthonor
orthonor­
matrix 
nonsquare 
with 
called 
an orthogonal matrix. 
mal col
umns. 
out orthogonal 
theor
by the next 
is given 
The most 
em. 
matrices 
important 
fact ab
A square matrix 
if and only 
if Q-1 = QT. 
Q is orthogonal 
if QT Q = I. This 
is true 
if and only 
em 5.4, Q is orthogonal 
Proof By Theor
if and 
only 
if Q is invertible 
and 
Q-1 = QT, by Theorem 
inverses: 
matrices 
are orthogonal 
that 
Show 
and find their 
the following 
[cos 
{J -sin 
{J] cos 
{J and 
B = sin 
{J 
of A are just 
Solulion The columns 
the standard 
basis 
vectors 
for IR3, which 
are 
A-' �A'� [� � �] 
orthonormal. 
Hence, 
and 
clearly 
A is orthogonal 

3.13. 

Theorem 5 . 5  

Example 5 . 1  

in lffi"  315 
5.1 Orthogona
lity 

Section 

isos 

Theorem 5 . 6  

erving 

that 
For B, we check 
directly 
BrB = [ cos
() sin()] [cos() 
-sin()] cos() 
cos () sin () 
-sin () 
[ cos2 
() + sin2 () 
() + sin () cos ()
-cos () sin 
[� �] =I 
] 
sin2 () + cos2 () 
-sin () cos 
() + cos () sin () 
Therefore, 
B is orthogonal, 
em 5.5, and 
by Theor
-sin() sin ()] cos() 
_1 T [ COS() B = B = 
matri
5. 7 is an example 
tion 
A in Example 
Remark Matrix 
of a permuta
x, a matrix 
the columns 
l, any 
obtained 
by permuting 
of an identity 
In genera
n X n per­
matrix. 
matrix is orthogona
l (see 
Exercise 
25). Matrix 
mutation 
B is the 
matrix of a rotation 
() in IR2. Any rotation 
is a length-pres
has the 
property that it 
the angle
through 
The word 
isometry literally 
means 
em sh
ows that 
as an isometry in geome
transformation 
(known 
next 
try). The 
theor
since 
preserving;' 
"length 
it is 
ry. Orthogonal 
every 
orthogona
tion 
l matri
x transforma
is an isomet
also 
matrices 
Greek roots 
from the 
derived 
preser
ve dot 
products. 
In fact, 
matrices 
are characterized 
orthogonal 
by either 
one of 
("equal") 
"). 
and 
metron ("measure
these 
properties. 
are equiv
statements 
The following 
Let Q be an n X n matrix. 
alent: 
a. Q is orthogonal. 
= llxll for every
b. llQxll 
xin IR". 
c. Qx·Qy=x·y foreveryxandyin!R". 
prove that 
(a)::::} (c)::::} (b)::::} (a). To do so, we will 
Proof We will 
to make use 
need 
ry. 
in IR", then 
of the fact that if x and y are (column) 
vectors 
x · y = x
QrQ =I, and we have 
Then 
that Q is orthogonal. 
(a)::::} (c) Assume 
Qx · Qy = (QxfQy = xrQrQ
y = xrly 
= xry = x · y 
yin IR". Then, 
(c)::::} (b) Assume 
that Qx 
· Qy = x 
taking 
y = x, 
· y for every x and 
·x, so llOxll = v'Qx· Qx = VX:-X = llxll. 
we have 
Qx· Qx = x
property (b) holds 
of Q. 
the ith 
q; denote 
(b)::::} (a) Assume 
that 
and let 
column 
operty (b ), we have 
63 in Section 
1.2 and pr
Using 
Exercise 
x·y = i(llx + rll2 -llx -rll2) 
= i(llQ(x + y)ll2 -llQ(x -y)ll2) 
= i(llQx + Qrll2 -llQx -Qrll2) 
= Qx·Qy 
Now if e; is the ith standard 
in IR". [This 
c).] 
shows that (b) ::::} ( 
for all x and y 
Consequent
basis 
then 
q; = Qe;. 
vector, 
ly, 
q;. % = Qe;. Qej = e;. ej = 1 if i = j 
{o if i * j 
matrix. 
mal set, 
an orthonor
of Q form 
the columns 
Thus, 
is an orthogonal 
so Q 

316  Chapter 
5 Orthogon

ality 

Theorem 5 . 1  

Theorem 5 . 8  

are just the rows of 
of orthogonal 

Theor

A and Bin Example 
at the orthogonal 
5.7, you may notice 
matrices 
Looking 
that 
do their columns 
form orthonorma
not only 
do their 
rows. In 
fact, 
every 
l sets-so 
prope
rty, 
orthogonal 
matrix has this 
as the next 
em shows. 
theor
If Q is an orthogonal 
matrix, 
form an orthonorma
its rows 
then 
l set. 
Q-1 = QT. Therefore, 
that 
em 5.5, we know 
Proof From 
(QT)-1 = co-1)-1 = Q = (QT)Y 
so QT is an orthogonal 
QT-which 
Thus, 
matrix. 
the columns of 
Q-form an orthonorma
l set. 
The final theorem 
in this 
section 
lists 
some 
other 
properties 
matrices. 
matrix. 
Let Q be an orthogonal 
a. Q-1 is orthogonal. 
b. <let Q = ::t:: l c. If 
of Q, then 
A is an eigenvalue 
IAI = 1. 
d. If 
Q1 and Q2 are orthogona
l n X n matrices, 
then 
so is Q1 Q2. 
proofs of 
( c) and leave the 
Proof We will 
prove 
property 
properties 
the remaining 
as exercises. 
of Q with 
A be an eigenvalue 
(c) Let 
Qv = AV, 
corresp
onding 
eigenve
ctor 
v. Then 
em 5.6(b), 
Theor
and, 
using 
we have 
Since 
JJvJJ * 0, this 
that 
implies 
IAI = 1. 
[o -01] 
Remark Property (c) holds 
even 
x 1 
for complex 
The matri
eigenval
ues. 
i and -i, both of which 
absolute 
have 
with 
is orthogonal 
eigenvalues 
value 
1. 

JJvJJ = JJQvJJ = JJAvJJ = IAI JJvJJ 

5 . 1  

which sets of vector

In Exercises 
orthogonal. 

1-6, determine 

..  I Exercises 
I. nHn [-�l 2. uirn m 
,_ [ Jf :H-!l ·· [:H-H Ul 

s are 

311 

Section 

in IR" 
5.1 Orthogona
lity 
1/v6] 1/v6 

- 1/v6 
1/\/2 

1/\/2 
1/\/2 

0 

0 
2/3 
- 2/3 
1/3 

0 

angle-pres

22. Prove 
Theor
em 5.S(a). 
23. Prove 
Theor
em 5.S(b). 
em 5.S(d). 
24. Prove 
Theor
25. Prove 
that 
every 
permuta
tion 
matrix is orthogona
l. 
26. If Q is an orthogonal 
matrix, prove 
matrix 
that any 
by rearranging 
obtained 
the rows 
of Q is also 
orthogonal. 
l 2 X 2 matrix 
27. Let 
Q be an orthogona
and let 
x and 
y 
be vectors 
between 
in IR2. If e is the angle 
x and 
y, 
between 
prove 
Qx and Qy is also 
the angle 
e. 
that 
(This proves 
the linear 
by 
that 
transformations 
defined 
orthogonal 
erving in IR2, a fact 
matrices 
are 
that is true 
in genera
l.) 
have 
2 X 2 matrix must 
that an orthogonal 
28. (a) Prove 
the form 
[a -b] or [a  b] 
where 
[:] is a unit 
vector. 
(b) Using 
show 
part (a), 
l 
that every 
2 X 2 matrix is of the 
form 
[cos e or sine sine] -cos e 
[cos e sine -sine] cos e 
where 
0 :::::: e < 21T. 
(c) Show 
orthogona
l 2 X 2 matrix 
that every 
corre­
to either 
sponds 
or a reflection 
a rotation 
in IR2. 
(d) Show 
that 
l 2 X 2 matrix 
an orthogona
Q cor­
responds 
in IR2 if <let 
to a rotation 
Q = 1 and a 
Q = -1 .  
reflection 
in IR2 if <let 
28 to determine 
29-32, use Exercise 

orthogona

b a 

b -a 

give the angle of rotation; 

whether 
tation 
or a 
if it is 

give the line of reflectio

orthogonal matrix represents a ro
n. If it is a rotation, 

In Exercises 
the given 
reflectio
a reflection, 
n. 
[1/\/2 - 1/\/2] [ - 1/2 
29. 1/\/2  1/\/2 30. - V3/2 
[- 1/2 
31. V3/2 

V3/2] 1/2 

V3/2] - 1/2 

set. 

ation 

of these 

form an ortho­

5.2 to express 

s. Give the 
[w]8 of w with respect to the basis 

vectors 
basis vector

for IR2 or IR3• Then use Theorem 

In Exercises 
gonal basis 
combin
w as a linear 
coordinate vector 

7-10, show that the given 
B = {v1, vJ of!R2 or B = v1, v2, v3 of!R3• 
7.v1 = [ _�], v2 = [�Jw = [ _�] 
8. v1 = [�],v2 = [-�Jw = [�] 

s to form an orthonormal 

orthogo­
s is orthonormal. If it is not, normalize the 

In Exercises 
nal set of vector
vector

11-15, determine whether the given 

9. v, � [ _ +' � [ l; � [ -:} w � [: l 
10. v, � [ l, � [ -J, � [ _n w � m 
11. [il [-n 
13.[!J[-!J[J 12' [il [ -iJ 
l •. [-iH!l Ul 
[ 1/2] [ 0 l [ v'3/2] [ 0 l 
16-21, determine 
[o -
1]  [ 1/\/2 1/\/2] 
18.u -! :i 
-sin2e l -cos e sine cos e 
-cos e sine 0 ! 2 I 2 I 2 I -2 -l] 
[cos e sine 
19. cos2 e sine 
20. [-l I -2 I 2 I 2 ! 2 

1/2 v6/3  - v'3/6  0 
15' - 1/2 '  1/v6 ' v'3/6 ' 1/\/2 
1/2 - 1/v6  -v'3/6 1/\/2 
whether 

In Exercises 
orthogonal. If it is, find its inverse. 
16. 1 
0

17. - 1/\/2 1/\/2 

the given 

matrix is 

B = 0, 

318  Chapter 
5 Orthogon
ality 
B be n X n orthogonal 
that 
vector 
x, a construction 
first 
a prescribed 
(a) Prove that A(A r + Br)B = A + B.  with 
is 
matrices
. 
A and 
33. Let 
tions
frequently 
useful 
in applica
.) 
if an upp
35. Prove that 
er triangular 
matrix is orthogonal, 
if <let 
prove that, 
(b) Use part (a) to 
A + <let 
if n > m, then there is 
no m X n matrix 
be a diagona
then 
it must 
l matrix. 
A + B is not invertible. 
then 
in !Rn. Partition 
x as 36. Prove that 
34. Let 
vector 
x be a unit 
B = {v1, ••• , vn} be an orthonorma
x in !Rn. 
that II Ax II = llxll for all 
such 
l basis for !Rn. 
37. Let 
x and yin !Rn, 
(a) Prove that, 
for any 
2) + · · · 
Q = [-�1--�--------------
1) + (x · v2)(y · v
x · y = (x · v1)(y · v
-] 
I�-----------
Let 
n) 
+ (x · vn)(y · v
is called 
(This 
identity 
y i I - C - xJyyr 
(b) What does 
Parse
val's Identity 
about the 
imply 
products 
the dot 
relationshi
p between 
x · y and 
(This 
that Q is orthogonal. 
Prove 
a 
ure gives 
proced
l basis for !Rn 
for finding 
quick 
an orthonorma
method 

[xla· [y]a? 

Parseval'

s Identit

A 

y.) 

O rth o g o n a l  
O rth o g o n a l  

Complements a n d  
Proiecli
o n s  

O rthogonal Complements 

w_j_ is pronounced 
"w perp:' 
w 
w 
e = w_j_ and 

1. The no­
section, 
we generalize 
in Chapter 
that we encountered 
two concepts 
In this 
of a normal vector 
be extended to 
tion 
orthogona
ts, and 
to a plane 
will 
l complemen
will 
of one vector 
the projection 
give 
rise 
onto 
another 
of orthogonal 
to the concept 
onto 
projection 
a subspace. 
l vector 
in that 
is orthogona
n to a plane 
A norma
plane. If the plane 
l to every 
vector 
it is a subspace 
passes 
then 
through 
the origin, 
W of IR3, as is span
(n). Hence, 
we have 
two subspaces 
of IR3 with 
the proper
vector 
of one is 
ty that every 
orthogonal 
to every 
vector 
of the other
. This 
is the idea 
behind 
tion. 
the following 
defini
of !Rn. We say that 
v in !Rn is orthogo­
a vector 
DefiniliOD Let Wbe a subspace 
vector 
nal to W if v is orthogonal 
that are 
in W. The set 
to every 
of all vectors 
to Wis called 
. That 
the 
orthogonal 
orthogonal complement of W, denoted W_j_
is, 
W_j_ = {vin!Rn:v·w 
= 0 for all win W} 
If Wis a plane 
through 
the origin 
through 
the origin 
in IR3 and e is the line 
perpen­
dicular 
to W (i.e., 
parallel 
to the 
every 
vector 
normal vector 
v on e is 
to W), then 
vector 
every 
orthogonal to 
W consists 
ver, 
e = W_L. Moreo
hence, 
win W; 
vectors 
every v 
w that are orthogonal to 
of those 
we also 
have 
W = e_L. 
on €; hence, 
5.5 illustrates 
situation. 
Figure 
this 

I e 
w = e_j_ 

precisel

Example 5 . 8  

Figure 5 . 5  

y 

Theorem 5 . 9  

Theorem 5 . 10 

319 

intersec­

i = 1, . . .  , k. 

Vis in W.L if and only 

if V · W; = 0 for all 

Section 
5.2 Orthogonal 
and Or
Complements 
thogonal 
Projections 
5.8, the orthogona
turned 
of a subspace 
l complement 
In Example 
out to be an­
t of the complement 
bspace. 
Also, 
the complemen
of a subspace was the origi­
other su
as proper
are true 
These 
nal subspace. 
properties 
in general 
and are proved 
ties 
(a) and 
be useful. 
also 
(b) of Theor
(Recall that 
em 5.9. Properties (c) and (d) will 
the 
B consists 
A and 
elemen
of their 
tion A n B of sets 
common 
ts. See Appendix A.) 
of !Rn. 
Let 
W be a subspace 
of!Rn. 
a. w.L is a subspace 
b. ( W.L).L = W c. w n w.L = {o} d. If W = span(W1, ... , Wk), then 
0 · w = 0 for all win W, 0 is in W.L. Let u and v be in W.L and let 
Proof (a) Since 
Then 
c be a scalar. 
u · w 
= v · w 
= 0 for all w in W 
Therefore, 
(u + v) ·w = u·w 
+ v·w 
so u + v is in W .L. We also have 
(cu) ·w = c(u·w) = c(O) = 0 
of !Rn. 
we see that cu 
that 
is in 
from which 
W .L. It follows 
W .L is a subspace 
prove 
(b) We will 
this 
proper
ty as Corollary 
proper
(c) You are asked 
this 
to prove 
ty in Exercise 
ty in Exercise 
this 
(d) You are asked 
to prove 
proper
the subspaces 
We can now express 
some 
fundamenta
l relationships 
involving 
associated 
with 
an m X n matrix. 
complement 
of 
of the row space 
Then 
Let A be an m X n matrix. 
the orthogonal 
complem
A, and the orthogonal 
null space of 
A is the 
A 
ent of the column space of 
AT: (row(A)).L = null(A) and (col(A)).L = null(AT) 
space of 
is the null 
x is in (row(A)) .L if and only 
in !Rn, then 
if x is orthogonal 
to 
Proof If x is a vector 
if Ax = 0, which 
is equivalent 
this 
every 
row of A. But 
is true 
if and only 
to x being 
To prove 
ity, 
we 
the second 
), so we have 
established the 
ident
ity. 
in null(A
first 
ident
fact that row(AT) = col(A). 
A by AT and use the 
replace 
simply 
: row(A), null(A), col(A), and null(AT). 
has four subspaces
Thus, an 
m X n matrix 
in !Rn, and the last 
two are orthogonal 
two are orthogonal 
complements 
The first 

5.12. 
23. 
24. 

= 0 + 0 = 0 

3 8 0   Chapter 
5 Orthogon

ality 

Example 5 . 9  

null(A) null(AT) 

./  �. 
/ 
col(A) [ffim 
row(A) 
Figure 5 . 6  The four fundamental 
subsp
aces 
m X n matrix 
tion 
a linear 
A defines 
transforma
in �m. The 
complements 
from 
is col(A). Moreover, 
transforma
into 
�m whose range 
this 
tion 
null(A) to 0 in 
sends 
m X n matri
ally. 
�m. Figure 
5.6 illustrates these 
ideas 
schematic
These 
four subspaces 
are called 
x A. 
the 
fundamental subspaces of the 
bases 
subspaces 
for the four fundamental 
Find 
of 
[-; 3  -I] 
- 1  0 A= 2 1 -2 
y Theor
em 5.10. 
and null 

and verif
3.48, we computed 
Solulion In Examples 
3.47, and 
column space, 
that 
row(A) = span(u1, u2, u3), where 
space 
of A. We found 
ll1 = [ 1  0  0 -1 ], u2 = [O 2  0  3 ] , U3 = [ 0  0  0 4 ] 
Also, 
null(A) = span(x1, x2), where 

for the row space, 

bases 

1 6  1 

3.45, 

�n 

- 1   1 
- 2   -3 

X1 = 1 , Xz = 0 
�  each 
that every 
to show 
To show 
that (row(A))1-= null(A), it is enough 
to 
sufficien
t?) 
exercise. 
xj, which 
is an easy 
(Why 
is this 

U; is orthogonal 

0 
0 

-4 

5.2 Orthogonal 
Section 
and Or
Complements 
thogonal 
Projections 
= span(a1, a2, a3), where 
The column 
A is col(A) 
space of 

3 8 1  

Ar. Row reduction 

space of 
the null 
produces 
need 
We still 
to compute 
2 -3 4 0  1 0  0 1 0 
1 - 1  2 1 0  0  0 6 0 
[Ari O ]  3 0  6 0 �  0 0 3 0 
6 - 1  3 0  0  0  0 0 0 
of Ar, then 
space 
that 

So, if y is in the null 
It follows 

y1 -y4, y2 = -6y4, 

and y3 = -3y4. 

1 1 -2 1 0  0  0  0  0 

0 

null(A') � { [ =:�]} � sp�( [ =m 

1 

- 1  

vector 
and it is easy to check 
that this 
to a1, a2, and a3. 
is orthogonal 
situa
to other 
5.9 is easily adapted 
of Example 
The method 
tions. 
by 
W be the subspace 
Let 
of IR5 spanned 
-3 1 
W1 =  5 , Wz=  2 , W3=  4 
5  3 5 
Find 
a basis 
by w1, w2, and w3 
W spanned 
Solution The subspace 
of 
space 
1 - 1  0 -3 1 - 1  A= 5 2 4 
5 3 5 

for W_j_. 

-2  - 1  

0 -2 - 1  

0 
-1 

0 

is the same 
as the column 

Example 5 . 10 

3 8 2   Chapter 
5 Orthogon

ality 

may proceed 

as in 

em 5.10, Wl_ = (col(A))J_ = null(AT), and we 
Therefore, 
by Theor
example. 
the previous 
We compute 
IA'loJ � [-: -3 5 
0  5  OJ [ 1  0  0  3  4  OJ 
-2 3  0 �  0  1  0  1  3  0 
2 
if y1 = -3y4 -4y5, y2 = -y4 -3y5, and 
y is in W1-if and only 
Hence, 
- 1  5  0  0  0  1  0  2  0 
- 1  4 
follows 
that - 3y4 -4y5 

y3 = -2y5. It 

wl_ = -2y5  =span 0 -2 

-y4 -3y5 

- 3  -4 
- 1  - 3  

Y4 
Ys 

1  0 
0 

v 

u 
Figure 5 . 1  v = proj0(v) + perp0(v) 

5.7. 

O rthogonal Proiecli
ons 

basis for WJ_. 
two vectors 
and these 
form a 
of a vector 
o vector 
in IR2, the projection 
that, 
Recall 
v onto 
a nonzer
by 
u is given 
(u•v) proj0(v) = -- u u·u 
j0(v), and we 
perp0(v) = v -proj0(v) is orthogonal to pro
the vector 
Furthermore, 
can decomp
ose v as 
as shown in Figure 
= perpu(v) is in 
w = proju
let W = span(u), then 
If we 
W1-. We 
(v) is in 
Wand w1-
"decomp
therefore 
have 
a way of 
osing" v into 
the sum of two vectors, 
one from Wand 
the other 
idea 
this 
+ wl_. We now generalize 
y, v = w 
W-namel
orthogonal to 
to !Rn. 
Definition Let W be a subspace 
of !Rn and let {u1, ... , uk} be an orthogonal 
basis 
orthogonal projection of v onto Wis 
v in !Rn, the 
for W. For any vector 
defined 
as ( U1 • V )  ( Uk· V ) 
+ -- uk 
projw(v) = -- u, + · · · 
U1 • U1 Uk" Uk 
of v orthogonal to W is the vector 
The 
perpw(v) = v -projw(v) 
in the definition 
summand 
Each 
of projw(v) 
is also 
a projection 
vec­
onto a 
single 
tor (or, equivale
ntly, 
the one-dimensional 
subspace 
spanned 
by it
-in our previous 
tion, 
the notation 
sense). Therefore, 
of the preceding 
with 
we can write 
defini

component 

Section 

5.2 Orthogonal 
and Or

Complements 

thogonal 

Projections 

3 8 3  

w 

F i g u r e  5 . 8  
P = P1 + Pz 

l projection 
U; are orthogonal, 
Since 
Wis the sum 
the orthogona
the vectors 
of v onto 
orthogonal. 
bspaces 
onto 
of its projections 
one-dimensional su
that are mutually 
Fig­
= projw(v), p1 = proj01(v), 
W = span(u1, u2), p 
5.8 illustrates 
this 
situation 
ure 
with 
2 = proj0,(v). As a special 
and p
geometric 
a nice 
ofprojw(v)
of the definition 
case 
, we now also have 
interpretation 
notation 
of Theor
gy, 
and terminolo
em 5.2. In terms 
of our present 
em states 
that theor
that 
if w is in the subspace 
W of !Rn, which 
has orthogonal 
basis 
{v1, v2, •.• , vd, then 
­
a sum of orthogonal 
onto 
mutually 
projections 
into 
Thus, w is decomp
osed 
orthogo
nal one-dimensional 
of W. 
subspaces 
is, 
The definition 
of orthogona
above 
seems 
to depend 
; that 
l basis
on the choice 
{u;, ... , u£} for W would 
basis 
and 
a "different" 
appear to give 
a different 
projw(v) 
For now, 
this 
is not the 
perpw(v). Fortun
ately, 
as we will 
prove. 
let's 
soon 
be 
case, 
with 
content 
an examp
le. 
v � [ -: ] . Find 
x - y + 2z � 0, and let 
tbe 
Let W be tbe pl one in II' with 
equation 
orthogonal 
Wand the component of v orthogonal 
projection 
of v onto 
to W. 
basis 
5.3, we found 
Solution In Example 
an orthogonal 
for W. Taking 
we have ll1 "V = 2 Uz • V = -2 

U1 • U1 = 2 Uz • Uz = 3 

Example 5 . 1 1  

3 8 4   Chapter 
5 Orthogon

ality 

1

Uz. Uz 

U1. U1 

projw(v) = -

Therefore, 
(u ·v)  (u ·v) 
- u1 + -2- u2 
� i[�l-f :J Ul 
perpw(v) = v -projw(v) = -� -i -; 
[ 3] [ �] [ �] 
and 
It 
of the plane. 
the equation 
it satisfies 
projw(v) is in 
W, since 
to see that 
It is easy 
[ -: l to W. ( S" F igmo 
W, since 
is equally 
easy 
to see that 
perpw(v) is orthogonal to 
it is a scalar 
multiple 
of 
l vootm 
thrnorna

5.9.) 

perpw(v) 

Theorem 5 . 1 1  

The Orthogonal Decomposition 

Figure 5 . 9  v = projw(v) + perpw(v) 
sition 
The next 
theorem 
shows that we can 
always 
find a 
decompo
with 
of a vector 
complement. 
and its orthogonal 
respect 
to a subspace 
Let W be a subspace 
there are unique 
of IW and let v be a vector 
in !Rn. Then 
vectors 
win Wand w_j_ 
in W_j_ such 
that 
v=w+w
_j_ 
Proof We need 
decompo
to show 
t such a 
two things: tha
is 
sition 
exists and that it 
unique. To show 
{u1, . . .  ,  uk} for W Let 
we choose 
basis 
an orthogonal 
existence, 
w = projw(v) and let 
w_j_ = perpw(v). Then 
w + w_j_ = projw(v) + perpw(v) = projw(v) + (v -projw(v)) = v 

Theorem 

3 8 5  

- 0 

= u;· v- u;· v= 0 

Section 
5.2 Orthogonal 
and Or
Complements 
thogonal 
Projections 
u1, ... , uk. To show 
it is a linear 
w = projw(v) is in W, 
Clearly, 
of the basis vectors 
combination 
since 
that 
wl. is in Wl., 
it is enough 
to show 
l to 
that wl. is orthogona
each 
vectors 
U;, by Theorem 
of the basis 
5.9(d). We compute 
U; • wl. = U; • perpw(v) 
= u;· (v -projw(v)) 
= u;· ( v -(:1.";Ju1-· · · -(�k.";Juk) 
= u-· v ---(u · 
' U1 ·u1 ' t  u;·u; ' ' 
( U1 "V) (U;"V) 
u) - · · · - --(u · 
u) -· · · 
= u-·v -0 -· · · --'-(u-· u.) -· · · 
(u·v) 
ui·ui z 
z 
z 
u; · uj = 0 for j -=F  i. This 
since 
proves 
that 
wl. is in 
Wl. and completes 
of the proof. 
part 
let's 
suppose 
osition, 
decomp
of this 
To show 
the uniqueness 
de­
we have 
composition 
v = W1 +wt, where
w1isin Wand wt is in wl.. 
Thenw+ w_L = W1 + wt,so 
But since 
these 
w -w1 is in Wand wt -wl. is in Wl. (because 
are subspaces), we 
know that 
is in 
this 
common 
W n Wl. 
vector 
= {O} [using 
Theor
em 5.9(c)]. Thus, 
of IR3 given 
x -y + 2z = 0, the orthogonal 
Example 
l Decomposition 
the Orthogona
Theorem. 
5.11 illustrated 
of v � [ -! ] wdh mped 
the subspace 
with 
by the plane 
equation 
to W ;,. � w + w", whece 
dernmpo,;tion 
w � pmjw(v) � Ul Md w" �  pecpw(v) � Hl 
decompo
sition 
The uniqueness 
of the orthogonal 
es that 
guarante
the definitions 
(v) do not depend 
on the 
of projw(v) and perpw
choice 
The 
of orthogonal 
basis. 
ty (b) of Theo­
us to 
em also 
tion 
Decomposi
Orthogonal 
proper
prove 
allows 
Theor
Decomposition 
te that prope
rem 5.9. We sta
as a corollary 
to the Orthogonal 
rty here 
em. 
Theor
of !Rn, then 
If Wis a subspace 

the existence 
another 

When W is 

C o r o l l a r
v 5 . 12 

3 8 6   Chapter 
5 Orthogon

ality 

Theorem 5 . 13 

C o r o l l a r
v 5 . 14 

w · x = 0. But this 
that w is in 
now implies 
Proof If w is in Wand xis in W_L
, then 
(W_L )_L. Hence, 
W C (W_L )_L. Now let v be in (W_L )_L. By Theor
em 5.1 1, we can write 
. But now 
v = w + w_L for (unique) 
win Wand w_L 
vectors 
in W_L
0 = v · w_L = ( w + w_L) · w_L = w · w_L + w_L · w_L = 0 + w_L · w_L = w_L · w_L 
that 
vis in W. This shows 
v = w + w_L = w, and thus 
so w_L = 0. Therefore, 
( W _L) _L C W 
we conclude 
that 
(W_L)_L 
and, 
since 
the reverse 
inclusion 
is also true, 
= W, as required. 
is also 
in 
of Wand W_L, expressed 
the dimensions 
relationshi
There 
p between 
a nice 
Theor
em 5.13. 
dim W + dim W _L = n 
If W is a subspace of !Rn, then 
Proof Let 
an orthogonal 
{u1, ... , uk} be 
for Wand let {v1
basis 
, ... , v1} be an orthog-
basis 
onal 
w_L = l. Let B = {u1, ... , Uk,vl, ... , v1}. 
dim w = k and dim 
for w_L. Then 
basis 
We claim 
that Bis an orthogonal 
for !Rn. 
We first note 
that, 
since 
each 
u; is in Wand each 
vj is in W_L
, 
U;'Vj = 0 fori = 1, .. .,kandj = 1, . .  . , l  
hence, 
by Theor
B is an orthogonal 
Thus, 
arly 
set and, 
is line
em 5.1. 
independent, 
in !Rn, the Orthogona
if vis a vector 
Next, 
Theor
l Decomposition 
v = 
em tells us that 
w + w_L for some 
win Wand w_L 
in W_L. Since 
w can be written 
as a linear 
combina­
of the vectors 
tion 
of the 
as a linear 
vectors 
combination 
U; and w_L can be written 
vj, v 
of the vectors 
combination 
as a linear 
can be 
written 
in B. Therefore, 
B spans 
!Rn also 
dim w + dim w_L = n 
and so is a basis for !Rn. It follows 
!Rn, or 
+ l = dim 
that k 
of a 
aces 
result to the fundamental 
apply 
As a lovely 
subsp
this 
bonus, 
when we 
matrix, we get a quick 
Theor
proof 
of the Rank 
em (Theor
as 
em 3.26), restated 
here 
Corollary 
The Rank Theorem If A is an m X n matrix, 
(A) = n 
then 
rank(A) + nullity
W_L = null(A), by Theorem 
= row(A
em 5.13, take W 
Proof In Theor
). Then 
(A). The result 
so dim W = rank(A) and dim W_L = nullity
. 
follows
get a counterpar
by taking 
Note 
that we 
t identity 
W = col(A) [and therefore 
W_L = null(AT)J: 
(A r) = m 
rank(A) + nullity

5.14. 

5.10, 

Section 
5.2 Orthogonal 
and Or
Complements 
thogonal 
Projections 
illustrated 
of the advantages 
with 
some 
Sections 
5.1 and 
of working 
5.2 have 
has an or­
not established 
orthogonal 
However, 
bases. 
we have 
that every 
subspace 
l basis, 
nor have 
we given 
for constructing 
a method 
such 
in 
a basis (except 
thogona
particular 
next 
are the subject of the 
5.3). These issues 
such 
examples, 
as Example 
section. 

3 8 1  

11-14, let W 

In Exercises 
given 

be the subspace spanned by the 
for W_j_. 
Find a basis 

vectors. 

5 . 2  

In Exercises 
for W _j_. 
and give a basis 

I Exercises 
1-6, find the orthogonal complement W_j_ of W 
: 2x - y = 0} 
1. W = { [;] 
: 3x + 4y = 0} 
3. w � { [ � l x + y - z 
2. W = { [;] 
� 0) 
4. W � { [:J 2x - y + 3z � 0) 
5. W � { [ � l x � I, y � -I, z � 3t) 
6. W � {[�]x � �t,y � +.z � 21) 
7. A =  [ � -: _!] - 1  - 1  1 
8. A = [-� � -� � ! l 

9 and 10, find bases for the 

7 and 8, find bases for the 

In Exercises 
space of A. Verify that every vector 
to every vector 
in null(A). 

2  2 -2 0  1 
-3 - 1  3  4  5 

row space and null 
in row( A) is 

In Exercises 
A and the null space of AT for the 
that every vector 
in null(A T). 
7 

10. Exercise 
9. Exercise 

orthogonal to every vector 

column space of 
exercise. Verify 

in col(A) is 

given 

8 

2 
2 
14. W1 = - 1  , Wz = 0 , W3 = 2 

4 
6 

2 

1 

- 1  

orthogonal 

- 1   -3  2 

15-18, find the orthogonal projection 

of v onto 

In Exercises 
the subspace W spanned by the 
vector
that the vector

s U; are orthogonal.) 

s u;. 

(You may assume 

of 

0. 

In Exercises 

In Exercises 

19-22,find the orthogonal decomposition 

3 8 8   Chapter 
5 Orthogon
ality 
W J..? 
that 
that v = 
w + w'. Is it necess
arily 
true 
w' is in 
Either 
or find a 
prove 
that 
counterexample. 
it is true 
v with respect to W 
19. v = [-�l W =span([�]) 
for !Rn and let 
26. Let {v1, ... , vn} be 
l basis 
an orthogona
that Wl. = 
arily 
W = span(v1, ... , vk)· Is it necess
true 
20.v � [ J W � 'P"•([:J) 
k+l' ... , vn)? Either 
span(v
prove 
that it 
is true 
or find a 
ple. 
counterexam
27-29, let W be a subspace of !Rn, and let x be 
21. F  [-H w � 'P""( [�].[-:])  27. Prove that 
a vector in !Rn. 
xis in W if and only 
if projw(x) = x. 
W if and only 
28. Prove 
if 
that xis orthogonal to 
projw(x) = 
22. F  [-; l W � 'P"" ( [J [-m 29. Prove 
that projw(projw(x)) = 
projw(x). 
l set in !Rn, and 
30. Let S = {v1, ... , vk} be an orthonorma
in !Rn. 
let x be a vector 
(a) Prove that 
llxll2 2: lx·v1l2 + lx·v2l2 + · · · 
+ lx·vkl2 
23. Prove Theor
em 5.9(c). 
(This 
inequality 
is called 
24. Prove Theor
em 5.9(d). 
in !Rn. Suppose 
of !Rn and v a vector 
is an equality 
w in Wand (b) Prove that Besse
l's Inequality 
if and 
subspace 
25. Let W be a 
only 
if xis in span(S). 
that 
w and 
with 
vectors 
w' are orthogonal 
and the OR Factorizatio n  
l (or or­
for const
ructing 
an orthogona
In this 
section, 
we present a simple 
method 
of !Rn. This 
l) basis for any subspace 
will 
thonorma
then 
method 
lead 
one of 
the 
us to 
useful 
of all matrix 
factoriza
most 
tions
. 
W of !Rn. The idea 
like 
orthogonal 
basis 
to find an 
to be able 
We would 
for a subspace 
an arbitrary 
basis 
{x1, ... , xd for Wand to "orthogona
is to begin 
with 
lize" it 
one 
vector 
at a time. 
We will 
illustrate 
the basic 
bspace 
construction with 
the su
W from 
Example 
5.3. 
Let W = span(x1, x2), where 
for W. 
an orthogonal basis 
Construct 
vector 
to it by 
that 
Solulion Starting 
with 
get a second 
x1, we 
is orthogonal 
of x2 orthogonal 
the compo
to x1 (Figure 
nent 
5.10). 

taking 

The Gram-Schmidt 

The Gram-Schmidl 

Process 

Bessel's Inequalit

Example 5 . 12 

Process 

y.) 

3 8 9  

Section 

QR Factori

and the 

Process 

5.3 The Gram-Schmidt 

zation 
w 
v2 orthogonal 

Constructing 

F i g u r e  5 . 1 0  

Algebraic
ally, 

we set 

v1 = x1, so 
v2 = perpxJxz) = x2 -projx, (xz) 

to x1 
= x  -(� )x 

2 

X1 •xi I 

Then {v1, v2} is an orthogonal 
dent set and therefore 

set of vectors 
for W, since 

a basis 

Remark Observe that this method 

mtocs. Jn Exrunpl'

5.12, if we had taken 

ly indepen-

depends 

{v1, v2} is a linear

in W. Hence, 
dim W = 2. 

4 
x, � [-fl and x, � [ i l we would have 

on the order of the original basis 

a different 

�  obtained 
Example 
5.12. 
quent vectors 
method 

orthogona
The generalization 
Then the process 
orthogona

is to 
l to all of the 
is known as the Gram-Schmidt 

for W. (Verif
iteratively 
vectors 
Process. 

of this method 

l basis 

y this.) 

to more than two vectors 

construct the components 

that have already 

been constructed. 

begins as in 
of subse­
The 

Theorem 5 . 15  The Gram-Schmidt Process 

Let {x1

, . . .  , xd be a basis 

for a subspace 

W of !Rn and define the following: 

_ (v2 : xk)vz _ ... 

Vz Vz 

{v1, .•. , vk} is an orthogonal 

basis 
for W. 

Then for each i = 1, . . .  , k, {v1, . . .  , v;} is an orthogonal 

for W;. In particular, 

basis 

l 

that, 

basis 

such a 

Stated 

basis. 

basis, 

of ll�r has an orthogona

for each i = 1, . . .  , k, {v1, . . .  , v;} is an or­

subspace 

for constructing 

succinc
and it gives 

tly, Theorem 5 .15 says that every 
an algorithm 

Proof We will prove by induction 
that, 
thogonal 
basis 
for W;. 

ality 

3 9 0   Chapter 
5 Orthogon
Jorgen 
Pedersen 
Gram 
(1850-1916) was a Danish 
actuary 
cian) 
statisti
(insurance 
who was 
interested 
in the science of 
mea­
surement. 
He first 
published 
the 
process 
in 
that 
bears his name 
an 1883 paper 
on least 
squares. 
Schmidt (1876-1959) was 
Erhard 
a German 
mathem
who 
atician 
studied under 
the great David 
Hilbert and is considered 
one 
of the branch 
of the founders 
of 
mathematics 
known 
as functional 
analysis. 
His contribution 
to the 
Gram-Schmidt 
Process 
came 
in a 
1907 paper 
equations, 
on integral 
in which 
out the details 
he wrote 
than 
of the method 
more 
explicitly 
Gram 
had done. 

So, by the Orthogonal 
definition, v1, . . .  , v; are linear 
combina
Therefore, 

Since v1 = x1, clearly 
{vi} is an (orthogonal) 
for some i < k, {v1, . . .  , v;} is an orthogonal 

V;+1 = X;+1 -projw,Cx;+1) = perpw,CX;+1) 
Decomposition Theorem, 

{v1, . . .  , V;+ d  is an orthogonal 
otherwise 

By the induction 
W;. Hence, 

V;+ J  X;+ J   . V1  . Vz 

{v1, . . .  , v;} is an orthogona

V; V; 
l basis 

V;+ i  * 0, since 

impossible, since 

Moreover, 

hypothesis, 

= _ (VI• X;+1) _ (V2 • X;+1) _ ... _ (V; • X;+1) 

for W1 = span(x1). Now assume 

V1 V1  Vz Vz 

basis for W;. Then 

tions 
set of vectors 
in W;+ i ·  
X;+1 = projw,CX;+1), which in turn 
{x1, . . .  , X;+ 
i} 
lin-
of i + ! 
in W;+ i ·  Consequently, {v1, . . .  , V;+ d  is a basis 
for W;+ 1, 

, . . .  , X;) and 
(Why?) We conclude that {v1, . . .  , V;+d is a set 
the proof. 
1. This completes 
for W, we simply 

W; = span(x1

mal basis 
by the Gram-Schmidt Process. 

that X;+ 1 is in W;. But this is 
is line
early 
since 

arly independent. 
independent 
vectors 
dim W;+ 1 = i + 
If we require 
orthogonal 
vectors 
replace 

an orthonor
produced 

V; by the unit vector 

q; = (l/llv;ll)v;. 

V;+ 1 is orthogonal 

for span(x1, . . .  , x;) = 

to W;. By 

. V; 

W;. 

implies 

need to normalize the 
That is, for each i, we 

of x1, . . .  ,  X; and, hence, are in 

Example 5 . 13 

Apply the Gram-Schmidt Process 
W = span(x1, x2, x3) of IR4, where 

to construct 

an orthonormal 

basis 

for the 

subspace 

Solulion First we note that {x1, x2, x3} is a linear
for W. We begin 

ly indepen
we compute the 

by setting v1 = x1. Next, 

dent set, so it forms a basis 
component of x2 orthogo

­

nal to W1 = span(v1): 

Section 

5.3 The Gram-Schmidt 

Process 

and the 
tion 3 9 1  

QR Factoriza

For hand calcula
When we are 
an orthonorma
without 
affecting 

tions, 

it is 
finished, 
we can rescale the orthogonal 
l set; thus, 

fraction
a good idea to "scale" v2 at this point to eliminate 
to obtain 
we can replace 

s. 
constructing 

set we are 

scalar multiple 

each V; by any convenient 
y, we replace 

v2 by 

the final result. 

Accordingl

We now find the 

W2 =  span(x1, xi) =  span(v1, v2) =  span(v1, v�) 

l to 
component of x3 orthogona

using 

the orthogonal 
basis {v1, v�}: 

we mrnle and"'"; � 2v, � [-!l 

{v1, v�, v�} for W. (Check to 

orthogonal 
l.) To obtain 

an orthonorma

are orthogona

l basis, 

basis 

vectors 

Agllin, 

�  We now have an 

these 
vector: 

make sure that 

we normalize each 

[-11 [-1/V6] [-V6/6] 
q3 =  C1:�11)v� = ( �) � =  1�V6 �/6 
2 2/V6 V6/3 

mal basis 

for W. 

Then {q1, q2, q3} is an orthonor

3 9 2   Chapter 
5 Orthogon

ality 

One of the important uses of the Gram-Schmidt Process 

nal basis that contains 

a specified 

vector. 

The next example 

is to construct 
illustrates 
this applica

an orthogo
tion. 

­

Example 5 . 14 

Find an orthogona

l basis for 

IR3 that contains the 

vector 

Solulion We first find any basis for IR3 containing 
v1• If we take 

�  then {v1, x2, x3} is clearly 

Process 

to this basis to obtain 

a basis for IR3. (Why?) We now apply the Gram-Schmidt 

and finally 

Then {v1, v�, v�} is an orthogonal 

basis 

for IR3 that contains 

v1. 

Similar

ly, given a unit 
the preceding method 

we can find an orthono
and then normalizing 

vector, 

that contains 

it by 
orthogonal 
vectors. 

the resulting 

rmal basis 

using 

is almost always 

Remark When the Gram-Schmidt Process 
error, leading 

tors q;. To avoid this loss of orthogon

some roundoff 

ality, 

is implemented 
to a loss of orthogona

on a computer, 
there 

lity in the vec­
are usually 

made. The 

q;, and as each q; is computed, 

as soon as they are computed, rather 
xj are modified 

q;. This procedure is known as the Modified Gram-Schmidt 

the remaining 

vectors 

V; are normalized 
vectors 

vectors 
give the 
to be orthogonal to 
however, 
Process. 
orthonor

In practice, 
mal bases. 

QR factoriza

a version 

of the 

some modifications 

than at the end, to 

tion is used to compute 

The  OB Factorization 
If A is an m X n matrix with linear
then applying the 
torization 

of A into the product of 

Gram-Schmidt Process 

ly indepen

dent columns 
columns 

(requiring 
yields 

a very 

to these 

a matrix Q with orthonormal columns 

that m 2 n), 

useful fac­
and an 

Section 

5.3 The Gram-Schmidt 

Process 

and the 

QR Factori

zation 

3 9 3  

matrix 

upper triangular 
R. This is the 
numerical approximation 
and to 

of eigenvalues, 
squares 

QR factorization, 
which we explore 

the problem 

ofleast 

and it has applications 
this section, 

to the 
at the end of 

To see how the QR factorization 

columns 
Gram-Schmidt Process to 

of A and let 

q1, . . .  , qn be the orthonorma

A with normalizations. 

approximation, 
which we discuss 
arises, let a1, . . .  , an be the (line
obtained 

in Chapter 7. 
arly independent) 
by applying 
5.15, we know that, 

l vectors 
From Theorem 

the 

for each i = 1, ... , n, 

W; = span(a1, . . .  , a;) = span(q1, . . .  , q;) 

Therefore, 

there are scalars r1;, r2;, • . •  , r;; such that 

That is, 

a, = rllq, 
az = r12q1 + Yzzqz 

which can be 

written 

in matrix form 

as 

r1nl r�n = QR 

rnn 

0 

Theorem 5 . 16 

Clearly, the matrix 

of R are all nonzero. 

Q has orthonorma
To see this, 
is in 

nal entries 
combination 
bination of a1, . . .  , a;_ 1, which is impossible, since 
We conclude that r;; 
must be invertible. 

-=fa 0 for i = 1, ... , n. Since 

of q1, . . .  , q;_1 and, hence, 

(See Exercise 
We have proved the following 

23.) 
theorem. 

l columns. It 

is also the case that the diago­

observe that if r;; = 0, then a; is a linear 
W;_1. But then a; would be a linear 

com­

a1, . . .  , a; are 

linear

R is upper triangular

ly independent. 
, it follows that it 

l columns and 

R is 

dent columns. Then A can be fac­

matrix. 

with linear

ly indepen

upper triangular 

The QR Factorization 

tored 
an invertible 

Let A be an m X n matrix 
as A = QR, where Q is an m X n matrix with orthonorma
•  We can 
•  The requirement 

suppose that A is an m X n matrix 
Q) = n, since 
So rank(A) = n too, and 

To prove this, 
orem 5.16. Then, since 
by Exercise 
61 in Section 3.5. 
therefore, 
linear
A are linearly 

we have Q = AR-1• Hence, 
its columns 

also arrange 
q; by -q; and r;; by -r;;. 

by the Fundamental 

ly independent. 

R is invertible, 

independent, 

ly independent 

Theorem. 

diagonal 

entries 

replace 

have linear

But rank( 

simply 

for the 

that A 

Remarks 

of R to be positive. 
If any r;; < 0, 
a necessary one. 
tion, 

columns is 

that has a QR factoriza

as in The­
rank(Q) = rank(A), 
are orthonormal and, 

consequently 

the columns 

of 

3 9 4   Chapter 
5 Orthogon

ality 

Example 5 . 15 

m X n matrix R. Then 
We will 
examine this 

matrix. 

modified form. If A is m X n, it is possible to find a sequence of orthogona

l matrices 

arbitrary 

matrices 

in a slightly 

tion can be extended  to 

•  The QR factoriza
A= QR, where Q = (Om-I· · ·  Q2Q1)-1 is an orthogonal 
Q1, .•. , Om-I such that Om-I· · ·  Q2Q1A is an upper triangular 

QR Factoriza
tion. 

The Modified 

approach in 

Explora

tion: 

so 

1/2 

1  2 

- 1  0  1 

from Example 

From Theorem 

1/2 Vs/10 

- 1/2 3Vs/10 

Q has orthonor

The orthonormal 

use the fact that 

Vs/10  V6/3 

Find a QR factorization 

Solulion The columns 
basis for col(A) produced 

of A are just the vectors 
5.13. 
by the Gram-Schmidt Process was 

of A=[-�� �1 
q1 = [=:;:1, qz = [��::1, q3 = [-��:1 
Q = [qi qz q3] = - 1/2 Vs/10 
[ 1/2  3Vs/10 
-v:/61 V6/6 
5.16, A = QR for some upper triangular 
QT Q = I. Therefore, 
QTA = QTQR =IR = 
We compute [ 1/2 
Vs/10 \/5/10 l =: 
R = Q'A = 3Vs/10 
2 �l 1 
- 1/2  [ 1 
1/2 l 3Vs/2 
-V6/6 � [: 1 
2. X1 = [ _ � l Xz = [ �] 
3 ... � [ J x, � [ n x, � m 

0 
V6/6 V6/3 1 

V6/3 
matrix 

- 1/2 
3Vs/10 

mal columns and, hence, 

R. To find R, we 

V6/2 

Vs 
0 

1/2 

0 

R 

..  I Exercises 

5 . 3  

1-4, the given vectors 

In Exercises 
Apply the Gram-Schmidt Process 
basis. 
basis. 

for IR2 or IR3• 
form a basis 
to obtain an orthog
onal 
to obtain an orthonormal 

Then normalize 

this basis 

Section 

5.3 The Gram-Schmidt 

Process 

and the 

QR Factori

zation 

3 9 5  

In Exercises 

5 and 6, the given vector

subspace W of IR3 or IR4. Apply the Gram-Schmidt Process 
to obtain an orthogonal basis 

for W 

s form a basis 
for a 

7 and 8, find the orthogonal decomposition 

of v 

w "' i• Emd", 

In Exercises 
with respect to the subspace W 

7. v � [ -:} 
8. F  [i} W asinExmise6 
9. [: � �] 
veoto' [:J 

11. Find an orthogonal 

Use the 
for the 

basis 

to find an orthogonal basis 
Gram-Schmidt Process 
column spaces of the matrices in Exercises 

9 and 10. 

for IR3 that contains 

the 

12. Find an orthogonal 

basis 

for IR4 that contains 

the 

vectors 

of the 

1/2 
1/2 
1/2 

In Exercises 

given exercise. 
9 

In Exercises 
matrix in the 
15. Exercise 

13 and 14, fill in the missing entries of Q 

15 and 16, find a QR factorization 

to make Q an orthogonal matrix. [ 1/\/2  1/\/3 *] 
13. Q =  0  1/\/3 * 
- 1/\/2 1/\/3 * [1/2 
2/Vi4 * l/Vi4 * 0 * -3/Vi4 * :] * * 
14. Q = 
17 and 18, the columns of Q were obtained by 
2] [ ' 
matrix R such that A =  QR. 
l -!] 3 � 3 
17.A = u 
- 1  'Q =  i 
2 3 
3] [ 1/v'6 
1/:3
18.A = [-j 
-� , Q  = 
- 1�V6 
if and only if A =  QR, where 

In Exercises 
applying the Gram-Schmidt Process 
Find the 
8 
7 
-2 

20. Prove that A is invertible 

19. If A is an orthogona

1/\/3 
1/\/3 
_ 

16. Exercise 

upper triangular 

to the columns of A. 

1  --3 

l matrix, 

of A. 

2/V6 

10 

4 

Q is orthogonal 
entries 

on its diagonal. 

and R is upper triangular 

21 and 22, use the method suggested by 
20 to compute A - i for the 

In Exercises 
Exercise 
exercise. 
21. Exercise 9 
23. Let A be an m X n matrix 

22. Exercise 15 
with linear

ly indepen

dent 

matrix A in the given 

with nonzero 

columns. Give an 
matrix 
triangular 
be invertible, 
using 
Theorem. 

proof 

alternative 
that the upper 
R in a QR factorization 
property (c) of the Fundamental 

of A must 

24. Let A be an m X n matrix with linear

ly independent 

columns 
be a QR factoriz
Show that A and Q have the same column 

and let A = QR 

ation of A. 
space. 

find a QR factorization 

Explorations 0 

The Modified QR Factorization 

as we have stated 
QR factorization 

it does not work and so 
of A. There is 

When the matrix A does not have linearly independent 
Process 
eralized 
that can be used, 
triangular 
method 
using 

of the Gra
that converts 
a sequence of orthogona
tion, 

a sequence of elementary 
matrices

to that of LU factoriza

m-Schmidt 
a gen­
cannot be used to develop 
Process 

m-Schmidt 
A into upper 
l matrices. The 

in which the matrix L is formed 

form one column at a time, 

but instead 

a modification 

is analogous 

we will explore 

columns, the Gra

a method 

using 

The  first 

thing we need is the "orthogonal 

analogue" of an elementary 

matrix; 

that is, we need to know how to construct 
form a given 

By Theorem 5.6, it will be necessary that llxll = II Qxll = llrll. Figure 5.1 1  suggests 

column of A-call it x-into the corresponding column 

an orthogonal 
matrix 

of R-call it y. 

Q that will tr

x in a line perpendicular 

way to proceed: 

We can reflect 

ans­

. 

a 

to x -y. If 

vector 

standard matrix 

Q of the reflection 

3.6 to find the 

in the direction 

in the direction 

u, and 

ofx -y, then uj_ = [-�:]is orthogonal to 
of u J_. 
-2d1d2] T d2 =I -2uu. 
-2d,d2 
1 -2 2 
(a) u = [fl (b) x = [:J,r = [�] 

is the unit 
we can use Exercise 26 in Section 
in the line through the origin 

1. Show that Q =  1 [l -2d2 
2. Compute Q for 
define an n X n matrix Q as 

of Q as follows. If u is any unit vector 
Q =I -2uur 

the definition 

generalize 

in !Rn, we 

We can 

y 

figure 5 . 1 1  

3 9 6  

Such a matrix is called 

a Householder matrix (or an elementary reflector). 

3. Prove that every 

Householder 

matrix Q satisfies 

the following 
: 

properties

(a)  Q is symmetric. 

(c) Q2 = I  
(b) Q is orthogonal. 

4. Prove that if Q is a Householder 

matrix corresponding 

to the unit vector 

(u) 
Qv = v ifv· u  = 0 

u, then  {-v ifv is in span
5. Compute Q foe u � [ -�] <md wdfy 

6.  Let 

Prnblem'3 ond 4. 

(1904-1993) 
Householder 
Alston 
field 
was one 
in the 
of the pioneers 
linear 
of numerical 
He 
algebra. 
was the first 
to present a systematic 
solving 
of algorith
treatment 
ms for 
problems 
involving linear 
systems. 
In addition 
the 
to introducing 
widely 
Householder trans­
used 
bear 
his name, 
formations 
he 
that 
the first 
was one of 
to advocate 
the 
systematic 
use of norms 
in linear 
algeb
ra. His 1964 book 
Analysis is 
considered 
a classic. 

x * y with llxll = llrll 
Householder 
lt in Problem 4.] 

corresponding 
Section 1.2 to the resu

ready to perform the triangularization 

7. Find Q and verify Problem 6 for 

Qx = y. [Hint: 

matrix Q satisfies 

The Theory 
in Numerical 

We are now 

by column. 

of Matrices 

8. Let x be the first column 

of A and let 

and set u = (1/ llx -rll)(x -y). Prove that the 
Apply Exercise 57 in 

of an m X n matrix A, column 

Show that if Q1 is the Householder 
with the block 
form 

matrix given by Problem 6, then Q1A is a matrix 

whereA1 is (m -l)X(n - 1). 

If we repeat Problem 8 on the matrix 

A 1, we use a 

Householder 
matrix P2 

such that 

where A2 is (m - 2) X (n - 2). 

2 = [ � :J. Show that Q

9. Set Q 

matrix and that 
2 is an orthogonal 

3 9 1  

10. Show that we can continue 

matrices 
matrix (i.e., 

r;1 = 0 if i > j). 

Qi, . . .  , Qm-i such that Q

1 1. Deduce that A = QR 
12. Use the 

method 

of this exploration 

with Q =  Qi Q

to find a QR 

in this fashion 

m X n 
m-i · · · Q2QiA = R is an upper triangular 
2 · · · Qm-i orthogonal. 

to find a sequence of orthogona

l 

factoriza
tion of 
3 
- 4  
- 5  

- 1  

3 

Approximating Eigen

with the QR Algorithm 

values 

best (and most widely 
of a matrix makes use of the 

used) 

methods 

for numerically approximating 

the 
The purpose of this ex­

is to introduce 

this method, 

and to show it at 

work in a 

QR factoriza
the QR algorithm, 

tion. 

One of the 
eigenvalues 
ploration 
few examples. For a 
numerical 
tions 

linear 

consult any 

good text on 

to use a CAS 

to perform the calcula­

t of this topic, 

more complete 

treatmen

Matrix Computations 

algebra. (You will find it 

in the problems 
Given a square matrix A, the first step is to 

F. Van 
Hopkins 
Johns 
1983). 

See G. H. Golub and C. 
Loan, 
(Baltimore: 
University Press, 
2. If A  = [ � �],find A1 and verify that it has the 

is appropria
1.  First 

prove that Ai is similar to 

te). Then we define Ai = RQ. 

factor it as 

values 

method 

helpful 

below.) 

as A. 

Continuing 

the algori
factor A2 = Q2R2 and set A3 = R2Q2, and so on. 
QkRk and then set Ak+i = RkQk. 

thm, we factor Ai as Ai= QiR1 and setA2 = RiQi. 

That is, fork 2: 1, we compute Ak = 

same eigenvalues 

as A. 
Then we 

A. Then prove that Ai has the same eigen­

A = QR (using 

whichever 

3. Prove that Ak is similar 
4. Continuing 

to A for all k 2: 1. 

Problem 2, compute A2, A3, A4, and A5, using 

two-decimal-

place 

accuracy. 

What do you notice? 

It can be shown that if the eigenvalues 

of A are all real and have distinct 

absolute 

values, then the matrices 
Ak approach 

5. What will be true of the diagona
6. Approximate 
the eigenvalues 
accuracy 
two-decimal-

U? 
of the following 
by applying 
five iterations. 

and perform at least 

matrices 

algorithm
. Use 

the QR 

an upper triangular 
U. 
of this matrix 

l entries 

matrix 

place 

(c) [ � � -�] (d) [ � 2 
(a) [� �] (b) [� �] 
-�] 
3]. What happens? 

A  = [ 2 

- 4  0  1  - 2  4  2 

algorithm 

to the matrix 

- 1  

- 2  

7. Apply the QR 

Why? 

3 9 8  

8. Shift  the eigenvalues 

0.9I. Apply the QR algorithm 
B = A + 
from the (approximate) eigenvalues 
eigenvalues 
of A. 

of B. Verify 

9. Let Q0 = Q and R 0 = 

R. First show that 

of the matrix in Problem 7  by replacing 
A with 
0.9 

to B and then shift back by subtracting 

that this method 

approximates the 

QOQI . . .  Qk-lAk = AQ0Q1 . . .  Qk-1 

for all k 2: 1. Then show that 

use the 

same approach 

[Hint: Repeatedly 
the "inside 
of Ak+i. 

out:'] Finally, deduce 

that (Q0Q1 · · · Qk)(Rk · · · R1R0) is the QR factorization 

used for the first 
equation, 

working 

from 

3 9 9  

4 0 0   Chapter 
5 Orthogon

ality 

Orthogonal Diagonalizalion ot svmmetric Matrices 

matrix 

ues. Indeed, 

[o - 1] 

We saw in Chapter 4 that a square 
eigenval
the matrix 1 0 
discovered that not all square matrices 
dramatic
our attention 
ally if we restrict 
in this section, 
matrix is always 

with real entries 
will not necessarily 
has complex eigenvalues 
are diagona
to real symmetric 
a real symmetric 

all of the eigenvalues of 
diagona

have real 
i and - i. We also 

matrices. As we will show 
matrix are real, 

lizable. 

lizable. 

and such a 

Recall that a symmetric 

studying 

the diagonalization 

matrix 
process 

is one that equals 
for a symmetric 

its own transpose. Let's 
2 X 2 matrix. 

begin by 

The situation changes 

Example 5 . 16 

If possible, 

diagonalize 

Solulion The characteristic 
which we see that A has eigenvalues 
ing eigenvectors, 
we find 

2 + A -6 = ( A  + 3 )(A - 2), from 

for the correspond­

vely. So A is diagona

respecti

p-1AP = [-� �] = D. 

However, 

we can do better. 
malize them to 

get the unit eigenve

and if we set P = [ v1 v2], then we know that 

that v1 and v2 are orthogonal. 

So, if we nor­

A1 = - 3  and A2 = 2. Solving 

2 -2 
polynomial of A is A 

the matrix A = [ 1 2] . 
v1 = [ _�] and v2 = [�] 
[ l/Vs] 
U1 = -2/Vs  and u2 = [2/Vs] l/Vs 
[ l/Vs 2/Vs] 

Q =  [u1 Uz] = -2/Vs l/Vs 

lizable, 

Observe 

ctors 

and then take 

we have Q-1AQ = D also. But now Q is an orthogonal matrix, 
orthonormal 
set of vectors. Therefore, 
a transpose!) 
easy, since 
checking is 

Q-1 only involves 

Q-1 = QT, and we have QTAQ = D. (Note that 

since {u1, u2} is an 

computing 

taking 

The situation 

in Example 

5.16 is 

the one tha

t interests 

us. It is important enough 

to warrant 

a new definition. 

D efi n iii 0 n A square matrix 

orthogonal 
matrix 

Q and a diagonal 

matrix D such that QT AQ = D. 

A is orthogonally diagonalizable 

if there 

exists 

an 

We are in

in finding 
diagonalizable. 

terested 
conditions 
Theorem 5.17 shows us where to look. 

under  which 

a matrix is orthogona

lly 

Section 

5.4 Orthogonal 

Diagonalization 
Matrices 

of Symmetric 

4 0 1  

Theorem 5 . 1 1  

If A is orthogona

lly diagonaliza

ble, then 

A is symmetric. 

Theorem 5 . 1 8  

If A is a real symmetric 

matrix, 

then the eigenvalues 

of A are real. 

Proof If A is orthogona
trix Q and a diagona
I =  QQT, so 

lly diagonaliza

l ma­
l matrix D such that QT AQ = D. Since Q-1 = QT, we have QT Q = 

ble, then  there 
exists 

an orthogona

But then 

since 

every 

diagonal 

matrix is symmetric. 

Hence, 

A is symmetric. 

Remark Theorem 5.17 shows that the orthogona

lly diagona

lizable 

all to be found among the symmetric 
matrix must be orthogona
indeed 
Finding a proof 
rest of this section. 

lly diagona

matrices. It does not say that every 
lizable. 
for this amazing 

it is a remarkable 
occupy us for much of the 

result will 

However, 

is true! 

are 

matrices 
symmetric 
fact that this 

We next prove that we don't 
matrices 

ing with symmetric 

with real entries. 

need to worry about complex eigenvalues 

when work­

z = a -bi (see Appendix C). To show 

Recall that the complex conjugate 

way to 
follows 

do this is to show 
that b = 0. 

that z = z, for then bi = - bi (or 2bi = O), from which 

number z = a + bi is the number 
that b = 0. One 
we need to show 
it 

of a complex 
that z is real, 

We can also extend the notion 

of complex conjugate to 
vectors 
are the complex 

by, 
conjugates of 

and matrices 

A to be 

defining 

ily to matrices; 

the matrix whose entries 

is, if A = [a;), then A = [ au] .  The rules 

for example, 
the entries of A; that 
extend eas
A andB. 
Proof Suppose that A is an eigenvalue 
Av= Av, and, taking 

of A with corresp

- -

in particu

complex conjugates, we have Av= Av. But then 

onding 

lar, we have AB = AB for compatible 

jugation 
for complex con
matrices 

eigenve

ctor v. Then 

since 

A is real. 
Taking 

the fact that A is symmetric, 
we have 

Av = Av = Av = Av = Av 

transposes and using 

vTA  = vTAT = (Avf = (Avf = Avr 

A(vrv) = vr(Av) = vr(Av) = (vrA)v = (Avr)v = A(vrv) 

Therefore, 

or (A  -A)(vrv) =  o. [a,� b,il __ [a,� b,il 

Nowifv =  .  , thenv-

. , so 

an + bni 

an - bni 

4 0 2   Chapter 
5 Orthogon

ality 

since 
Hence, 

v * 0 (because 
A is real. 

it is an eigenvector). We conclude that A  - A  = 

0, or A  = A. 

Theorem 4.20 showed 

that, 
eigenvalues 

for any square matrix, 
are linearly independent. 

eigenve
For symmetric 

ctors 
matrices, 

corresponding 

something 

to distinct 
stronger 
is true: 

Such eigenvectors 

are orthogonal. 

Theorem 5 . 19 

If A is a symmetric 
eigenvalues 

of A are orthogonal. 

matrix, then any 

two eigenvectors 

correspond

ing to distinct 

1 = A1v1 and Av2 = A2v2. Using AT= A and the fact that x · y = xTy 

ctors corresponding to  the 

distinct 

Proof Let v1 and v2 be eigenve
A1 * A2 so that Av
for any two vectors 

x and y in !Rn, we have 

(A1 - A2) (v1 • v2) = 0. But A1 - A2 * 0, so v1 • v2 = 0, as we wished 

to show. 

Hence, 

(vfAT)Vz 
= vi(A2v2) 

A2(vfv2) =  A2(v1 • v2) 

(vf A)v2 =  vf(Av2) 

eigenvalues 

Example 5 . 11 

Verify 

the result of Theorem 5.19 

for 

polynomial of A is - A3 + 6A 2 -9A + 4 = - (A - 4) · 

eigenvalues 

of A are A1 = 4 and A2 = 1. The 

Solulion The characteristic 
(A -1)2, from which it follows 
corresponding 

eigensp

that the 

aces are 

�  (Check this.) We easily verify that 

from which it follows 
(Why?) 

that  every 

vector 

in £4 is orthogonal 
in £1. 

to every vector 

Roman Note thot [-�] • [-i] � !. Thos, 'igenve

need not be orthogonal. 

same eigenvalue 

do;s conesponding 
to the 

Section 

5.4 Orthogonal 
Diagona

lization of 
Matrices 

Symmetric 

4 0 3  

We can now prove the main result 

of this section. 

It is called the Spectra

rem, since the set 
(Technically, 
matrix. 
we should 
there is 
a corresponding result 

of a matrix is sometimes 
call Theorem 5.20 
for matrices 
.) 

of eigenvalues 

with complex 

entries

called 

the Real Spectral Theorem, 
since 

l Theo­
the spectrum of the 

Theorem 5 . 2 0  

The Spectral Theorem 

Let A be an n X n real matrix. 

diagonalizable. 

Then A is symmetric 

if and 

only if it is orthogona

lly 

on n. For n = 1, there 

Theorem 5.17. To prove 
the "only 
if" 
to do, 
a 
k X k real symmet­

Let n = k + 1 and let 

lly diagonalizable. 

that every 

is nothing 

since 

of A and let v1 be a correspond
that v1 is a unit 

ing eigenvector. 
vector, 

since 

have an eigenvector 

correspond
v1 to an orthonormal 

ing 
basis 

tion, 

y in diagona

we proceed 

already proved 

the "if" part as 

by induction 

l form. Now assume 

real eigenvalues 

Let A1 be one of the eigenvalues 

Proof We have 
implica
1 X 1 matrix is alread
ric matrix with 

is orthogona
matrix with real eigenval
ues. 

A be an n X n real symmetric 
Spectrum is a Latin 
word 
meaning 
� 
vibrate, 
"image:' 
they 
atoms 
When 
emit 
light. 
And when 
light 
passes 
it spreads out 
through 
a prism, 
into 
a spectrum 
-a band 
of 
Then v1 is a real vector 
rainbow 
colors. Vibration 
otherwise 
frequencies 
to the 
correspond 
eigenvalues 
tor 
of a certain 
opera
to A1. Using the Gra
{v1, v2, . . .  , vn} of !Rn. Now we form the matrix 
and are 
in the 
visible 
as bright 
lines 
spectrum 
is emitted 
oflight that 
from a prism. 
Thus, 
we can liter-
ally 
see the eigenvalues 
of the atom 
in its spectrum, 
and for this 
rea-
son, 
it is appropriate 
that 
the word 
spectrum has come to 
be appli
ed 
all eigenvalues 
to the set of 
of a 
matrix 
(or operator). 

Ql = [v1 Vz ... vnJ 

Then Q1 is orthogonal, 

(why?)  and we  can 

it and we will still 

m-Schmidt Process, 

we can normalize 

we can extend 

assume 

and 

David Hilbert 

1905, the German 
of Gottingen in 
he delivered 
In a lecture 
mathematician 
at the University 
on certain infini
linear 
te-dimensional 
acting 
( 1 862-1 943) considered 
operators 
of a quadratic 
spaces. 
vector 
Out of this 
lecture 
the notion 
many 
arose 
form in infinitely 
variables, 
and it was in this 
context 
Hilbert first 
used 
spectrum to mean a 
the term 
that 
complete 
set of eigenvalu
es. The spaces 
in question 
are now 
called 
them 
major contributions 
Hilbert 
integral 
of mathematics, among 
areas 
to many 
made 
of mathema
, geomet
equations, 
number theory
tics. 
ry, and the foundations 
In 1900, at the 
Second 
International 
Congress of 
Mathematicians 
in Paris, 
Hilbert 
gave 
entitled 
an address 
"The Problems 
In it, he challenged 
mathematicians 
of Mathematics:' 
to solve 
23 problems 
coming 
during the 
tal importance 
of fundamen
of the 
problems 
been 
have 
century. Many 
others 
solved-some 
may never 
were 
d. Nevert
proved 
false-
and some 
heless, 
true, 
be solve
rt's 
Hilbe
speec
h energized 
the mathema
tical 
community 
and is 
often 
as the most 
regarded 
influential 
speec
h ever 
given 
about 
mathema
tics. 

Hilbert 

spaces. 

4 0 4   Chapter 
5 Orthogon

ality 

sincevf(A1v1) = A1(vfv1) = A1(v1 · v1) = A1 and vf(A1v1) = A1(vfv1) = A1(v; · v1) = 0 

{v1, v2, •.. , vn} is an orthonor

for i of-1, because 

mal set. 

But 

so B is symmetric. 

Therefore, 

B has the block 
form 

�  and A1 is symmetric. 

of B is equal to the 

nomial 
Exercise 39 in Section 
istic 

polynomial 

Furthermore, 

B is similar to 

characteristic 
4.3, the characteristic 
that the eigenvalues 

poly­
polynomial of A, by Theorem 4.22. By 
polynomial of A1 divides the 
of A 

character­
of A 1 are also eigenvalues 

A (why?), so the 

characteristic 

of A. It follows 

..-...  and, hence, 

are real. 

real symmetric 
Hence, 
D1. Now let 

there is 

We also see that A1 has real entries. 

(Why?) Thus, A1 is a k X k 

matrix with real eigenval
an orthogona

hypothesis 
l matrix P2 such that PiA1P2 is a diagonal 

applies 
matrix-say, 

induction 

ues, so the 

to it. 

Then Q2 is an orthogonal 
Consequently, 

(k +  l)X(k +  1) matrix, and therefore 

so is Q  =  Q1 Q2. 

QrAQ = (Q1Q2fA(Q1Qz) = (QIQf)A(Q1Qz) =  QI(QfAQ1)Qz =  QIBQ2 

for all n 2 1, an n X n real symmetric 

diagonalizable. 

which is a diagonal 

matrix. 

This completes 

the induction 
step, 

and we 

matrix with real eigenvalues 
lly 

conclude that, 
is orthogona

Example 5 . 18 

Orthogona

lly diagonalize 
the matrix 

Solution This is the matrix from Example 
eigenspaces 

of A are 

5.17. We have already found 

that the 

Section 

lization of 
Matrices 

Symmetric 

4 0 5  

We need three 

orthonorma

the Gram-Schmidt Process 

to 

we apply 

. First, 

l eigenvectors

5.4 Orthogonal 
Diagona
[-�] and r-il 
nl and [!] 

to obtain 

;n E, 

The new wctm, 

vectors, 
vectors 

which h., been rnmtructed 

to be orthogoml to [-� l ;, ''ill 
,._...  (why?) and '° ;, o,thogonal to [: l Thu,, we haw th'"  mutually 
- 1/v6] 2/v6 
mns. We find that [1/v3 
Q =  l/v3 
Q'AQ � [� � �] 

and all we need to do is normalize them 
as its colu

- 1/v2 
0 
l/v2 

a matrix Q with these 

and construct 

rward to verify that 

is straightfo

mthogona

and it 

- 1/v6 

l/v3 

l 

allows 

The Spectral Theorem 

A = QDQT, where Q is orthogonal 
are just the eigenvalues 
using the 
q1, . . .  , qn, then, 
column-

D 
of A, and if the columns of Q are the orthonor
we have 

row representation 

us to write a real 
symmetric 
and D is diagonal. 

matrix 
The diagona

A in the form 
l entries of 
mal vectors 

of the product, 

the spectral 

This is called 
matrix, 
tion onto 
decomposition 

by Exercise 62 in Section 
the subspace 
spanned 

decomposition 

of A. Each of the terms A;q;qT is a rank 1 
ral 

the matrix of the projec­

by q;. (See Exercise 25.) For this reason, 
the spect

3.5, and q;qT is actually 

is sometimes referred 

to as the projection form of the Spectral 

Theorem. 

4 0 6   Chapter 
5 Orthogon

ality 

Example 5 . 19 

l/VJ 

1/3 

1/3 

Find the 

spectral decomposition 

of the matrix A from Example 
5.18. 

l/VJ] =  1/3 

q3 =  2/\/6 

qi=  l/VJ  , 

Therefore, 

1/3 
1/3 
1/3 

Solulion From Example 

5.18, we have: [ l/VJ] 

[- 1/\/6] 
- 1/\/6 [1/3 
1/3] 1/3 
[ 1/2 � - 1�2] 
- 1�2 0 1/2 [ 1/6 
1/6] - 1/3 
so  [t t t
l [ ! 0 
=4ttt+ 0 0  t t t 
by qi and q3. (See Exercise 26.)  4 

The rank 2 matrix q2qi + q3qr is the 
subspace 

two terms A2q2qi + ,\3q3qr 

matrix of a projection 

,\2 = ,\3, so we 

In this example, 

easily verified. 

- 1/3 
2/3 
- 1/3 

matrix A explic­

could combine 

a symmetric 

which can be 

the last 

the plane) 

itly in terms of its eigenvalues 
matrix with given eigenvalues 

and eigenvectors. 
and (orthonormal) eigenvectors. 

spectral decomposition expresses 
This gives 

spanned 

Observe 

(i.e., 

- 1/3 
1/6 

that the 

to get 

-! 0 

1/6 

onto the two-dimensional 

us a way of constructing 

a 

Example 5 .2 0  

Finda2 X 2 matrixwitheigenvalu

es,\1= 3and,\2 = -2andcorresp

ondingeigenvectors 

Section 

5.4 Orthogonal 

Diagonalization 
Matrices 

of Symmetric 

401 

Solution We begin by normalizing 
{q1, q2}, with 

the vectors 

to obtain 

an orthonormal 

basis 

Now, we compute 

the matrix A whose spectral decomposition is 

A  =  A1q1qf +  A2qzqi 

=  3[i] [t �] - 2[-i] [-� tl 
-25  25 
25  25 
=  3[!s !1] [ 16 -�] 
= [-J �] 
�  It is easy to check that A has the 

25 - 2 25 

properties. 

desired 

12 16  12 

(Do this.) 

5 . 4  

12. If b oF 0, 

Orthogonally diagonalize the matrices in Exercises 
by finding an orthogonal matrix Q and a diagonal 
matrix D such that QT AQ = D. 

I Exercises 
1-10 
2. A =  [-1 3] 3 -1 
I.A =[� :J 
4. A =  [ 9 - 2] - 2 6 
[ 1 V2] 
3. A =  V2 O 
5. A  = [ � : : 
] 6. A  = [ � ! 
�] 
7. A =  u : -�] 8. A =  [� � ;] 
9. A  = [ � � � ! ] I 0. A  = [ � � � �: 

of A is nonnega
symmetric 
matrix B. 

1 1 .  If b * 0 ,  orthogona

lly diagonalize 

orthogona

lly diogonolire 

b  a

A  = [a
b]. 
A  = [ � � n 

orthogon

13. Let A and B be 
and let c be a scalar. 
lly diagonalizable: 

matrices 
Use the 
Theorem to prove that the following 
orthogona
(a) A +  B 

ally diagonalizable n X n 
Spectral 
are 
matrices 

(b) cA  (c) A2 

14. If A is an invertible 
matrix 

nalizable, 

show that A-1 is orthogon

that is orthogona

lly diago­

ally diagonalizable. 

15. If A and Bare orthogona

lly diagonaliza

ble and 

AB = 

BA, show that 

AB is orthogona

lly diagonalizable. 

16. If A is a symmetric 

matrix, 

show that every eigenvalue 

tive if and only if A = B2 for some 

17-20, find a spectral decomposition 

of the 

In Exercises 
matrix in the given 

exercise. 

4 0 8   Chapter 
5 Orthogon

ality 

2 X 2 matrix with 

1 
5 

2 
8 

s A1 and A2 and corresponding orthogonal 

18. Exercise 
20. Exercise 

17. Exercise 
19. Exercise 
In Exercises 
eigenvalue
eigenvectors v1 and v2• 

21 and 22, find a symmetric 
21.A1 = - l, A2 = 2, v1 = [�], v2 = [ _�] 
22. A1 = 3,A2 = -3,v1 = [�], v2 = [-�] 
23 and 24, find a symmetric 

In Exercises 
eigenvalues A1, A2, and ,\3 and corres
eigenvectors v1, v2, and v3. 

3 X 3 matrix with 

ponding orthogonal 

23 A, �  1,A, � 2,A, � 3,v,  � [}, � [-:]. 

25. Let q be a unit vector 

in !Rn and let W be the subspace 

by q. Show that the orthogona

v onto W 

(as defined 

in Sections 

l projection 
1.2 and 5.2) is 

of a 

spanned 
vector 
given by 

projw(v) = (qqT)v 

y in !Rn, x · y = xTy.] 

is thus qq T. 

for x and 

matrix of this projection 

and that the 
[Hint: Remember that, 
q1, . . .  , qd be 

26. Let {

an orthonorma
l set 

of vectors 

in !Rn 

and let W be the 
(a) Show that the matrix of the orthogona

spanned by this set. 

subspace 

l projection 

by 

onto W is given 

p  = q1qf + . . .  +  qkql 
(c) Let Q = [q1 · · · 

(b) Show that the 
symmetric 

projection 

matrix P in part (a) is 
P2 = P. 

and satisfies 

qk] be then X k matrix whose 
vectors 

of W. 

columns are 
Show that P 

the orthonormal basis 
= QQT and deduce 

that rank(P) = k. 

27. Let A be 

an n X n real matrix, all of whose eigenvalues 
are real. 
Prove that there 
and an upper triangular 
This very useful result 
ization 
tral Theorem.] 

exist 
matrix 
is known as Schur's Triangular­

an orthogonal 
matrix 
T such that QT AQ = T. 

Theorem. [Hint: Adapt the proof 

of the Spec­

Q 

28. Let A be a nilpotent 

tion 4.2). Prove that there is 
such that Q
27.] 
diagonal. 

T AQ is upper triangular 
[Hint: Use Exercise 

matrix (see Exercise 56 in Sec­
an orthogona
l matrix Q 
with zeros 

on its 

Applications 

Quadratic Forms 
An expression 
of the form 

ax2 + by2 + cxy 

is called 

a quadratic 

form in x and y. Similarly, 

ax2 + by2 + cz2 + dxy + exz + fyz 

is a quadratic 
which has total 
form, but x2 + y2 + x is not. 

form in x, y, and z. In words
degree 

form is a sum of terms, 
, a quadratic 
5x2 -3y2 + 2xy is a quadratic 
two in the variables. Therefore, 
as follows: 
matrices 

We can represent quadratic 

each of 

forms using 

ax2 + by2 + cxy = [x y][c;2 

c�2J[;J 

Section 

5.5 Applications 

4 0 9  

and 

ax2 + by2 + cz2 + dxy + exz + fyz =  [x y z] [d�2 

y these.) Each has the form x

vation leads 

us to 

the following 

general 

definition. 

�  (Verif

e/2 
T Ax, where the matrix 
A is symmetric. 

Defi n ition A quadratic form in n variables 

form 

is a function 

d/2 e/2] [x] 
f :  !Rn ---+ IR of the 

b f/2  y 
f/2  c  z 
This obser­

where A is a symmetric 
associated 

with f 

n X n matrix and x is in !Rn. We refer 

to A as the matrix 

Example 5 . 2 1  

What is the quadratic form with associated 

Solution If x =  [ :J then 

matrix 

A = 
[ 2 - 3]? 

- 3  5 

Observe that the off-diagonal entries 

the coefficient -6 of x1x2• This is true 
n variables 

xT Ax as follows: 

xTAx =  a11x� + a22x� + · · · 

generally. We can expand a quadratic 
form in 

a12 =  a21 =  -3 of A are combined to give 
+ annx� + 2: 2a;jxixj 

i<j 

Thus, if i * j, the coefficient 
of X;Xj is 2aij. 

Example 5 . 2 2  

Find the matrix associated 

with the quadratic 

f (x1, x2, x3) =  2x� - xi + 5xf + 6x1x2 -3x1x3 

form 

Solution The coefficients 
coefficients 
of the 

of the squared terms x;2 go on the diagonal 
oduct terms x;xj are split 

as a;;, and the 
between aij and aji· This gives 

cross-pr

0 -�] 

3 
- 1  

4 1 0   Chapter 
5 Orthogon

ality 

f(x,.x,, x,) � [x, x, x,J[ _� -! -�J[::l 

so 

as you can easily check. 

In the case of a quadratic 
in IR3. Some examples 

form f(x, y) in 

are shown in Figure 

a surface 

Observe that the effect of holding 

to the yz or xz planes, 
sections 

are easy to identify

cross 

the graph parallel 
all of these 
cross sections 

so f(x, y) 2 0 for all values 

opening downward 
a saddle 
point. 

parabolas 
producing 

we get by holding 

5.12. 

two variables, the graph of z = j(x, y) is 
of 
For the graphs in Figure 
5.12, 
example, 
in Figure 5.12(a), the 

x or y constant is to 
respecti
vely. 
.  For 

take a cross 

section 

x or y constant are all parabolas 
5.12(c), holding 
parabolas 

of x and y. In 
Figure 
and holding

opening 
x constant gives 
opening upward, 

y constant gives 

upward, 

z 

z 

y 

(b) z = -2x2 - 3y2 
z 

x 

y 

x 

(a) z = 2x2 + 3y2 

z 

x 

y 

(c) z = 2x2 - 3y2 
of quadratic 
f (x, y) 

forms 

Figure 5 . 1 2  Graphs 

(d) z = 2x2 

5.5 Applications 

4 1 1  

What makes this type of analysis 
quite 
have no cross-product terms. The matrix 
diagonal 

For example, 

matrix. 

easy is the fact that these 
forms 
associated with such a quadratic 

quadratic 

form is a 

Section 
2x2- 3y2= [x y][� -�J[;J 

the matrix of a quadratic 

In general, 
tion 5.4 that such matrices 
show that, 
means of a suitable change 

can always 
for every quadratic 
of variabl

Let f(x) = xT Ax be a quadratic 

e. 
form in n variabl

form is a symmetric 

be diagonalized. 

matrix, 
We will now use this fact to 

and we saw in Sec­

form, we can elimina

te the cross-product terms by 

By the Spectral Theorem, 

matrix. 
there is 
that is, QT AQ = D, where D is a diagonal 
now set 

an orthogona
matrix displaying 

es, with A a symmetric 
l matrix Q that diagonalizes 
A; 

n X n 

the eigenvalues 

of A. We 

X =  Qy Or, equivalently, 

y = Q-1X = QTX 

Substi

tution into the 

quadratic 

form yields 
xTAx = (QyfA(Qy) 
= yTQTAQy 
= yTDy 
form without 

cross-pr

which is a quadratic 
more, if the eigenvalues 

of A are A1, . . .  , An, then Q can be chosen 
so that 

oduct terms, 

since 

D is diagonal. 

Further­

Ify = [yl 
becomes 

Yn] T, then, 

with respect 

to these 

new variables, the quadratic 

form 

yTDy = A1Y12 + · · . + Any; 

is called 

This process 
lowing 
theorem, 
will become 

clear 

diagonalizing a 

quadratic form. We have just proved the fol­

known as the Principal 
in the next 

Axes Theorem. (The reason 
.) 

subsection

for this name 

Theorem 5 . 2 1  

The Principal 

Axes Theorem 

form can be diagonalized. 

the quadratic 

Every quadratic 
ric matrix associated with 

matrix such that QT AQ = D is a diagonal 
x = Qy transforms 
which has no cross-pr
y = [y1 Yn f, then 

the quadratic 

if A is then X n symmet­

Specific
form xT Ax and if 
matrix, then the change 

Q is an orthogonal 

of A are A1, •.. , An and 

of variable 
form yTDy, 

form xT Ax into the quadratic 

oduct terms. If the eigenvalues 

ally, 

xTAx = yTDy = A1y� + · · · 

+ Any; 

4 1 2   Chapter 
5 Orthogon

ality 

Example 5 .2 3   Find a change 

of variable 

the quadratic 

form 

with no cross-pr

into one 
Solulion The matrix off is 

oduct terms. 

that transforms 

f (x1, x2) = 5xf + 4x1x2 + 2x� 
A = [� �] 

�  (Check this.) Ifwe set 

with eigenvalues 

then QT AQ = D. 

converts f into 

ctors 
are 

unit eigenve

The change of variable 
x = Qy, where 

A1 = 6 and A2 = 1. Corresponding 
q -[2/Vs] and q2 = [ l/Vs] 
I -l/Vs -2/Vs 
= [2/Vs l/Vs] and D = [6 o1] 
Q l/Vs -2/Vs 0 
x = [::] and y = [;:] 
f(y) = f(y1, yi} = [Y1 Y2 l [ � �] [;:] = 6yf + Yi 
f(x) = xT Ax at x = [ -�]. We have 

form xT Ax and the new one 
in the following 
In Example 

yT Dy (referred 

quadratic 

are equal 

sense. 

5.23, 

The original 
pal Axes Theorem) 
to evaluate 

to in the Princi­
suppose we want 

j(-1,3) = 5(-1)2 + 4(-1)(3) + 2(3)2 = 11 
In terms of the new variables, [Y1] =  = TX= [2/Vs l/Vs] [-1] = [ l/Vs] 
y  Q l/Vs -2/Vs 3  -7 /Vs 
= 11 

f(yl, yi} = 6yf + y� = 6(1/Vs)2 + (- 7  /Vs)2 = 55/5 

exactly 

as before. 

y2 

so 

The Principal Axes 

Theorem has some interesting 

We will consider 
form can take on. 

two of these. 

The first relates 

to the 

and important 
possible values that a quadratic 

consequences. 

form f(x) = xTAx is classified as 
the followin

one of 

g: 

for all x -=fa 0 
for all x -=fa 0 

Definition A quadratic 
de.finite if f(x) > 0 
semidefinite if f(x) ::=:: 0 for all x 
de.finite if j(x) < 0 
semidefinite if f(x) :s 0 for all x 

1. positive 
2. positive 
5. indefinite if j(x) takes 

3. negative 
4. negative 

on both positive and negative 

values 

Section 

5.5 Applications 

4 1 3  

A symmetric 
tive definite, negative 

f(x) = xT Ax has the corresp

matrix A is called positive 
property. 

definite, positive 
semidefinite, or indefinite if the associ

onding 

semidefinite, nega­
ated quadratic form 

The quadratic 

forms in parts (a), (b ), (c), and (d) of Figure 5.12 are positive 
indefinite, 

negative definite, 
Theorem makes it easy to tell if a quadratic 

and positive semidefinite, 
form has one of 

respecti
vely. 
these 

The Principal Axes 
properties

. 

definite, 

Theorem 5 . 2 2  

and only 

Let A be an n X n symmetric 
a. positive definite if 
b. positive semidefinite 
c. negative definite 
d. negative semidefinite 
if and only 
e. indefinite 

matrix. 
if all of the 

The quadratic 

form f(x) = xT Ax is 
eigenvalues of A are positive. 
eigenvalues of A are negative. 

if and only 

if all of the eigenvalues 

if and only 
if A has both positive and negative 

if all of the 

eigenvalues of A are non 
positive. 

eigenvalues. 

if and only 

if all of the 

of A are nonnegative

. 

You are asked to prove Theorem 5.22 in Exercise 27. 

Example 5 . 2 4  

Classify f(x, y, z) = 3.x2 + 3y2 + 3z2 -2xy -2xz -2yz as positive definite, 
definite, 

indefinite, 

or none of these. 

negative 

Solution The matrix associ

ated with f is 

[-� -� =�] 

- 1   - 1  3 

which has eigenvalues 
tive,f is a positive definite 

1, 4, and 4. (Verif
form. 

quadratic 

y this.) Since 

all of these 

eigenvalues 

are posi­

If a quadratic 

form f(x) =  xT Ax is positive definite, 
Similar

at the origin. 

occurs 

value of f(x) is 0 and it 
form has a maximum 

then, since f(O) 

= 0, the 

ly, a negative definite 

Thus, Theorem 5.22 allows us to solve 

minimum 
quadratic 
certain 
types 
of problem 

of maxima/minima 
that falls into this category 

easily, without 
is the constrained 

at the origin. 
problems 

resorting to calcul
optimization 
values 

problem. 
of a quadratic 

us. A type 

It is often important 

to know the maximum or minimum 

arise 

ts. (Such problems 

not only in mathematics 

certain constrain

form subject to 
but also in statistics, 
physics, 
the extreme values 
finding 
In the case of a quadratic 
means. The graph of z = f(x, y) 
the point (x, y) to the 
points that lie simultaneously on the 
to the xy plane. 
est and 
lowest 
form and corresponding 

surface 
These points form a curve lying 
points on this curve. 

cs.) We will 
engineering, 
and economi
of f(x) = xT Ax subject to the const
raint 
what the problem 
form in two variabl
in IR3, and the const
raint llxll = 1 restricts 
perpendicular 

be interested 
in 
that II xii = 1 .  

Thus, we are considering 

es, we can visualize 

and on the unit cylinder 

unit circle in the xy-plane. 

Figure 5.13 shows this situation 

for the quadratic 

and we want the high­

on the surface, 

is a surface 

in Figure 

surface 

those 

5.12(c). 

4 1 4   Chapter 
5 Orthogon

ality 

z 

Theorem 5 . 2 3  

y 

of z = 2x2 -3y2 with 
x2 + y2 = 1 
Figure 5 . 1 3  The intersection 
the 
cylinder 
of f(x, y) = 2x2 -3y2 (the high­
which 
are 2 and -3, respecti
vely, 

the maximum 
and minimum 
points on the curve 

of intersection) 

values 

associa

ted matrix. Theorem 

5.23 shows that this is 

In this case, 

est and lowest 
are just the eigenvalues 
always 

the case. 

of the 

n X n symmetric 
of A be A1 :::::: A2:::::: ···:::::: Aw Then the following 
nt II xii = 1: 

Let f(x) = xT Ax be a quadratic 
Let the 
to the constrai
a. Ai 2: f(x) 2: An 
b.  The maximum value of f(x) 

form with associated 

eigenvalues 

matrix A. 
are true, 

is a unit 

when x 

corresponding 

to A1. 
c.  The minimum 
value of f(x) 
correspond
ing to Aw 

is A 1, and it occurs 
is An, and it occurs 

subject 

when x 

is a unit 

eigenvec

tor 

eigenvec

tor 

Proof As usual, 
orthogonal matrix 

we begin by orthogona
such that QT AQ is the 

lly diagonalizing 
diagonal 

A. Accordingl
y, let Q be an 

matrix 

Then, by the Principal 
yTDy. Now note that y 

Axes Theorem, 
the change 
= QTx implies 
that 

of variable 

xT Ax = 
x = Qy gives 

using x · x = xTx, we see that 

vector, so is the corresp

onding 

llrll = Wr  = �  = 

of xT Ax 
y, and the 
values 

since QT=  Q-1. Hence, 
llxll = 1. Thus, if x is a unit 
and yTDy are the same. 

Section 

5.5 Applications 

4 1 5  

Example 5 . 2 5  

(a) To prove property (a), we observe that 

ify = [y1 · ·· Yn]T, then 

f (x) = xTAx = yTD y 

+ Any; 
= A1Yl + Azyi + · · · 
:::; A1yf + A1Yi + · · · 
+ A1y; 
+ y;) 
= A1 (yf + Yi + · · · 
= A1 llrll2 
= A1 

Thus, f(x) s A1 for all x such  that 

(See Exercise 37.) 
(b) If q1 is a unit eigenvector corresponding 

llxll = 1. The proof that f(x)  2: 

An is sim

ilar. 

to A1, then Aq1 = A1q1 and 

f(q1) = qfAq1 = qf A1Q1 = A1(qfq1) = A1 
quadratic 

This shows that the 
erty (a), it is the maximum 
(c) You are asked to prove this property in Exerc
ise 38. 

form actually 

value of f(x) and it occurs when x = q1. 

takes on the value A

1, and so, by prop­

5.23, 

values 

Find the maximum 

and minimum values 

of the quadratic 

Solution In Example 

subject to 
the constraint 
for which each of these 
occurs. 

4x1x2 + 2xi 
A2 = 1, with corresponding unit eigenvectors 

form f(x1, x2) = 5xi + 
xi + xi = 1, and determine 
_ [2/Vs] and _ [ 1/Vs] 
Qi -1/Vs Qz --2/Vs 
value off is 6 when x1 = 2/Vs and x2 = 1/Vs. The mini­
off is 1 when x1 = 1/Vs and x2 = -2/Vs. (Observe that these 

we found thatfhas the associated 

opposite directions
-since 

the maximum 

-q1 and -q2 are als

of x1 and x2 

A1 = 6 and 

Therefore, 
mum value 
values 
vectors 

occur twice-in 
for A1 and A2, respecti
vely.) 

extreme 

eigenvalues 

o unit eigen­

Graphing 
The general 

Quadratic Equations 
form of a quadratic 

equation 

in two variables 

x and y is 

ax2 + by2 + cxy + dx + ey + f = 0 

where at least 
called 
of a (double) 
tions are 
are called 

one of a, b, and c 

(or conics), since 

is nonzero. 

The graphs of such quadratic 
sections 

equations 
are 
cross 

they can be 

obtained 

by taking 

conic sections 

as a specia

circles 

slicing 

it with a plane). The most important 

cone (i.e., 
the ellipses (with 
the nondegenerate conics. Figure 5.14 shows how they 
a cone to result in a single 

l case), hyperbolas, 
arise. 

ble for a cross 

It is also possi
section of 
or a pair of lines. These are 
called 
The graph of a nondegene
rate conic 
the coordinate axes if its equation 

degenerate conics. (See Exercises 
is said to be in standard 
position 
expressed 

to 
in one of the forms in Figure 5.15. 

of the conic 
sec­
and parabolas

can be 

line, 

59-64.) 

. These 

relative 

point, a straight 

4 1 6   Chapter 
5 Orthogon

ality 

Circle  Ellipse 

Parabola 

Hyperbola 

Figure 5 . 1 4  The nondegene
rate 
conics 
Ellipse or Circle: 2 + 2 = l; a, b > 0 
x2 y2 

a b 
y 

b 

y 

b 

- b  

a > b  

y 

a 

- a  

a = b  

- b  

a < b  
Hyperbola 

y 

y 

b 

- b  

x2 _ .i__ 
a2 b2 -1, a, b  > 0 

r_  x2 

b2 a2 - 1,  a, b  > 0 

-

Parabola 

y 

y 

y 

y 

y =  ax2, a >  0 

y =  ax2, a <  0 

x  =  ay

2, a >  0 

x  =  ay

2, a <  0 

Figure 5 . 1 5  

Nondegene
rate 
conics 

in standard 
position 

Example 5 . 2 6  

Section 

5.5 Applications 

4 1 1  

write 

If possible, 
standard 

(a) 4x2 + 9y2 = 36  (b) 4x2 -9y2 + 1 = 0  (c) 4x2 -9y = 0 

each of the following 

y the resulting 

position and identif

quadratic 

equations 

graph. 

Solulion (a) The equation 

4x2 + 9y2 = 36 can be written 

in the form 

in the form 
in 

of a conic 

x2 yz 
9  4 
the x-axis at ( ± 3, 0) and the 
y-axis at (O, ± 2). 

graph is an ellipse intersecting 

so its 
(b) The equation 

4x2 -9y2 +  1 = 0 can be written 

in the form 

so its graph is a hyperbola, 
(c) The equation 

4x2 -9y = 0 can be written 
in the form 

opening 

up and down, intersecting 

the y-axis at (O, ±t). 

- +- = 1 
9  4 

y2 x2 
1 - 1 = 1 

y = -x2 

4 
9 

so its graph is a parabola 

opening upward. 

If a quadratic 

equation 

contains too many terms 

to be written 

5.15, then its graph is not 

in Figure 
terms but no xy term, the graph of the conic has been translated 
position. 

in standard 

position. 

out of standard 

in one of 

the forms 
When there are additional 

whose equation is 

x2 + 2y2 -6x + Sy +  9 =  0 
(x2 -6x) + (2y2 + Sy) =  - 9  
(x2 -6x) +  2(y2 + 4y) =  - 9  

Solulion We begin by grouping 

the x and y terms separately 
to get 

or 

or 

Next, we complete 

on the two expressions 

the squares 
in parentheses 
(x2 -6x +  9) +  2(y2 + 4y +  4) = -9 +  9  +  S 

to obtain 

(x - 3)2 +  2(y +  2)2 = S 

We now make the 
tutions 
tion into 

substi

x' = x -3 and y' 

= y +  2, 

turning 

the above 

equa­

(x')2 +  2(y')2 = S 

or -- + --

(x')2 (y')2 
s  4 

= I  

Example 5 . 2 1  

Identif

y and graph 

the conic 

4 1 8   Chapter 
5 Orthogon

ality 

This is the equation of an ellipse in standard position in the x '  y '  
intersec
coordinate 
system 
dard position 3 units 

coordinate system, 
in the x'y' 
is at x = 3, y = - 2, so the ellipse has been translated 
out of stan­

ting the x' -axis at ( ± 2 \/2, O) and the y'-axis at (O, ± 2). The origin 

down. Its graph is sh

right and 
2 units 

own in Figure 5.16. 

to the 

y 

y' 

2 

x 
x' 

- 2  

- 2  

-4 
ellip
A translated 
se 

Figure 5 . 1 6  

If a quadratic 

equation contains 

a cross-product term, 

then it represents a conic 

that has been rotated. 

Example 5 . 2 8  

Identif

y and graph the conic whose 
5x2 + 4xy + 2y2 = 6 

equation 

is 

Solulion The left-hand side of the equation 
matrix form as xT Ax = 6, where 

is a quadratic 

form, so we can write it 

in 

In Example 
orthogona

we found that the 

5.23, 
lly diagonalizes 

eigenvalues 

of A are 6 and 1, and a matrix Q that 

of this 
Observe that <let Q = - 1. In this example, we will interchange 
matrix 
by Exercise 28 in Section 

to + 1. Then Q will be the matrix of a rotation, 

to make the determina

5.1. It is always 

the columns 

nt equal 

possible to rearrange the columns 
nt equal to + 1 .  (Why?) We set 

of an 

........... orthogonal 

matrix Q to make its determina

A is [2/Vs 

Q = l/Vs 

A = [� �] 
l/Vs] -2/Vs 
[ l/Vs 2/Vs] 

Q = -2/Vs l/Vs 

instead, 
so that 

Section 

5.5 Applications 

4 1 9  

equation into the form 

(x')TDx' = 6 

y 

3 

x' 

Figure 5 . 1 1  A rotated 

ellip
se 

These are just the columns 
that these 
variable is 

are orthonor
just a rotation. 

Example 5 . 2 9  

The change ofvariablex = Qx' converts 
by means of a rotation. 

the given 

If x'  = [;:],then this 
(x')2 + 6(y')2 = 6 or (x')2 
an ellipse in the x' y' coordinate sy
6 

+ (y')2 =  1 
stem. 

equation is just 

which represents 

and e� = [ �] in the new 
of the x' and y' axes.) But, from x = Qx', we have 

coordinate 

system. 

To graph this ellipse, we need to know which vectors play the 

roles 

of e; = [ �] 

(These 

two vectors 

locate the positions 

and 

Qe; = [ l/Vs 2/Vs] [l] 
-2/Vs l/Vs 0 [ l/Vs] -2/Vs 
1  [ l/Vs 2/Vs] [Q] [2/Vs] 
Qez = -2/Vs l/Vs 1 = l/Vs 
The graph is shown in Figure 5.17.  4 

which are the eigenve
perfectly 
with the 

q 1  and q2 of Q, 
agrees 

Axes Theorem is 

mal vectors 

of A! The fact 

ctors 

fact that the change of 

You can now 

see why the Principal 

ric matrix A arises 
of A give the directions 
of the 

as the coefficient matrix of a quadratic 
axes of the corresponding 
to be both rotated 
5.29. 

for the graph of a conic 
as illustrated 
in Example 

dard position, 

principal 

It is possible 

so named. 
equation, 
graph. 

If a real 
symmet­
the eigenve

ctors 

and translated 
out of stan­

In matrix form, 

2 

is 

Identif

The cross-

is to elimina

te the cross-pr

oduct term first. 

whose equation 

y and graph the conic 

Solution The strategy 

product term comes from the 

the equation is xTAx + Bx +  4 = 0, where 
as in Example 5.28 

+ 2y --x -- y + 4 =  0 
2 28 4 
5x + 4xy 
Vs  Vs 
A  = [ � �] and B = [ -� -�] 
Example 5.28, [ l/Vs 2/Vs] 
Q = -2/Vs l/Vs 
I  [ 28  4 
] [ l/Vs 2/Vs] [x'] 
Bx= BQx = -Vs -Vs -2/Vs l/Vs y' 

xTAx = (x'fDx' = (x')2 + 6(y')2 

But now we also have 

quadratic 

x = Qx', where 

Then, as in 

by setting 

= -4x' -12y' 

form xT Ax, which we diagonalize 

4 2 0   Chapter 
5 Orthogon

ality 

y \ 2 

y' 

x 

x" 

x' 

- 2  - 1  

y" 

- 1  

- 2  � - 4  

Figure 5 . 1 8  

Thus, in terms of x' and y', the given equation becomes 

(x')2 + 6(y')2 -4x' -12y '  + 4 = 0 

To bring the conic represented 
by  this 
to translate 
We have 

the x'y' axes. We do so by completing 

equation 

into standard position, 
we need 
as in Example 
the squares, 

5.27. 

((x')2 -4x' + 4) + 6((y ')2 -2y' + 1) = -4 + 4 + 6 = 6 

or 

(x' - 2)2 + 6(y' - 1)2 = 6 
us the translation 
equations 

This gives 

In the x"y" coordinate 

x" = x' - 2 and y" = y ' -
system, 

is simply 

the equation 
(x")2 + 6(y")2 = 6 
in Example 

which is the equation 
first rotating 

and then transla

of an ellipse (as 

ting. 

The resulting 

5.28). We can sketch 

graph is shown in Figure 5.18. 4 

this ellipse by 

The general 

form of a quadratic 

equation 

in three 

variables 
x, y, and z is 

ax2 + by2 + cz2 + dxy +  exz + 

where at least 
is called 

one of a, b, . . .  ,f is nonzero. 

a quadric surface (or quadric). Once again, 

to recognize 

fyz + gx + hy +  iz + j = 0 
The graph of such a 

quadratic 
equation 
a quadric 
we need 

Ellipsoid: :::2 + b2 + :::2 = 1 
x2 y2 z2 
a  c z 

4 2 1  

Section 
5.5 Applications 
of one sheet: --,, + b2 - :::2 = I 
x2 y2 z2 
a"'  c 
z 

Hyperboloid 

x 

y 
of two sheets: -;;, -+--t2 -c2 = -I 
x2 y2 z2 
z 

Hyperboloid 

Elliptic 

cone: z2 = -;;, + b2 
z 

x2 y2 

y 
x 
paraboloid: z = -;;, + b2 
x2 y2 
z 

Elliptic 

y 
x 
paraboloid: z = -;;, - p 
x2 y2 
z 

Hyperbolic 

y 

x 
Figure 5 . 1 9  Quadric surfaces 

4 2 2   Chapter 
5 Orthogon

ality 

Example 5 .3 0  

Identif

y the quadric 

to put it into standard position. 
Figure 
are obtained 

Some quadrics 
by permuting 

5.19; others 

the variables. 

in standard position are shown in 

surface 

5x2 + l ly2 + 2z2 + 16xy + 20xz -4yz = 36 

whose equation is 

Solulion The equation can be written 

in matrix form as xT Ax = 36, where 

10] -2 

2 

8 
1 1  
- 2  

We find the 
eigenvectors 

eigenvalues 

of A to be 18, 9, and -9, with corresponding 

orthogonal 

respecti
vely. 

We normalize 

them to obtain 

and form the orthogonal 

matrix 

Note that in order for Q to be the 
is true in this case. 
sign of the 

determina

we require 

matrix of a rotation, 

nt.) Therefore, 

[! -! =ll 

<let Q = 1, which 

(Otherwise, <let Q = - 1, and swapping two columns changes 
18(x')2 + 9(y')2 -9(z')2 = 36 (x')2 (y')2 (z')2 

or -- + ----- = 1 

x = Qx', we get xTAx = (x')Dx' = 36, so 

2  4  4 

of variable 

the 

and, with the change 

From Figure 5.19, we recognize 
sheet. 
and z' axes are in 
respecti

The x', y', 
vely. The graph is shown in Figure 5.20. 

this equation as the equation 
the eige

of a hyperboloid of one 
nvectors q1, q2, and q3, 
the directions of 

Section 

5.5 Applications 

4 2 3  

z 

z' 

F i g u r e  5 . 2 0  

A hyperboloid 
of one sheet 
position 
nonstandard 

in 

We can 

also identif
position using 
will be asked to do so in the exercises. 

the "comple

te-the-

y and graph quadrics 

that have been translated 

squares method" of Examples 

ndard 
5.27 and 5.29. 

out of sta
You 

Q u a d ratic Forms 
In Exercises 
for the given 

5 . 5  

quadratic form f(x) = xT Ax 

1-6, evaluate the 
A and x. 

I Exercises 
[ � -� l x = [::] 
[� !lx = [;] 
[ _� -!lx = [!] 
u 0 -}� [�] 
2 u 0 -}� [-:] 
2 [: 2 :Jx�m 0 

1. A =  

2.A = 

3.A = 

4.A = 

5.A = 

6.A = 

matrix A associat

ed 

quadratic 
form. 

7-12,find the symmetric 

In Exercises 
with the given 
7. xl + 2xf + 6x1x2 
8. X1X2 
9. 3x2 -3xy - y2 
10. x� - x� + 8X1X2 -6XzX3 
11. 5xl -xi + 2x� + 2x1x2 -4x1x3 + 4x2x3 

12. 2x2 -3y2 + z2 -4xz 
13. 2xl + 5x� -4x1x2 

14. x2 + 8xy + y2 
15. 7xl + xi + xj + 8X1X2 + 8X1X3 -16XzX3 
16. xl + xi + 3xj -4x1x2 
17. x2 + z2 -2xy + 2yz 
18.2xy + 2xz + 2yz 

forms in 

Diagonalize the quadratic 
by 
finding an orthogonal matrix Q such that the change of 
x = Qy transforms the given form into one with no 
variable 
cross-pr
oduct 
Give Q and the new quadratic 
form. 

Exercises 

terms. 

13-18 

of axes to put the conic 

graph, give its equation 

of axes to put the conic 
in the 

equation 

4 2 4   Chapter 
5 Orthogon
ality 
19-26 as 

Exercises 

semidefinite, negative 

definite, 

forms in 

positive 

2 - y2 - z2 -2xy -2xz -2yz 

Classify each of the quadratic 
definite, 
positive 
negative 
semidefinite, or indefinite. 
19. xf + 2x} 
20. xf + xi -2x,x2 
22. x2 + y2 + 4xy 
21. -2x2 -2y2 + 2xy 
23. 2xf + 2xi + 2x� + 2x1x2 + 2x1x3 + 2x2x3 
24. xf + xi + x� + 2x1x3 25. xi + x� - x� + 4x1x2 
26. -x
27. Prove Theorem 

28. Let A = [ � �] be a symmetric 
a( x + �y y + ( d -�2)y2.] 

that A is positive definite 
<let A > 0. [Hint: 

if and 
ax2 + 2bxy + dy2 = 

29. Let B be an invertible 
matrix. 

5.22. 

positive definite. 

Show that A = BTB is 

2 X 2 matrix. 
Prove 
and 
only if a > 0 

symmetric 
an invertible 

30. Let A be a positive definite 
that there exists 
BTB. [Hint: Use the 
QDQT. Then show 
some invertible 
matrix C.] 

matrix B such that A = 
Spectral Theorem to write A 
= 
as cT c for 
factored 
that D can be 

matrix. Show 

n X n 

31. Let A and B be positive definite 

symmetric 

are positive definite. 

and let c be a positive scalar. Show that the 
matrices 

(a) cA  (b) A2  (c) A +  B 

matrices 
following 
(d) A_, (First show that A is necessarily 
symmetric 

.) 
Show 
matrix. 
a positive definite 
matrix B 
= B2• (Such a matrix B is called a square 

that there is 
such that A 
root of A.) 

32. Let A be a positive definite 

symmetric 

invertible

val­

determine 

33-36,find the maximum and 

minimum 
subject 

In Exercises 
ues of the quadratic form f(x) in the given exercise, 
to the constraint llxll = 1, and 
which 
these occur. 
33. Exercise 
20 
35. Exercise 
23 
the proof 
37. Finish 
of Theorem 5.23(a). 
38. Prove Theorem 
5.23(c). 

34. Exercise 22 
36. Exercise 24 

the values of x for 

Q u a d ratic E q u a t i o n s  

39-44, identify the graph 

G r a p h i n g  
equation. 
of the given 
In Exercises 
39. x2 + 5y2 = 25 40. x2 - y2 - 4 = 0 
42. 2x2 + y2 - 8 = 0 
41. x2 - y - 1  = 0 
43. 3x2 = y2 - 1  44. x = -2y2 

with the given equa­

the curve. 

anslation 

in standard form. 

2 + lOx -3y = - 13 

coordinate system, and sketch 

45-50, use a tr
In Exercises 
in 
n. Identify the 
in stan
dard positio
the translated coordinate system, and sketch the curve. 
45. x2 + y2 -4x -4y + 4 = 0 
46. 4x2 + 2y2 -Sx + l2y + 6 = 0 
47. 9x2 -4y2 -4y =  37  48. x
49. 2y2 + 4x + Sy = 0 
50.2y2 -3x2 -18x -20y + 1 1= 0  
51-54, use a rotation 
In Exercises 
in 
standard positio
n. Identify the graph, give its 
rotated 
51. x2 + xy + y2 = 6 52. 4x2 + lOxy + 4y2 = 9 
53. 4x2 + 6xy -4y2 = 5  54. 
3x2 -2xy + 3y2 = 8 
55-58, identify the conic 
In Exercises 
tion and give its equation 
55. 3x2 -4xy + 3y2 -28v'2x + 22Vly + 84 = 0 
56. 6x2 - 4xy + 9y2 -20x -lOy - 5 = 0 
57.2xy + 2\/2x - 1  = 0 
58. x2 -2xy + y2 + 4 V2x - 4 = 0 
Sometimes the graph 
of a quadratic equation 
line, a pair of straight lines, or a single point. We refer to 
that 
such a graph 
es, 
the equation 
59-64, identify 
case there is no graph at all and we refer to the 
in which 
an imaginary conic. In Exercises 
conic as 
the conic 
ate or 
degener
with the gi
imaginary and, where possib
59. x2 - y2 = 0  60. x2 + 2y2 + 2 = 0 
61. 3x2 + y2 = 0 62. x2 + 2xy + y2 = 0 
63. x2 -2xy + y2 + 2Vlx - 2Vly = 0 
64. 2x2 + 2xy + 2y2 + 2\/2x - 2Vly + 6 = 0 
and let 
65. Let A be a 
k be a 
2 X 2 matrix 
scalar. Prove that the 
graph of the quadratic 
equation 
(a) a hyperbola 
xTAx = k 

either 
le, sketch the graph. 

if k * 0 and <let A < 0 

symmetric 

(b) an ellipse, circle, 

or imaginary conic 

is 

if k * 0 and 
y conic 

if 

or an imaginar

detA > 0 
lines 
(c) a pair of straight 
k * 0 and <let A 
= 0 
(d) a pair of straight 
lines 
and detA  * 0 
line if k = 0 
[Hint: Use the Principal 

(e) a straight 

or a single point if k = 0 
and <let A 

= 0 

Axes Theorem.] 

is a straight 

as a degenerate 
is not satisfied for any values of the variabl

conic. It is also possible 

ven equation as 

Chapter 
Review 

4 2 5  

74. Let A be a real 2 X 2 matrix with complex 
eigenvalues 
xk+i = Axk 
system 
4.43 shows that if v 
ctor corresponding to 
A = a -bi, then 
P =  [Rev Im v] is 
invertible 

A = a :±: bi such that b =F 0 and I A I = 1. Prove that 
A =  P[: -�JP-1. Set B = (PPT)-1. Show that the 

every trajectory 
lies on 
is an eigenve
the matrix 

of the dynamical 
an ellipse. [Hint: Theorem 

quadratic 
and prove that if x lies on this ellipse, so 
does Ax.] 

an ellipse for all k > 0, 

xTBx = k defines 

and 

66-73, identify the quadric with the given 

and give its equation in 

In Exercises 
standard form. 
equation 
66. 4x2 + 4y2 + 4z2 + 4xy + 4xz + 4yz =  8 
67. x2 + y2 + z2 -4yz = 1 
68. -x2 - y2 - z2 + 4xy + 4xz + 4yz = 12 
69.2xy + z = 0 
70. 16x2 + 100y2 +  9z2 -24xz - 60x -80z = 0 
71. x2 + y2 -2z2 + 4xy -2xz + 2yz - x + y + z = 0 
72. 10x2 + 25y2 + 10z2 -40xz + 20\/2x + soy + 
73. l lx2 + l ly2 + 14z2 + 2xy + 8xz -8yz -12x + 

20\/2z = 15 

12y + 12z =  6 

Chapter 
Review 
Kev Definitions 

and Concepts 

subspaces 

fundamental 
of a matrix, 
380 
Gram-Schmidt Process, 
orthogonal 
basis, 
370 
orthogonal 
complement 
of a subspace, 
378 
orthogonal 
matrix, 374 

389 

orthogonal pro
jection, 
382 
set of vectors, 
369 
orthogonal 
Decomposition 
Orthogonal 
Theorem, 
384 
orthogona
matrix, 400 
orthonormal 

ble 

lly diagonaliza

374-376 

372 
orthonorma
l set of vectors, 
properties of orthogonal 
matrices, 
393 
QR factorization, 
Rank Theorem, 
386 
spectral decomposition, 
Spectral Theorem, 

405 

403 

basis,  372 

Review Questions 

1. Mark each of the following 
(a) Every orthonormal set 
independent. 
(b) Every nonzero subspace 

statements 
true or false: 
of vectors 
is linear
ly 
of u;gn has an orthogona

matrix with orthonormal rows, 

basis. 

(c) If A is a square 
(e) If A is a matrix with det A = 1, then A is an 
(f) If A is an m X n matrix such that (row(A))_j_ = u;gn, 

then A is an orthogona
l matrix 

(d) Every orthogona

is invertible. 

orthogona

l matrix. 

l matrix. 

l 

zero matrix. 
of u;gn and v is a vector 

then A must be the 
(g) If W is a subspace 
(h) If A is a symmetric, 
(i) Every orthogonally 

in u;gn such 
that projw(v) = 0, then v must be the zero vector. 
then A 2 = I. 
orthogona
diagonalizable 
is invertible. 

l matrix, 

matrix 

exists 

eigenvalues. 
2. Find all values 

(j) Given any 

of a and b such that 

n real numbers 

A1, .•. , An, there 
a symmetric n X n matrix with A1, .•. , An as its 
\ [H [ J [fl ) i< an mthogonal 
[ v] 8 of v = [-�] with 
B � ml [ J [-� l) orn' 

orthogonal 

basis 2 

<et of vedm. 

vector 

to the 

3. Find the coordinate 

respect 

4 2 6   Chapter 
5 Orthogon

ality 
{v1, v2} oflR2 is [v]8 = [-3] 1/2 . 

v with respect 

of a vector 

to an 

4. The coordinate 
orthonorma

vector 

l basis B = 

with respect 

to 

to 

v. 

the 

basis 

l basis 

values 

for all vectors 

matrix, find all possible 

orthogonal 

of a, b, and c. 

In Questions 
9. W is the 
2x -Sy= 0 

10. W is the line in IR3 with parametric 

, find all possible vectors 

If v1 = 4/ 5 

x and 
matrix. 

for W = span{x1, x2, xJ. 

for IR4 that contains 

[3/5] 

17. Find an orthogonal 

16. Find an orthogonal 

7. If Q is an orthogonal 

an orthonorma
is an orthonorma
l set. 

4/7Vs - 15/7Vs 2/7Vs 
matrix. 

for W _j_. 
9-12,find a basis 
line in IR2 with general equation 

15. (a) Apply the Gram-Schmidt Process 

8. If Q is an n X n matrix such that the angles 
L(Q x, Qy) and L(x, y) 
are equal 
y in IR", prove that Q is an orthogonal 

to find an orthogona
(b) Use the result of part (a) to find a QR factorization 

2��] is an 
5. Show that [-�j� 2�7 
6. If [ 1�2 : ] is an orthogonal 
n X n matrix and {v1, ••• , vk} is 
l set in !Rn, prove that {Qv1, .•. , Qvk} 

� -[!J·� -[iJ·� -[rJ 
of A-[! � ] 
vecto" m '"d [ J 
w -ml· + x, + x, + x. -0} om' 
18. Let A = [ � 2 
-� l · 
lly diagonalize A. 
decomposition of A. 
[-� -� -� � -�] 
A= 2 1  4 8 9 
v -[-�] 

(a) Orthogona
A3 = -2 and eigenspaces 
19. Find a symmetric 
20. If {v1, v2, •.• , v"} is an 
ues c1, c2, •.• , en and corresponding 

prove that A is a symmetric 

x = t 
y = 2t 
z = - t  

,\1 = ,\2 = 1, 

matrix with eigenval­
eigenvectors 

matrix with eigenvalues 

14. Find the orthogonal 

four fundamental 
of 

(b) Give the spectral 

3 - 5  6 - 1  7 

for the subspace 

13. Find bases 

decomposition 

for each of the 

subspaces 

orthonor

equations 

mal basis 

for !Rn and 

- 1  1 2 

basis 

of 

Vector Spaces 

6 . 0  Intro d u ction :  Fib o nacci in (Vector> Space 

The Fibonacci 

sequence was introduced in Section 

4.6. It is the sequence 

Algebra is generous; 
more than 

she often gives 
is asked of her. 

( 1 71 7- 1 783) 

tive integers 

-Jean 
le Rond 
d'Alembert 
1 968, p. 481 
In Carl B. Boyer 
Wiley, 

of nonnega
is the sum of the two terms preceding 
2 + 3 = 
sequence is completely 

A History of Mathematics 

0, 1, 1, 

13, . . .  
with the property that after 

2, 3, 

5, 8, 

the first two terms, each term 

it. Thus 0 +  1  = 1, 1  +  1  = 

sequence by f0,f1,f2, .•• , then the entire 

2, 1  + 2 = 

3, 

sequence x0, x1, x2, x3, . . .  as 

let's 

2, . . .  ) 

notation, 

write a 

becomes 

with vector 

By analogy 

this notion. 

determined 

The Fibonacci 

sequence then 

We now generalize 

5, and so on. 
If we denote 

Defi n ition A Fibonacci-type sequence is any sequence 

the terms of the Fibonacci 
by specifying 
that 

x = [xo, X1, X2, X3, . . .  ) 

Jo = O,f1 = 1 and fn = fn-l + fn-l for n 2 2 
f = [f0,J1,J2,f3, •.. ) = [O, 1, 1, 
such that x0 and x1 are real numbers and Xn = xn-l + Xn-z for n 2 2. 
x2, •.. ) and y = [y0, y1, y2, •.• ) to be the 
ex =  [cx0, cx1, cx2, ••• ) 

Problem 1 Write down the first five terms 
By analogy 

If c is a scalar, we can likewise define the scalar multiple 

[1, \/2, 1  + 

For example, 

with vectors 

again, 

of three 

let's 

of a sequence by 

x =  [x0, x1, x2, x3, •.• ) 

\/2, 1 + 2\/2, 2  + 3\/2, . . .  ) is a Fibonacci-t

more Fibonacci-t

define the sum of two sequences 
sequence 

ype sequence. 
ype sequences. 
x = [x0, x1, 

421 

4 2 8   Chapter 
6 Vector 

Spaces 

Problem 2 (a) Using your examples 

the sums of various 
sulting 

sequences 

from Problem 1 or other 
pairs of Fibonacci
appear to be Fibonacci-type? 
scalar multiples 

(b) Compute various 

examples, compute 

-type sequences. 

Do the re­

of your Fibonacci-type se­
appear to be 

sequences 

from Problem 

quences 
Fibonacci-type? 

1. Do the resulting 
Problem 3 (a) Prove that if x and y are Fibonacci-type sequences, 
like 11;r, Fib is clo
show that Fib has much more in common with 11;r. 

type sequence. 
-type sequences 
and scalar multiplication. 

all Fibonacci
sed under addition 

(b) Prove that if x is a Fibonacci

a Fibonacci-

ex is also 

the set of 

denote 

Let's 

then so is x + y. 

by Fib. Problem 3 shows that, 
The next exercises 

-type sequence and c is a scalar, then 

Problem 4 Review 

satisfy all of these 
Fibonac

ci-type sequence 

properties? 

the algebraic 

properties 
What Fibonacci-t

in Theorem 1.1. 
plays 
the role ofO? For a 
x, what is -x? Is -xalso a Fibonacci
-type sequence? 

of vectors 
ype sequence 

e1, e2, •.• , en-The Fibonacci 

basis vectors 

Does Fib 

Problem 5 In !Rn, we have the standard 

sequence f = [O, 1, 1, 
two terms are 

What about e3, e4, .•• ? Do these 
Problem 6 Let x = [x0, x1, x2, .•. ) be a Fibonacci

0 and 1. What sequence 

e in Fib plays 

vectors 

the role of e1? 
in Fib? 

2, . . .  ) can be thought of as the analogue 

have analogues 

-type sequence. 

of e2 because its first 

Show that x is a 

linear 

combina

tion of e and f. 

Problem 1 Show that e and f are linearly indepen

dent. (That is, show that if 

ce + df = 0, then c = d = 0.) 
value 

to assign 

Problem 8 Given your answers 

to the "dimension

" of Fib? Why? 

to Problems 

6 and 7, what would be a sensible 

Problem 9 Are there 

any geometric 

in Fib? That is, if 

sequences 
2
, r3, . . .  ) 

[1, r, r

is a Fibonacci

-type sequence, 

what are the possible values 

of r? 
of geometric 
Fibonac

Problem 10 Find a "basis" for Fib consisting 
Problem 11 Using your answer 

to Problem 10, give an alternative 

derivation of 

ci-type sequences. 

Binet's 

formula [formula (5) in Section 
4.6]: 

n Vs  2  Vs  2 

_ 1_(1 + Vs)n _
_ 1_(1 - Vs)n 
j, = 
sequence f = [f0,f1,f2, .•. ). [Hint: 
l = [10, 11, 12, 13, .•. ) = [2, 1, 3, 4, . . .  ) 

type sequence 

terms of the Fibonacci 

for the 
the basis 

from Problem 10.] 

The Lucas sequence is the Fibonacci-

Express 

f in terms of 

from Problem 10 to find an analogue 

ofBinet's 

formula 

Problem 12 Use the 
term Zn of the 

for the nth 

Problem 13 Prove 

that the Fibonacci 

basis 
Lucas sequence. 
fn-1 + fn+ I  =Zn for n 2:  1 
C = [1, 1, 2,  3, 

ype sequences 

and Lucas 

_... [Hint: 

The Fibonacci-t

form a basis for Fib. (Why?)] 
In this Introduct
behaves in 

sequences 
infinite 
that is the 

like IR2
subject of this chapter. 

ion, we have seen that the collection 
many respects 

sequences. This useful analogy leads 

, even though 
to the general notion 
of a vector 
space 

+ = [1, 0, 1, 1, 
. . .  ) 
Fib of all Fibonacci
the "vectors" are actually 

. . .  ) and f

-type 

sequences 

are rela

ted by the identity 

The Lucas 
sequence 
is named 
(see 
Edouard 
Lucas 
page 336). 

after 

Section 

6.1 Vector 

Spaces 
and Subsp
aces 4 2 9  

Vector Spaces a n d  Subspaces 

in many respects. In particu

and the algebra 
both vectors and 

of vectors 
lar, we can add 
properties that result 

two opera­
(Theorem 1.1 and Theorem 3.2) are identical in both settings. In this section, 
in a wide variety 
we will therefore 

In Chapters 1 and 3, we saw that the algebra 
are similar 
and we can multiply 
both by scalars. The 
tions 
we use these 
of examples. By proving general theorems about these "vectors:' 
simultaneousl
of algebra: its ability 
them into a general setting. 

y be proving results 
about all of these 

to take properties from a concrete 

to define generalized 
"vectors" 

like !Rn, and abstract 

examples. This is the real power 

properties 

that arise 

of matrices 

from these 

setting, 

matrices, 

plication 

( 1 809-

V. 

Geometrico, 

by cu. If the 

multiple 

u + 0 = 

are called vectors. 

have been defined. 

If u and v are in 

space and its elements 

an element 0 in V, called 

a zero vector, such that 
u. 

called addition and scalar 

an element -u in V such that u + (-u) = 0. 

V, the sum of u and v is denoted 
of u by c is denoted 

8. (c + d)u = cu+ du 
9. c(du) = (cd)u 
10.lu= u 

c is a scalar, the scalar 
hold for all u, v, and w in V and for all scalars c and d, then V is 

Defi n ition Let V be a set on which two operations, 
multiplication, 
by u + v, and if 
following 
axioms 
called 
a vector 
1. u + v is in V. 
2. u + v = v + u 
4. There exists 
5. For each u in V, there is 
6. cu is in 

Closure 
addition 
under 
tician 
n mathema
The Germa
ivity 
Commutat
Grassmann 
Hermann 
tivity 
3. (u + v) + w = u + (v + w)  Associa
credited 
1 877) is generally 
with 
the idea 
introducing 
first 
of a 
space 
vector 
(although 
he did 
Closure 
under 
scalar multi
not call it that) 
in 1 844. Unfor­
7. C ( U + V) = CU + CV 
tunat
ely, 
his work 
was very 
diffi­
Distributivity 
and did not receive 
to read 
cult 
Distribut
ivity 
the attention 
One 
it deserved. 
person 
who did study 
it was the 
Italian 
mathematician 
Giuseppe 
( 1 858-1 932). In his 
Peano 
•  By "scalars" 
1 888 book Calcolo 
clarified 
Grassm
ann's 
Peano 
earlier 
laid 
work and 
down 
the 
axioms 
for a vector 
space 
as 
them 
we know 
today. Peano
's 
U, n, and 
E (for 
remarkable 
book 
is also 
for 
introducing 
operations 
on sets. 
His notations 
and "is 
"intersection;' 
"union;' 
are the 
an element of") 
ones 
we 
still 
use, 
although 
they were not 
•  The definition 
immedia
tely 
accepted 
by other 
mathematicians. 
Peano
's axio­
space 
of a vector 
matic 
definition 
little influ
ence 
also 
had very 
for 
many years. 
Acceptance 
came 
in 1 9 1 8, after 
Hermann 
Wey! 
( 1 885-1955) repeated it in 
Space, Time, Matter, an 
his book 
's general 
duction 
intro
to Einstein
theory 
of relativity. 

numbers. Accordingly, we should 
pos­
space over the real numbers). It is also 
numbers or to belong to "ll.P, where p is prime. 
In these 
space over "ll.P, respecti
Most of 
space or a vector 
vely. 
spaces, 
omit the adjective 
If 
"real:' 
that we are working 

to V as a real vector 
refer 
sible for scalars to be complex 
cases, 
V is called 
our examples 
something 
number system. 

of vector 
specify 
of addition 
axioms 1 through 10. We need to pay particular 

we need to 
and scalar multiplication 
and to verify 
to axioms 1 and 6 (closure), 
attention 

of. Neither does it specify what the operations 
plication
and Exercises 
5-7. 

not specify what the set  V consists 
called "addition" 
but they 

multiply, and divide 
algebra, such a number system 
of a vector 
space  does 

in which, 
according to the usual 
is called 
a field. 

we can add, subtract, 
In abstract 

and "scalar 
need not be. See Example 
6.6 

In fact, the scalars can be chosen from 

speaking, 
arithmetic. 

Often, they will be familiar, 

roughly 
laws of 

space (or a vector 

We will now look at several 

a complex vector 

V and the operations 

any number system 

so we will usually 

mean the  real 

will be real vector 

we will usually 

is referred 
to as 

'' look like. 

examples 

In each case, 

spaces. 

the set 

space:' 

over the real 

a "vector 

assume 

multi­

Remarks 

4 3 0   Chapter 
6 Vector 

Spaces 

of a zero vector 

in V), and axiom 5 (each vector 

in V must have 

axiom 4 (the existence 
a negative 

in V). 

n 2: 1, !Rn is a vector 

For any 
multiplication. 
the remaining 

Example 6 . 1  

space 

with the usual 

operations 

Axioms 1 and 6 follow from the definitions 
of these 
axioms 

follow from Theorem 

1 . 1 .  

of addition 
and 

and scalar 
operations, 

Example 6 . 2  

scalar multiplica

all 2 X 3 matrices 

with the usual operations 

is a vector 
tion. 

and matrix 
that the sum of two 2 X 3 matrices 
another 

The set of 
addition 
We know 
ing a 2 X 3 matrix by a scalar gives 
The remaining 
axioms 
2 X 3 zero matrix, 
There is nothing 

of all m X n matrices 

l about 2 X 3 matrices. 
forms a vector 

hence, 
, the zero vector 
0 is the 
of a 2 X 3 matrix A is just the 2 X 3 matrix -A. 
For any positive integers 
with the usual 
of matrix 

m and n, 

space 
Here the "vectors" 
is also a 2 X 3 matrix and that multiply­
2 X 3 matrix; 

of matrix 
matrices. 

follow from Theorem 3.2. In particular

and the negative 

operations 

are actually 

we have closure. 

specia

the set 
addition 

space 
This vector 

and matrix scalar multiplica

tion. 

space 

is denoted 

Mmn· 

Example 6 . 3  

Let <;IP 2 denote 
Define addition 

of all polynomials of degree 
the  set 
and scalar multiplication 
in the usual 

p(x) =  a0 + a1x + a2x2 and q(x) =  b0 + b1x + b2x2 

2  or less with real coefficients. 
way. (See Appendix D.) If 

are in 

<!P 2, then 

has degree 

at most 2 and so is in <!P2• If c is a scalar, then 

cp(x) =  ca0 + ca1x + ca2x2 

t is, the polynomial all of whose 
1x + a2x2 is the poly­
We 
to verify the remaining 
axioms. 

for Exercise 12. With p(x) and q(x) as above, 

axioms 1 and 6. 

is also in 

The zero vector 

<!P2. This verifies 
are zero. 

coefficients 

nomial -p(x) = -a0 - a1x - a

will check axiom 2 and leave 
we have 

0 is the zero polynomial-tha
The negative 
of a polynomial 
2x2• It is now easy 
the others 

p(x) = a0 + a
(x) =  (a0 + a1x + a2x2) + (b0 + b1x + b2x2) 
=  (a0 + b0) + (a1 + b1)x + (a2 + b2)x2 
=  (b0 + a0) + (b1 + a1)x + (b2 + a2)x2 
=  (b0 + b1x + b2x2) + (a0 + a1x + a2x2) 
=  q(x) + p(x) 

p(x) + q

where the third equality 
commutative. 

follows 

from  the  fact  that 

addition 

of real numbers is 

Example 6 . 4  

Section 
6.1 Vector 
Spaces 
and Subsp
aces 4 3 1  
set <!J' n of all polynomials of degree 

less than or 

as is the set 

<!J' of all polynomials. 

In general, for any fixed n 2: 0, the 
to n is a vector 
Let ?Jf denote 

space, 

alued 

equal 

on the real 
line. 

If f and g are 

by 

cf are defined 

functions 

the set of all real-v

two such functions 

off+ g at x is obtained 

defined 
and c is a scalar, then f + g and 

(f + g)(x) =  f (x)  + 
g(x) and (cj)(x) =  cf (x) 
that is,f0(x) =  0 for all x. The negative 
in ?Jf is the 
by ( -j) (x) =  -f(x) [Figure 
?Jf is a vector 

6.1 (b) J. The zero vector 
6.l(c)]. 

6.1 (a) J .  Similarly, the value 

words, the value 
by the scalar c [Figure 

In other 
and g at x [Figure 
tiplied 
f0 that is identic
function 
Axioms 

1 and 6 are obviously 
true. 
space. 

Exercise 13. Thus, 

of cf at x is just the value 

-f defined 

by adding 

ally zero; 

Verification of the remaining 

axioms 

is left as 

together 

the values off 
off at x mul­

constant function 
f is the 

of a function

y 

(x,f(x) + g (x)) 

I 

(a) 

y 

y 

(x, 2/(x)) \ 2f 

I (x,  -3/(x)) 

(b) 

/ (x, -f(x)) 
The graphs of 

(a) f, g, and f + g, (b) f, 2f, and -3f, and (c) f and -f 

Figure 6 . 1  

(c) 

-f 

4 3 2   Chapter 
6 Vector 

Spaces 

Example 6 . 5  

6.4, we could also have considered 

only those 

functions 

defined on 

This approach 

also produces a vector 

space, 

[a, b] of the real line. 

In Example 
by ?f [a, b]. 
some closed 
interval 
denoted 
"1l_ of integers 
in which it fails (a counterexample
under scalar multiplication. 
For example, 

The  set 
strate this, 
instance 
closure 

with the usual operations 
to find 

scalar t is (t)(2) = t, which is not an integer. 
every x in "1l_ and every scalar c (i.e., 

it is enough 

axiom 6 fails). 

that one of the ten axioms fails and to give a specific 

is not a vector 

space. 

To demon­

). In this case, 
we find that we do not have 
Thus, it is not true that ex is in "1l_ for 
of the integer 2 by the 

the multiple 

Example 6 . 6  

Let V = IR2 with the usual 
multiplica

tion: 

definition 

of addition 

but the following 

definition 
of scalar 

Then, for example, 

so axiom 10 fails. [In fact, the other nine 
not need to look into them, 
example 
axioms in the order 

rather 
in which they have been 
given.] 

because V has already 

shows the value of looking ahead, 

failed to be a vector 
than working 

space. 
This 
through the list of 

axioms are all true (check this), but we do 

Example 6 . 1  

Let C2 denote 
scalar multiplication 
example, 

the set of all ordered 
as in IR2, except 

and 

Using properties 
axioms 

of the complex numb
C2 is a complex 

Therefore, 

hold. 

In general, en is a complex 

pairs of complex 

and 
here the scalars are complex numb
ers. For 

numbers. Define addition 

6 -31 

2 -31  4 

2 - 3i (l- i)(2- 3i) - 1- Si 

.] + [-3 + 2i]  [-2 + �i] 
[ 1  + i
(1- i)[l + i]=[(l - i)(l +i)]=[ 2 ] 
space.  4 
for all n 2 1. 
4 

ers, it is straightforward 
vector 

z; (with the usual 

over ZP for all n 2: 1. 

definitions 

of addition 

to check that all ten 

vector 

space 

space 

by 

and multiplication 

Example 6 . 8  

If p is prime, 
scalars 

from "ll_p) is a vector 

the set 

Section 

6.1 Vector 

Spaces 
and Subsp
aces 4 3 3  

Before we consider 

ful properties 
of vector 
for vector 

spaces in 

examples, 

further 
spaces. It is important 

we state 

a theorem that contains 

some use­
to note that, by proving this theorem 
proving it for every specific vector 

general, 

we are actually 

space. 

Theorem 6 . 1  

space, 

u a vector 

in V, and c a scalar. 

Let V be a vector 
a. Ou= 0 
b. co= 0 
c. (- l)u = -u 
d. If 

cu = 0, then c = 0 or u = 0. 

Proof We prove properties 
ties as exercises. 
(b) We have 

(b) and 

( d) and leave 

the proofs of the remaining 

proper­

by vector 

space 

cO = c(O 
axioms 4 and 7. Adding 

+ 0) = cO + cO 
the negative 

of cO to both sides 

produces 

co + (-co) = (co + co) + (-co) 

which implies 

(d) Suppose cu = 0. 
c = 0, there 

is nothing 

4 

u = lu 

assume 

o = co +  (co+ (-co)) 
= co +  0 
= co 
To show that either 
to prove.) Then, since 

c = 0 or u = 0, let's 

c * 0, its reciprocal 

that c *  0. (If 
l/c is defined, 

by axioms 
5 and 3 
by axiom 
5 by axiom 
by axiom 
by axiom 
1 -(cu) c 
1 -0 c 
0  by proper
ty (b) 

and 

1 0  

9 

We will write u - v for u + ( -v), thereby defining 

also exploit 
the associa
for the sum 
of three 

tivity 
vectors 

property of addition 
and, more generally, 

to unambiguou

of vectors. 
sly write u + v + w 

We will 

subtraction 

for a linear 

combination of vectors. 

Subspaces 
We have seen that, 
giving 
rise to the 
subspace 

in !Rn, it is possible 
notion 
of a subspace. 

for one vector 
For example, a plane through the origin is 

to sit inside 

another 
one, 

space 

a 

of IR3. We now extend this concept 

to general vector 

spaces. 

4 3 4   Chapter 
6 Vector 

Spaces 

Defi nition A subset W of a vector 
itself 
with the same scalars, 

a vector 

space 

space V is called 

a subspace of V if W is 
as V. 

and scalar multiplication 

addition, 

As in !Rn, checking to see whether 

space 
testing only two of the 

a subset 
space 

W of a vector 

ten vector 

axioms. 

We prove this observation 

V is a subspace 

of 

V involves 
as a theorem. 

Theorem 6 . 2  

space 

and let W be a nonempty subset of V. Then W is a sub­

V if and only if the following 

conditions 

hold: 

Let V be a vector 
space of 
a. If u and v are in 
b. If u is in 

W, then u + v is in W. 

W and c is a scalar, then cu is in W. 

Proof Assume 
10. In particula

that W is a subspace 
r, axiom 1 

of V. Then W satisfies 
space 
(a) and axiom 6 
is condition 
(b). 

vector 

axioms 1 to 

is condition 
is a subset 
that  W 
axioms 1 and6 hold. 

of a vector 

space V, satisfying condi­

Axioms 2, 3, 

7, 8, 9, 

and 10 hold 

Conversely, assume 

(a) and (b). By hypothesis, 

in W. (We say that 

tions 
in W because they are true for all vectors 
vectors 
W inherits these 
and 5 to be checked. 
at least 
Since W is nonempty, it contains 
Theorem 6.l(a) imply that Ou= 0 is also 
also in W, 

one vector 
in W. This is axiom 4. 
by taking c = - 1  in condition 

properties 

Theorem 6.l(c). 

If u is in 

V, then, 

using 

in V and thus are true in particular 

from V.) This leaves 
4 

for those 
axioms 

u. Then condition 

(b) and 

(b ), we have that -u = ( -l)u is 

Remark Since 

text of !Rn to general vector 
all of the 
spaces, 
Chapter 3 are subspaces 
of IR3. 
through the origin 
are subspaces 

Theorem 6.2 generalizes 
the notion 
subspaces 
of !Rn in the current 
context. 

from the 

of a subspace 
of !Rn that we encountered 
In particular, lines 

and planes 

con­
in 

Example 6 . 9  

We have already 
vector 
Hence, 

space. 

shown that the 

set <!/' n of all polynomials with degree 

<!/' n is a subspace 

of the vector 

at most n is a 

space<!/' of all polynomials. 4 

Example 6 . 1 0  

Let W be the set 

of symmetric 

n X n matrices. 

Show that W is a subspace 

of Mnn-

Solulion Clearly, W is nonempty, so we need only check conditions 
(a) and (b) in 

Theorem 6.2. Let A and B be in W and let c be a scalar. Then AT = A and BT = B, 

from which it follows 

that 

Therefore, 

A + B is symmetric 

is in W. Similar

ly, 

and, hence, 
(cAf = cAT = cA 

so cA is symmetric 
and scalar multiplica

and, thus, 
tion. 

is in W. We have shown that W is closed under addition 

Therefore, 

it is a subspace 

of Mnn' by Theorem 

6.2. 4 

Example 6 . 1 1  

real-valued functions 

Section 

6.1 Vector 

Let C(;S be the set of all continuous 
of all differentiable 
the set 
real-valued 
subspaces of�, the vector 
space 

Spaces 
and Subsp
aces 4 3 5  
IR and let 0J be 
IR. Show that C(;S and 0J are 
on IR. 
Solution From calculus, we know that if f and g are continuous 
scalar, then f + g and cf are also continuo
C(;S is closed 
scalar multiplication and so 
is a subspace 
f + g and cf Indeed, 
So 0J is also closed under addition 

functions 
and c is a 
us. Hence, 
under addition 
and 
then so are 
of�. If f andg are differentiable, 

alued functions defined 

making it a subspace 

and scalar multiplication, 

defined on 

defined on 

functions 

of all real-v

(j + g)' =  f '  + g '  and (cf)' =  c (j') 
by 0J C C(;S), making 

that every 

in C(;S (denoted 

...+ 

is continuo

so rt/' C 0J, and thus rt/' is a 

0J a subspace 

us. Conse­

ofC(;S. It is also 

of calculus 

It is a theorem 

quently, 0J is contained 
of 0J. We therefore 

the case that every 
subspace 

polynomial function 

is differentiable, 

have a hierarchy 

of subspaces 

of�, one inside 

the other: 

differentiable 

function 

of�. 

This hierar

chy is depicted 

in Figure 
6.2. 

Figure 6 . 2  The hierarchy 

of subsp

aces 
of� 

There are other su

bspaces 

of � that can be 

placed 

into this hierarchy. 

Some of 

these 

are explored 

in the exercises. 

In the preceding discussion, 

we could have restricted 

[a, b]. Then the 

correspond

our attention 
ing subspaces 

to functions 
of � [a, b] 

defined 

on a closed interval 

would be C(;S [a, b], 0J [a, b], and rt/' [a, b]. 
f" + f =  0 

Let S be the 

set of all functions 

That is, 

S is the 

solution set of Equation 

(1). Show that 

S is a subspace 

of�-

(1) 

Example 6 . 12 

that satisfy the 

differential 

equation 

4 3 6   Chapter 
6 Vector 

Spaces 

Solulion S is nonempty, since 
f and g be in S and let c be a scalar. Then 

(f + g)" + (f + g) = (j" + g") + (f + g) 

the zero function 

clearly 

satisfies 

Equation 

(1). Let 

" + g) 

= (f" + f) + (g
= O+O 
= O  
is in S. Similarly, 

which shows that f + g 

(cf)" + cf= cf" + cf 

= c(f" + f) 
= co 
= O  

so cf is also in 
of?F. 

Therefore, 

S. 
S is closed under addition 

and scalar multiplication 

and is a subspace 

any specific 

solu­

differential 

for finding 

that certain 
spaces 

Equation 

The differential 

( 1) is an 
of such equations 
actually 
solve 
than the zero function

The solution sets 
equation. 
Example 
6.12 
solutions, 
other 
to this 
tions 

we did not 

type of equation 
As you gain experience 

working 

example 

of a homogeneous linear 
are always 

Equation 
(1) (i.e., 
). We will discuss 

subspaces of ?F. Note that in 
we did not find 
techniques 

in Section 6.7. 
resemble one another

with vector 

spaces 

you will notice 
and subspaces, 
consider 
the vector 
vely, 
are, respecti

. For example, 
vector 

examples 

tend to 

of these 

spaces 

IR4, <!/'3, and M22• Typical elements 

u � m p(x) �a +  bx+ ex' + dx', and A � [: !] 

operations 
are essentially the same in all three 
settings. To highli
ry steps 

we will perform the necessa

and scalar multi­
in 

of addition 

involving 

the vector 

in the three 

ght the similari
ties, 
spaces 
side 

vector 

space 

of IR4• 

is a subspace 
(b) Show that the  set  W 
subspace 
( c) Show that the set W 

of<!!' 3. 

of all matrices 

of all polynomials of the  form 

a  + bx - bx2 + ax3 is a 

of the form [ _: �] is a subspace 

of M22. 

In the 
words 
of Yogi Berra, 
"It's 
again:' 
all over 
deja vu 

Any calcula
tions 
plication 
the next example 
by side. 

Example 6 . 13 

(a) Show that the set W of all vectors 

of the form 

and 

and 

Then 

Then 

the zero 

polynomial. 

(b) W is nonempty because it con­
tains 

(c) W is  nonempty because it con­
tains 

Section 

Spaces 
and Subsp
6.1 Vector 
aces 431 
the zero matrix 0. (Take a 

Solution 
(a) W is nonempty because it con­
tains the zero vector 

0. (Take a = b = 
(Take a = 
b = 0.) Let A and B be in W-say, 
b = O.)Letp(x)andq(x)bein W-say, 
0.) Let u and v be in W-say, 
p(x) = a + bx -bx2 + ax3 
A = [ _� �] B = [ _� �] A+B= 
q(x) = c + dx -dx2 + cx3 
p(x) + q(x) = (a + c) + (b + d)x -(b + d)x2 + (a + c)x3 
[ a+ c -(b + d) b + d] a + c 
Then u+v= [ a+ c l b+d -b - d a + c [ a+ c l b+d -(b + d) a+ c 
so u + v 
so p(x) + q(x) is also 
so A + B 
kp(x) = ka + kbx -kbx2 + kax3 
[ ka kb] kA = -kb ka 
so kA is in W. 
Theorem 6.4 

so ku 
IR4 that is clo
scalar multiplication. 
a subspace 

Thus, W is a nonempty subset of 
and 
W is 
of iR4, by Theorem 6.2. 

M22 that is closed under addition 
scalar 
Therefore, 
a subspace of M22, by 

<;IP 3 that is closed under addition 
scalar multiplicat
ion. Therefore, 
a subspace 

Thus, W is a nonempty subset of 
and 
W is 

Thus, W is a nonempty subset of 
and 
W is 

ly, if k is a scalar, then 

if k is a scalar, then 

if k is a scalar, then 

sed under addition 
Therefore, 

right form). 
Similarly, 

of<;if 3 by Theorem 6.2. 

is also in W (because it has 

in W (because it has 

so kp (x) 

in W (because 

the right form). 

the right form). 

is also 

has the 

Similarly, 

multiplica

Similar

tion. 

is in W. 

is in W. 

it 

Example 6 . 14 

Example 

6.13 shows that it is often 
in common. 
and other 

possible 
Consequently, we can apply our knowledge 
examples. We will encounter 
this idea several 

matrices, 

examples 
that, 

to relate 

on the surface, 

of 

have nothing 

appear to 
!Rn to polynomials, 
times in 

this chapter and will make it precise 

in Section 

6.5. 

space, 

If V is a vector 
only the zero vector, is also a subspace 
simply 

note that the two closure 

of itself. 
the zero subspace. 

The set {O}, 

then V is clearly 

a subspace 
of V, called 
conditions 

0 + 0 = 0 and c 0 = 0 for any scalar 

c 
subspaces of V. 

the trivial 

of Theorem 6.2 are satisfied: 

The subspaces 

{O} and V are called 

consisting 

To show this, 

of 
we 

4 3 8   Chapter 
6 Vector 

Spaces 

An examination of the proof 

of Theorem 6.2 reveals 

the following 

useful fact: 

If W is a subspace of a vector 

space 

V, then W contains 

the zero vector 
0 of V. 

tent with, and analogous 

of!R3 if and only if they contain 

the origin

This fact is consis
spaces 
must contain 

to, the fact that lines 

and planes 
are sub­
. The requirement 
that every 
subspace 
that a set is not a subspace. 

0 is sometimes 

useful in showing 

Example 6 . 15 

Let W be the set of 
all 2 X 2 

matrices 
of the form 

Is W a subspace 

of M22? 

Solulion Each matrix in W has the 
( 1, 1) entry. Since 

the zero matrix 0 =  [� �] 

property that its ( 1, 2) entry 

is one more than its 

does not have this property, it is not in W. Hence, 

W is not a subspace of M22• 

Example 6 . 16 

of M22? (Since <let 0 =  0, the zero 

Let W be the set 

of all 2 X 2 

matrices 

with determina
matrix is in W, so the method 

nt equal to 0. Is W a subspace 

of Example 
6.15 is 

of no use to us.) 
Solulion Let 

A =  [ � �] and B =  [ � �] 

Then <let A =  <let B =  0, so A and B 
=  [� �] 
so det(A + B) = 1 =fa 0, and 
of M22. 

and so is not a subspace 

therefore 

addition 

are in W. But 

A  + B 

A + Bis not 

in W. Thus, W is not 

closed under 

set of vectors 

carries 

over easily from !Rn to general vector 

sets 
Spanning 
The notion 
of a spanning 
spaces. 

Defi n ition If S =  {v1, v2, . . •  , vd is a set of vectors 

the set of 
all linear 
and is denoted 
a spanning set for V and V is said to be spanned by S. 

of v1, v2, . . .  , vk is 
by span(v1, v2, . . .  , vk) or span(S). 

combinations 

space V, then 

in a vector 
the span of v1, v2, . . .  , vk 

called 
If V = span(S ), then S is called 

6.1 Vector 

Spaces 
and Subsp
aces 4 3 9  

Section 
x2 span l!J' 2• 

Example 6 . 11 

Show that the polynomials 1, x, and 

Solution By its very definition, 
nation of 1, 

x, and x2• Therefore, 

a polynomial p (x) = 
l!J'2 = span(l, x, x2). 

a + bx + cx

2 is a linear 

combi­

Example 

xn). However, 
all polynomials
to be infinite, 
l!J' = span(l, x, x2, • • •  ). 

be generalized 

to show that l!J' n = span(l, x, x2, . . .  , 
6.17 can clearly 
space of 
no finite set of polynomials can possibly span l!J', the vector 
. (See Exercise 
44 in Section 6.2.) But, if we  allow  a 
spanning set 
then clearly 

the set of all nonnega

tive powers of x will do. That is, 

Example 6 . 18 

Show that M23 = span(E11, E12, E13, E21, E22, E23), where 

Ell=  0  E12 =  0  E13 =  0 

[� 0 �] [� �] [� 0 �] 
E21 = [ � 0 �] E22 = [� 0 �] E23 = [� 0 �] 

0 

0 

(That is, 

E;j is the matrix with a 1 in row i, columnj and zeros 

elsewhere

.) 

Solution We need only observe that 

Extending 

this example, we see that, 
ces Eij, where i = 1, . . .  , m andj = 1, . . .  , n. 

in general, 

Mmn is spanned 

by the mn matri­

Example 6 . 19 

In l!J'2, determine 

whether 

r(x) =  1  -4x + 6x2 is in 

p(x) =  1 -x + x2 and q(x) = 2 + x -3x2 

span(p(x), q(x)), where 

that cp(x) + dq(x) = r (x). This 

Solution We are looking for scalars c and d such 
means that 

c(l  - x + x

2) +  d(2 + x -3x2) =  1  -4x + 6x2 

Regrouping 

powers of x, we have 

according 
(c + 2d) + ( - c +  d)x + (c -3d)x2 =  1  -4x + 6x2 

Equating 

the coefficients 

of like powers of x gives 

c + 2d = 1 
- c  +  d = -4 
c -3d = 6 

440  Chapter 
6 Vector 

Spaces 

which is easily solved to give c =  3 
r(x) is in span
(p(x), q(x)). (Check this.) 

and d = - 1. Therefore, 

r(x) = 3p(x) -q(x), so 

Example 6 . 2 0  

In ?F, determine 

whether 

sin 2x is in span(sin x, cos 

x). 

S o lution We set c sin x + d 
equation 
x. Setting 

is true. Since 
x = 0, we have 

cos x = sin 
are functions, 

2x and try to determine 
c and d so 
that this 
the equation must be true for all values 

these 

of 

c sin O+d cos O=sinO or c(O)+ d(l)= O  

from which we see that d = 0. Setting 

x = 1T /2, we get 

c sin(7r/2) + d cos('TT/2) = sin(7T) or  c(l) +  d(O) =  0 
But this implies 

2x = O(sin x) + O(cos x) =  0 for 

that sin 

all x, which 

zero function. We 
conclude 

that sin 2x is not in 

c =  0. 
since 

giving 
is absurd, 
span(sinx, cos x)

sin 2x is not the 
. 

Remark It is true that sin 

example, we have the double 
a linear 

combina

tion. 

2x can be written 

in terms 

of sin x and  cos 

angle 

formula sin 2x = 2 sin x cos x. However, 

x. For 
this is not 

Example 6 . 2 1  

In M22, describe 

the span 

of A  = [ � � l B  = [ � � l and C = [ � �]. 

Solution Every linear 

combination 

of A, B, and C is of the form 

cA + dB + eC =  c[�  �] +  d[�  �] + e[� �] 

= [c +  d c + e] 

c + e d 

is symmetric, 

This matrix 
metric 
in span

2 X 2 matrices
(A, B, C). To show this, 

. In fact, we have equality; 

that is, every symmetric 

2 X 2 matrix 

the subspace 

of sym­
is 

2 X 2 matrix. Setting 

so span(A, B, C) is contained 
within 
we let [; �] be a symmetric 
[x y] = [c +  d  c + 
y z  c +e d 
that c = x 

e] 

and solving 

for c and 

d, we find 

-z, d = z, and e =  -x + y + z. Therefore, 

�  (Check this.) It follows 

that span(A, B, C) is the subspace 

of symmetric 

2 X 2 matrices 

. .+ 

Section 

Spaces 
6.1 Vector 
and Subsp
aces 4 4 1  

As was the case in 

!Rn, the span of a set of vectors 
them. The next theorem makes this result 

is always 

a subspace 
precise. 

of the vector 
It generalizes 

that contains 

space 
Theorem 3.19. 

Theorem 6 . 3  Let Vi, v2, •.. , vk b e  vectors 
a. span(vi, v2, •.• , vk) is 
b. span (vi, v2, .•. , vk) is 

space 

in a vector 
V. 
of V. 
subspace 

a subspace 
the smallest 

Vi, v2, •.. , vk. 

of V that contains 

by V. 

Proof (a) The proofofproperty (a) is identical to the proofofTheorem 3.19, with 
!Rn replaced 
(b) To establish 

property (b ), we need to show that any subspace 

Vi, v2, .•• , vk also contains 

span( vi, v2, .•• , vk)· Accordingl
span(vi, v2, .•. , vk) is contained 

Vi, v2, . . .  , vk. Then, since 

that contains 
plication, 
vk. Therefore, 

Ci Vi  + c2v2 + · · · + ckvk of Vi, v2, •.. , 

every linear combination 

W is closed under addition 

y, let W be a subspace 

it contains 

and scalar multi­

of V that contains 

in W. 

of V 

1 Exercises 

6 . 1  

1-11, determine whether 

multi­

set, together 

operations 

space. If it is not, list 

the given 
of addition 

and scalar 
all of the axioms 

In Exercises 
with the specified 
plication, 
is a vector 
that fail to hold. 
in IR2 of the form [: l with the 
1. The set of all vectors 
addition 
and scalar multiplication 
all vectors [;] in IR2 with x 2: 0, y 2: 0 (i.e., 
addition 

2. The set of 

the first quadran
scalar multiplica

t), with the usual 
tion 

vector 

vector 

usual 

and 

6. IR2, with the usual 

defined 

scalar multiplica

tion but addition 

Y1 Yz Yi + Yz +  1 

by [x,] + [x2] = [x, + x2 + l] 
0 
of all positive real numbers, with addition 
EB 
by c 0 x = xc 
by x EB y 

= xy and scalar multiplication 

of all rational 

numbers, with the usual 

addition 

7. The set 

defined 
defined 

8. The set 

and multiplication 
er triangular 

9. The set of all upp

usual 

matrix addition 

2 X 2 matrices, 
and scalar multiplication 

with the 

3. The set of all vectors 

[�] in IR2 with xy 2: 0 (i.e., 

the 

addition 

union of the first and third quadran
ts), with the usual 
vector 
and scalar multiplication 
4. The set of all vectors [;] in IR2 with x 2: y, with the 
usual 
5. IR2, with the usual 
defined 

addition 

addition 

and scalar multiplication 

vector 

by 

but scalar multiplication 

of the form [: � l 

10. The  set  of 

all 2  X  2 matrices 

where ad = 0, with the usual 
scalar multiplication 

matrix addition 

and 

11. The set 

of all skew-symmetric 

n X n matrices, 
and scalar multiplication 

with the 

matrix addition 

usual 
(see page 162). 

12. Finish 

verifying 

(see Example 

6.3). 

space 

that <!f 2 is a vector 
that ge is a vector 

space 

13. Finish 

verifying 

(see Example 

6.4). 

E:v  In Exercises 
14-17, determ

442  Chapter 
6 Vector 
Spaces 
in C2 of the form [�],with the 

with the specified 
plication, 
the axioms that fail to hold. 
14. The set 

operations 
is a complex vector 

ine whether the given set, together 
multi­
of addition and scalar 
space. If it is not, list 
all of 

usual vector 

15. The set Mmn(C) of all m X n complex matrices, 

with 

the usual matrix addition 

tion 

but scalar 

given (fixed) matrix 

addition 

and scalar multiplica

n X n matrices 

of all vectors 
addition 

diagonal 
idempotent n X n matrices 

and scalar multiplication 
C2, with the usual vector 
by c[z1] = [�z1] 

30. V = Mnn' W = {A in Mnn : <let A = 1} 
31. V = Mnn' W is the set of 
32. V = Mnn> W is the set of 
33. V = Mnn' W = {A in Mnn : AB = BA}, where B is a 
34. V = C!P2, W = {bx + cx2} 
35. V = C!P2, W = {a+ bx+ cx2: a + b + c = 
O} 
36. V = C!P2, W = {a+ bx+ cx2: abc = O} 
37. V = C!P, W is the set of all polynomials 
3 
38. V = ':ffe, W = {f in '2F :f(-x) = f(x
)} 
th . d" t d 77  If "t . t l" t 41. v -'!:Y, w - m ';'Y. 0 -0 
39. V = ':ffe, W = {fin '2F :f(-x) = -f(x)} 
18-21, determine whether the given 
space �ver e in zca e lLp· 1 is no , is ·tt;. 
multipli- _ a;-;  _ {f . a;-; ·f( ) _ } 
Jlh 42. V -'!:Y, W is the set of all mtegrable 
set, together 40. V = ':ffe, w = {fin '2F :f(O) = 
l} 
in "11._� with an even number of Jlh 43. V = CZJJ, W = {fin CZl! :f'(x) 2: 0 for all x} 
and scalar � 44. V = ':ffe, W = ct;(2l, the set 
in "11._� with an odd number of Jlh 45. V = ':ffe, W = {fin '2F :  = lim f (x) = 00} 

_ u.::  . 

vector addition 

of addition and scalar 

multiplication 

operations 

continuous 
second 

of all functions 

derivatives 

funct10ns 

addition 

and scalar 

of degree 

vector 

Z2  CZ2 

with 

. 

. 

16. The set 

multiplication 
defined 

.  . 

17. !Rn, with the usual 
In Exercises 
with the 
specified 
catzon, 
is a �ector 
all of the axzoms that fail to hold. 
18. The set of 
all vectors 
"ll2 with the 
usual 
of all vectors 
"ll2 with the 
usual 
Mmn("llp) of all 

ls, over 
multiplication 
19. The set 
ls, over 
multiplication 
20. The set 

from "11._P' over "11._P with the usual 

m X n matrices 

scalar multiplication 

vector 

addition 

and scalar 46. Let V be a vector 
with entries 47. Let V be a vector 
be a subspace 
48. Let V be a vector 

and  an example 
of V. 

and multiplication 

matrix addition 

X->0 
space 

with subspaces 

U and W. Prove 
with v = [R2 to show that U U W need not 
U and W. Give 

with subspaces 

space 

of V. 

space 

with subspaces 

U and W. 

that U n W is a subspace 

21. "ll6, over "ll3 with the usual 
(Think this one through 

addition 
carefull
y!) 

Define the sum of U and W to be 

ine whether 

6.2 to determ

22. Prove Theorem 6.l(a). 
23. Prove Theorem 
6.l(c). 
In Exercises 

24-45, use Theorem 

W is a subspace of V 
24. v � �'. w � mii 25. v � �'. w � trn 
26. v = IR3, w = f [ � l) 
27. v� �'. w � lUJl 

l a + b +l 

U + W = { u + w : u is in U, w is in W} 

(a) If V = IR3, U is the x-axis, and W is the y-axis, 
of a vector space 
(b) If U and W are subspaces 
of V. 

what is U + W? 
prove that U + 

W is a subspace 
spaces, 

define the Cartesian 

49. If U and V are vector 

V, 

product of U and V to be 

V. Prove 
that 
of V X V. 

U X  V = { (u, v) : u is in U and v is in V} 

of a vector 

50. Let W be a subspace 

Prove that U X V is a vector 

space. 
space 
Li = { (w, w): w is in W }  is a subspace 

In Exercises 

51 and 52, let A = [ 1  1] and 
B = 1 0 . Determine whether C is in span(A, B). 
[l - 1]  - 1  1 
[l 2]  [3 - 5] 
3  4 

5 - 1  

51. c = 

52. c = 

Section 

6.2 Linear 

Independence, 
and Dimension 

Basis, 

443 

i] [o - 1]? 

1 ' 1 0 

53 and 54, let 
p(x) =  1 -2x, q(x) = x - x2, 
In Exercises 
and r(x) = -2 + 3x + x2• Determine whether s(x) is in 
span(p(x), q(x), r(x) ). 
53. s(x) = 3 -5x - x2 54. s(x) =  1 + x + x
55-58, let f(x) = sin2x and g(x) = cos2x. 
In Exercises 
h(x) is in span(j(x), g(x) ). 
Determine whether 
55. h(x) =  1  56. h(x) =cos 2x 
57. h (x) = sin 2x  58. h (x) = sin x 

59. Is M22 spanned by [ � � l [ � � l [ � � l [ � -�]? 

2 

Writing Project The Rise of Vector Spaces 

by 1 + x, x + x2, 1 + x2? 
by 1 + x + 

2x2, 2 + x + 

61. Is <!!'2 spanned 
62. Is <!!'2 spanned 
63. Prove that every 

-1 + x + 2x2? 

2x2, 
has a 

vector 

space 

unique 

zero 

vector. 

64. Prove that for every 
vector 
a unique v' in V such 

there is 

space 
v in a vector 
V, 
that v + v' = 0. 

ue invented 
in 1827 by 
widespread acceptance 

nn and Giuseppe 

As noted in the sidebar on 
Hermann Grassma
idea of a vector 
work had  its 
origins 
August 
Mobius 
of the vector 

and the vector space 
in barycentric 
coordinates, a techniq
strip 

page 429, in the late 19th century, the mathematicians 
troducing the 
ental in in
y. Grassmann's 

axioms that we use toda

Peano were instrum

fame). However, 

Ferdinand 

(ofMobius 

space 

space conce
Write a report on the 

pt did not come until 
history 

spaces. 
and the contributions 

the early 
Discuss 

of vector 

20th century. 
the origins 

of the 
of 
Why was the math­

slow to adopt these 

of Grassmann and Peano. 

a vector 
space 
ematical 
community 
1 .  Carl B. Boyer and Uta C. Merzbach, 

ideas, and 
A Histor
2. Jean-Luc, Dorier (1995), A General Outline 

how did acceptance 
y of Mathematics (Third Edition) 
of the Genesis 

of Vector Space 

(Hoboken, 

NJ: Wiley, 201 1). 

come about? 

notion 

3. Victor 

Theory, Historia 
J. Katz, A Histor
ing, MA: Addison 

Mathematica 22 (1995), pp. 227-261. 
y of Mathematics: 
Wesley Longman, 
2008). 

An Introductio

n (Third Edition) 
(Read­

f linear Independenc

e. Basis, a n d  Dimension 

In this section, 
to general 
vector 
the proofs of the 

we extend 
of linear 
spaces, 
generalizing 
the results 
theorems carry over; 
we simply 

independence, 
of Sections 
replace 

basis, 
!Rn by the vector 
V. 

the notions 

space 

2.3 and 3.5. In most cases, 

and dimension 

linear 

Independenc

e 

Defi n ition A set of vectors 
pendent if there 

{v1, v2, . . .  , vd in a vector 

are scalars c1, c2, . . .  , ck, at least 

one of which 

space V is linearly de­
is not zero, 

such that 

A set of 

vectors that 
linear

is not 

ly dependent is said to be linearly independent. 

444  Chapter 
6 Vector 

Spaces 

As in 

IJ�r, {v1, v2, .•• , vd is line
c1v1 + c2v2 + · · · 
{v1, v2, .•. , vk} in a vector 
We also have the following 

A set of vectors 
if at least 

the vectors 

one of 

arly independent in a vector 

+ ckvk = 0 implies 
useful alternative 

formulation 

space 
c1 = 0, c2 = 0, . . .  , ck = 0 
oflinear 

V if and 
only if 

dependence. 

Theorem 6 . 4  

space 
can be expressed 

V is linearly dependent if and 
only 
combination 
of the others. 

as a linear 

Proof The proof 

is identical 

to that of Theorem 
2.5. 

As a specia

l case of Theorem 6.4, note that a set of 

two vectors 

is linearly depen­

dent if and only if one is a scalar multiple 

of the other. 

Example 6 . 2 2  

In l!f 2, the set 

{l + x + x2, 1 - x + 3x2, 1 + 3x - x2} is linear

ly dependent, 

since 

2(1 + x + x2) -(1 - x + 3x2) =  1 + 3x - x2 

Example 6 . 2 3  

In M22, let 

A = [�  �l B = [� - 1] C =  [2 OJ 

0 ,  1  1 

Then A + B = C, so the set 
{A, B, C

} is linearly dependent. 

Example 6 . 2 4  

In ':IF, the set {sin2x, cos2x, cos 2x} is linear

ly dependent, 

since 

Example 6 . 2 5  

cos 2x = cos2x -sin2x 

Solution 

Show that the set 

{l, x, x2, . . .  , xn} is linearly indepen

dent in <!P w 
0, c1, .•. , en are scalars such that 
= c0 + c1x + c2x2 + · · · + cnxn is zero for all values of x. But 
Co· 1 + C1X + CzX2 + · · · 
+ CnXn = 0 
that c0 = c1 = c2 = · · · = en = 0. 
p (x) 
have more than 
at most n cannot 
of degree 

Then the polynomial 
a polynomial 
So p (x) must be the zero polynomial, meaning 
Therefore, {l, x, 

x2, . . .  , xn} is linearly indepen
dent. 

n zeros (see 
Appendix D). 

1 Suppose that c

2 We begin, 

by assuming 

solution, 

that 
as in the first 
+ CnXn = Q 
p(x) = Co + C1X + CzX2 + · · · 
tute x = 0 to obtain 

for all x, we can substi

Since 

this is true 

c0 = 0. This leaves 

C1X + CzX2 + · · · 
+ CnXn = 0 

l� Solution 

6.2 Linear 

Independence, 
and Dimension 

445 

Taking 

derivatives, 

we obtain 

Section 
C1 + 2CzX + 3C3X2 + · · · + ncnxn-I = 0 

Basis, 
2c2x + 3c3x2 + · · · + ncnxn-I = 0 
c0 = c1 = c2 = · · · = en = 0, and {1, x, 
4 

2 = 0. Continuing 

in this fashion, 

we 

and setting x 
and setting 
find that k!ck = 0 fork = 0, . . .  , n. Therefore, 
x2, . . .  'xn} is linear

= 0, we see that c
x = 0, we find that 2c2 = 0,  so c
ly independent. 

1 = 0. Differentiating 

Example 6 . 2 6  

In <;!/'2, determine 
Solution Let c1, c2, and c3 be scalars such 

whether 

that 

the set {1 + x, x + x2, 1 + x2} is linearly independent. 

Then 

This implies 
that 

C1 +  c3 = 0 
C1 + Cz  = 0 
C2 + C3 = 0 

the solution to 
which is c1 = c2 = c3 = 0. It follows 
dent. 
linear
ly indepen

that {1 + x, x + x2, 1  + x2} is 

2.23(b). The system 
correspondence 

between 

<;!/' 2 and 

of equations 

Remark Compare Example 

that arises 
IR3 that relates 

is exactly 

6.26 with Example 
the same. This is because of the 

I  + rn [iJ x + x' -[: J 1  + x' -m 

and produces the columns 
to solve. Thus, showing that {1 + x, x + x2, 1 + x2} is linear
lent to showing that 

of the coefficient matrix of the linear 

ly indepen

system 

that we have 
dent is equiva­

is line

arly independent. 
This can be 

done simply 

by establishing 

that the 
matrix 

[i : �] 

has rank 3, by the 
Fundamental 

Theorem of Invertible 
Matrices

. 

446  Chapter 
6 Vector 

Spaces 
In ?JP, determine 

Example 6 . 2 1  

whether 
S o lulion The functions 
only if one of 
that this is not 
the same zeros, 

them is 
a scalar multiple 
the case, since, 
none of which are zeros 

of the 
for example, 

{sin x, cos x} is line

the set 
f(x) = sin x and g(x) = cos x are linear
other. But it is clear 

any nonzero multiple 

ly dependent 
if and 
from their 
graphs 
ofj(x) 

= sin x has 

arly independent. 

This approach 

may not always 

direct, 

more computational method. 

ofg(x) =cos x. 
be appropria
Suppose c and d are scalars such 

use, so we offer the 

te to 

that 

following 

x = 0, we obtain 

Setting 
set {sin x, cos x} is linearly independent. 

x = n/2, we obtain 

c sin x + d cos x = 

0 
d = 0, and setting 

c = 0. Therefore, 

the 

Although the 

definitions 
in terms of finite sets of vectors, 
follows: 

of linear 

dependence 

and independence 

are phrased 

we can extend the concepts 

to infinite sets as 

A set S of vectors 
many linearly dependent vectors. A set of 
said to be linearly independent. 

in a vector 

space V is linearly dependent if it contains 

finitely 

vectors 

that is not line

arly dependent is 

Note that for 
example 

of an infinite 

finite sets of vectors, 

this is 
ly independent vectors. 

set of linear

just the original 

definition. 

Following 

is an 

Example 6 . 2 8  

ly independent. 

subset T of S that is linear
power of x in T and let 
such that 

Solulion Suppose there is 
a finite 
the highest 
scalars 

In <lP, show that S = {l, x, x2, . . .  } is linear
xn be the lowest 
en, Cn+l' . . .  , cm, not all zero, 
Cn+i = · · · = cm= 0, which is a contradict

to that used in Example 

But, by an argument 

similar 

ly dependent vectors, 

so it is linear

linear

6.25, 
S cannot 

this implies 
contain 

ion. Hence, 
ly independent. 

finitely 
many 

ly dependent. Let xm be 
power of x in T. Then there 
are 

that en = 

Bases 
The important 
spaces. 

concept 

of a basis 

now can be extended 

easily to arbitrary 
vector 

Defi n ition A subset 
1. l3 spans V and 
2. l3 is linear

ly independent. 

l3 of a vector 

space 

V is a basis for V if 

Section 

6.2 Linear 

Independence, 
and Dimension 

Basis, 

441 

Example 6 . 2 9  

If e; is the ith 
called 

column 

the standard basis 

for !Rn. 

of the n X n identity 

matrix, 

then { e1, e2, . . .  , en} is a basis for !Rn, 

Example 6 . 3 0  

{ l ,  x, x2, • . •  

, xn} is a basis 

for '!P n' called the standard 
basis for '!P n-

Example 6 . 3 1  

The set [ = {E11, . • .  , E1n, E21, • • •  , E2n, Em1, • • •  , Emn} is a basis 
matrices 

for Mmn' where the 
6.18. [ is called the standard 
basis for Mmn­
Mmn-It is easy to show that 
[ is linearly inde­

We have already 

Eij are as 

in Example 

defined 

seen that [ spans 
[ is a basis for Mmn-

y this!) Hence, 

pendent. (Verif

Example 6 . 3 2  

Show that B = {l + x, x + x2, 1 + x2} is a basis for 

'!1'2• 

Solution We have alread
show that B spans '!1'2, let a + bx + cx
show that there 

are scalars c1, c2, and c3 such that 

y shown that B is linearly independent, 
in Example 
To 

2 be an arbitrary 

6.26. 
polynomial in '!1'2• We must 

or, equivalentl

y, 
(c1 + c3) + (c1 + c2)x + (c2 + c3)x2 = a + bx + cx2 
the linear 

of like powers of x, we obtain 

system 

Equating 

coefficients 

whi<h has a solution, 

since 

is invertible. 
it exists

.) Therefore, 

B is a basis 

(We do not need to know what the solution is; we only need to know that 

the weffideot matcix [ i : �] has rnok 3 aod, hence, 
matrix [ i O �] is the 

the correspondence 

between '!1'2 and IR3, as indicated 

key to Example 

for '!P 2• 

6.32. 

We rnn 

in 

Ro11ark Obmve that the 

tely obtain 

immedia
it using 
the Remark following 

Example 

6.26. 

448  Chapter 
6 Vector 

Spaces 

Example 6 . 3 3  

Show that B = {l, x, x2, • • •  } is a basis 
for <!P. 
S o lulion In Example 
since 

polynomial is a linear 

clearly 
every 

6.28, 

we saw that B is linearly independent. 
It also spans <!P, 

combination 

of (finitely many) powers of x . .+ 

Example 6 . 3 4  

Find bases 

for the 

three 

vector 

spaces in Example 

6.13: 

Solulion Once again, 
similarities 
take us until 

among them. In a strong 
Section 

we will work the three 
sense, 

examples 
side by side to highli
ght the 
they are all 
the same example, 
perfectly 
precis
e. 

6.5 to make this idea 

(a) Since 

(b) Since 

a + bx - bx2 + ax3 

= a(l + x3) + b(x - x2) 

we have W1 = span(u, v), where 

we have W2 = span(u(x), v(x)), 
where 

and 

u(x) = 1 + x3 
v(x) = x -x2 

linear
Since {u, v} is clearly 
ly in­
it is also a basis 
for W1. 

dependent, 

Since {u(x), v (x)} is clearly 
ent, it is also  a basis 

lin­

early independ
for W2. 

but it will 

we have W3 = span(U, V), where 

- b  a = a 0 1 + - 1  0 

(c) Since [ a  bJ [ 1 OJ b[ 0 1 J 
U = [ 1 0J and V = [ O 1 J 
- 1  0 
for W3• .+ 

0 1 

{ U, V} is clearly 
linear
ly in­

dependent, 

it is also 

a basis 

Since 

Coordina1es 
with respect to a basis 
Section 
for subspaces of !Rn. We now extend this concept 
to arbitrary 
vector 

3.5 introduced the idea of the coordinates 

of a vector 

spaces. 

Theorem 6 . 5  

Let V be a vector 
exactly 
one way to 

and let B be a basis 

space 
write v as a linear combination 
vectors 

vector 
in B. 

for V. For every 

of the basis 

v in V, there is 

Proof The proofis 
infinite, 
linear 
since 

the same as the proof 
combinations 

ofTheorem 3.29. 
definition, 
finite. 

are, by 

It works even ifthe 
B is 

basis 

Section 

6.2 Linear 

Independence, 
and Dimension 

Basis, 

449 

The converse of Theorem 6.5 is also true. 
every vector 

space V with the property that 
combination 
the unique 
makes sense. 

representation 
representation of a vector 

of the vectors 

Since 

in B, then 
property characterizes 

a basis. 
to a basis is unique, 

with respect 

That is, if B is a set of vectors 
in V can be written uniquely 

in a vector 
as a linear 
B is a basis for V (see Exercise 30). In this sense, 

the next definition 

Defi n ition Let B = { v1, v2, .•. , vn} be a basis for a vector 

= c1v1 + c2v2 + · · · + cnvn-Then c1, c2, •.• , en are called 

in V, and write v 

vector 
coordinates of v with respect to B, and 

the column 

vector 

space 

V. Let v be a 

the 

is called 

the coordinate vector 

of v with respect to B. 

Observe that if the basis 

B of Vhas n vectors, 

then [ v l B  is a (column) vector 

in !Rn. 

Example 6 . 3 5  

[p(x)] B of p (x) = 2 -3x + 5x2 with respect 
to the stan­

vector 

Find the coordinate 
dard basis 

B = {l, x, x2} of(!J>2. 
Solution The polynomial 

p(x) is already 

a linear 

combina

tion of 1, x, and 

x2, so 

This is the correspondence 

and it can easily be generalized 
to show 

between (!J>2 and IR3 that we remarked 
on after 
of a 
vector 

that the coordinate 

6.26, 

Example 
polynomial 

with respect 

to the 

standard 
basis 

B = {l, x, x2, . . .  , x"} is just the vector 

Remark The order in which the basis 

vectors 
vector. For example, 

in a coordinate 

the entries 

appear in B affects the order 

of 

in Example 

6.35, 

assume 

that the 

4 5 0   Chapter 
6 Vector 

Spaces 

!3' = {x2, x, l}. Then the 
to !3' is 
standard basis vectors 
p (x) = 2 -3x + 5x2 with respect 

are ordered as 

coordinate 

vector 

of 

[p(x) ]a � Hl 
[A] B of A = [2

- 1] 3 with respect 

4 

to the standard basis 

Example 6 . 3 6  

Find the coordinate vector 
l3 = {E11, E12, E21> E22} of M22• 
Solulion Since 

we have 

This is the correspondence 

between M22 and IR4 that we noted before the intro­
6.13. It too can easily be generalized to 

to Example 

give a correspondence 

duction 
between Mmn and !Rmn. 

Example 6 . 3 1  

[p(x) ] 8  of p (x) = 1 + 2x - x2 with respect 
to the basis 

Find the coordinate vector 
C = {l + x, x + x2, 1 + x2} of\)}>2. 
Solulion We need to find c1, c2, and c3 such that 

c1(1 + x) + c2(x + x2) + c3(1 + x2) = 1 + 2x - x2 
(c1 + c3) + (c1 + c2)x + (c2 + c3)x

2 = 1 + 2x - x2 
this means we need to solve the 
system 

6.32, 

or, equivalentl

y, 

As in 

Example 

c1 +  c3 =  1 
2 
C1 + c2 
c2 + c3 = - 1  

whose so

lution is found to be c1 = 2, c2 = 0, c3 = - 1. Therefore, 

[Since 
correct

this result 
.] 

Section 
says that p (x) = 2 (1  + x) - (1 

Basis, 
6.2 Linear 
and Dimension 
Independence, 
easy to check that it is 4 
+ x2), it is 

4 5 1  

The next theorem shows that the 

process 
operations 

of forming 
of addition 

coordinate 

vectors 
is com­
multiplication. 

and scalar 

patible 

with the vector 

space 

Theorem 6 . 6  

for a vector 

space 

V. Let u and v be vectors 

in 

Let B = {v1, Vz, . . .  , vn} be a basis 
e be a scalar. Then 
V and let 
a. [u + vl s  = [ul s  + [vl s  
b. [cu]s = e[u ]s 

Proof We begin by writing 

u and v in terms 

of the basis vectors-say, as 

Then, using 

vector 

space 

properties, 

we have 

and 

so 

and 

. 

.  . 

ez + dz  ez  dz 
+ .  . 
.  . 

lei+ d1] [e1l ld1] 
en + dn  en  dn [eel] [ell eez  ez 

[cu] 8  =  : = e : = e[u ] 8  

een  en 

An easy co

rollary 

to Theorem 6.6 states 

that coordinate 

vectors 

preser

ve linear 

combina

tions: 

You are asked to prove this corollary 

in Exercise 31. 

The most useful aspect 
information from a general 
space 
to 3 at our disposal. We will explore 
For now, we have the following 

of coordinate vectors 
vector 

to !Rn, where we have the tools 

is that they allow us to transf

er 
1 

this idea in some detail 

in Sections 

of Chapters 
6.3 and 6.6. 

useful theorem. 

(1) 

4 5 2   Chapter 
6 Vector 

Spaces 

Theorem 6 . 1  

Let B = {v1, v2, ••• , v"} b e  a basis for a vector 

be vectors 
{ [ u1] 13, . . .  , [ uk J 13} is line

in V. Then {u1, . . .  , ud is line
ent in 

IR". 
arly independ

space V and let 

u1, . . .  ,  uk 

arly independent in V if and only if 

Proof Assume 

that {u1, . . .  , ud is line

dent in V and let 

arly indepen
cdu1] 13  + · · · + cdud13 = 0 

in IR". But then we have 

[c1u1 + · · · + ckuk] 13  = 0 

c1u1 + ··· + ckuk with respect 

to 

using 
B are all zero. 

Equation (
That is, 

1), so the 

coordinates 

of the vector 

The linear 
{ [ u1] 13, . . .  , [ uk J 13} is linear

independence 
ly indepen
dent. 

The converse implica

c1u1 + · · · + ckuk = Ov1 + Ov2 + · · · + Ov" = 0 

of {u1, . . .  ,  uk} now forces c1 = c2 = · · · = ck = 0, so 
V; = 0 · VI + · · · +  1 · V; + · · · + 0 • V n 

is left as Exercise 32. 

which uses similar 

in the special 

U; = V;, we have 

case where 

ideas, 

tion, 

Observe that, 

Dimension 
The definition 
number of vectors 
basis, 
that different 
bases 

of dimension 

is the same for a vector 
for the space. 

space 
a vector 

Since 

in a basis 

we need to show that this definition 

makes sense; 

contain 
Part (a) of the next theorem generalizes 
Theorem 2.8. 

for the same vector 

space 

as for a subspace of !Rn -the 
space 
can have more than one 
that is, we need to establish 

the same number of vectors. 

Theorem 6 . 8  

Let B = {v1, v2, . . .  , vn} be a basis 
a. Any set of more than n vectors 
b. Any set of 
fewer than n vectors 

V. 
for a vector 
space 
in V must be linear
ly dependent. 
in V cannot span V. 

2.8. 

vectors 

in !Rn and, hence, 

in V, with m > n. Then { [ u1 ] 13, . . .  , 

of more  than  n 
This means that {u1, . . .  ,  um} is linearly dependent as well, by 

Proof (a) Let {u1, . . .  , um} be a set of vectors 
[ um J 13} is a  set 
by Theorem 
Theorem 
6.7. 
(b) Let {u1, . . .  , um} be a set of vectors 
in V, with m < n. Then S = { [ u1J13, . . .  , [ um J 13} 
is a set of fewer than n vectors in !Rn. Now span(u1, . . .  ,  um) =  V if and 
only if 
span(S) =  !Rn (see Exercise 33). But span(S) is just the column 
then  X m 
space of 
matrix 

is linear

ly dependent, 

A =  [ [u1]13 ·· ·  [ umJ13 ] 

so dim(span(S)) = dim(col(A)) :s m < n. Hence, 
does not span V. 

S cannot 

span !Rn, so {u1, . . .  , um} 

Now we extend 

Theorem 3.23. 

Section 

6.2 Linear 

Independence, 
and Dimension 

Basis, 

4 5 3  

Theorem 6 . 9  

The Basis Theorem 

If a vector 
vectors. 

space V has a basis with n vectors, then every basis for V has exactly 

n 

word. However, 

it 

The proof 

is easier 

word for 

virtually 

of Theorem 3.23 also works here, 

to make use of Theorem 6.8. 

P r o o f  Let B be a basis for V with n vectors 

vectors. By Theorem 6.8, m ::::: n; otherwise, B' would be 
Since n ::::: m and m ::::: n, we must have n = m, as required. 

n ::::: m, since 

basis 
in V is linear
independent. 

Now use Theorem 6.8 with  the 
roles 
of V with m vectors, Theor

and let B' be another 

ly dependent. Hence, 

em 6.8 implies 

of B and  B' 

basis for V with m 
linearly dependent. 
interchanged. 

Since B' is a 

that any set of more than m vectors 
B is a basis and 

is, therefore, 
linearly 

The following 

definition  now 

makes sense, 

since the 

number of vectors 

in a 

(finite) 

basis 

does not depend 

on the choice 

of basis. 

space V is called 

finite-dimensional 

of finitely 

Defi n ition A vector 
sisting 
ber of vectors 
in a basis 
A vector 
to be zero. 

many vectors. The dimension 

for V. The dimension 

if it has a basis con­
by dim V, is the num -

of V, denoted 
of the zero vector 
space 
basis is called 

infinite-dimension

al. 

space tha

t has no finite 

{ 0} is defined 

Example 6 . 3 8  

the standard 
bspace 

basis for !Rn has n vectors, 
is just the 
A two-dimensional 
the  origin. 
ly independent 
(i.e., 

Since 
dimensional su
through 
linear
origin. Any 
linear
Theorem. 
as shown in Table 6.1. 

vector 
is spanned 
therefore 
span IR3, by the Fundamental 
dent vectors must 
according to dimension, 

nonparallel) 
ly indepen
of IR3 are now completely classified 

dim !Rn = n. In the case 
by its basis 

of IR3, a one­
and thus 
is a line 
of two 
the 

span of a single nonzero 

vectors and 

The subspaces 

is a plane through 

subspace 

three 

v 

Table 6 . 1  
dimV 
3 
2 

0 

IR3 
Plane 
Line through the origin 

the origin 

through 

{O} 

Example 6 . 3 9  

The standard 
basis 
n + 1. 

for <!J> n contains 

n +  1 vectors 

(see Example 6.30), 

so dim <!J> n = 

4 5 4   Chapter 
6 Vector 

Spaces 

Example 6 .4 0  

The standard 
dimMmn = mn. 

basis for  Mmn contains 

mn vectors 

(see Example 6.31), so 

Example 6 . 4 1  

they each contain 

the infinite 

linearly 

Both <!P and 9F are infinite-dimensional, 

dent set { l ,  x, x2, ••• } (see Exercise 

indepen

since 

44). 

Example 6 .4 2  

of the vector 

space W of symmetric 

2  X  2 matrices 
(see 

Find the dimension 
Example 

6.10). 

Solulion A symmetric 

2 X 2 

matrix is 
of the form 

so W is spanned 

by the 

[� �] = a[� �] + b[� �] + c[� �] 
set s = { [� �l [� �l [� �]} 

If S is line

arly independent, 

then it will be a basis for W. Setting 

we obtain 

from which 
dent and 

it immedia

is, therefore, 

tely follows 
0. Hence, 
a basis for W. We conclude that dim W = 3. 

that a = b = c = 

S is linear

ly indepen­

The dimension 

of a vector 

space 

is its "magic 

number:' Knowing 

the dimension 

space 

of a vector 
plify the work needed 
illust
examples 
rate. 

V provides 

us with much information about V and can 

greatly 
as the next few theorems 

sim­
and 

in certain 

types of calcula
tions, 

Theorem 6 . 10 

with dim V = n. Then: 
dent set in V contains at 
most n vectors. 

space 
ly indepen

Let V be a vector 
a. Any linear
b. Any spanning 
c. Any linear
d.  Any 
e.  Any 
f. Any spanning 

set for V 

ly independent set of 

contains 

at least n vectors. 
exactly 
n vectors 
of exactly 
dent set in V can be extended 

spanning set for V consisting 
linear

ly indepen

set for V can be reduced 

to a basis 
for V. 

n vectors 

is a basis for V. 

to a basis 

for V. 

in V is a basis 
for V. 

Section 

6.2 Linear 

Independence, 
and Dimension 

Basis, 

4 5 5  

of the 

dent set of 

of exactly 

is some vector 

n vectors. 

n vectors 

exactly 
v in V that is not a linear 

- 1 vectors 

in V. If S does not span V, 
vectors 
in S. 
linearly indepen
dent 
We conclude that S must 

combination 
that is still 
impossible, by Theorem 6.S(a). 

Proof The proofs of properties (a) and (b) follow from parts (a) and (b) of Theo­
rem 6.8, respecti
vely. 
(c) Let S be a linearly indepen
then there 
Inserting v into S produces a set S' with n + 1 vectors 
(see Exercise 54). But this is 
span V and therefor
e be a basis for V. 
(d) Let S be a spanning set for V consisting 
dependent, 
then some vector 
v in S is a linear 
away leaves a set 
S' with n 
impossible, by Theorem 6.S(b 
therefore 
(e) Let S be a linearly independent set of vectors 
V and so consists 
then, 
combina
independent. 
a larger, 
indepen
process 
Therefore, 
S* is 
(f) You are asked to prove this property in Exercise 

linearly independ
dent set in 
stops, we have a linearly independent set S* that contains 

linearly 
and expand it into 
no linearly 
by Theorem 6.S(a). 
When the 
S and also spans V. 

n vectors, 
the proof of property (c), there 
in S. Inserting 

tion of the others. 
v 
spans V (see Exercise 
linearly indepen

ent set. Eventuall
more than 

y, this process 
n vectors, 

eat the process 
must stop, 

). We conclude that S must be 

combina
that still 

v into S produces a set S' that is still 

55). But this is 
dent and 

does not span V, we can rep

by the Basis Theorem. 

If S is linearly 
Throwing 

S spans V, it is a basis 

tion of the vectors 

for V that extends 

V can contain 

is some vector 

of exactly 

be a basis 
for V. 

If S' still 

in V. If 

a basis 

since 

as in 

for 
V, 

56. 

If S does not span 
v in V that is not a linear 

S. 

You should 

view Theorem 6.10 as, in 

part, a labor-sa

instances, 
of vectors 

it can dramatic
is line

arly independent, 
a spanning 

set, or a basis. 

ally decrease 

the amount 

of work needed 

ving device. In many 
to check that a set 

2

2

22

S is a basis 
for V. 

whether 

In each case, 

determine 

(a) V = rzf
(b) V = M
(c) V = rzf

, S = {l + x, 2  - x + x 2, 3x -2x2, -1 + 3x + x2} 
-�], [ � _ �]} 
, S = { [ � 
�], [ � 
, S = {l + x,x + x2, 1 + x2} 
dim(\]f 2) = 3 and S contains 
\)f 2. 
dim ( \]f 2) = 3 and S 
dim(M22) = 4 and S contains 
three 

Solution (a) Since 
dent, by Theorem 
(b) Since 
rem 6.lO(b). 
( c) Since 
early 
S is line
(This is 
using 

arly independent; 
the same problem as in 

contains 
three 
\]f 2, by Theorem 6.10( 
6.26. 
Example 

we did this in Example 

S is not a basis 
for M22. 

S is not a basis for 

Theorem 6.10!) 

independent 

or if it spans 

6.lO(a). 

Hence, 

Hence, 

{l + x, 1 - x} to 

Extend 

Solution First note that {l + x, 1 - x} is 
((lf 2) = 3, we 

vector-one 

need a third 

a basis 

for \)f 2. 

four vectors, S is linear

ly depen­

vectors, S cannot 

span M22, by Theo­

vectors, S will be a basis 

c) or (d). It is easier 

Therefore, 

for rzf 2 if it is lin -
to show that 
S is a basis 
easier 

for \]f 2• 

it becomes 

6.32-but see how much 

linear

that is not linear

dim 
ly independent. 
(Why?) Since 
two. 

ly dependent on the first 

Example 6 .4 3  

Example 6 .44 

4 5 6   Chapter 
6 Vector 

Spaces 

as in the proof 
We could 
and error. However, it 
is easier 

proceed, 

of Theorem 6.IO(e), 
to proceed 

to find such a vector 
in a different 

in practice 

way. 

using 
trial 

We enlarge 

the given set of vectors 

by throwing 

in the entire 

standard 

basis for !!f 2. 

This gives 

S = {l  + x, 1  -x, 1, x, x2} 

added, 

that was 

by Theorem 

-in this case, 

arly dependent, 

Now S is line
vectors
first vector 
is linear
{l + x, 1  -x, x} is line
is line
1  -x, x2} is a basis 

6.IO(a), so we need to throw away some 
starting 
with the 

We use Theorem 
6.lO(f), 

two. Which ones? 
1. Since 

1  = i(l + x) + i(l -x), the set {l + x, 1  -x, l} 
ly, x = i(l  + x) -i(l  -x), so 
we check that {l + x, 1  -x, x2} 
{l  + x, 

so we throw away 1. Similar
arly dependent als
o. Finally, 
(Can you see a quick 

way to tell this?) Therefore, 

arly independent. 

ly dependent, 

for l!f2 that extends 

{l + x, 1  -x}. 

6.42, 

In Example 

the vector 
This is an example 

the vector 
space M22 of all 2 X 2 

space 
matrices
of a general result, 

2 X 2 
. As we showed, 

of 
M22. 
theorem of this section 

dim W = 3 ::=::: 4 = dim 

W of symmetric 

as the final 

matrices 

is a subspace 

shows. 

Theorem 6 . 1 1  

of a finite-dimensional 
vector 

space 

V. Then: 

subspace 

Let W be a 
a. W is finite-dimensiona
b.  dim 

W = dim V if and only if W = V. 

l and dim 

W ::=::: dim V. 

n vectors) 

then di
(containing 

V = n. If W = {O}, 
then any basis B for V 
in V. But B can be reduced 

Proof (a) Let dim 
nonzero, 
contained 
tors), by Theorem 6.IO(f). Hence, 
rtainly 
(b) If W = V, then ce
V = n, then any basis 
B for W consists 
early 
in V and, hence, 
dent vectors 
fore, V = span(B) = W. 

indepen

certainly 

m(W) =  0 ::=::: n = dim V.  If 
spans W, since 
B' for W 

W is 
W is 
(containing 
at most n vec­
and dim(W) ::=::: n =dim V. 
hand, if dim W = dim 
But these 
are then n lin­

dim W = dim V. On the other 
n vectors. 
for V, by Theorem 6.IO(c). There­

of exactly 

a basis 

W is finite-dimensional 

to a basis 

I Exercises 

6 . 2  

1-4, test the sets of matrices for linear 

In Exercises 
dence 
one of the matrices as a linear 

indepen­
in M22. For those that are linearly dependent
, express 
combination 
of the others. 

4· { [ � � l [ � � l [ � � l [ � �] } 
5. {x, 1  + x} in l!f 1 

In Exercises 
pendence. For those 
of the polynomials as a linear 
6. {l  + x, 1  + x2, 1  -x + x2} in l!f 2 
7. {x, 

2x - x2, 3x + 2x2} in l!f 2 

combinat

5-9, test the sets of polynomials for linear 

inde­
, express 
one 

that are linearly dependent

ion of the others. 

Section 

6.2 Linear 

Independence, 
and Dimension 

Basis, 

451 

20. V = Mzz

, 

� 15. If f and g are in 

2 + x3} in <;5}3 

8. {2x, x - x2, 1  + x3, 2 - x
9. {l - 2x, 3x + x2 - x3, 1 + x2 + 2x3, 3 + 2x + 3x3} in <;5}3 
in <;IF, For those that are linearly dependent
in­
In Exercises 
10-14, test the sets of functions 
for linear 
dependence 
, 
express 
combination 
of the 
one of the functions as a linear 
others. 
10. {l, sin x, cos x}  11. {l, sin2x, cos2x} 
12. {eX, e-x} 
14. {sin x, sin 2x, sin 3x} 

13. {l, ln(2x), ln(x2)} 

all functions 

space of 
nt 

C(b (JJ, the vector 
derivatives, then the determina
with continuous 
I f(x) g(x) I 
j'(x) g'(x) 

the Wronskian off and g [named after 
the 
Hoene­
J 6sef Maria 
French 
(1776-1853), who worked 
on the 
of 
theory 
nts and the philosophy of mathematics]. 

W(x) = 
is called 
Polish-
Wronski 
determina
Show that f and g are linearly indepen
Wronskian 
some x such that W(x) * O). 

mathematician 

dent if their 

identica

is not 

� 16. In general, 

the Wronskian 
nt 

determina

lly zero (that is, if there is 
of f1, • • •  Jn in C(b (n-il is the 
fz(x) 
j{(x) 

W(x) = 

21. V = M22

, 

B = { [� �l [� �l [ _� �l [� �l [� �]} 
22. V =  <;IP2, B = {x, 1  + x, x -x2} 
23. V =  <;IP2, B = {l -x, 1  -x2, x -x2} 
24. V =  <;IP2, B  = {l, 1  + 2x + 3x2} 
25. V =  <;IP2, B = {l, 2 -x, 3  -x2, x + 2x2} 
26. Find the coordinate 

vector 

with 

respect 

to the basis B = {E22, E21, Ew E11} of M22• 

of A = [ � ! ] 
of A = [� !] with respect 
[ � � l [ � �]} 

to the basis B = { [ � � l [ � � l 

27. Find the coordinate 

vector 

of M22. 

with respect 

28. Find the coordinate 

vector 

to the basis B = {l + x, 1 -x, x2} of<;JP2. 

of p (x) =  1  + 2x + 3x2 
of p (x) = 2 - x + 3x2 with 

29. Find the coordinate vector 

respect 

to the basis B = {l, 1  + x, - 1  + x2} of <;JP 2. 

30. Let B be a set 

of vectors 
the property that every 
as a linear 
uniquely 
Prove that B is a basis 
for V. 

in a vector 
vector 

V with 
written 

combination 
of the 

in V can be 
in B. 

vectors 

space 

ally zero. 

Repeat Exercises 

and f1, . . .  ,fn are linearly independent, 
provided 
W(x) 
is not identic
using 
10-14 
test. 
the Wronskian 
17. Let {u, v, w} be a linear
space V. 
+ v, v + w, u + w} linear
prove that it is or give a counterexample 

a vector 
(a) Is {u 
(b) Is {u -v, v -w, u -w} linear

Either 
to show that it is not. 

dent set of vectors 

ly indepen

in 

ly independent? 

ly independent? 

in V, and let c1, . . .  , ck be scalars. Show that 

let u1, . . .  , uk 

for a vector 

space V, 

32. Finish 

31. Let B be a basis 
be vectors 
[ c1U1 + . . .  +  ckud s  = C1 [u1l s  + . . .  + cdud s· 
the proof of Theorem 6.7 by showing 

{ [ u1] 8, . . .  ,  [ uk] 8} is 
{ u1, . . .  , uk} is linear

that if 
dent in IR" then 
linear
ly independ
V. 
an 
33. Let { u1, . . .  , um} be a set of vectors in 
B be a basis 
for V. 
to B. Prove that 

n-dimensional 
Let S = { [ u1] 8, . . .  , [um] 8} be the set of coordinate 
vectors 

V and let 

ly indepen

vector 

ent in 

space 

xample 

prove that it is or give a countere

Either 
to show that it is not. 

18-25, determine whether the set B is a basis 

In Exercises 
for the vector 

span(u1, • . •  , um) = V if and only if span(S) = !Rn. 
space V 
and give a basis for V 
18. V = M22, B = { [ �  � l [ � � l [ _ � -�]} 
{[  ] [ 
l9.V =M22, B= �  �' � -�' � �' � -� � 36.V ={p(x)in<;IP2:xp'(x)=p(x)} 
} 35. V =  {p(x)in<;JP2:p(l) = O} 

] [  ] [  ]

of {u1, . . .  , um} with respect 

34. V = {p(x) in <;JP2: p(O) = O} 

34-39,find the dimension 

In Exercises 

space V 
of the vector 

4 5 8   Chapter 
6 Vector 
Spaces 
39. V = {A in M22: AB= BA}, where B = [01 1
1] 

37. V =  {A 
38. V = {A in M22: A is skew-symmetric} 

in M22: A is upper 

triangular} 

dimension 

of the vector 

space 

40. Find a formula for the 

of symmetric 
a formula 

41. Find 

n X n matrices. 
for the dimension 
n X n matrices. 

of skew-symmetric 

of the vector 

space 

42. Let U and W be subspaces 

of a finite-dimensional 

54. Let 

S = {v1, . . .  , vn} be a linearly indepen
space V. Show that if v is a vector 

dent set in 
in V that is 
= { v1, • • •  , vn, v} is still 

a vector 
not in span(S ), then S' 
independent. 

linearly 

55. Let 

S = {v1, • • •  , vn} be a spanning 

set for a vector 
V. Show that ifvn is in span(v1, . . .  , vn_ 1), then 

space 
S' = {v1, • . •  , vn_1} is still 

a spanning 
set for V. 

56. Prove Theorem 6.lO(f). 
57. Let {v1, • • •  , vJ be a basis for a vector 
V 
that 

c1, • . .  , cn be nonzero scalars. Prove 

and let 
{c1V1, . . .  , Cnvn} is also 

a basis for V. 

space 

58. Let {v1, • • •  , vJ be a basis for a vector 

space 

V. Prove 

that 

{vi, V1 + V2, V1 + Vz + V3, . . .  , V1 + . . .  + vJ 

is also 

a basis for V. 

space 

in Exercise 48 

Grassmann's Identity: 

V. Prove 

U + W is defined 

The subspace 

6.1. Let B = {v1, . . .  , vk} be a basis for 

vector 
dim(U + W) = dimU +dim W -dim(U n W) 
[Hint: 
B to a basis C of U and a basis 
D of W. 
of Section 
Prove that C U D  is a basis for U +  W.] 
U n W. Extend 
finite-dimensiona
(a) Find a formula for dim(U X  V) 
(b) If W is a subspace 

dim W, where Li =  {(w, w) : w 

of V, show that dim Li = 

49 in Section 
6.1.) 

V. (See Exercise 

and dim 

l vector 

spaces. 

is in W}. 

43. Let U and V be 

i!f is infinite

in terms of dim U 

-dimensional. 

Show that there is 
combination 

of 

44. Prove that the vector 

space 
Suppose it has a finite 

{l + x, 1 + x + x

basis. 
[Hint: 
some polynomial that is not a linear 
this basis.] 
45. Extend 

for llf 2. 
2} to a basis 
46. Extend { [ � � l [ � �]} to a basis 
for M22• 
47.Extend{[� �l [� �l [� -�]}toabasisforM22• 
48. Extend { [ � �], [ � �] } to a basis 
for span(l, 1 + x, 2x) in llf 1. 
for span(l - 2x, 2x - x2, 1 - x2, 1 + x2) 
in i!f 2. 
for span(l -x, x - x2, 1 - x2, 1 -2x + 
x2) in i!f 2• 
[ _ � -�]) in M22. 

for span( [ � OJ [O l] [-1 

49. Find a basis 
50. Find a basis 

1 , 1 0 , 1 

52. Find a basis 

51. Find a basis 

space of 

symmetric 

matrices

vector 

for the 

2 X 2 

. 

53. Find a basis for 

span(sin2x, cos2x, cos 2x) 

in '!fa. 

ct real numbers. Define 

(x -a0)· • ·(x -a;_1)(x -a;+1)· • ·(x -an) 
(a; -a0) • • ·(a; - a;_ 1)(a; - a;+1) • • ·(a; - an) 

Let a0, a1, • • •  , an be n + 1 distin
polynomials p0(x), p1 (x), . . .  , Pn(x) by 
( )  
P; x  = 
These are called the Lagrange polynomials associated 
with a0, a1, . . .  , aw [Joseph-Loui
s Lagrange 
was born in Italy but spent most of his life in Germany and 
France. He made important 
contributions 
to such fields 
as 
astronomy, mechanics, and the 
number 
theory, algebra, 
calculus 
was the 
of variations. 
of a determinant 
the volume interpretation 

(1736-1813) 
In 1773, Lagrange 
4).] 
59. (a) Compute the Lagrange polynomials associ

ated 

with a0 = 1, a1 = 2, a2 = 3. 

1 if i = j 

ly indepen

60. (a) 

use Exercise 

(b) Show, in general, that 

in i!f w [Hint: 

Prove that the set B 
of Lagrange polynomials 
is linear

{o if i * j 
p;(a) = 
= {p0(x), p1 (x), . . .  , Pn(x)} 
Set c0p0(x) + · · · + cnpn(x) =  0 and 
for i!f w 
(b) Deduce that B is a basis 
polynomial in i!f n' it follows 
61. If q(x) is an arbitrary 
Exercise 
60(b) 
q(x) =  CoPo(x) + . . .  + cnpn(x) (1) 
+ · · · + q(an)Pn(x) is the 

(a) Show that C; = q(a;) for i =  0, . . .  , n, 

for some scalars c0, . . .  , cw 

and deduce 

59(b).] 

that 

dent 

that q(x) = q(a0)p0(x) 
unique 
basis 

representation 

of q(x) with respect 
to the 

from 

B. 

first to give 

(see Chapter 

Section 

6.2 Linear 

Independence, 
and Dimension 

Basis, 

4 5 9  

the func­

(b) Show that for any n + 1 points (a0, c0), (a1, c1), ..• , 
(an, en) with distinct 

first components, 
( 1 )  is the unique 

l of degree 

by Equation 

tion q(x) defined 
polynomia
at most n that passes 
through all of the points. This formula is known 
as the Lagrange 
formula. (Com­
pare this formula with Problem 19 in Explora­
tion: 
Chapter 
4.) 

Applications 

Geometric 

(c) Use the Lagrange 

interpolation 
formula to find 
at most 2 that passes 

the polynomial of degree 
through the points 

of Determina
nts in 

interpolation 

(i) (1, 6), (2, - 1), and (3, - 2) 
(ii) ( -1, 10), (0, 5), and (3, 2) 

62. Use the 

Lagrange interpola

if a polynomial in 
the zero 
polynomial. 
63. Find a formula 

tion formula to show that 

i!f n has n + 1 zeros, then it must be 
for z;. (Why?) Count the 

the 

This is the 
in Mnn(!f_p)· [Hint: 
number of different 
bases 
number of ways to construct 
a basis 
at a time.] 

for z;, one vector 

same as determining 

for the number of invertible 

matrices 

Exploration 

Magic S quares 

The engraving 
the many mathematical 
on the wall 
an array 

shown on page 461 is Albrecht 
in this engraving 

is the chart 
in the upper right-hand corner. (It is enlarged 
We can think 

Durer's Melancholia I (1514). Among 
of numbers that hangs 
shown.) Such 
of it as a 4 X 4 

of numbers is 

known as a magic square. 

in the detail 

artifacts 

matrix 

3 
10 
6 
15 

2 
1 1  
7 
14 

the numbers in each row, in each column, and in both diagonals 

Observe that 
the same 
that Durer cleverly 
the date of the 

placed 
engraving

sum: 34. Observe 

that the entries 

further 
the 15 and 14 adjacent to each 
.) These observations 

Defi n ition An n x n matrix M is called a magic square if the sum of the 

lead to the following 

are the integers 

in the last row, giving 

1 ,  2, . . .  , 16. (Note 

definition. 

other 

have 

is the same in each 

the weight of M, denoted 

row, each column, and both diagonals
1, 2, . . .  , n2 exactly 
once, 

wt(M). If M is an n X n magic square that 
then M is called 

. This common 

a classical 

each of the entries 

entries 
sum is called 
contains 
magic square. 

1 .  If M is a classical 

show that 

n X n magic square, 
wt(M) = _n(_n_2 _+_l_) 
2 
2.4.] 

[Hint: Use Exerc

ise 51 in Section 
2. Find a classical 

amples 

related in any way? 

3  X  3 magic square. 

Find a different 

one. Are your two ex­

4 6 0  

3. Clearly, the 3  X  3 
matrix 

with all entries 

to Problem 2, find a 3 X  3 

entries 

1. Using your answer 

weight 
are dif
all of whose 
square with distinct 
entries 
Let Magn denote 
n X n magic squares 

of weight 

0. 

the set of 

a method 

ferent. Describe 
and weight 
all n X n magic squares, and let Mag� denote 

w for any real number w. 

equal to t is a magic square with 

magic square with weight 
1, 
for constructing 
magic 
a 3  X  3 

the set of 

all 

4. (a) Prove that Mag3 is a subspace 
of M33. 

(b) Prove that Mag� is a subspace of Mag3. 
M = M0 + kJ 

5. Use Problems 

w, then we can write 

magic square of weight 

where M0 is a 3 X 3 
of ones, 
appropria
Let's 

and k is a scalar. What 
te value of k.] 
try to find a way of describing 

3 and 4 to show that if M is a 3 X  3 
M as 

magic square with weight 

matrix consisting 
must k be? [Hint: Show that M -kJ is in Maij for an 

0, J is the 3 X 3 

entirely 

all 3 X 3 

magic squares. Let 

be a magic square with weight 0. 
onals 
b, . . .  , i. 

give rise to a system 

on the rows, columns, and diag­
The conditions 
linear 
homogeneous 
a, 

equations 

of eight 

in the variables 

6. Write out this system 

of equations 

and solve 

it. [Note: 

Using a CAS will 

facilitate the calcula
tions.] 

4 6 1  

7. Find the 

dimension 

By doing a substitution, 

if necessary, use 

your solution to Problem 6 to show that M can be written 

of Mag�. 

Hint: 

[ s  -s -t  t l 

in the form 

M = -s + t 0  s -t 
- t  s + t -s 
of Mag3• [Hint: 

of Problems 

5 and 7.] 

of a 3 X  3 
magic 
certain 
rows, 
columns, 

8. Find the 
dimension 
9. Can you find a direct 

Combine the results 
way of showing that the (2, 2) entry 
w must be w/3? [Hint: Add and subtract 
of the central entry.
magic square of weight 

0, obtained 

a multiple 

square with weight 
and diagonals 

to leave 

] 

10. Let M be a 3 X  3 
3 X  3 
magic square as in Problem 5. If M has the form  given 
in Problem 7, write 
out an 
of M. Show that this is the equation 
equation for the 
sum of the squares 
of a circle in the variables 
plot it. Show that there 
sand t integers. Using Problem 8, show 
points (s, t) on this circle with both 
eight 
these 
eight points give rise to eigh
magic 
squares 

of the entries 
s and t, and carefully 

magic squares. How are 

that 
these 

related to one another? 

t classical 

from a classical 

are exactly 

3 X 3 

4 6 2  

Section 

6.3 Change 

of Basis 

4 6 3  

C h a n g e  of 

Basis 

the molecular 

greatly 
structure 
of zinc, 

a problem described 

using one coordinate  system 

may be 

to a new coordinate 

system. This switch 

is usually 

a change 

mathematics 
for a vector 

that you have prob­

es, a process 

algebra, a basis provides 
vectors. 

via the notion 
y a particular 

of variabl
courses. In linear 
space, 
simplif
shown in Figure 6.3(a). A scientist 
of the bonds between the atoms, the angles 
facilitated 
algebra. The standard basis 
the best choice. 

of coordinate 
problem. For example, 
studying 

will be greatly 
linear 

by intro­

the lengths 

and so on. Such an analysis 
use of the tools of 

system 

coordinate 

basis will often 

by performing 

In many applications, 
solved more easily by switching 
accomplished 
ably encountered 
in other 
us with a 
Choosing the right 
consider 
zinc might wish to measure 
between these 
ducing 
and the associated 
Figure 
than the 
basis, 
atoms of zinc. 

bonds, 
coordinates 

and making 
standard 

standard 

this case 
6.3(b) shows, in 
since 

axes are not always 

xyz coordinate 
vectors 
these 

{u, v, w} is prob
align 

As 
for IR3 
of basis 
ably a better choice 
nicely with the 

bonds between the 

figure 6 . 3  

(a) 

(b) 

v 

Matrices 

Change-of-Basis 
Figure 6.4 shows two different 
basis. 
system 
while Figure 

shows the coordinate 
arises 

Figure 6.4(a) 
6.4(b) 

basis C = {v1,  vz}, where 

coordinate systems 

from the 

for IR2, each arising 
related to the basis 

!3 = {u,,  u2}, 

from a different 

The same vector 

x is shown relative 

to each coordinate system. It is clear 

from the 

diagrams 

that the coordinate vectors 

of x with respect 

to !3 and C are 
[x]8 = [�] and [x]c = [ _�] 

vely. 
respecti
vectors. 
One way to find the 

is a direct 
p is to 

connection 
use [ x] 8 to calculate 

out that there 

relationshi

It turns 

between the two coordinate 

4 6 4   Chapter 
6 Vector 

Spaces 

y 

y 

2 

- 2  

- 4  

- 2  

x 

- 4  

Figure 6 . 4  

(a) 

(b) 

Example 6 .4 5  

Then we can find [x]c 
there 
for such problems. We illustrate 

is a better way to proceed-one 
this approach 

by writing 

x as a linear 

that will provide us with a general 

combination 

ofv1 and v2• However, 
mechanism 

in the next example. 

1 
3 

find [x]c, given that [x]8 =  . 

Using the bases 
Solulion Since x = u1 + 3u2, writing 
coordinates of x with respect 
required 

B and C above, 
u1 = [-�] -3[�] + 2[�] = - 3v1 + 2v2 
u2 = [ _ �] = 3 [ �] [ �] = 3v1 -V2 

and 

so 

to C. We find that 
u1 and u2 in terms ofv1 and v2 will give us the 

This gives 

[x]c = [ _�] 

in agreement with Figure 6.4(b)

. 

This method 

but it has one big 

We can now find 

may not look any easier 
advantage: 

than the one 

suggested 

prior to Example 

6.45, 

y in IR2 
[ y] c from [ y] 8 for any vector 

Section 

6.3 Change 

of Basis 

4 6 5  

with very little 
different 

work. Let's 
additional 
point of view. From 

x = u1 + 3u2, we have 

look at the 

calculations 
in Example 

6.45 from a 

by Theorem 6.6. Thus, 

[x]c = [ [u1Jdu2lcl[�] 
= [-� -�][�] 

where P is the 
very nicely. 

matrix whose columns 

are [ u1] c and [ u2] c· This 

procedure generalizes 

[u1] C• . . .  , 

Defi n ition Let B = { u1, . . .  , u"} and C = { v1, . . .  , v"} be bases for a vector 

are the coordinate vectors 

by Pc<-B and is called 

the 

space V. Then  X  n matrix who
[unlc of the vectors 
change-o

f-basis matrix from B to C. That is, 

in B with respect 

se columns 

to C is denoted 
Pc<-B = [ [ u1Jdu2lc · · · 
basis and C as the "new" 
basis. 
"old" 
the old 
vectors 
6.45 is a specia

by writing 

obtained 

[ un lcl 

Theorem 6.12 shows that Example 

Think of B as the 

are just the coordinate 
the new 
result. 

ones. 

Then the columns 

of Pc<-B 
basis vectors 

in terms of 
l case of a general 

Theorem 6 . 12 

for a vector 

space V and let 

Let B = {u1, . . .  , uJ and C = {v1, . . .  , vn} be bases 
matrix from B to C. Then 

Pc<-B be the change-of-basis 
a. Pc,_s[x]8 = [x]c for all x in V. 
b. PC<-8 is the unique 
c. Pc<-B is invertibl

matrix 

e and (Pc,_8)-1 = Ps<-c· 

P with the property that P [ x] 8 = [ x] c for all 

x in V. 

Proof (a) Let x be in V and let 

[x] c  = [C1U1 + . .. + CnunJ c  

= C1 [ u1J c  + .. .  + Cn [ unJ c  

� I [ u.J c  [ u"]'tl 

= Pc<-s[x]s 

4 6 6   Chapter 
6 Vector 

Spaces 

ith 

that P = Pc.-B· 

It follows 

basis vector 

x = U;, the ith 

column of Pc,_8, by definition. 

in V. Taking 
column 
of P is 

P; = Pe; = P [ u;] B = [ U;] c 
in ll�r, by Theorem 6.7. Hence, 
{u1, . . .  , un} is line

(b) Suppose that P is an n X n matrix with the property that P [x] 8 = [x] c for all x 
in B, we see that [x] 8 = [ u;] 8 = e;, so the 
Pc.-8 = [ [u1J c  · · ·  [ u]c is invert­
For all x in V, we have P C<-B [ x] 8 = [ x] c· Solving for [ x l t» we find that 
•  You may find it helpful 

which is the ith 
( c) Since 
independent 
ible, 

for all x in V. Therefore, 
the uniqueness 

from C to B. Thus, 

(P c.-8) -1 is a matrix that changes 

arly independent in V, the set { [u1

transformation) 
it is a linear 
nate system 
to another
[x]c as output; 
input 
and returns 
a schematic representa
gives 

to think 
of change 
from !Rn to itself 
. The transformation 

tion (indeed, 
that simply switches 
correspond

from one coordi­
Pc<-B accepts 
[x ] 8  as 
(Pc,_8) -1 = p8,_c does just the opposite. Figure 
6.5 

by the Fundamental 

tion of the process. 

have (Pc.-8) -1 = P8.-c. 

property (b), we must 

Jc, . . .  , [ unld is line
arly 

[x ] 8  = (Pc<-B) -1 [x]c 

as a transforma

ing to 

of basis 

bases 

Theorem. 

by 

Remarks 

x • 
v 
[ Jc/  � [ ]B 
[x]c • 
[x]B • 
/Multipli
cation� 
by Pc�B 
Multipli
cation 
by PB�c = (Pc�B)-1 
notation C +--- B as 
to C. 

vectors with 
respect 

think of the 
that P c<-t3 [ x] 8 is a linear 
of this combination 
is [xJc, the 

vectors of 
one basis 

to remember 

the result 

is which, 

be coordinate 

com­

to 

with respect 

Pc.-8 and P8.-c for the 

find the coordinate 

bases 

B = {1, x, x2} and C = 
vector of p(x) = 1 + 2x - x2 

is easy, so we find 

P B<-c first. 

Observe that the 

Figure 6 . 5  Change 
of basis 
•  The columns 
of C:' It is also helpful 

the other 
saying 
bination 
columns of Pc.-8 must themselves 

basis. To remember which basis 

of Pc<-B· But since 

of the columns 

"B in terms 

of Pc.-8 are the coordinate 

Example 6 .4 6  

Find the change-of-basis 
matrices 
{I + x, x + x2, 1 + x2} of\]]>2. Then 
with respect to C. 

Solution Changing 
coordinate 
vectors 

for C in terms of B are 
to a standard 
basis 

Section 

6.3 Change 
of Basis 

461 

(Look back at the 

Remark 

following 

Example 

6.26.) 

It follows 

that 

To find P C<-l3• we could express each 
tors in C (do this), but it is much easier 

vector 

Theorem 6.12(c). 

We find that 

Pc<--13 = (P13,_c)-1 = 

It now follows 

that 

[p(x)] c = Pc,_13[p(x)]13 

[-�-2::1 -�-2::1 -�-2::1 l 

in l3 as a linear 
to use the fact that Pc,_13 = (P13,_c)-1, by 

combination 

of the vec­

which agrees 

with Example 

6.37. 

Remark Ifwe do not 

need Pc,_13 explic

itly, 

we can find [p(x) le from [p(x) ]13 and 

P13,_c using 

Gaussian 

elimina

tion. 

Row reduction 
produces 

It is worth repeating the observation in Example 

6.46: 

Changing to a standard 

(See the next section 

on using 

Gauss-Jordan 

columns of P E<-l3 are the coordinate 

standard 
basis 

If£ is the 

basis 
is easy. 
then the 
are usually 

"visible:' 

We make use of this observation again 

for a vector 

vectors 

space V and l3 is any other 
of l3 with respect 
in the next example. 

to £, and these 

basis, 

elimina
tion.) 

[II [p(x) lcl 

Example 6 .41 

In M22, let l3 be the basis {E11, E21, E12, E22} and let 

C be the basis {A, B, C, D}, where 
C-[ 1 OJ [ 1  1] [ 1 
- O  O' - o o' - 1  �] 

B-

A-

Find the change-of-basis 

matrix 

Pc,_13 and verify that 

[XJc = Pc,_13[X]13 for X = 

[� !]. 

4 6 8   Chapter 
6 Vector 

Spaces 

since 

directl

combina

four linear 

tion problems 

1 To solve 

coefficients 

here we are lucky, 

l3 
of the form 

by inspection. 

we can find the required 

Solulion 
with respect 

y, we must find the coordinate vectors of 

to C. This involves 

this problem 
solving 

If X = [� !l then 

X = aA + bB + cC + dD, where X is in l3 and we must find a, b, c, and d. However, 
Clearly, E11 = A, E21 = -B + C, E12 = -A + B, and E22 = -C + D. Thus, 

[Eulc � m [£,,],�[-I} [E,, ], � [-�l [E,,Jc � [-!] 
rn  Pc�,;� [ [Eulc [E,.], [E,,Jc [E,,Jc] [� -: -� -!] 
[X]s � rn 0 - 1  -!][;] 
to C of the matrix [=] 
Pc+-13[X ]13 = [i 
[� !] = x 

-A-B-C+4D= -[� �]- [� �]- [� �] +4[� �] 

This is the coordinate vector 

- 1  1 
1 0 
0 0 

with respect 

and 

as it should 
be. 

2 We can compute Pc..-13 in a different 

way, as follows. As you will be 

Solulion 
asked to prove in Exercise 21, if£ is another 
(PE..-c) -1pE+-l3· If£ is the standard 
tion. 

basis, 

We have 

basis 

for M22> then Pc..-13 = Pc..-EPE..-13 = 

then PE+-l3 and PE+-c can be 

found by inspec­

Section 

6.3 Change 

of Basis 

4 6 9  

�  (Do you see why?) Therefore, 

Pc+-B = (Pt:+-c) -1Pt:+-B 

0  0 

0 1 
1 0 

0 1 
1 0 
0  0 

1 
0 1 
0  0 

1 - 1  
0 1 
0 0 

[j J[j 0  0 �] 
-�m 0  0 �] 
[j - 1  0 
� [j 0 - 1  -�] - 1  1 
.+ 

method 
advantage 
tions. It has the disadvantage 
using a CAS will facilitate 
finding 
For certain 

of not requiring 
the computa­
that we find a 
of requiring 
a matrix inverse, 

is preferable 

to the first. 

has the 

1 0 
0 0 

solution. 

so in 

problems, though, 
we are about to describe 

yet 

may be just as easy to use. In any event, 
which you may find best of all. 

which agrees 

with the first 

Remark The second 
combina

tion of any linear 
matrix inverse. 
general the 
the first method 
a third app

roach, 

second 

However, 
method 

The Gauss-Jordan 
Finding 
the change-of-basis 
by inspection. 
Finding 
as easy, but requires 
it by 
by Gauss-Jordan elimina
method 
bases, 

hand, then (except 

as in Example 

Melhod for Compuling 

matrix to a standard 

Malrix 
be done 
is almost 
basis 
the change-of-basis matrix from a standard 
of a matrix inverse, as in Example 
6.46. 
If we do 
case) 

a Change-of-Basis 
basis is easy and can 

we will usually 

necessary inverse 

the calculation 

for the 2 X  2 
tion. 

We now look at 

find the 
a modification 

of the Gauss-Jordan 

that can be used to find the change-of-basis matrix 

Suppose B = {u1, . . .  , u"} and C = {v1, .•. , vn} are bases 
and Pc+-B is the change-of-basis matrix from B to C. The ith 

column 
of P is 

6.47. 

for a vector 

space V 

between two nonstandard 

so U; = p1;v1 + · · · 

+ Pnivn-If£ is any basis 

for V, then 

[ u;]t; = [P1;V1 + · · · 

+ Pnivn lt: = P1dv1Jt: + · · · 
+ Pndvn lt: 
in matrix 
form as 

This can be rewritten 

410  Chapter 
6 Vector 

Spaces 

which we can solve by 

There are n such systems 
but the coefficient 
solve 
all the systems 

matrix 

to the augmented 

applying 

Gauss-Jordan elimination 
[ [v1 L�·  · · · 
[vn lt:I [u;] EJ 
of equations 
matrix [ [ v1] E • • • 
simultaneously by row reducing 
[ [v1Jt: · · · 
[vn lt:l [u1Jt: · · ·  [un lt:l = [ CIBJ 

of Pc,_8, 
I l E> ••• ' [ v n l E}' by Theorem 6. 7. 
are [ v1] E> ••• ,  [ vn] E has the 

each column 
Hence, 
we can 
the n X 2n augmented 
matrix 

to be solved, 
[ vn] E] is the same in each case. 

ly independent, 

so is { [ v 

one for 

row echelon form, 

by the Fundamental 

Theorem. 

It follows 

n X n identity 

{ V1' . . .  ' v n} is linear
I for its reduced 

Since 
Therefore, 
matrix 
that Gauss-Jordan 

the matrix C whose columns 

elimination 

will necessarily 

produce 

[ CIBJ---+ [IIPJ 

We have proved 

the following 
theorem. 

where P = Pc<--8. 
Let l3 = {u1, .•. , un} and C = {v1, . . .  , vn}  be bases 
[ C IBJ ---+ [I IPc.-al 

space V. Let 
B =  [ [u1lt: . . .  [un lt:l andC =  [ [v1] ,<:  . . .  [vn lt:L where£ is anybasisforV. 
Then row reduction 
applied 

to the n X 2n augmented 

for a vector 

matrix  [ C 

I B ]  produces 

If£ is a standard 

basis, 
B and C =  PE<--C
6.47. 

B  = PE<--
Example 

is particu
larly 
this method 
· We illustrate 
this method 

easy to use, since 
by reworking 

in that case 
the problem 

in 

Theorem 6 . 13 

Example 6 .4 8  

Rework 
Example 
Solution Taking 

6.47 using 
£ to be the standard basis 

the Gauss-Jordan 

method. 
for M22, we see that 
and C = PE.-c = 

B = 

B  = PE<--

0 
1 
0 
Row reduction 
produces 

ICIB J  � [l 1  1  1 

1  1 0 
0 1 0 
0  0 1 0 

y this row reduction

(Verif

0 

0  0 

� 
1 0 
0  0 

0 fl 0 
[l 0 
[l 0 
:: 0 1 
[l 0  0 
1 0 - 1  -�] 
0 0 f: 0  1 
,�, � [l 0 - 1  -�] 

0 - 1  1 
0 1 0 
0 0 0 

0 
0 
0 
0 1 
0  0 

- 1  1 
1 0 
0 0 

.) It follows 

that 

P

as we found before. 

� 

Section 

6.3 Change 

of Basis 

411 

I Exercises 

6 . 3  

1-4: 

In Exercises 
(a) Find the coordinate vector
bases B and C, respectively. 
respect to the 
change-of-basis matrix Pc,_8from B to C. 
(b) Find the 
( c) Use your answer to part (b) 
compare your an
(d) Find the change-of-basis matrix P8,_cfrom C to B. 
(e) Use your answers to parts (c) and 
and compare your answer with 

s [x]8 and [x]c of x with 
to compute [ x] c, and 
(d) to compute [x]8, 

the one found in part (a). 

one found in part (a). 

swer with the 

1. x = [�], B = { [�], [�]}, 
C = { [ � l [ _ �]} in �2 
2. x = [ _ � l B = { [ � l [ �] } , 
C = { [ � l [ �] } in �2 
3. x � [ _ H B � u n m. [ � J l. 
c � mH:WJl in�' 
4.x� [;Js� mJm.[m 
c� mrnrnJlin�' 
5-8, follow the instructions 
1-4 
of x. 
5.p(x) = 2  -x,B = {l,x}, C = {x, 1 + x}in0\ 
6. p(x) = 1 + 3x, B = {l + x, 1 -x}, 
7. p(x) = 1 + x2, B = {l + x + x2, x + x2, x2}, 
C = {l, x, x2} in rzf 2 
8. p(x) = 4 -2x - x2, B = {x, 1 + x2, x + x2}, 

In Exercises 
using p(x) instead 

C = {l, 1 + x, x2} in rzf 2 

C = {2x, 4} in 0\ 

for 

In Exercises 
Exercises 

9 and 10, follow the instructions 
of x. 
1-4 using A instead 
9. A = [ � _ �], B = the standard basis, 
c={[� -�l[� �],[� �],[� �] }inM22 
10. A = [l 1] 1  1 , 
B = { [� �l [� �l [� �l [� �] }, 
C = {[� �l [� �l [� �l [� �]} inM22 
11 and 12, follow the instructions 
of x. 
1-4 using f (x) instead 
1 1 .  f(x) = 2 sin x -3 cos x, B = {sin x + cos x, cos x}, 
C = {sinx + cosx, sinx -cosx} in span
C = {cos x - sin 
() = 60° to obtain 
14. Repeat Exercise 13 with() = 135°. 
15.Let B andC bebasesfor�2. If C  = {[�],[�]}and 

(sin x, 
cos x) 
the xy-axes in the plane counterclockwise 
an angle 
through 
Use the 
methods 
x' y'-coordinates 
are (3, 2) and (b) the xy-coordinates 
whose x'y' -coordinates 
-4). 

of this section 
of the point whose xy-coordinates 

In Exercises 
Exercises 

12. f(x) = sin x, B ={sin x + cos x, cos x}, 

x, sin x + cos x} in span

-axes. 
to find (a) the 

13. Rotate 

are (4, 

new x' y' 

of the point 

(sin x, 
cos x) 

for 

the cha

nge-of-basis matrix from B to C is 

Pc<-B = [ _ � -�] 
1 -x + x2} and the change-of-basis 

for rzf 2. If B = {x, 1 + x, 
matrix 

from B to C is 

find B. 

16. Let Band C be bases 

for Exercises 

find C. 

412  Chapter 
6 Vector 

Spaces 

of degree n 

us, you learn 

that a Taylor polynomial 

In calcul
about a is a polynomial of the form 
p(x) = a0 + a1(x - a) + a2(x - a)2 + · · · 
+ a"(x - a)" 
where an * 0. In other words, 
it is a polynomial that has 
of powers of x - a instead 
been expanded in terms 
of pow­
ers of x. Taylor polynomials are very useful for approximat­
ing functions that are "well 

(x - a)2, ••• , (x - a)"} is a basis 
6.7.) This fact allows us to use 

for rtl' n for any real number a. (Do you see 

The set 
show this? Try using Theorem 
the techniques of this section 
e a polynomial as a 
Taylor polynomial about 
17. Express p (x) = 1  + 2x -5x2 as a Taylor polynomial 

to rewrit
a given 

18. Express p(x) = 1  + 2x -sx2 as a Taylor polynomial 
about a =  -2. 
p (x) = x3 as a Taylor polynomial 
19. Express 
about a = - 1. 
20. Express p (x) = x3 as a Taylor polynomial abou
t a = t. 
21. Let B, C, and V be bases 
for a finite-dimensiona
l vec­
B = {v1, .•. , vn}. Let P be an invertible 
for i = 1, . . .  , n. Prove that C = {u1, •.• , uJ is a basis 

vector 
22. Let V be an n-dimensional 

B = {l, x - a, 

with basis 
n X n matrix 

behaved" 
near x = a. 

Pv+-cPc+-B = Pv+-B 

tor space V. 
Prove that 

a quick way to 

space 

and set 

a. 

for V and show that P = Ps+-c· 

about a =  1. 

linear Tra n stormalions 

We encountered 
formations 
mations 

linear 

in Section 
from IR" to !Rm. In this section, 

transformations 

between arbitrary 
vector 

spaces. 

3.6 in the context 

of matrix trans­

we extend 

this concept to linear 

transfor­

transformation 

Defi n ition A linear 
W is a mapping T : V ---+ W such that, 
1. T(u + v) = T(u) + T(v) 
2. T(cu) = cT(u) 

from a vector 

space V to a vector 

space 

for all u and v in V and for all scalars c, 

It is straightforward 

to show that this definition is 
combina

tions. That is, 

that T preser

ve all linear 

equivalent 

to the requirement 

T :  V ---+ W is a linear 

transformation if and 
only if 

T(c1v1 + c2v2 + · · · 
for all v1, . . .  , vk in V and scalars c1, . . .  , ck. 

+ ckvk) = c1 T(v1) + c2T(v2) + · · · 

+ ckT(vk) 

Example 6 .4 9  

Every matrix transforma
matrix, 

transforma
tion. 
then the transformation TA : IR" ---+ !Rm defined 

tion is 

a linear 

by 

That  is, 

if A is an m X n 

is a linear 

transforma

tion. This is a restatement 

of Theorem 

3.30. 

TA (x) = Ax for x in IR" 

6.4 Linear 

Transformations 

413 

Example 6 . 5 0  

Section 
Define T :  Mnn---+ Mnn by T(A) =AT. Show that T is a linear 
Solution We check that, 

for A and B in Mnn and scalars c, 

T(A + B) = (A + Bf = AT + BT = T(A) +  T(B) 

transforma

tion. 

and 
Therefore, 

T is a linear 

transforma

tion. 

T(cA) = (cAf = cA T = cT(A) 

Example 6 . 5 1  

Let D be the differential 
linear 

transforma

tion. 

operator 

D : 0J ---+ '2F defined 

by D(j) = f'. Show that D is a 

Solution Let f and g be differentiable 
calculus, we know that 

functions 

and let 

c be a scalar. Then, from 

D(f + g) = (j + g) ' = f'  + g' 

= D(j) + D(g) 

and 
Hence, 

D is a linear 

transforma

D(cj) = (cf) ' = cf' = cD(j) 
tion. 

Example 6 . 5 2  

function 
transforma

on [a, b] is integrable. 
tion. 

The 

In calculus, you learn 

that every 

continuous 

t integra

shows tha

next example 

tion is a linear 

Define S :  � [a, b ]  ---+ IR: by S(j) = J: f (x) dx. Show that S is a linear 

Solution Let f and g be in� [a, b]. Then 

transforma

tion. 

S(f + g) = r(j + g)(x)dx 
a 
= r (j(x) + g(x)) dx 
a 
= rf(x) dx + rg(x)dx 
a 
a 
S(cf) = r (cj)(x) dx 
a 
= f cf(x)dx 
a 
= cff(x)dx 
a 

= S(f) + S(g) 

= cS(f) 

and 

It follows 

that S is linear

. 

414  Chapter 
6 Vector 

Spaces 

Example 6 . 5 3  Show that none of the 
(a) T :  M22---+ IR defined 
(b) T :  IR ---+ IR defined 
(c) T :  IR ---+ IR defined 

following 
by T(A) = <let A 
by T(x)  = 
by T(x) = x + 1 

2x 

transformations 

is linear

: 

Solution In each case, 
properties of a linear 

transformation fails to hold. 

we give 

a specific 

counterexample 

to show 

that one of the 

(a) Let A =  [� �] andB = [� �]. ThenA + B  = [� �l so 
T(A + B) = <let (A + B) = I� �I = 1 

But 

T(A) + T(B) = detA + detB = I� �I + I� �I = 0 + 0 

= 0 

so T(A + B) * T(A) + T(B) and T is not 
(b) Letx = 1 andy = 2. Then 

linear. 

T(x + y) =  T(3) = 23 = 8 * 6  = 21 + 22 =  T(x) + T(y) 

so T is not linear. 
(c) Letx = 1 andy = 2. Then 

Therefore, T is not linear

. 

T(x + y) =  T(3) =  3 +  1 = 4 * 5  = (1 + 1) + (2 + 1) =  T(x) + T(y) 

4 

�  it fails 

transformations?) 

Remark Example 

6.53(c) 

shows that you need to be careful when you encounter 

the word "linear:' As a function, T(x) = x +  1 is linear
its graph is a straight 
from the vector space 
line. 
transformation 
(Which linear 
functions 

However, 
to satisfy the 

it is not a linear 

from IR to IR will also be linear 

IR to itself, since 

definition. 

, since 

There are two specia

l linear 

transformations 

that deserve to be singled 
out. 

Example 6 . 5 4  

(a) For any vector 
vector 

in V to the zero vector 

spaces 

V and W, the transforma

tion T0 : V ---+ W 

that maps every 

the zero transformation. That is, 

in W is called 
T0(v) = 0 for all v in V 

(b) For any 
to itself 

space 
the identit

is called 

vector 

V, the transformation I : V---+ V that maps every 
That is, 
y transformation. 
J(v) = v forallv in V 

vector 

in V 

(If it is important 
proofs that the zero 

to identif
and identity 

space V, we may 
transformations 
are linear 

y the vector 

The 

write Iv for clarity.) 

are left as easy exercises. 4 

Section 

6.4 Linear 

Transformations 

415 

_.,  lowing 

Properties of linear 
In Chapter 3, all linear 
properties 
theorem 
proof for linear 
tions 
straightforward. 

were directly related 

Transformations 
transforma

were matrix 
to properties 

is easy to prove for matrix 
transforma
in general 

tions 

transformations, 

and their 

of the matrices 
transforma
tions. 
takes a bit more care, 

involved. The fol­
(Do it!) The full 

but it is  still 

Theorem 6 . 1 4  Let T :  V---+ W be a linear 

transforma

tion. 

Then: 

a. T(O) = 0 
b. T (-v) = - T(v) for all v in V. 
c. T(u -v) = T(u) -T(v) for all 

u and v in V. 

_.,  (a) Let v be any 

Proof We prove properties 
Exercise 2 1 .  

vector 
for each step?) 

in V. Then T(O) = T(Ov) = OT(v) = 0, as required. 
(Can you 

give a reason 
(c) T(u - v) = T(u + ( - l)v) = T(u) + ( -l ) T(v) = T(u) -T(v) 

(a) and (c) and leave the proof of property 
(b) for 

Remark Property (a) can be useful 

in showing 

that certain transforma

tions 
are 

Example 

not linear
. As an ill
1 * 0, so T is not 
of transforma
tions 
linear. Example 

consider 

ustration, 
linear
that do map the zero 

so T( 0) = det 0 = 0, but we have seen that T(A) = det A is not linear

6.53(a) is a case in point: The zero vector 

to the zero 

vector 

vector 

, by Theorem 6.14(a). Be warned, however, 

that there 

is the 2 X 2 zero matrix 0, 

are lots 
not 

6.53(b ). If T(x) = 2x, then T(O) = 2° = 

. 

but that are still 

The most important property of a linear 
by its effect on a basis 

determined 

transforma
for V. The next example 

tion T : V ---+ W is that T is 

shows what 

completely 
this means. 

transformation from IR2 to <!J' 2 such that 
r[�] = 2 -3x + x2 and r[�] = 1  - x2 

Find r[-�J and r[�J. 

_., 

B = { [�],[�]}is a basis 

Solution Since 
span(B). Solving 

for IR2 (why?), every vector 

in IR2 is in 

Example 6 . 5 5  

Suppose T is a linear 

416  Chapter 
6 Vector 

Spaces 

we find that c1 = - 7 and c2 = 3. Therefore, 

= - 7r[�] + 3T[�] 

= - 7(2  -3x + x2) + 3( 1  - x2) 
= - 1 1  + 21x -10x2 

II-

Similarly, 

so 

we discover that [:] = (3a -2b)[�] + (b - a)[�] 
r[:J = r((3a -2b)[�J + (b - a)[�]) 
= (3a -2b)r[�J + (b -a)r[�J 
= (Sa -3b) + (-9a + 6b)x + (4a -3b)x2 

= (3a -2b)(2  -3x + x2) + (b  -a)(l  - x2) 

The proof 

theorem 

of the general 

(Note tha\by setting a = - 1  and b = 2, we recover the solution r[ -�] = - 1 1  + 
2lx-lOx .) 
4 
l3 = {v1, ••• , vn} be a 
set for V. Then T(B) = {T(v1), ••• , T(vn)} spans the 
are scalars c1, ..• , en 

Proof The range of T is the set of 
v is in V. Let T(v) be in the range of T. Since l3 spans V, there 
such that 

straightforward. 

in W that are of the 

transformation 

all vectors 

and let 

is quite 

spanning 

range of T. 

form T(v), where 

Theorem 6 . 15  Let T :  V -+  W be a linear 

Applying 

T and using 

the fact that it is a linear 

T(v) = T(c1v1 + · · · 

we see that 
+ cnvn) = c1T(v1) + · · · 
+ cnT(vn) 

transformation, 

In other 

words, T(v) is 

Theorem 6.15 applies, in particular
T(B) would then be a basis 

in span(T(B)), as required. 
, when l3 is a basis 
for the 
6.5. 

in Section 

this issue 

the case. We 

will address 

in this case, 
always 

range of T. Unfortunately, 

for V. You might guess 
this is not 

that, 

Composition 
In Section 
extends 

of linear 
3.6, we defined 
linear 

to general 

the composition of matrix transforma
transformations 

in an obvious 
way. 

tions. 

The definition 

Transformations 

S 0 T is read 
"S of T:' 

Section 

6.4 Linear 

Transforma
tions 

411 

Defi n ition If T :  U ---+ V and S : V ---+ W are linear 
composition of 

S with T is the mapping S 0 T, defined 

transforma
by 
(S 0 T)(u) = S(T(u)) 

tions, 

then the 

where u is in U. 

Example 6 . 5 6  

Observe that S 0 T is a mapping from U to W (see Figure 6.6). Notice 

the definition 

to make sense, 

the range of T must be contained 

also that for 
in the domain of S. 

linear 

Let T : 

transformations 

r[:] = a + (a + b)x and S(p(x)) = xp(x) 

w 
u 
• 
s --+  S(T(u)) = (S 0 T)(u) 
T - T(u) 
U• 
• 
of linear 
transform
ations 

v 
Figure 6 . 6  Composition 
IR2 ---+ <!/' 1 and S : <!/' 1 ---+ <!/' 2 be the 
Find (S 0 T) [ _ �] and (S 0 T) [:]. 
(S 0 T)[ _�] = s(r[ _�]) = S(3 + (3 - 2)x) = S(3 + x) = x(3 + x) 
(S 0 T)[:J = s( r[:]) = S(a + (a +  b)x) = x(a + (a +  b)x) 

= ax + (a + b)x2 

Solution We compute 

= 3x + x2 

defined 

and 

by 

Chapter 3 showed 

that  the composition of two matrix transformations 

was 

another matrix transforma

tion. 

In general, 

we have the following 
theorem. 

Theorem 6 . 16 

If T : U ---+ V 
linear 

transforma

tion. 

and S : V ---+ W are linear 

transforma

tions, 

then S 0 T : U ---+ W is a 

418  Chapter 
6 Vector 

Spaces 

Proof Let u and v be in U and let 

c be a scalar. Then 

(S 0 T)(u + v) 

and 

= S(T(u + v)) 
= S(T(u) +  T(v)) 
= S(T(u)) + S(T(v)) 

= (S 0 T)(u) + (S 0 T)(v) 

since 
since 

Tis linear 
S is linear 

(S 0 T)(cu) = S(T(cu)) 
= c(S 0 T)(u) 

= S(cT(u)) 
= cS(T(u)) 

since 
since 

Tis linear 
S is linear 

S 0 T is a linear 

Therefore, 

tion. 
transforma
transforma
properties 
of linear 
, are related 

mirror 
to the algebraic 

in turn

tions 

transformations 

is associa

The algebraic 

which, 
formations, 
example, com
position of linear 
are linear 
transforma

tions, 
then 

those 

of matrix 
trans­
properties 
of matrices
. For 
tive. That is, if R, S, and T 

these 
provided 
that given 
in Section 

3.6. 
The next example 

transforma
tions. 

compositions 

make sense. The 

proof of this property is identical 

to 

gives 

another 

useful (but not surprising) 

property of linear 

Example 6 . 5 1  

transformations 
and let I : V 

� V be the 
iden­

� W be linear 

� V and T : V 

(T 0 J)(v) = T(I(v)) = T(v) 

Let S : U 
tity transformation. 
Then for every v in V, we have 

ly, I o S = S. 

Since T 0 I and T have the same value at every v in their 
T o I = T. Similar
show that Ti (v) = T2(v) for every v in V. 

Remark The method 

that two linear 

transforma

6.57 is worth noting. 

of Example 
tions Ti and T2 (both from V to W) are equal. 

Suppose we want 
to 

to show 

It suffices 

domain, 

it follows 

that 

Further properties 
oflinear 

transformations 
are explored 

in the exercises. 

Inverses or Linear 

Transrormalions 

Defi n iliOD A linear 
transforma

tion T' : W � V such that 

transforma

T' o T = Iv and T 0 T' = Iw 

In this case, T' is called 

an inverse 
for T. 

tion T :  V � Wis invertible 

if there 

is a linear 

Section 

6.4 Linear 

Transformations 

419 

Example 6 . 5 8  

next section 

have been omitted from this definition. 

Remarks 

could 

T' 0 T 

we will see in the 

tions. However, 

W of T do not have to be the same, as they do in 

the case of invertible 
matrix transforma
that V and W must be very closely related. 
t that T' be linear 

•  The domain V and codomain 
•  The requiremen
For, as we will see in Theorem 6.24, if T' is any 
•  If T' is an inverse for T, then the definition implies 
T : IR2 ---+ r!J> 1 and T' : r!J> 1 ---+ IR2 defined by 
b)x and T'(c + dx) = [d � J 

mapping 
= Iw, then T' is forced to be linear 
too. 

r[:] = a + (a + 

= Iv and T 0 T' 
T' is invertible 

as well. 
that T is an inverse for T'. 

from W to V such that 

that the mappings 

Hence, 

Verify 

are inverses. 

and 

Solution We compute 

(T '  o r)[:J = r'( r[:]) = T'(a +(a+ b)x) = [(a+:)_ a] [:] 
(T 0 T')(c + dx) = T(T '(c + dx)) = r[ c ] = c + (c + (d -c))x = c + dx 
T' o T = 
of each 4 

IIR2 and T 0 T' = I21'1• Therefore, 

T and  T' 

Hence, 
other. 

are inverses 

d - c 

As was the case for invertible 

unique 

if they exist. 

The following 

inverses 
matrices, 
theorem is 
the analogue 

of linear 

of Theorem 3.6. 

transforma

tions 
are 

Theorem 6 . 1 1  

If T is an invertible 
linear 

transformation, 

then its inverse is unique. 

Proof The proof is the same as that of Theorem 3.6, with products of 
placed 
proof 

matrices 
tions. (You are asked to complete 

by compositions 
in Exercise 

re­
this 

transforma

of linear 

3 1 .) 

Thanks to Theorem 6.17, if T is invertible, 

by r-1 (pronounced 
be denoted 
dress the 
issue 
finding 

its inverse when it exists. 

of determining 

we can 

will 
to the inverse of T. It 
"T inverse"). In the next two sections, 
we will ad­
and 

transformation is invertible 

when a given linear 

refer 

tion for which 

tion for which 

tion for which 

T is a linear 

by T(A) = AB  -BA, where B 

Spaces 

by 

by 

6 . 4  

fixed n X n matrix 

is a fixed n X n matrix 

y  z x - y  1 

by T(A) = AB, where B is a 

c  d 0  c + d  

• • • ann 

In Exercises 
transformation. 

4 8 0   Chapter 
6 Vector 

..  I Exercises 
1-12, determine whether 
1. T :  M22 --,lo  M22 defined 
r[a  b] = [a + b O J 
2. T :  M22 --,lo  M22 defined 
r[w  x] [ 1  w - z] 
3. T :  Mnn --,lo  Mnn defined 
4. T :  Mnn --,lo  Mnn defined 
5. T :  Mnn --,lo  IR defined 
6. T :  Mnn --,lo  IR defined 
7. T :  Mnn --,lo  IR defined 
8. T :  <JP2 --,lo  <!P2 defined 
9. T :  <!P2 --,lo  <!P2 defined 
(b + l)x + (c + l
11. T :  gji --,lo gji defined 
b(x + 1) + b(x + 1)2 
10. T :  gjf---,l> gji defined 
12. T :  gji --,lo  IR defined 
14. Let T :  IR2 --,lo  IR3 be a linear 
Find r[�J and r[�J. 
15. Let T :  IR2 --,lo  <JP2 be a linear 
T [ �] = 1 -2x and T [ _ �] = x + 2x2 
Find r[ -�] and r[� J. 

by T(A) = tr(A) 
by T(A) = alla22 
by T(A) = rank(A) 
by T(a + bx + cx2) = (a + 1) + 
)x2 
by T(a + bx + cx2) = a + 
by T(j) = f (x2) 
by T(j) = (j(x) )2 
by T(j) = j(c), where c is a fixed 

13. Show that the transforma

ple 6.56 are both linear. 

transforma

transforma

Sand T in Exam­

tions 

scalar 

T(l) = 3 -2x, T(x) = 4x - x2, and T(x2) = 2 + 2x2 

2 

transforma

Find T(4 - x + 3x2) and T(a + bx + cx2). 

Find T(6 + x -4x2) and T(a + bx+ cx2). 

transforma
T(l  + x) = 1 + x2, T(x + x2) = x - x2, 
T( 1 + x2) = 1 + x + x

16. Let T :  <JP2 --,lo  <JP2 be a linear 
17. Let T :  <!P2 --,lo  <!P2 be a linear 
18. Let T :  M22 --,lo  IR be a linear 
OJ = 1 
1] = 2 0  ' 
r[� 
r[� 
1] = 3 
�] = 4 
r[� 
r[� 
�]and r[: �]. 
Find r[� 
19. Let T :  M22 --,lo  IR be a linear 
r[; :J = aw+ bx+ cy + 
for all[; :J in M22. 

are scalars a, b, 

c, and d such that 

transforma

transforma

tion. 

there 

0  ' 

dz 

, 

0 

x'. 

no linear 

transforma

20. Show that there is 

tion T : IR3 --,lo  <JP 2 
such that {] � 1 + x, r[�] � 2 - x + 
t!J � -2 + 2x' 
22. Let {v1, •.• , vn} be a 
--,lo  V be a linear 
� 23. Let T :  <JP n --,lo <JP n be a linear 

let T : V 
Prove that if 
T (v1) = V1, T(v2) = V2 . . .  , T(vn) = vn, then T is the 
identity 
tion such that 
T(xk) = kxk-I fork = 0, 1, . . .  , n. Show that T must 
be the differential 

for a vector 
tion. 
transforma

21. Prove Theorem 
6.14(b). 

transformation on V. 

transforma

space 
V and 

basis 

operator D. 

tion for which 

tion for which 

Show that 

24. Let v1, • • •  , v" be vectors 

(a) If {T(v1), . . .  , T(vn)} is linear

in a vector 
transforma

space V and let 
tion. 
ly independent 

---+ W be a linear 

T : V 

independent in 

in V, then 
ly independent 
in W. 

arly independent 

true that if 

show that { v1, • • •  , vJ is linearly 
(b) Show that the converse 
of part (a) is false. 
That is, it is not necessarily 
{v1, . • .  , vn} is linear
{T(v1), • • •  , T(v)} is line
Illustrate 
25. Define linear 

T :  IR2---+ IR2• 
this with an example 
transformations S : IR2 ---+ M22 and 

T : IR2 ---+ IR2 by s[�] = [a: b 
a� b] and T[�] [2c
Compute (S 0 T) [ �] and (S 0 T) [;]. Can you 
( T 0 S) [;]? If so, compute 

compute 

26. Define linear 

transformations 

S : <!P 1 ---+ <!P 2 and 

it. 

T :  <!P2---+ <!P1 by 

)x + 2bx2 
S(a + bx) = a + (a + b 
and T(a + bx + cx2) = b + 2cx 

Compute (S 0 T)(3 + 2x - x2) and 
(S 0 T)(a + bx + cx2). Can you compute 
( T 0 S) (a + bx)? If so, compute 
transformations S : <;if" ---+ <;if" and 
Find (S 0 T)(p(x)) and (T 0 S)(p(x)). [Hint: Remember 

S(p(x)) = p(x + 1) and T(p(x)) = p'(x) 

T: <!Pn-+<!Pn by 

it. 

� 27. Define linear 

the Chain Rule.] 

� 28. Define linear 

Section 

6.5 The Kernel 
of a Linear 
and Range 
tion 4 8 1  
Transforma
29 and 30, verify that Sand T are inverses. 
by s[x] = [4x + y] and T: IR2-+ IR2 
by T[x] = [ x - y ] 

In Exercises 
29. S :  IR2-+ IR2 defined 

y 3x + y 

defined 

30. S :  <!P1---+ <!P1 defined 
(-4a + b) +  2ax 

y  - 3x + 4y 
by S(a + bx) = 
and T :  <!P1---+ <!P1 defined 

in W, 
V. 

by 

T(a + bx) = b/2 + (a + 2b)x 

transformation 

such that 

_: d] 

31. Prove Theorem 6.17. 
32. Let T :  V---+ V be a linear 

T oT =I. 

(b) Give an example 

only if T(v) = ±v. 
with V = IR2• 

(a) Show that {v, T(v)} is linear
T 0 T = T. 
(a) Show that {v, T(v)} is linear

of such a linear 

33. Let T :  V---+ V be a linear 

transforma

tion such that 

ly dependent 

if and 

ly dependent 

if and 

transformation 

transforma
tion 

of such a linear 

(S + T)(v) = S(v) + T(v) 

only if T(v) = v or T(v) = 0. 
(b) Give an example 
with V = IR2. 
space W is denoted by ;£', ( V, W). If S and T are 
of all linear 
from a vector 
space V 
of S and T by 

The set 
transformations 
in ;£', ( V, W ), we can define the sum S +  T 
to a vector 
for all v in 
we define the scalar 
cT of T by c to be 

V If c is a scalar, 
in V Then S +  T 

for all v 
from V to W 
34. Prove that S +  T 
35. Prove that H',(V, W) is a vector 
tion. 
transformations 

tion and scalar multiplica
36. Let R, S, and T be linear 

and cT are linear 
space 

transforma
tions. 
with this addi­

(a) R o (S +  T) = R o S + R 
(b) c(R 0 S) = (cR) 0 S = R 0 (cS) for any scalar c 

make sense. 

following 

(cT)(v) = cT(v) 

operations 

multiple 

and cT are both transformations 

such that 
Prove that: 

the 

transforma

T :  <;if n-+ <;if n by 

tions S :  <;if n---+ <;if n and 
S(p (x)) = p(x + 1) and T(p(x)) = xp'(x) 

Find (S 0 T)(p(x)) and (T 0 S)(p(x)). 
II  The Kernel and R a n g e  of a linear 

o T 

The null space and column 
with a matrix. 
linear 

transforma

tion. 

space are two of the fundamental subsp

In this section, 

we extend 

these 

notions to the 

kernel 

aces associated 
and range 

of a 

Transformation 

Spaces 
4 8 2   Chapter 
6 Vector 
kernel is derived 
from 
The word 
word 
cyrnel, a 
the Old English 
form of the word 
corn, meaning 
or "grain:' 
"seed" 
Like 
a kernel 
of 
kernel 
trans­
corn, the 
of a linear 
is its "core" 
or "seed" 
in 
formation 
the sense 
it carries 
that 
informa­
of the import
many 
tion 
ant 
about 
properties 
of the transforma
tion. 

The range of T, denoted 
vectors 
in V under T. That is, 

Defi n ition Let T: V---+ Wbe a linear 
ker(T), is the set of 

transformation. 
in V that are mapped by T to 0 in W. That is, 

all vectors 

The kernel of T, denoted 

ker(T) = {v in V: T(v) = O} 

range(T), 

is the set 

of all vectors 

in W that are images of 

range(T) = {T(v): v in V} 

={w in W :w =  T(v)forsomev in V} 

Example 6 . 5 9  

Let A be an m X n matrix 
from !Rn to !Rm defined 
the column 

space 
of A. 
The kernel 
of T is 

and let 

T = TA be the corresponding 

matrix 

transformation 

by T(v) =Av. Then, as we saw in Chapter 3, the range 

of T is 

ker(T) = {vin !Rn: T(v) = O} 
= {vin!Rn: Av = O} 
= null(A) 
transformation 

In words, 
the kernel 
sponding matrix. 

of a matrix 

is just the null space of the corre­

Example 6 . 6 0  

by 

operator D :  <;if 3 ---+ <!P2 defined 

and range of the differential 

Find the kernel 
D(p(x)) = p'(x). 
Solulion Since D(a + bx+ cx2 + dx3) = b + 2cx + 3dx2, we have 
ker(D) = {a+ bx+ cx2 + dx3: D(a + bx+ cx2 + dx3) = O} 
that b = c = 

But b + 2cx + 3dx2 = 0 if and only 
d = 0. Therefore, 

= {a + bx+ cx2 + dx3: b + 

if b = 2c = 3d = 0, which implies 

2cx + 3dx2 = O} 

ker(D) = {a + bx+ cx2 + dx3: b = c = d = O} 

In other 

words, 

the kernel 

= {a: a in IR} 
of D is the set of constant polynomials

. 

The range of D is all of<!P2, since every polynomial 

the derivative) 

of some polynomial 

in <!P3. To be specific, 

in <!P2 is the image under D (i.e., 
if a +  bx+ cx2 is in <!P2, then 

y 

b 
2 

b 
2 

1 2 
Figure 6 . 1  b Ify = -- +bx 2  ' 
rydx = 0 
0 
then 

Section 
Let S : <!/' 1 �  IR be the linear 

Example 6 . 6 1  

and range of S. 
Find the kernel 
Solution In detail, 
we have 

Therefore, 

by 

transformation defined 

of a Linear 
Transforma
tion 4 8 3  

S(a + bx) = r(a + bx

6.5 The Kernel 
and Range 
S(p(x)) = rp(x)dx 0 
)dx 0 =[ax+ !x2I 
=(a+%)-o=a+% 
ker(S) = {a + bx: S(a + bx) = O} {a + bx : a + % 
= 0} 
= -! } 
{a + bx : a 
{-!+bx} 
if a is an arbitrary 
{a dx = [axn =a -o =a 

whose graphs have the 
distri
buted above and 

as the image under 

real number, then 

polynomials 

of all those 

linear 

6.7). 

interval 

lly, ker(S) consists 

Geometrica
property that the area between the line and the x-axis is equally 
below the axis on the 

The range of S is IR, since 
[O, l] (see Figure 
every real number can be obtained 
S of some polynomial in <!/'1. For example, 
so a = S(a). 

0 

�  M22 be the linear 

and range of T. 

Let T  :  M22 
T(A) = AT. Find the kernel 
Solution We see that 

ker(T) = {A in M22: T(A) = O} 

But if AT= 0, then A = (A T)T = oT = 0. It follows 

= {AinM22: AT = O} 

that ker(T) = { O}. 

Since, 
we deduce 

for any matrix A in M22, we have A = (AT)T = T(AT) (and AT is in M22), 
that range(T) = M22. 

In all of these 

examples, 

the kernel 
and co domain, 
and column 

and range of a linear 

transformation are sub
we are 
this is perhaps not surpris

of the transformation. 
a matrix, 
for granted, 

so we need to prove that 

space of 
anything 

not take 

respecti
vely, 

Since 

we should 

of the domain 

spaces 
generalizing 
the null space 
ing. Nevertheless, 
it is not a 

coincidence. 

­

­

Example 6 . 6 2  

transforma

tion  defined 

by taking 

transposes: 

4 8 4   Chapter 
6 Vector 

Spaces 

Theorem 6 . 18 

Let T :  V ---+ W b e  a linear 
a.  The kernel 
of V. 
b. The range of T is a subspace 
of W. 

of T is a subspace 

transforma
tion. 
Then: 

T(O) = 0, the zero 

Proof (a) Since 
Let u and v be in ker(T) and let c be a scalar. Then T(u) = T(v) = 0, so 
T(u + v)  = T(u) + T(v)  = 0 + 0 

of V is in ker(T), 

vector 

= 0 

so ker(T) is nonempty. 

T(cu)  = cT(u) = cO = 0 

u + v and cu are in 

ker(T), and ker(T) is a subspace of V. 

and 
Therefore, 
(b) Since 0 = T(O), the zero vector 
so range(T) is nonempty. 
Let T(u) and T(v) be in the range of T and let c be a scalar. Then T(u) + T(v) = 
T(u + v) is the image of the vector 
V, so is cu, 
hence T(u) + T(v) is in range (T). Similar
of W that is 
and hence cT(u) is in rang
Therefore, 
and thus it 
of W. 
and scalar multipl
closed 
under addition 

ly, cT(u) = T(cu). Since 
u is in 
range(T) is a nonempty subset 
is a subspace 

u and v are in V, so is u + v, and 

u + v. Since 

of W is in rang

ication, 

e(T), 

e(T). 

Figure 6.8 gives  a 

schematic 

representation 

of the kernel 

and range of a linear 

transforma

tion. 

�  range(T) 
0  •o 
v 
Figure 6 . 8  The kernel 
and range 
of T : V --+  W 

T 

W 

In Chapter 3, we 

space 
these 

and the nullity 
definitions 

defined 
the rank 
of a matrix to 
transforma
tions. 

to linear 

of a matrix 

to be the 

dimension 

be the dimension 

of its null space. We 

of its column 
now extend 

Let T :  V ---+ W be a linear 

Definition 
dimension 
the dimension 

of the range of T and is denoted 
of T and is denoted 

of the kernel 

tion. 

The rank of T is the 
by rank(T). The nullity of T is 
by nullity(

transforma

T). 

Example 6 . 6 3  

and T = TA is the matrix transformation defined 
and the null 

of T are the column 

by T(v) =Av, then 
A, respecti
vely, 

space of 

If A is a matrix 
the range and kernel 
by Example 

6.59. 

space 
3.5, we have 

from Section 

Hence, 
rank(T) = rank(A) and nullity

(T) = nullity

(A) 

Section 

6.5 The Kernel 
and Range 

of a Linear 
Transforma
tion 4 8 5  
tion D : l!P3 �  <!J2 defined 

by 

nullity 

of the linear 

transforma

Example 6 . 6 4  

Find the rank and the 
D(p(x)) = p'(x). 

Example 6 . 6 5  

Find the rank and the nullity 

transformation 

of the linear 

S : <!J 

Solution In Example 

6.60, 

we computed range 
rank(D) = dim <!f 2 =  3 

(D) =  <!J2, so 

of D is the set 

of all constant 

polynomials: ker(D) = {a: a in IR}  = {a 

· 1 : a 

The kernel 
in IR}. Hence, 

{1} is a basis for ker(D), so 
nullity

(D) = dim(ker(D)) =  1 

Solution From Example 

6.61, range(S) = IR and rank(S) =dim IR = 1. Also, 

1 �  IR defined 

by 

= rp(x) dx 

0 

S(p(x)) 

ker(S) = { -� +  bx : b 
in IR} 

= {b(- t + x): b in IR} 
= span(-t + x) 
for ker(S). Therefore, 

so { -t + x} is a basis 

nullity(S

) = dim(ker(S)) = 1. 

Example 6 . 6 6  

Find the rank and the nullity 
T(A) = AT. 

of the 

linear 

transformation T :  

Solution In Example 

6.62, 

we found that range(T) = M22 and ker(T) = {O}. Hence, 

rank(T) = dim M22 = 4 and nullity

(T) = dim{O} =  0 

M22 �  M22 defined 

by 

In Chapter 3, we 

(A) = n. This is the 
the matrix transformation T = TA has !Rn as its domain, 

saw that the rank 
(A) + nullity

of an m X n matrix A are rela
Rank Theorem (Theorem 3.26). 
we could rewrite 

and nullity 

by the formula rank
Since 
relationshi

ted 

the 

p as 

rank(A) + nullity
Theorem extends 

(A) = dim !Rn 
very nicely to general linear 

transforma­

This version 
tions, 

of the Rank 

as you can see from the last three 

examples: 

rank(D) + nullity
rank(S) + nullity
rank( T) + nullity

(D) =  3  +  1  =  4  = 
(S) =  1  +  1  =  2  = 
( T) =  4  +  0  =  4  = 

dim l!P3 

dim M22 

dim <!J 1 

Example 
Example 
Example 

6.64 
6.65 
6.66 

4 8 6   Chapter 
6 Vector 

Spaces 
Let T : V ---+ W be a linear 

Theorem 

V into a vector space 

W. Then 

Theorem 6 . 19  The Rank 

transforma

tion from a finite-dimensiona

l vector 

space 

rank( T) + nullity

( T) = dim V 

In the next 

section, 

you will see how to adapt the 
proof 
of the result.  For now, 

we give an alternative proof that does not use 

of Theorem 

3.26 to prove 

this version 
matrices

. 

set, it can be ex

for V, by Theorem 6.28. 

Proof Let dim V = n and let {v1, . . .  , vk} be a basis 
dim(ker(T)) = k] .  Since {v1, . . .  , vd is a linearly independent 
to a basis 
Ifwe can show that the set 

y(T) = 
for ker(T)[so that nullit
tended 
Let B = {v1, . . .  , vk> vk+1, . . .  , vn} be such a basis. 
for range(T), then we 

will have rank(T) = dim(range(T)) = n - k and thus 

C = {T(vk+1), . . .  , T(vn)} is a basis 
(T) = k + (n - k) = n = dim V 
in the range of T. To show that C spans 

rank(T) + nullity
C is contained 
in the range of T. Then v is in V, and since 

the range of T, let 
for V, we can 

as required. 

B is a basis 

Certainly 
T(v) be a vector 
find scalars c1, . . .  , en such that 

Since v1, .•. , vk are in 

the kernel 

of T, we have T(v1) = ·  ·  · 
T(v) = T(c1V1 + . . .  + ckvk + Ck+1Vk+1 + . . .  + cnvn) 

= T(vk) = 0, so 

= c1T(v1) + ·  ·  · + ckT(vk) + ck+1T(vk+1) + ·  ·  · + cnT(vn) 
= Ck+1T(vk+1) + . . .  + cnT(vn) 
by C. 

such that 

range of T is spanned 

This shows that the 

ly independent, suppose that there 

To show that C is linear
Then T(ck+IVk+I + ··· + cnvn) = 0, which means that Ck+!Vk+l + ··· + cnvn is in the 

are scalars ck+ 1, . . .  , en 

expressible as a linear 

combination 

of the 

basis 

of T and is, hence, 

kernel 
v1, . . .  , vk ofker(T)-say, 

vectors 

But now 
and the linear 

independence 

en = 0, which means C is linear

shown that C is a basis 

We have 

of B forces c1 = · · · = en = 0. In particu
lar, ck+1 = · · · = 

ly independent. 

for the range of T, so, by our comments 

above, 

the 

proof 

is complete

. 

We have verified 
allows 

the Rank 
this theorem 
us to find 
only half the work. The following 

Theorem for Examples 
the rank and nullity 

6.64, 
6.65, 
of a linear 
the process. 

and 6.66. 
transforma

illustrate 

examples 

tion with 

In practice, 

Section 

6.5 The Kernel 
and Range 

of a Linear 
Transforma
tion 481 
2 �  <!/' 3 defined 

T :  <!/' 

by 

Find the 
T(p(x)) = xp(x). (Check that T really 

of the linear 
.) 

rank and nullity 

is linear

transformation 

Example 6 . 6 1  

� 

Example 6 . 6 8  

2 X  2 matrices. 

Define a linear 

transfor­

Solution In detail, 
we have 

T(a  + bx +  cx2) = ax + bx2 + cx3 

It follows 

that 

ker(T) =  {a  + 

bx+  cx2: T(a  + bx+  cx2) = O} 
= {a + bx+  cx2: ax+ bx2 + cx3 = O} 
= {a +  bx+  cx2: a  =  b  =  c = 
= {o} 

y(T) = dim(ker(T)) = 0. The Rank Theorem implies 

(T) = 3 -0 = 3 

rank(T) =  dim <!/'2 -nullity

o} 

that 

so we have nullit

6.67, 

it would be 

Remarll In Example 

{x, x2, x3} is easily seen to be a basis 
(the rank or the nullity 
of a linear 
Theorem can then be used to find the 
knowing 

which way to proceed. 

for the range of 

T. Usually, though, 
transformation) 
will be easier 

easy to find the rank 
of T first, 
the two 
one of 
to compute; 
the Rank 
better at 

other. With practice, 

you will become 

just as 

since 

space of 

all symmetric 

Let W be the vector 

mation T : W �  <!/' 2 by r[: �] = (a  - b) + (b - c)x + (c - a)x2 
iJ)-'"-- (Check that T is linear
ker( T) = { [: �] : T [: �] = 0} 

Solution The nullity 
as follows: 

of T is easier 

of T. 

.) Find the rank and nullity 

to compute directly than the 
rank, 

so we proceed 

{ [: �] : (a  -b) + (b - c)x + (c - a)x2 = O} 
{ [: �]:(a - b) = (b  - c) = (c - a) = 0} 
{[: �]: a= b= c} 
{ [� �]} = span([� �]) 

kernel 

of T, so nullity( 

for the 
6.42 tell us that rank(T) = dim W -nullity(

T) = dim (ker ( 
T)) = 1. 

T) = 

Therefore, { [ � �] } is a basis 

The Rank Theorem and Example 
3 - 1  = 2. 

4 8 8   Chapter 
6 Vector 

Spaces 

one-to-one 
We now investigate 
the discussion 

and Onto Linear 
criteria 

transformation 
are the very important properties 
one-to-

for a linear 

Transformations 

to be invertible. 
one and onto. 

The keys to 

Definition 
distinct 
vectors 

A linear 
transforma
in V to distinct 

tion T :  V---+ W is called one-to-one 
in W. If range( T) = W, then Tis called onto. 

vectors 

if T maps 

•  The definition 

Remarks 

of one-to-one may 

be written 

more formally as follows: 

T :  V---+ W is one-to-

one if, for all u and v 
in V, 
u * v implies 

that T(u) * T(v) 

The above 

statement 

is equivalent 

to the following: 

T :  V---+ W is one-to-

one if, for all u and v in V, 
T(u) = T(v) implies 
that u = v 

Figure 

6.9 illustrates 

two statements. 

these 

w 
v 
(a) T is one-to-one 
•  Another 

v 
(b) T is not one-to-one 

way to write the 

definition 

of onto is as 

follows: 

w 

Fioure 6 . 9  

T : V---+ W is onto if, for all w in W, there is 
at least 

one v in V such that 

w = T(v) 

In other 
words, given 
for an arbitrary 

w in W, does ther

e exist 
solve this equation 

w, we can 

some v in V such that w = T(v)? If, 

for v, then T is onto 

(see Figure 6.10). 

Section 

6.5 The Kernel 
and Range 

of a Linear 
Transforma
tion 4 8 9  

Example 6 . 6 9  

Which of the following 

linear 

v 

(a) T is onto 

Figure 6 . 1 0  

w 

w 
v 
(b) T is not onto 
by r[;] � [ x � y] 

are one-to-one? 

transforma

tions 

onto? 

= p'(x) 

(') T !I'-> !I' defined 
(b) D :  <!/'3---+ <!1'2 defined 
(c) T :  M22---+ M22 defined 

Solution (a) Let r[;:J = r[;:J. Then 

by D(p(x)) 
by T(A) = AT 

of IR3. To be specific, there 

vector 

is no 

equations, 

we see that x1 = x2 and 

so 2x1 = 2x2 and x1 - y1 = x2 - y2• Solving these 

y1 = Yz. Hence, [;:] = [;:], so T is one-to-one. 
It-"- [; l in !I' 'uch th" 

r[; l � m (Why not?) 

range is not all 

since its 

T is not onto, 

since 

6.60, 

distinct 

(b) In Example 
we showed 
polynomials in 
to-one, 
x3 -=F x3 + 1, but D(x3) = 3x2 = D(x3 + 1). 
(c) Let A and B be in M22, with T(A) = T(B). 
(BTl = B. 
In Example 
Hence, 

T is one-to-one. 

T is onto. 

Hence, 

that range(D) = <!/'2, so D is onto.D is not one­
<!/'3 can have the same derivative. 

For example, 

Then AT = BT, so A = (AT)T = 
we showed 

that range(T) = M22• 

6.62, 

It turns 

out that there is 

a very 

simple criterion 

for determining 

whether a linear 

transformation is one-to-one. 

4 

Theorem 6 . 2 0  

A linear 

transformation T :  V ---+ W is one-to-

one if and only ifker(T) = {O}. 

4 9 0   Chapter 
6 Vector 

Spaces 

Example 6 .10 

Show that the linear tra

that T is one-to-

one. If v is in the kernel 

T(O) = 0, so T(v) = T(O). Since T is one-to-one, this implies 
vector 

Proof Assume 
we also know that 
v = 0, so the only 
one, let u and v be 
in Vwith T(u) = T(v). Then T(u -v) = T(u) -T(v) = 0, which implies 
is in the kernel 
u = v. This proves 

in the kernel 
that ker(T) = {O}. To show that T is one-to-

that u - v 
have u - v = 0 or, equivale

(T) = {O}, so we must 

of T, then T(v)  = 0. But 

of T is the zero vector. 

Conversely, assume 

that T is one-to-
one. 

of T. But ker

ntly, 

that 

nsformation T : IR2 ---+ rtP 1 defined by 
r[�] =  a  + (a  + b)x 

is one-to-one and onto. 

Solulion If [ �] is in the kernel 

of T, then 

0  = r[�] = a +  (a +  b)x 

It follows 

that a = 0 and a + 

b = 0. Hence, 

quently, ker( T) = { [ �] } , and 

Rank Theorem, 

By the 

T is one-to-one, by Theorem 6.20. 

b = 0, and therefore [ �] 

[�]. Conse-

rank(T) = dim IR2 -nullity

(T) =  2  - 0  =  2 

Therefore, 
range(T) = IR2. It follows tha
t T is onto. 

the range of T is a two-dimensional 

subspace 

of IR2, and hence 

For linear 

transforma

�  only if rank(T) = dim W. (Why?) The proof 

ties of one-to-one and onto are closely related. Observe first that for a linear 
y(T) = 0, and T is onto if and 
formation T :  V ---+ W, ker(T) = {O} if and 
of the next theorem essentially 
uses the 

between two n-dimensiona

the proper­
trans­

only if nullit

spaces, 

l vector 

tions 

method 

of Example 

6.70. 

Theorem 6 . 2 1  Let dim V = dim W = n .  Then a linear 

transformation T :  

V ---+ W is one-to-
one 

if and only if it is onto. 

Proof Assume 
remark 

preceding 

that T is one-to-one. 
Then nullit

y(T) = 0 by Theorem 6.20 and the 

Theorem 6.21. The Rank Theorem implies 
that 
rank( T) = dim V -nullity

( T) =  n  - 0  =  n 

Therefore, 

T is onto. 

Theorem, 

Conversely, assume 

that T is onto. 

Then rank(T) = dim W = n. By the Rank 

nullity

( T) = dim V -rank( T) =  n  - n  =  0 

Hence, 

ker(T) = {O}, and T is one-to-one. 

Section 

6.5 The Kernel 
and Range 

of a Linear 
Transforma
tion 4 9 1  

In Section 6.4, 
the image of a basis 
give a condition 

we pointed out that if T : V ---+ W is a linear 
for V under T need not be a basis for the range of T. We can now 

transformation, 

then 

that ensures 

that a basis 

for V will be mapped by T to a basis for W. 

Theorem 6 . 2 2  

independent set in V, then T(S )  = {T(v1), . . .  , T(vk)} is a linearly indepen­

one linear 

transforma

tion. 

If S = {v1, . . .  , vk} is a lin­

Let T :  V ---+ W be a one-to-
early 
dent set in W. 

Proof Let c1, . . .  , ck be scalars such that 

Then T(c1v1 + ·  ·  · + ckvk) 
T. But, since 

= 0, which implies 

that c1v1 + ·  ·  · + ckvk is in 

the kernel of 

T is one-to-one, ker(T) = {O}, by Theorem 

6.20. 

Hence, 

c1v1 + ·  ·  · + ckvk = 0 

But, since 
{T(v1), . . .  , T(vk)} is linear

{v1, • • •  , vd is line

arly independent, 
ly indepen
dent. 

all of the scalars C; must be 

0. Therefore, 

coronarv 6 . 2 3  

Let dim  V 
maps a basis 

= dim  W = n. Then a one-to-one 
linear 
for V to a basis for W. 

transforma

tion T :  V ---+ W 

Example 6 .1 1  

Let T :  IR2---+ \fP1 be the linear 

= {v1, . • .  , vn} be a 

Proof Let B 
T(vn)} is a linear
W. But, by Theorem 6.15, T(B) spans the range of T. Moreover, 
rem 6.21, so range(T) = W. Therefore, 

for V. By Theorem 6.22, 

T is onto, 
which completes 

dent set in W, so we need only show that  T(B) spans 

by Theo­
the proof. 

T(B) spans W, 

T(B) = {T(v1), • • •  , 

ly indepen

basis 

transforma

defined 

by 
6.70, 

tion from Example 

r[:J =a +  (a +  b)x 
basis [ = { e1, e2} for IR2 is mapped to a basis 

the standard 

Then, by Corollary 

T([) = {T(e1), T(e2)} of\fP1• We find that 

6.23, 

T(e1) =  r[�] = 1 + x  and  T

(e2) =  r[�] = x 

It follows 

that { 1  + x, x} is a basis 

for \fP 1. 

We can now 

determine 

which linear 

transforma

tions 

T : V ---+ W are invertible. 

Theorem 6 .2 4  

transformation 

T :  V 

---+ W is invertible 

if and only if it is one-to-one 

A linear 
and onto. 

Spaces 

4 9 2   Chapter 
6 Vector 
W �  V such 

Proof Assume 

that 

that T is invertible. Then there 

exists 

a linear 

tion T-1 : 
transforma

To show that T is one-to-one, let v be 

in the kernel 

of T. Then T(v) = 0. 

Therefore, 

T-1 0 T =  I

v and  T 0 T

-1 = Iw 

Y-1(T(v)) = Y-1(0) =:> (T-1 0 T)(v) =  0 

=:> I(v)  =  0 
=:> v= O  

which establishes 

that ker(T) = { O}. Therefore, 

T is one-to-one, by Theorem 6.20. 

To show that T is onto, 

let w be in W and let v =  T-1(w). Then 

T(v) =  T(T-1(w)) 
= (T o  T-1)(w) 
= I(w) 
= w  

Conversely, assume 

image of v under T. Since 

which shows that w is the 

that T is one-to-one and onto. 
and rank(T) = dim W. We need to show that there 

T': W �  V such that T' 0 T = Iv and T 0  T' = Iw. 

T is onto, 

Let w be in W. Since 

only one such vector 

There is 
then T(v) = T(v'); the fact that T is one-to-one then implies 
makes sense 
It follows 

to define a mapping 
that 

T': W �  V by setting 

some vector 
vector 

there 
v, since, 

if v' is another 

exists 

T'(w) = v. 

v is in V, this shows that T is onto. 

This means 
exists 

a linear 

that nullity(T) 

transformation 

= 0 

v in V such that T(v) = w. 
in V such that T ( 
v') = w, 

that v = v'. It therefore 

and 

(T' o T)(v) = T'(T(v)) = T'(w) = v 
(T 0 T')(w) =  T(T'(w)) =  T(v) = w 

It then follows 
transforma

tion. 

that T' 0 T 

= Iv and T 0 T' 

= Iw. Now we must 

To this end, 

let w1 and w2 be in W and let 
T(v1) = w1 and T(v2) = w2• Then v1 = T'(w1) and v

2 = T'(w2) and 

show that T' 

is a linear 
c1 and  c2 be scalars. As above, 

let 

T'(c1w1 + c2w2) = T'(c1T(v1) + c2T(v2)) 

= T'(T(c1v1 + c2v2)) 
= I(c1V1 + CzVz) 

Consequently, T' is linear

, so, by Theorem 6.17, T' = T-1• 

Section 

6.5 The Kernel 
and Range 

of a Linear 
Transforma
tion 4 9 3  

of Vector  Spaces 

The words 
isomorphism and 
isomorphic are derived 
from 
the Greek 
words 
isos, meaning 
and 
"equal;' 
morph, meaning 
tively 
e:' Thus, 
"shap
figura
speak­
ing, 
isomorphic vector 
spaces have 
"equal 
shapes:' 

Isomorphisms 
We now are in a position to 
spaces to 
be "essentially the same:' 

T :  V �  W is called 

A linear 

Definition 
is one
an iso­
morphism from V to W, then we say that V is isomorphic to W and write 

transformation 
If V and W are two vector 

spaces such tha

-to-one and onto. 

an isomorphism if it 

t there is 

describe, 

in concrete terms, 

what it means for two vector 

V = W. 

Example 6 .12 

Show that <!P n-I and !Rn are isomorphic. 

of forming 

Solution The process 
with one possible 
not use the term 

[p(x) ], 0, where£ = { 1, x, ... , x"-1} is the 

isomorphism (as we observed 
there). Specific
isomorphism 

define T :  <;JP n-i �  !Rn by T(p(x) )  = 

vector of a polynomial 
we did 

<;JP n-i · That is, 

in Section 6.2, 

basis for 

although 

already 

the coordinate 

standard 

ally, 

us 

provides 

Theorem 6.6 shows that T is a linear 
an-1Xn-l is in the kernel 

of T, then 

transforma

tion. 

If p(x) = a0 + a1x + ·  ·  · + 

a0 = a1 = · · · = an-i = 0, so p(x) = 0. Therefore, 

dim <!Pn-i = dim IR" = n, T is also onto, 

Hence, 
to-one. 
Since 
an isomorphism, 

and <!P n-i =  IR". 

ker(T) = {O}, and T is one­
by Theorem 6.21. Thus, T is 

Example 6 .13 

!Rm" are isomorphic. 

n and 

Show that Mm
Solution Once again, 
is an isomorphism. 

the coordinate 

mapping 

from Mmn to !Rm" (as in Example 

6.36) 

The details 

of the proof 

are left as an exercise. 

In fact, the easiest 

way to tell if two vector 
spaces 
as the next 
theorem 

shows. 

check their 

dimensions, 

are isomorphic is simply 

to 

4 9 4   Chapter 
6 Vector 

Spaces 

Theorem 6 . 2 5  Let V and W be two finite-dimensiona

l vector 

spaces 

(over 

the same field of scalars). 

Then V is isomorphic to W if and only if dim V = dim W. 

Proof Let n = dim V. If V is isomorphic to W, then there is 
T :  V ---+ W. Since 
that 

y(T) = 0. The Rank Theorem then implies 

T is one-to-

an isomorphism 

one, nullit

rank( T) = dim V -nullity

( T) = n  -0 = n 
subspace 
the range of T is an n-dimensional 
so dim W = n, as we wished to 
show. 

Therefore, 
W = range(T), 

of W. But, since 

n. LetB = {v1, .•. , vn} 

T is onto, 

Conversely, assume 
for V and 

that Vand Whavethesamedimension, 

let C = {w1, . . .  , wn} be a basis for W. We 

be a basis 
transformation T :  V---+ W and then show 
vector 
basis B-say, 

v in V can be written 

uniquely 

that T is one-to-
combina

one and onto. 
tion of the vectors 
in the 

as a linear 

will define a linear 

An arbitrary 

We define T by 

.-..  It is straightforw
suppose v is in the kernel 

of T. 

Then 

ard to check that T is linear. (Do so.) To see that T is one-to-one, 

and the linear 

independence 

of C forces c1 = ·  ·  · 

= en = 0. But then 

so ker(T) = {O}, 
by Theorem 6.21. Therefore, 

and V =  W. 
Show that u;gn and <lP n are not isomorphic. 

that T is one-to-

meaning 

one. Since 

T is an isomorphism, 

dim u;gn = n * n + 1 = dim <lP n' u;gn and <lP n are not isomorphic, 

by 

Solution Since 
Theorem 6.25. 

dim V = dim W, T is also onto, 

Example 6 .14 

Example 6 .15 

of all symmetric 

2 X 2 matrices. 

Show that W is isomorphic 

space 

Let W be the vector 
to IFR3. 

so  W =  IFR3, by Theorem 6.25. 

Solution In Example 
6.42, 
T :  W ---+ IFR3. What is it?) 

we showed that dim W = 3. Hence, 

(There is 

an obvious 

candidate 

dim W = dim IFR3, 
for an isomorphism 

Section 

of a Linear 
Transforma
tion 4 9 5  

6.5 The Kernel 
and Range 
over 22, and hence M22(Z2) � Zi. 

spaces over the 
space 
space 

M22(Z2) of all 2 X 2 matrices 

have all been real vector 

we 
complex numbers C or ZP' where p is 
from 

Remark Our examples 
are true for vector 
the vector 
4 as a vector 

have proved 
prime. 
For example, 
22 has dimension 

but the theorems 

with entries 

spaces, 

..  I Exercises 

6 . 5  

1. Let T :  M22---+ M22 

be the linear 

transformation 

defined 

by 

transforma

tion defined 

by 

if any, of the following 

polynomials are 

in 

j]h 4. Let T :  '!!'2---+ '!!'2 be the 

ker(T)? 

linear 

T(p(x)) = xp'(x). 

(a) Which, 
(i) 1 (ii) x (iii) 
(c) Describe ker
5-8, find bases for the 

(b) Which, 

range(T)? 

x2 

(T) and range(T). 

if any, of the 

polynomials in part (a) are in 

(a) Which, 

ker(T)? 

if any, of the 

following 

matrices 

are in 

(ii) [� �] (iii) [3 OJ 0 - 3  

transformation defined 

if any, of the matrices 

in part (a) are in 

(T) and range(T). 

ker(T)? 

2. Let T :  M22 ---+ IR be the linear 

by T(A) = tr(A). 

range(T)? 

(b) Which, 

(c) Describe ker
(a) Which, 
(i) [ 1 2] -1 3 
range(T)? 
(c) Describe ker
(i) 0 (ii) 2 (iii) Vl /2 

(b) Which, 

(T) and range(T). 

if any, of the 

following 

matrices 

are in 

(ii) [� �] (iii) [1 3] 0 -1 

if any, of the 

following 

scalars 

are in 

6. Exercise 2 
8. Exercise 4 

transformations 

In Exercises 
linear 
case, state the nullity and rank of T and verify the 
Theorem. 

of the 
In each 
Rank 

T in the indicated exercises. 

and range 

kernel 

5. Exercise 1 7. Exercise 

3 

Rank Theorem 

In Exercises 
and then use the 
9. T :  M22---+ IR defined 

nk of T 
to find the other. 

9-14, find either the nullity or the ra
2  [a  b]  [a - b] 
by TCpCx)) = [���n 

_ 
by T c  d = c 

10. T :  r;;2---+ 1R2 defined 

d 

by T(A) = AB, where 

by 

ker(T)? 

defined 

transformation 

following 

if any, of the 

3. Let T :  '1P2---+ IR2 be the linear 

T(a + bx + cx2) = [a - b] b + c 
(a) Which, 
(i) 1 + x (ii) x - x2 (iii) 1 + x - x2 
(i) [ �] (ii) [�] (iii) [�] 
(c) Describe ker(T) and range(T). 

if any, of the 

following 

polynomials are in 

(b) Which, 

vectors 

range(T)? 

are in 

by T(A) = AB -BA, where 

11. T :  M22 ---+ M22 defined 

B=[ 1 -1] -1 1 
B = [� �] 
12. T :  M22 ---+ M22 defined 
J]h 13. T :  '1P2---+ IR defined 
14. T :  M33---+ M33 defined 
15-20, determ

by T(p(x)) = p'(O) 

by T(A) = A  -Ar 

ine whether 

In Exercises 
mation T is (a) one-to-one and (b) onto. 
15. T :  IR2---+ IR2 defined 

by T[x] = [2x - y] 

y  x + 2y 

the linear 

transfor­

by 
b
-
-

17. T :  <.5P 2 ---+ IR3 defined 

18. T :  <.5P2---+ IR2 defined 

Spaces 

4 9 6   Chapter 
6 Vector 
16. T :  IR2---+ <.5P2 defined 
by r[:J =  (a -2b) + (3a +  b)x +(a +  b)x2 
3c] c - a  
T(a + bx +  cx2) =[a� b
by T(p(x)) = [���;] 
19. T :IR3---+ M22definedbyr[:c] =[a - b b - c] 
by r[ � l � 
20. 1"' nl'--+ w defined 
[a +  b + c b  - 2c] 
21-26, determine 

b -2c a - c 
2 X 2 

In Exercises 
isomorphic. 
If they 
T :  V---+ W. 
21. V = D3 (diagonal 
3 X  3 
3 X 3 
22. V = S3 (symmetric 

V and W are 
whether 
are, give 
an explicit 
isomorphism 
), W = IR3 
matrices
matrices), W = U3 (upper 

, where W is the 

all symmetric 

a + b  b + c  

matrices 

3 x 3 matrices), W = S� (skew­

23. V = S3 (symmetric 

triangular 
3 X 3 

matrices) 

vector 
of 

space 

3 X 3 

p'(x) is 

symmetric 

an isomorphism. 

by T(p(x)) = p(x) + 
by T(p(x)) = p(x -2) 

F+"S7  25. V = C, W = IR2 

matrices) 
24. V = <.5P2, W = {p(x) in <.5P3: p(O) = O} 
26. V = {A in M22: tr(A) = O}, W = IR2 
� 27. Show that T :  <.5P n---+ <.5P n defined 
28. Show that T :  <.5P n---+ <.5P n defined 
is an isomorphism. 
29. �how_ that T :  <.IP_n---+ <.5P n defined 
30. (a) Show that 'ii; [O, l] 
(b) Show that� [O, l] =<'.{;;[a, a + l] for all a. 

� [O, l ]---+ � [2, 3] by letting 
whose value 
[2, 3].] 

1s an 1Somorph1sm. 

= 'ii; [2, 3]. [Hint: Define T :  
at x is (T(j))(x) = j(x - 2) for x in 

by T(p(x)) = xnp( �) 

T(j) be the function 

so is 

T :  U---+ V be 

transforma
tions. 

V---+ W and 

34. Let S : 

S :  V---+ W and T :  U---+ V be linear 

so is S 0 T. 

31.Showthat 'ii;[ O,l ]  = �[0,2 ] .  
32.Showthat'ii;[ a,b ]  =  'ii;[ c,d] foralla < b andc < d. 
33. Let 
transforma
tions. 

(b) Prove that if S and T are both onto, 
linear 

(a) Prove that if Sand T are both one-to-one, 
S 0 T. 
(a) Prove that if S 0  T is one-to-
(b) Prove that if S 0  T is onto, 
(a) Prove that if dim V < dim W, then T cannot 
36. Let a0, a1, ••• , an be n +  1 distinct 

T :  V---+ W be a linear 
transforma
spaces. 
finite-dimensional 
vector 

(b) Prove that if dim V > dim W, then T 

one, so 
so is S. 

one-to-one. 
Define T :  <.5P n ---+ !Rn+ I by 

real numbers. 

tion between two 

35. Let 

cannot 

onto. 

is T. 

be 

be 

Prove that T is an isomorphism. 
vector 
transformation 

T 0  T. Use the Rank Theorem 

37. If V is a finite-dimensional 
space and 
T :  V---+ V is 
rank(T) = rank(T2), 
to help show that the 

a linear 
prove that range(T)n ker(T) = {O}. [Hint: T2 denotes 
kernels 

of T and T2 are the same.] 
38. Let U and W be subspaces 

of a finite-dimensiona

such that 

l 

space 

vector 
u - w. 

V. Define T :  U X W ---+ V by T(u, w) = 

(a) Prove that T is a linear 
e(T) = U + W. 
(c) Show that ker(T) = U n  W. [Hint: See Exercise 

(b) Show that rang

transformation. 

50 

in Section 
6.1.] 

(d) Prove Grassmann's Identit

y: 

dim(U + W) = dimU +dim W -dim(U n W) 
results 
[Hint: Apply the Rank Theorem, 
using 
(a) and (b) and Exercise 
in Section 
6.2.] 

43(b) 

Section 

6.6 The Matrix 

of a Linear 
Transforma
tion 491 

The Matrix 01 a linear Transtormation 

transforma

that a linear 

Theorem 6.15 showed 
mined by its effect on a spanning set for V. In particul
basis 
for V, then we can compute T(v) for any 
vector 
We implicitly used 
the process. 
the standa
in Theorem 
3.31 

tion T :  V � W is completely 
6.55 illustrated 
transformations 
transforma
tion 
transformation 
linear 
we will show that every 
between 
tion. 
as a matrix transforma
can be represented 

ar, if we know how T acts on a 
v in V. Example 
this important property of linear 
of a linear 

to help us compute 

rd matrix 

l vector 

spaces 

deter­

Suppose that V is an n-dimensional 
vector 
transforma

finite-dimensiona

T :  !Rn� !Rm. In this section, 
� W is a linear 
� !Rn. At the same time, 
!Rm. Figure 6.1 1  illustrates 

space, 
respect
phism R : V 
S(w) = [wJc, which allows 

and T : V 
ively. 

us to 
associate 
the relationshi
ps. 

Then the coordinate vector 

space, 

tion. Let B and C be bases for V and W, 

W is an m-dimensional 
vector 

mapping 

R ( v) = [ v] 6 defines 

an isomor­

� !Rm given by 

we have an isomorphism S : W 

the image T(v) with the vector 

[T(v)Jc in 

v v  T 
• 
UR [Rn 
. 

-------

[v]sS0T0R-l 

� 

T(v) w 
• 
ls [Rnl 
• [T(v)Jc 

R-1 

figure 6 . 1 1  

S o  T o  R-1 : [Rn� [Rm 

Since 

R is an isomorphism, 

it is invertible, 

so we may form the 

composite mapping 

which maps [v]6 to [T(v)Jc. Since this mapping 
tion. 
from Chapter 3 that it is a matrix 
standard 
What, then, is  the 
transforma
-1? We would like to find the 
m  X  n matrix A such that A [ v J 6 
matrix 
(S 0 T 0R-1)([v]6). 0r,since(S0 T 0 R-1)([v]6) = [T(v) ]c, werequire 

of S 0 T 0 R

goes from !Rn to !Rm, we know 

A [v]s = [ T(v)]c 

out to be surpris

It turns 
columns of 
But, if B = {v1, • • •  , vn} is a basis 

ingly 
A are the images 

for V, then 

easy to find. The basic 
of the standard basis vectors 

for !Rn under S 0 T 0 R
that of Theorem 3.31. The 

idea is 

-1• 

R(v;) = [v;]6 

0 +--ith entry 

0 

4 9 8   Chapter 
6 Vector 

Spaces 

Theorem 6 . 2 6  

so R-1(e;) = v;. Therefore, 

the ith column 

(S 0  T 0 R-1)(e;) =  S(T(R-1(e;))) 

of the matrix A we 

seek is given by 

=  S(T(v;)) 
=  [ T(v;) l e  

basis C of W. 

which is the 

coordinate 
We summarize 

this discussion 

as a theorem

. 

vector 

of T(v;) with respect 

to the 

two finite-dimensiona

Let V and W be 
tively, 
by 
m X n matrix A defined 

where B = {v1, ••• , vJ. If T :  V ---+ W is a linear 
A =  [ [T(v1)lc ! [T(v2)lc ! ·  ·  · ! [T(vn)lcl 

l vector 

spaces with bases 

B and C, respec­

satisfies 

transformation, 
then the 

for every 

vector 
v in V. 

A[v]8 =  [ T(v)J c  

pect to the bases B 

p is illustrated 

below. (Recall 

the matrix of T with  res
that TA 

denotes 

multiplication 

A in Theorem 6.26 is called 

by A.) 

The matrix 

and C. The relationshi
T v  -----+  T(v) 
TA 
J, 
J, 
•  The matrix 

of a linear 

Remarks 

[v]8 -----+  A [v]8 =  [ T(v)J c  

transforma

tion T with respect 

to bases B and C is some­

times 
right, 

by [T]c<--B· Note the direction 

of the arrow: 
denoted 
final equation 
as for T: V---+ W). With this notation, the 

right-to-l
in Theorem 6.26 becomes 

eft (not left-to­

[ TJ c<--B[v]s = [ T(v)J c  

Observe that the Bs in 
other. In words, this equation says, "The matrix 
v gives 

the coordinate vector 

the subscripts 

for T(v):' 

appear side by side and appear to "cancel" 

each 
the coordinate vector 
for 

for T times 

V = W and B = C, we write [ T] 8 (instead 

case where 

of [ T] B<--B). 

In the special 
that 

6.26 then states 

Theorem 

•  The matrix of a linear 

That is, for every 
vector 
by Theorem 6.26-namely, 

[ TJs[vJs = [ T(v)Js 

A [v]8 = [ T(v)J c  

(You are asked to prove this in Exercise 39.) 

transformation with respect 

to given bases 

is unique. 

v in V, there is 

only one matrix 

A with the property specified 

Example 6 .16 

Section 

6.6 The Matrix 

of a Linear 
Transforma
tion 4 9 9  

called a commutative 
with the vector 
but equivalent, 

v and get 
ways. If, as 

have 

for the 

clearer 

we denote 

coordinate 

because we can start 

mapping R commutes 

the coordinate mappings 

then we can summarize 

Theorem 6.26 is sometimes 

term commutative becomes 

this "commutativity" 

and w to [w]c by Rand 
by 

when V = W and B = C, for 

suggesting 
that the 
(provided we use the 

The reason 
then R = S too, and we 

in the upper left-hand corner 
in two different, 
that map v to  [ v J 8 

diagram 
to [T(v)Jc in the lower right-hand corner 
before, 
S, respecti
vely, 

•  The diagram that follows 
s 0  T = TA 0 R 
R 0  T = TA 0 R 
of T-namely, TA = Tl Tl 8-where 
•  The matrix [ TJc<--B depends on 
C. Rearranging 
Let T : IR3 ---+ IR2 be the linear 
mote ix of T whh '"P"' to B ond C and vecify Themem 6.26 foe v � [ _ � l · 

transformation T 
it is required). 
B and 
will affect the matrix  [ TJc._8. [See 

and let B = { e1, e2, eJ and C = { e2, ei} be bases 

the order of the vectors 
basis 

for IR3 and !R2, respecti
vely. 

matrix version 

with the linear 

within either 

the vectors 

transforma

tion defined 

in the bases 

Example 

Find the 

6.77(b).] 

by 

Solution First, 

we compute 

Next, 

we need their 

coordinate 

vectors 

with respect 

to C. Since 

we have 

Therefore, 

the matrix 

of T with respect 

to B and C is 

= [� - 2  -�] 

5 0 0   Chapter 
6 Vector 

Spaces 

To verify Theorem 

6.26 for v, we first compute 

T(v) � T[ J � [ ��] 
[vi s� u-
A[v]8 � [: -� -�1[ J � [ ��] �  [ T(v)J, 

[ T(v)l e  = [ ��L = [ ��] 

we confirm 

facts, 

that 

Then 

_.,..  (Check these.) 

and 

Using all of these 

Let D :  l!P3 �  <!P2 be the 

Example 6 .11 

{l, x, x2, x3} 

differential 

operator 

D(p(x))  = p'(x). Let B = 

to Band C. 

{l, x, x2} be bases 

for l!P3 and l!P2, respecti
vely. 

and C = 
(a) Find the matrix A of D with respect 
(b) Find the matrix A' of D with respect to B' and C, where B' = {x3, x2, x, l}. 
(c) Using part (a), compute 
Theorem 6.26. 
Solulion FirstnotethatD(a + bx +  cx2 + dx3) = b + 2cx + 3dx2• (SeeExample6.60.) 
(a) Since 
D(x3) = 3x2, their 

D(S -x + 2x3) and D(a + bx+ ex2 + dx3) to verify 

[ D ( l )J, � [H [D(x)J, � [H [ D (x') J, � [H [ D (x') J, � m 

B under Dare D(l) = 0, D(x) = 1, D(x2) = 2x, and 
vectors 

of the basis 
coordinate 

with respect to C are 

the images 

A  =  [DJc.-s =  [ [D (l) l e  ! [D (x) l e  ! [D (x2) l e  ! [D (x3) l e l  

Consequently, 

=  0  0  2  0 

[O 1 0 OJ 
= [� � � �i 3 0  0  0 

0  0  0 3 

(b) Since 

the basis 

B' is just B in the reverse 
order, 

A '=  [D b-s·  = [ [D (x3)l e  ! [D (x2)l e  ! [D(x)l e  ! [D ( l )l e l  

we see that 

Section 

of a Linear 
Transforma
tion 5 0 1  
6.6 The Matrix 
bases l3 and C affects the matrix of a 

in the 
of the vectors 
to these bases
.) 

getting 

the coordinate 

ows that the order 

tion with respect 
we compute 

(This sh
transforma
( c) First 
vector 

D(S -x + 2x3) = - 1  + 6x2 directly, 
[D(S -x + 2x') ], � [-1 + 6x' ], � [-�] 

On the 

other 

hand, 

so 

[D (S -x + 2x3) le 
. .+ 

case as an exercise 

which agrees 

with Theorem 6.26. 

We leave 
proof 

of the general 

Since 

the linear 

tion in Example 

transforma
ally no advantage 
to using 
ever, 
as it is very well-suited 
basic idea behind 

large 
to computer 
approach. 

6. 77 
matrix approach 
ones-the 
tion. 
implementa
Example 

examples-espec
ially 

of this transforma

this indirect 

the matrix 

in other 

is easy to 

use directl
tion to do 
calcula

y, there is 
re­
tions. How­

may be simpler, 
the 

6.78 illustrates 

Let T : <!J' 2 �  <!J' 2 be the linear 

transformation defined 
T(p(x)) = p(2x - 1) 

by 

(a) Find the matrix of T with respect to£ =  {l, x, x2}. 
(b) Compute T(3 + 2x - x2) indirec
Solution (a) We see that 

tly, using 

part (a). 

T(l) = 1, T(x) = 2x -1, T(x2) = (2x -1)2 = 1 -4x + 4x2 

so the 

coordinate 

vectors 

are 

Therefore, 

Example 6 .18 

5 0 2   Chapter 
6 Vector 

Spaces 

(b) We apply Theorem 6.26 as follows: The coordinate 
with respect to E is 

Therefore, 

by Theorem 

6.26, 

[T(3 + 2x - x2) ] £  = [T(p(x))] £  

vector of 

p(x) = 3 + 2x - x2 

[p(x)] ,  � [ J 
[T ] £lp(x)] £  [� -� -:J[J Ul 

Example 6 .19 

It follows 
computing 

that T(3 + 2x - x2) = 0 · 1 + 8 · x - 4 · x2 = 8x -4x2. [Verif
T(3 + 2x - x2) = 3 + 2(2x -1) -(2x -1)2 directly.] 

y this by 

The matrix of a linear 

transforma

Example 

6.79 shows its application to a traditional 

problem. 

tion can sometimes 
calculus 

be used in surprising 

ways. 

space of 

function
the subspace 
all differentiable 
the set B = {e3X, xe3X, x2e3x} is linearly 

s. Consider 

W of 

Let <f/J be the vector 
<f/J given 

by W = span(e3x, xe3X, x2e3x). Since 
dent (why?), it is a basis 
for W. 

indepen
(a) Show that the differential 
(b)  Find 
the matrix of D with respect 
to B. 
(c) Compute the derivative 
and verify it using 

operator D maps W into itself. 
of 5e3x + 2xe3x - x2e3x indire

ctly, 

part (a). 

using Theorem 6.26, 

Solulion (a) Applying D 

to a general element of W, we see that 

D(ae3x + bxe3x + cx2e3x) = (3a + b )e3x + (3b + 2c)xe3x + 3cx2e3x 

�  (check this), which is again 

in W. 

(b) Using the formula in part (a), we see that 

D(e3x) = 3e3X, D(xe3x) = e3x + 3xe3X, D(x2e3x) = 2xe3x + 3x2e3x 

so 

It follows 

that 

Section 

6.6 The Matrix 

of a Linear 
Transforma
tion 5 0 3  

(c) Forf(x) = 5e3x + 2xe3x - x2e3X, we see by 

that 

inspection 

V(x)]a � [ J 
l s= [� � �i [ �i = [ 1:] 
4 

= D(j(x)) = 17e3x + 4xe3x -3x2e3x, in 

0  0 3 - 1   - 3  

is easier 

agreement 

Hence, 

by Theorem 6.26, 

we have 

[D(j(x)) l s= [Dl s[f(x) 

which, 
with the formula 

in turn, 
in part (a). 

implies 

thatf'(x) 

Remark The point of Example 
once the 

6. 79 
formula 

is not that this method 
in part (a) has been established, 

than direct 

there 

is 

nt is that matrix methods 

can be used 

at all in what 

surface, to 

be a calculus 

problem. We will explore 

this idea further in 

to do. What is significa

differentiation. Indeed, 
little 
appears, 
Example 

on the 
6.83. 

Example 6 . 8 0  

the matrix 

Let V be an n-dimensional 
vector 
V. What is 
)? What if B * C? 
order 
Solution Let B = {v1, . . .  , vJ. 

of the basis vectors

of I with respect 

to bases 

space 

and let 

I be the identity 
B and C of V if B = C (including 

transformation on 
the 

Then J(v1) = v1, . . .  , J(vn) = vn, so 

and, if B = C, 

= In 
identity 
case B * C, we have 

[Il s  = [ [J(v1)l s  ! [J(vz)l s  ! ·  ·  · ! [J(vn)l s l  
=  [ e1 ! ez ! ·  ·  · ! en l 
[J(v1)l e  = [vil e, · ·., [J(vn)l e  = [vn l e  
[IJe..-s = [ [vile!· · ·! [vn l e l  

what you expected, 

matrix. (T
his is 

?) 

isn't it

the n X  n 

In the 

so 

the change-of-basis 

matrix 

=  Pe..-s 
from B to C. 

of Composite 

Malrices 
We now generalize 
Theorems 
easily find the inverse of a linear 
spaces 

(if it exists). 

and Inverse 
3.32 and 3.33 to get a theorem that will allow us to 

Transformations 

linear 

transforma

tion between finite-dimensiona

l vector 

5 0 4   Chapter 
6 Vector 

Spaces 

Theorem 6 .21 

Let T : U �  V and S : V �  W be linear 

Let U, V, and W b e  finite-dimensional 
vector 
respect

ively. 

spaces 

with bases B, C, and D, 

transforma

tions. Then 

the matrices:' 

Remarks 

•  In words
•  Notice 
of [ S 0  T]v<----6 is 

other 

, this theorem says, "The matrix of the composite is the product of 

how the "inner subscripts" 

and appear to cancel each 

out, leaving 

the "outer 

subscripts" in 

the form D +--- B. 
C must match 

[ S 0  T] v<----6 

and 

Proof We will show that corresponding columns 
[ SJ v<--c l  T]c<----6 are the same. 

Let V; be the ith basis vector 

of the matrices 

in B. Then the ith column 

[(S 0  T)(v;)] v  = [ S(T(v;)J v 

= [ s l v<--c l  Tl C<--6 [ v; ]  

[ S l v<--c l  T( v;) ] c 
6 
But [ v; ] 6 

= e; (why?), so 

by two applications 

of Theorem 6.26. 

[ S 0  Tl v<----6 

is  the 

ith column 

of the matrix [ Slv<--c lTlc<----6. Therefore, 
the ith columns 
prove. 
as we wished to 

and [ S l v<--c [ T] C<--6 are the same, 

of 

6.56. 

Use matrix methods 
Example 

to compute (S 0  T) [:] 
Solution Recall that T :  IR2 �  <J/'1 and S: <JP,� <!P2 are defined 
r[:] = a + (a + b )x and S(a + bx) = ax + bx2 

for the linear 

by 

transformations 

S and T of 

Choosing the standard 
bases 

£, [', and £" for IR

2, <JP 1, and <JP 2, respecti
vely, 

we see that 

�  (Verif

y these.) By Theorem 6.27, 

the matrix 

of S 0 T with respect 

to [ and£" is 

Example 6 . 8 1  

Section 

6.6 The Matrix 

of a Linear 
Transforma
tion 5 0 5  

Thus, by Theorem 6.26, 

y, (S 0  T) [ �] = ax + (a + b 

Consequentl
Example 
6.56. 

)x2, which agrees 

with the solution to 

In Theorem 6.24, 

we proved that a linear 

transforma

if it is one-to-one and onto (i.e., 
volved 
find the 

are finite-dimensional, 
inverse of such a linear 

we can use the matrix 
transformation. 

if it is an isomorphism). When the vector 

tion is invertible 
if and only 
spaces 
have developed to 

methods we 

in­

Theorem 6 . 2 8  

C be bases 
Let T :  V---+ W be a linear 
and W and let B and 
and only if the matrix [ T] C+-B is invertible

transforma

for V and W, respecti
vely. 
. In this case, 

tion between n-dimensional 
vector 
V 
if 

Then T is invertible 

spaces 

of T and r-1 (if T is invertible) 
are n  X n. If T is 

we have 

In=  [Iv 

Theorem 6.27, 

Proof Observe that the matrices 
invertible, 

then r-1 0 T = Iv. Applying 
l s=  [Y-1 0  TJs 
[ Y-1 J B+-c[ T] C
This shows that [ T] c.-B is invertible 
and that ( [ T]c..-8)-1 = [ r-1] B+-C· 
c..-B is invertible. 
ker(T) = {O}. (Why?) To this end, 
that A = [ T J 
Then T(v) = 0, so 
A [v]a = [ Tlc..-a[v]a = [ T(v)Jc = [Ol e= 0 

�  is enough 

Conversely, assume 

to show that 

+-B 

To show that T is invertible, 

it 
of T. 
let v be in the kernel 

which means that [ v] 8 
tal Theorem, 
this implies 

is in the null space of 
= 0, which, 

that [ v J 8 

the invertible 
implies 

in turn, 

matrix A. By the Fundamen­

that v = 0, as required. 

Example 6 . 8 2  

In Example 

6.70, the linear 

transforma

tion T :  IR2---+ !Jl1 defined 

by 

r[�] 

=a +  (a +  b)x 

was shown to be one-to-

Find r-1. 
one and onto and hence invertible. 

5 0 6   Chapter 
6 Vector 

Spaces 

Solulion In Example 
bases£ and£' for IR2 and <!P1, respectively, 

6.81, we found the matrix of T with respect 

to be 

to the standard 

By Theorem 6.28, 

By Theorem 6.26, 

that the 

it follows 

[Y-11£+-£'=([ T] £'+-£)-1=[l 0]-1

[ T]£'+-£ = [� �] 

matrix of r-1 with respect 
to£' and [ is 

1  1 - 1  1 

=[ 1 OJ 
[ _� �][�] 
[b : a

] 

This means that 

(Note that  the 
irrelevant

choice 

.) 

of the standard 
basis 

makes this last calculation 

virtually 

The next exam

ple, a continuation 

used in certain 
usually 
parts. Contrast 

evaluated 

integration 
in a calculus course 

problems 

this approach with our method. 

6.79, 

of Example 
in calculus. The specific 
by means of two applications 

can be 
is 
by 

shows that matrices 

integral 

we consider 

of integration 

4 

Example 6 . 8 3  

Show that the differential 
x2e3x) of'20, is invertible, 

and use 

operator, 

restricted 

to the subspace 

this fact to find the 

integral 

W = span(e3x, xe3x, 

I x2e3x dx 

Solulion In Example 
B = { e3X, xe3X, x2e3x} of W to be 

6.79, 

we found the matrix 

of D with respect 

to the basis 

By Theorem 6.28, 

therefore, 

D is invertible 

on W, and the matrix of D-1 is 

ol-1 [t 2 =  0 
3  0  �:i 

Section 

6.6 The Matrix 

of a Linear 
Transforma
tion 5 0 1  

integration 

Since 
on W. We want 

is antidifferentiation, 
to integra

te the function 

x2e3x whose coordinate vector 

this is the matrix corresponding to 

integration 
is 

Consequently, by Theorem 6.26, 

[� 

It follows 

that 

(To be fully correct, we need to add a constant of integra
because we are working with 
tions, 
linear 
zero vectors, 

the constant of integration to 

transforma

be zero as 
well.) 

forcing 

tion. 

which must send zero vectors 

to 

It does not show up here 

Warning In general, 

differentiation 

Exercise 22.) What the preceding example 
times 

27-30 explore 

is. Exercises 

this idea further. 

is not an invertible 
shows is that, 
suitably 

tion. 
it some­

transforma

restricted, 

(See 

Change of Basis and Similarilv 

Suppose T : V --+ V is a linear 

to wonder 

is natural 

V. It 
to this 
out that the answer 
we first considered 
in Chapter 4. 
6.12 suggests 

Figure 

tion and 13 and C are two different 
and [T]c are related. It turns 

[T]8 

bases 

for 

how, if at all, the matrices 

transforma

question 

is quite 

satisfying and relates to some questions 

one way to address 
from the upper left-hand corner 

the diagram 
different, but equivalent, ways 
since 
if the "upper" 

both are equal to T. However, 

the arrows 
around 
in two 
to the lower right-hand corner 
y knew, 
of T is with respect 

this problem. 

Chasing 

version 

we alread

to the 

shows that I 0 T = T 0 I, something 
v  } basis C 
v  T 
-
i I 
v  } basis 13 
v  T 
i1 
-
JoT= ToJ 

Figure 6 . 1 2  

•T(v) 

•T(v) 

• V 

• V 

5 0 8   Chapter 
6 Vector 

Spaces 

Theorem 6 . 2 9  

Let V be a finite-dimensional 
vector 
be a linear 

transforma

tion. 

Then 

space 

with bases 

B and C and let T : 

then T = I 0  T = T 0 I is with 

to C in its domain and with respect 

to B in its codomain. 

Thus, the matrix of 

basis C and the "lower" 
respect 
T in this case 

is [ T] B<-C· But 

version 

is with respect 

to B, 

[T]a.--c = [I 0 T]a.--c = [IJa.--dTlc.--c 
[ T]a.-c = [T 0 I]a.-c = [ T]a.--a[I]a.--c 

and 

Therefore, [Ila.--d T]c.-c = [ T]a.--a [I]a.-c· 
we know that 
basis 

6.80, 
to B. If we denote 

From Example 
matrix from C 

p-1 = (P5.--c)-1 = Pc.--a 

[I]8.-c = P8.-c, the (invertible) 
this matrix by P, then we 

also have 

change-of­

With this notation, 

so 
Thus, 

the matrices 
We summarize 

[ T] 8 and [ T] c are similar, 
the foregoing 

in the terminology 
as a theorem

discussion 

of Section 

4.4. 

. 

V �  V 

where P is the change-of-basis 

matrix from C to B. 

Remark As an aid in remembering 

from C to B, and not B to C, it is instructi
written 
and must appear to cancel, 

in full detail. 

the "outer 

leaving 

As shown below, the "inner subscripts" must 

that P must be the change-of-basis matrix 
ve to look 

at what Theorem 

6.29 says when 
be the same (all Bs) 

subscripts;' 

which are both Cs. 

Same 

Theorem 6.29 is often used when we are trying 
transformation is particu

which the matrix 
can ask whether 
diagonal 
matrix. 

of a linear 
a basis 
there is 
Example 
6.84 illustrates 

this application. 

C of V such that the 

to find a basis 
larly 

matrix [ T J c of T : V �  V is a 

simple. 

with respect 
to 
For example, we 

Example 6 . 8 4  

Let T :  IR:2 � IR:2 be defined by 

If possible, find a basis 

of T with respect 

to C is diagonal. 

[x]  [ 
Ty = 2x + 2y 

x + 3y] 
C for IR:2 such that the matrix 

Solution The matrix of T with respect 

of a Linear 
Transforma
tion 5 0 9  

Section 

6.6 The Matrix 
[T]E = [� �] 

to the standard basis E is 

This matrix is diagonaliza

4.24. 

ble, as we saw in Example 

p = [� -�J and D = [4 0] 0 - 1  

Indeed, 

if 

then P-1 [ T] Ep = D. If we let C be the basis of 
P is the cha
[T]c = P-1 [T]EP = D 

nge-of-basis matrix PE.-c from C to E. By Theorem 6.29, 

IR2 consisting 

of the columns 

of P, then 

C = { [ �], [ _ �] } is diagonal. 

so the 

matrix of T with respect 

to the basis 

Remarks 

We find that 

[ T ]  c direct
ly. 

by computing 

•  It is easy to check that the solution above is correct 
r[�]  [!] = 4[�] + o[ _�J and r[ _�J [-�] = o[�]  [ _�J 
•  The general 

[ r[�JL [�] and [ r[ _�JL [ _�J 

that form the 

columns of [T]c are 

vectors 

6.84 is to take the stan­
for its 

lizable by finding 

procedure for a problem like Example 
whether it is diagona
proceeds 

as in Chapter 4. The solution then 

dard matrix  [ T] E and determine 
eigensp
example. 

in agreement with our solution above. 

Thus, the coordinate 

exactly 

aces, 

bases 

as in the preceding 

Example 

6.84 motivates 

the following 

definition. 

Definition 
linear 
such that the 

matrix 

Let V be a finite-dimensional 
vector 

and let T :  V ---+ V be a 

transformation. 

Then T is called 

if there 

is a basis 

C for V 

space 
diagonalizable 
matrix. 

[ T ]  c is a diagonal 

only if the matrix [ T] 8 is diagonalizable. 

It is not hard to show that if B is any basis 
This is essentially 
in the last example. You are asked to prove this resu
Sometimes 

case, 

it is easiest 
to a "nonstanda
d matrix. 

to write 
rd" basis. 

respect 
to find the 

standar

down the matrix 
We can then reverse the process 

of a linear 

transforma

Exercise 

42. 
tion with 
of Example 
6.84 

lt in general in 

for V, then T is diagona

lizable 

if and 

what we did, for a special 

We illustrate 

this idea by revisiting 
Example 

3.59. 

Example 6 . 8 5  

Let e be the line 

through 

the origin 

standard 

matrix of the projection 

in IR2 with direction 
vector 
onto e. 

d = [ �:]. Find the 

5 1 0   Chapter 
6 Vector 

Spaces 

projection. 

There is no harm  in 
any nonzero 

Solulion Let T denote the 
a unit vector 
direction 
vector 
a unit vector, 
d' is  also 

(i.e., df + d � = 1), since 
for -€. Let d' = [ -d�2] so  that 
[ T(d)Jv = [�] and [ T(d')Jv = [�] 

the set D = {d, d'} is an orthonor
As Figure 6.13 shows, T(d) = d and T(d') = 0. Therefore, 

multiple 

assuming 
that d is 
of d can serve as a 

d and d' are orthogonal. 

Since 
for IR2. 
mal basis 

y 

Figure 6 . 1 3  Projection 
onto-€ 
[ T]v = [� �] 

The change-of-basis 

matrix from D 

to the standard 
basis 

[ is 

so 

so the 

change-of-basis matrix 

from E to D is 

By Theorem 6.29, 

then, 

the standard 
of T is 

matrix 

which agrees 

with part (b) ofExample 

3.59. 

Let T : <!P 2 ---+ <!P 2 be the linear 

Example 6 . 8 6  

The change-of-basis 

It follows 

that the 

to the basis 
(a) Find the matrix of T with respect 
(b) Show that T is diagonalizable and find a 
basis 
nal matrix. 
Solution (a) In Example 
standard basis£ = {l, x, x2} is 

we found tha

6.78, 

by 

matrix 

matrix 

from B to£ is 

transformation defined 

of T with respect 
to B is 

C for <!P 2 such that [ 

t the matrix of T with respect 
to the 

6.6 The Matrix 
of a Linear 
Transforma
tion 5 1 1  
Section 
T(p(x)) = p(2x -1) 
B = {l + x, 1 - x, x2} of<!f 2. 
T] c is a diago­
[T], � [� -� -:l 
p = PE<-B = [011 -� o�l 
-1 2 0 -:J[i -i �] 
[T]B = p-1 [T]EP u -! �m 
[-� � -:i 
0 0 4 
of [ T]E are 1, 2, and 4 
[�JTJ Hl 
� -: l and D � [ � � � l 
-
P � [ � 
C = {l, -1 + x, 1 - 2x + x2} 

matrix from a 
of C in terms of £. It 

(why?), so we know from Theorem 4.25 

of P are thus the coordinate 

corresponding 

change-of-basis 

eigenvalues are 

vectors 

to these 

basis 

respecti
vely. 

Therefore, 
setting 

we have p-i [ T] Ep = D. Furthermore, 
P is the 
C to £, and the columns 
follows 

that 

and [Tlc = D. 

�  (Check this.) 
�  (b) The eigenvalues 

that  [ T] E is diagonalizable. 

Eigenvectors 

5 1 2   Chapter 
6 Vector 

Spaces 

The preceding ideas can be generalized  to 

relate 

[ T]c'<--B' of a linear 
and C' are bases 

for W. (See Exercise 

transformation T :  V---+ W, where 13and13' are bases 
Theorem 
the Fundamental 

by revisiting 

We conclude this section 

44.) 

Matrices 

and incorpo

rating 

some results 

from this chapter. 

the matrices 
[ T ]  C<--B and 
for V and C 

of Invertible 

Theorem 6 . 3 0  

The Fundamental 

Theorem of Invertible 

Matrices: Version 4 

an n X n matrix and let T : V ---+ W 
be a linear 
statements 

13 and C of V and W, respecti
vely, 

transformation whose 

is A. The 

to bases 

row echelon 

. 
unique 

Let A be 
matrix  [ T] C<--B with respect 
are equival
following 
ent: 
a. A is invertible
b in !Rn. 
b. Ax = b has a 
solution for every 
c. Ax = 0 has only the trivial solution. 
d. The reduced 
form of A is Iw 
e. A is a product of elementary 
matrices
f. rank(A) = n 
g. nullity
(A) = 0 
h.  The 
column 
i. The column 
j. The column 
k. The row vectors 

vectors 
vectors 
vectors of 
A form a 

of A are linear
of A span !Rn. 

1. The row vectors 

for !Rn. 
basis 

. 

basis for !Rn. 

row vectors of 
A form a 

m. The 
n. detA * 0 
o. 0 is not an eigenvalue 
of A. 
p. T is invertible. 
q. T is one-to-one. 
r. T is onto. 
s. ker(T) = {O} 
t. range(T) = W 

ly independent. 

of A are linearly independent. 
of A span !Rn. 

ence (q)-¢:? (s) is Theorem 6.20, 

of 
A is n X n, we must have dim V = dim W = n. From Theorems 6.21 and 
statements 

Since 
we get (p) -¢:? ( q) -¢:? ( r). Finally, 

Proof The equival
onto. 
6.24, 
by Theorem 6.28, 

the last five 

which implies 

we connect 

that (a) -¢:? (p ). 

and (r)-¢:? (t) is the definition 

to the others 

..  I Exercises 

6 . 6  

1-12, find the matrix [ T] C<--B of the linear 

In Exercises 
of V and W, respectively. Verify Theorem 
transformation 
by computing T(v) directly and using the th
eorem. 
1. T : <!P 
by T(a + bx) = b -ax, 

T : V---+ W 
with res

1---+ <!P 1 defined 

pect to the bases 13 and C 

6.26 for the vector 

v 

13 =  C =  {l,x},v = p(x) =  4  + 2x 

2. T :  <!P 1---+ <!P 1 defined 

by T(a + bx) = b -ax, 

13 = {l + x, 1  -x}, C = {l, x}, v = p(x) =  4 + 2x 

by T(p(x) )  = p(x + 2), 
3. T :  <!P2---+ <!P2 defined 
13 = {l, x, x2}, C = {l, x + 2, (x + 2)2}, 
v = p(x) = a + bx + cx2 

.. 

4. T : <J/l2---+ <J/l2 defined 

6. T :  <J/l2---+ IR2 defined 

by T(p(x) )  = p(x + 2), 
B = {l,x + 2, (x +  2)2},C = {l,x, x2}, 

v = p(x) = a + bx + cx2 
by T(p(x) )  = [���� l 
5. T :  <J/l2---+ IR2 defined 
v = p(x) = a + bx +  cx2 
B = {l, x, x2}, C = {e1, ez}, 
by T(p(x) )  = [���� l 
B={x2,x,l},C= {[�],[�]}, 
v = p(x) = a  + bx + cx2 
r[:J [a �a
2b l B � WH-�]}. 
c� mHJ[:ll· v� n 
7 with v = [ �]. 
{E11, E12, E21, E22}, v = A  = [: �] 

8. Repeat Exercise 
defined 
9. T: M22---+ M22 

by T(A) = Ar, B = C = 

7. T :  IR2---+ IR3 defined 

by 

-1 1 

by T(A) = AB -BA, where 

11. T: 

M22---+ M22 

10. Repeat Exercise 
C =  {E12, E21>EwE11}. 

9 with B = {Ew E21, E 12> E 11} and 
defined 

B= [ l  -l], B= C= {E11, E12>E21, E22}, 
v = A  = [: �] 
v = A = [: �] 

defined 
C = {E11, E12> E21, E22}, 

by T(A) = A - Ar, B = 

12. T :  M22---+ M22 

� 13. Consider 

itself. 

the subspace 

W of'.2ll, given by 

differential 

operator 

W = span(sin x, cos x). 

(a) Show that the 
(c) Compute the derivative 

B = {sin x, cos x}. 
ctly, 
indire
agrees 
withj'(x) as 

Theorem 6.26, 
computed 

(b) Find the matrix of D with respect 

off(x) = 3 sin 
and verify that it 
directly. 

D maps W into 
to 

using 

x - 5 cos x 

the subspace 

W of'.2ll, given by 

6.6 The Matrix 
of a Linear 
Transforma
tion 5 1 3  
Section 
ilili_ 14. Consider 
(a) Show that the 
to B = { e2X, e -zx}. 
W = span(e2x, e-2x). 
(c) Compute the derivative 
W = span(e2X, 
(a) Find the matrix of D with respect 

of D with respect 
of f(x) = e2x -3e-zx 
using Theorem 
and verify that it 
ctly, 
6.26, 
with j'(x) as computed 
directly. 
the subspace 

ilili_ 15. Consider 

W of'.2ll, given by 

e2x cos x, e2x sin x). 

indire
agrees 

differential 

(b) Find the matrix 

D maps W into 

operator 

itself. 

(b) Computethederivative

off(x) 

e2x cos x, e2x sin x}. 
2e2x sin x indire
ctly, 
verify that it agrees 

to B = {e2x, 
= 3e2x - e2x cos x+ 
and 

Theorem 6.26, 

using 
with f' (x) as computed directly. 
W of'.2ll, given by 

ilili_ 16. Consider 

the subspace 

W = span(cos x, sin x, x cos x, x sin x). 

(a) Find the matrix of D with respect to B = {cos x, 

p(l) 

defined 

Theorem 

In Exercises 
transformations 

off(x) =cos x + 2x cos x 
and verify that it 
directly. 

(b) by finding the matrices of S and T separatel
y and using 
Theorem 
17. T :  <J/l1---+ IR2 defined 

sin x, x cos x, x sin x}. 
(b) Compute the derivative 
tly, using 
indirec
6.26, 
with f' (x) as computed 
agrees 
17 and 18, T : U ---+ V and S : V ---+ W are linear 
W, respectively. Compute [S 0 Tl v+-B in two ways: 
and B, C, and D are bases for U, V, and 
finding S 0 T directly and then computing its matrix and 
(a) by 
6.27. 
by T(p(x)) = [p(O)], S :  IR2---+ IR2 
bys[�] = [;a--2� l B = {l, x}, 
19-26, determine 
6.82 to find T-1• 

C = D = {e1, ez} 
18. T :  <J/l1---+ <J/l2 defined 
B = {l,x}, C = D = {l,x, x2} 
S :  <JP2---+ <JP2 defined 

In Exercises 
by considerin
mation T is invertible 
to the standard bases. If T is invertibl
and the method of Example 
19. T in Exercise 1  20.  T 
21. T in Exercise 3 
22. T : <Jfl2-+ <Jfl2definedby T(p(x)) = p'(x) 
� 23. T :  <J/l2---+ <JP2 defined 

e, use Theorem 6.28 

by T(p(x)) = p(x + 1), 
by S(p(x) )  = p(x + 1), 

g its matrix with respect 

whether the linear 

by T(p(x)) = p(x) + p' (x) 

in Exercise 5 

transfor­

by T(A) = AB, where 

24. T :  M22 � M22 defined 

5 1 4   Chapter 
6 Vector 
Spaces 
B = [� �] 
6.83 
27-30, use the method of Example 
� In Exercises 
27. f (sin x - 3 cos x) dx. (See Exercise 13.) 
28. f 5e-2x dx. (See Exercise 14.) 
29. f (e2x cos x -2e2x sin x)dx. (See Exercise 15.) 
30. f (x cos x + x 

25. T in Exercise 1 1  26. T in Exercise 12 

sin x) dx. (See Exercise 16.) 

e the given integral. 

to evaluat

it to compute the orthogona
where 

l projection 

of v onto W, 

with Example 

Compare your answer 
[Hint: Find an orthogona
IR3 =  W + WJ_ using 
Example 

l decomposition of IR3 as 
l basis 

an orthogona

5.3.] 

for W. See 

5.1 1 .  

39. Let T :  V � W be a linear 
l vector 

transforma
spaces 
for V and W, respecti
vely. 

and let l3 and C be 
Show that the matrix 
That is, if A is a 

tion between 

to l3 and C is unique. 

that A [ v 

] 8  =  [ T(v) l e  for all v in V, then 
Find values 

ofv that will show 

finite-dimensiona
bases 
of T with respect 
matrix such 
A =  [ Tlc+-B· [Hint: 
this, 

31-36, a linear 

In Exercises 
given. If possib
[ T] c of T with respect to C is diagonal. 

le, find a basis 

31. T :  IR2 � IR2 defined 

32. T :  IR2 � IR2 defined 

at a time.] 

T :  V �V is 

one column 

transformation 
C for V such that the matrix 

by r[ �] [ -4b ] a + Sb 
by r[�] = [: : �] 

40-45, let T :  V � W be a linear 
In Exercises 
Let l3 and C be bases for V and W, respectively, and let 
spaces V and W 
tion between 
finite-dimensional 
A =  [T]c+-B· 
T) = nullit
40. Show that nullity(
41. Show that rank(T) = rank(A). 
42. If V = W and l3 = C, show that T is diagona

vector 

y(A). 

and only if A is diagonalizable. 

transforma­

lizable if 

by T(a + bx)= (4a + 2b) + 

(a + 3b)x 

33. T : <!/'1 � <!/'1 defined 
34. T :  <!1'2 � <!1'2 defined 
� 35. T: <!/'1 � <!/'1 defined 
36. T :  <!1'2 � <!1'2 defined 
37. Let€ be the line 

by T(p(x)) = p(x + 1) 
by T(p(x)) = p(x) + xp'(x) 
by T(p(x)) = p(3x + 2) 
the origin 

d = [�:].Use the method 

through 

vector 

find the 

standard 

2z = 0. Use the 
38. Let W be the plane 

matrix 
of a reflection 
in IR3 with equation 
d matrix of an orthogonal 

in e. 
x - y + 
6.85 to find 
projection 
is correct by using 

that your 
answer 

of Example 

method 

the standar
onto W. Verify 

based proof 

to give a matrix­

of this section 

45. If dim V = n and dim 

what is the relationshi
Prove your assertion. 

of the Rank Theorem (Theorem 6.19). 
for V and W, respecti
vely, 
p between [ T] c+-B and [ T] c· +-B'? 

43. Use the results 
44. If !3' and C' are also bases 
�(V, W) = 
Mmn-(See the exercises 
tion <p: �(V, W) �  Mmn that is an 
then V* = V. 

and C be bases 
mapping 
a linear 
.] 
isomorphism
space, 

W = m, prove that 
for Section 6.4.) [Hint: 
Let l3 
vely. Show that the 
W, respecti

cp(T) =  [ T]c._8, for T in �(V, W), defines 
transforma

the vector 
finite-dimensional, 

space V* = �(V, IR). Prove that 

then the dual space of V is 
if V is 

46. If V is a vector 

for V and 

in IR2 with direction 
of Example 
6.85 to 

Exploration 

Tilings, L attices, and the 
Crystallographic Restriction 

of many culture

Repeating patterns 
ture of crystals 
often exhibits 
repeti
artwork 
s. Tiling 
do not overlap and 
duced many 
shapes (Figure 
6.14). 

works in which he explored 

found in nature 
are frequently 
and mosaics found in the 
as do the 
tion, 
(or tessella
tion) is covering 
of a plane 

and in art. The 
tilings 

molecular 
struc­

by shapes that 

leave no gaps. The Dutch artist 

M.  C. Escher ( 1898-1972) pro­

the possibility 

of tiling 

a plane using 

fanciful 

Figure 6 . 1 4  M. C. 

Escher's "Symmetry 
Drawing 
El03" 

515 

symmetry 
figure 6 . 1 1  Rotational 
r's "Symmetry 
El03"  M. C. 
Esche
Drawing 
El03" 

figure 6 . 1 5  Invariance 
under 
translation 
r's "Symmetry 
M. C. 
Esche
Drawing 
• 
• 
•  •  • 
• 
•  •  "l" 
•  • 
• 
• 
•  • • 
u 
•  •  •  •  • 
• 
figure 6 . 1 6  A lattice 

which we assume 
pattern 
has the 
(corresponding 
been moved at all. We say that the pattern 
translat
has translational 

symmetry 
has transla
metry in infinitely 

many direct
ions. 
two vectors 

ional symmetry in these 

In this exploration, 

to be infinite 

If a pattern 

shown in Figure 

1 .  Let the 
in Figure 

we will be interested 

in patterns 

such as those 

and repeating in all directions 

in Figure 
6.14, 
of the plane. 

Such a 

shifted 

dent vectors) 

property that it can be 
(or translated
to two linearly indepen
is invaria
pattern 
6.15 . 
shown in Figure 
in two directions, 
it has 

so that it appears not to 
have 
and has 
nt under translations 
ions. For example, the 
in Figure 
6.14 

in the directions 

tional symmetry 

) in at least 

direct

translational 

sym­

two directions 

linear combination 

6.15 be denoted 

by u and v. Show that the 

pattern 
of u and v-that is, by any vector 

under translation 
of the form au + bv, where a and b are integers. 

by any integer 

6.14 is invariant 

For any 

two linearly independent vectors 

u and v in IR2, the set 

of points deter­

combinations 

of u and v is called 

a lattice. 

Figure 6.16 

mined by all integer 
shows an 

linear 
of a lattice. 

example 

vectors 

u and v of Figure 

6.15. 

2. Draw the lattice corresponding to the 
Figure 6.14 also exhibits 

nt under such a rotation. 

about some 

rotational symmetry. That is, it is possible to rotate 
point and have it appear unchanged. We 

of 120° about the point 0, as shown in Figure 6.17. We call 0 a center 

For example, 

of Figure 6.14 is invariant 

the pattern 

say that it is 

pattern 

the entire 
invaria
under a rotation 
of rotational 

Note that 

symmetry (or a rotation 
center). 
if a pattern 

is based 

on an underlying 
lattice, 

then any symmetries 

of the 

pattern 

must also be possessed 

by the lattice. 

5 1 6  

why, if a point 0 is a rotation 

through 

every integer 

center 
then it is 
multiple 
of e. Deduce that if 0 < e :s 360°, 

an angle e, 

through 

/ e must be an integer. 

(If 360 / e = n, we say the pattern 

or lattice has 

n-fold 

3. Explain 
a rotation center 
then 360 
rotati

onal symmetry.) 
4. What is the smallest 
Problem 2? Does the pattern 
this angle? 

positive angle of rotational 
in Figure 6.14 also have rotational 
symmetry 

symmetry 

for the lattice in 

through 

5. Take various 
lattice that has 

to draw a 
rotational 
symmetry 
you draw a lattice with eight-fold rotational 

values 

. Try 

< e :s 360° and 360/e is an integer
of e such that 0 
through 
the angle e. 
In particular, can 
symmetry? 
of e that are possible 
angles 
The technique 

of rotational symmetry 
for a 
we will use is to 
trans­
a rotation about the 
gly, let Re denote 
ndard basis for IR2. Then the 

consider 

Accordin

e and let £ be the sta

rotation 

We will show 
that values 
restricted. 
bases. 

in terms of different 

lattice are severely 
formations 
origin 
matrix of Re is 

through an angle 

standard 

- sine] cos e 
Re E -
. sme 
[ l -[cos e 
{ u, v}. Compute the matrix [ R 0] a-

numerical) 

6. Referring 

to Problems 

2 and 4, take the origin to 
(a) What is the actual 
(i.e., 
(b) Let B be the basis 

of u and v. 
be at the 
value of [Re] E  in this case? 

tails 

7. In general, 

let u and v be any two linearly indepen

suppose that the lattice determined 
an angle 
form 

e. If B = {u, v}, show that the matrix 

by u and v is invariant 

of R11 with respect 

in IR2 and 
dent vectors 
under a rotation 
through 
to B must have the 

where a, b, c, and d are integers. 

8. In the terminology 
and notation 

integer. 
[Hint: 

Use Exercise 35 in Section 

of Problem 7, show that 2 cos e must be an 
4.4 and Theorem 6.29.] 

of rotational 

9. Using Problem 8, make a list of all possible values 
Record 

of e, with 0 < e :s 360°, 
that can be angles 
the corresponding 
ues of n, where n = 360/e, to show that a lattice can haven-fold rotational 
if and only if n = 1, 2, 3, 

was first proved by W Barlow in 1894. 

known as the crystallogra

6. This result, 

symmetry 

symmetry 

phic restriction, 

of a lattice. 

4, or 

val­

10. In the library 
for each of the five possible 
angle of rotational 
lographic 

restriction. 

or on the Internet, 

see whether 
types of rotational 
symmetry-tha
is one of 

you can find an 
Escher tiling 
t is, where the smallest 
specified 

by the crystal­

of the 

pattern 

those 

symmetry 

5 1 1  

5 1 8   Chapter 
6 Vector 

Spaces 

Applications 

Homogeneous 

Linear 

Differential 

Equalions 

Jlh  In Exercises 

function 

69-72 in Section 

4.6, we showed 

that if y = y(t) is a twice-differentia

ble 

that satisfies the 
differential 

equation 

y" + ay '  + by=  0 

(1) 

then y is of the 

form 

roots 

of the 

associa

(The case 

if ,\ 1 and ,\2 are distinct 
b =  0. 
in this section 
the vector 
particular 

space of 
attention 

where ,\1 = ,\2 was left unresolved.) Example 
( 1) forms a subspace 
these 
functions. In this section, 
by vector 
to the role played 
bases, 
we consider 
class 

to Equation 
further, paying 
we pursue 
spaces, 
and dimension. 
of examples. A differential 

equation ,\ 2 + a,\ + 
of 9F, 
20 
6.12 and Exercise 

ted characteristic 

show that the set of 
solutions 

To set the stage, 

a simpler 

equation 

ideas 

of the form 

y' + ay =  0 

(2) 

� 

linear 

, homogeneous, 

a first-order

fact that the highest 

is called 
refers to the 
"homogeneous" means that the right-hand side is zero. 
tion is "linear"
satisfies 
Equation 

derivative 

?) A solution 

to Equation 

oft. 
(2) for all values 
It is easy to check that one 

solution to 

Equation 

(2) is a differentiable 
y = y (t) that 

function 

(2) is y = e - at. (Do it.) However, 

("First-

order" 

equation. 
is a first derivative, 

and 
Do you see why the equa­

that is involved 

differential 

all solutions-and this is where vector 
We 

spaces 

come in. 

we would like to describe 
have the following 
theorem. 

Theorem 6 . 3 1  The set 

S of all solutions 

toy' + ay = 0 is a subspace of?F. 

P r o o f  Since 
and y be 

the zero function 

certainl
functions 
two differentiable 
x' + ax =  0 and  y' 

oft that 

y satisfies 

Equation 

(2), S is nonempty. Let x 

are in S and let c be a scalar

. Then 

+ ay =  0 

rules 

so, using 

(x + y)'  + a(x + y) = x' + y' + ax + ay = (x' + ax) + (y'  + ay) =  0  +  0  =  0 

for differentiation, 

we have 

and 

(cy)' + a(cy) = cy'  + c(ay) = c(y' + ay) = c·O =  0 

Hence, 

x + y and 

cy are also in S, so S is a subspace 

of ?F. 

Now we will show that S is a one-dimensional 

subspace 

basis. 

To this end, 

let x = x(t) be in S. Then, for all t, 

of 9F and that { e -at} is a 

x'(t) + ax(t) =  0 or x'(t) = -ax(t) 

Section 

6.7 Applications 

5 1 9  

Theorem 6 . 3 2  

If S is the 

solution space of 

y' + ay = 0, then dim S =  1 and 

{e-a1} is a basis for 

S. 

Define a new function 

Chain Rule for differentia

tion, 

z (t) = x(t) eat. Then, by the 
z'(t) = x(t)aeat + x'(t)eat 
= ax (t)eat -ax (t)eat 
= O  
ally zero, 

x (t)eat = z (t) = k for all t 

Since 

z' is identic

z must be a constant function

-say, z(t) = k. But this 

means that 

so x(t)  = ke-a1• Therefore, all solutions 
(2) are scalar multiples of the 
single solution y = e -at. We have proved the following 
theorem. 

to Equation 

One model for population 

assumes 
proportional to the size of the population. 

growth 

tion is 
few restrictions 
(such as limited 
population 
is its 
derivative 
portional to 

its size can be written 

at time t is p(t), then the growth 

p' (t). Our assumption 

that the growth 

food, or the like) 

space, 

rate, 

rate of the 

popula­
that the growth 
This model works well 
if there 
are 
on growth. If the size of the 

or rate of change 
rate of the 

of the population, 
population 

is pro­

as 
p'(t)  = kp(t) 

p (t)  = cekt 

where k is the proportiona
p' -kp = 0, so, by Theorem 6.32, 

lity constant. 

Thus, p satisfies 

the differential 

equation 

for some scalar 

c. The constants c and k are determined using 

experimental 

data. 

Example 6 . 8 1  

of humans 

coli (or E. coli, for short) 

The bacterium Escherichia 
and other 
intestines 
of the bacte
into the environment. 
Under laboratory conditions, 
into two every 20 minutes. If we start 
cell, 
be after 

severe health risks 
each cell 
E. coli 

is commonly found in the 
if it escapes 
rium divides 

mammals. It poses 

with a single 

1 day? 

how many will there 

need to use differential 
equations 

to solve 

this problem, but we 

the basic method. 

illustrate 
c and k, we use the data given in the statement 
of the 

. If 
we take 1 unit of time to be 20 minutes, then we are given that p(O) =  1 and p(l) = 2. 
Therefore, 

problem

c = c · 1  = cek·o = 1 and 2  = eek·! = i 

The Andromeda 

Solution We do not 
in order to 
will, 
To determine 

E. coli is mentioned 
in Michael 
's novel 
Crichton
Strain (New 
York: Dell, 1969), 
the "villain" 
although 
in that 
novel 
was supposedly an alien 
In 
virus. 
E. coli contaminated 
real life, 
the 
town 
water 
supply 
of Walke
rton, 
Ontario, 
in 2000, 
resulting 
in seven 
deaths and 
of 
hundreds 
causing 
people to become 
usly 
ill. 
serio

that k = ln 2, so 

It follows 

After 1 day, t = 72, so the 
(see Figure 6.18). 

p(t)  = etln2 = eln2' = 2t 
number of bacteria 

cells 

will be p(72) 

= 272 =  4.72 X 1021 

x 1021 
4.72 

5 2 0   Chapter 
6 Vector 
Spaces 
5 x 1021 
p(t) 
4 x 1021 
3 x 1021 
I I I I I I I I I I 
2 x 1021 
1 x 1021 
0 '---+---+--I --+--.+---+ �-+-'-- t 
0 10 20 30 40 50 60 70"'72 
Figure 6 . 1 8  Exponential 
growth 

Radioactive 

substances 
at time t, then the 

the substance 
rate of decay of a substance 

rate of decay is m'(t). Physicists 

the mass of 

If m(t) denotes 
that is, 

have found that the 

decay by emitting 

radiation. 

is proportional to 
its mass; 
m '(t) = km(t) or m '  -km= 0 
constant. Applying 
Theorem 

6.32, 

we have 

where k is a negative 

m (t) = cekt 

for some constant 
called its half-life. 

c. The time required 

for half 

of a radioacti

ve substance 

to decay is 

Example 6 . 8 8  

decayed to 37 mg. 

a 100 mg sample of radon-222 

After 5.5 days, 
(a) Find a formula for m(t), the mass remaining 
after t days. 
(b) What is the half-life ofradon-222? 
(c) When will only 10 mg remain? 

100 = m (O) = cek·o = c • 1 = c 
Solution (a) From m(t) = c/1, we have 
m (t) = lOOekt 
we are given that m(5.5) = 37. Therefore, 

so 
With time measured in days, 

100e5·5k = 37 
e55k = 0.37 

k = =  -0.18 

5.5k = ln(0.37) 
ln(0.37) 
5.5 

so 
Solving 

for k, we find 

so 

Therefore, 

m(t) = lOOe-0·181. 

Section 

6.7 Applications 

5 2 1  

m(t) 

100 

80 

60 

"so 

40 

I I I I I I I I 

20 

3.85 : "" 
tive 
decay 

Figure 6 . 1 9  Radioac

2 

6 

4 

8  1 0 

(b) To find the 
Solving this equation, 

we find 

half-life of radon-222, 

we need the value 

oft for which m(t) = 50. 

so 
Hence, 

and 

l00e-0·18t = 50 
e-o.1st = 0.50 
- 0. 18t = ln(t) = -ln 2 

t = - =  3.85 

ln 2 
0.18 

has a half-life of approximately 

3.85 days. (See Figure 
6.19.) 

need to determine 

the value oft such that m(t) = 10. That is, we 

must solve 

Thus, radon-222 
(c) We 
the equation 

IOOe-o.1st = 10 or e-o.1st =  0.1 

-0.18t = ln 0.1. Thus, 

yields 

Taking 

the natural 

of both sides 

logarithm 
ln 0.1 
-0.18 
after 

t = -- =   12.79 

so 10 mg of the sample will remain 

approximately 

12.79 days. 

Linear 

See 
Algebra by S. 
H. 
A. J. Insel, 
Friedberg, 
and L. E. 
Spence 
(Englewo
NJ: 
od Cliffs, 
Prentice-Ha
ll, 1979). 

is also a subspace 
Part (a) of 
Our approach here is to 
part (b) of Theorem 6.33 as well, 
methods. 

The solution set S of the second-
of ?F (Exercise 

which extends 
use the power of vector 

Theorem 6.33, 

20), and it 

order 

differential 

turns out that the 

equation y" + ay' + by = 0 
of S is 2. 
dimension 
us to 

doing so allows 

by Theorem 4.40. 
obtain 
with our previous 

implied 

a result that we could not obtain 

Theorem 6.32, is 

spaces; 

5 2 2   Chapter 
6 Vector 

Spaces 

Theorem 6 . 3 3  

Let S be the solution space of 

of the characteristic 

equation A 2 + a,\ + b = 

0. 

roots 

y" + ay' + by= 0 

and let A 1 and A2 be the 

a. If A1 * A2, then {e;\'t, e;\21} is a basis for S. 
•  Observe that what the theorem says, in other 

b. If A1 = A2, then {eA11, te;\11} is a basis for S. 

Remarks 

y" + ay' + by= 0 are of the 
form 

words, is that the solutions of 

in the first 
case and 

•  Compare Theorem 6.33 with Theorem 4.38. 

case. 

in the second 

and linear 
continuous 

recurrence 
mathematics 

relations 
and the latter 

have much in common. 
to discrete 

mathematics, 

Linear 
Although 

differential 
equations 
the former belong to 

there 

are many parallels

. 

Proof (a) We first show 
characteristic 

equation and let f(t) = eAt. Then 

that {eA11, eA21} is contained 

in S. Let A be any 

root of the 

from which it follows 

J'(t) = 

AeAt and J"(t) = A2eAt 

that f" + af' + bf= A2eAt + aAeAt + beAt 

= (A2 + a,\ + b
= 0 · eAt = 0 

)eAt 

e,f is in S. But, since A1 and A2 are roots 

Therefor
means that eA,t and eA,t are in 
S. 
{eA11, eA21} is also 
linear

The set 

ly independent, since 

if 

of the characteristic 

equation, 

this 

y 

Y =er 
eA2 ------------

Figure 6 . 2 0  

setting t = 0, we have 

then, 

c1 + c2 = 0 or c2 =  - c1 

Next, 

we sett =  1 to obtain 

But eA, - eA2 * 0, since eA, - eA2 = 0 implies 

possible if 
{eA11, eA2t} is line

A1 * A2. (See Figure 6.20.) 
dim S = 2, {eA,t, eA21} must be a basis 
for S. 

arly indepen
dent. 

We deduce 

Since 

(b) You are asked to 

prove this property in Exercise 2 1 .  

that eA, = e\ which is clearly 
that c1 = 0 and, hence, 

im­
c2 = 0, so 

Section 

6.7 Applications 

5 2 3  

Example 6 . 8 9  

Find all solutions of y 11 -Sy' + 6y = 0. 
Solution The characteristic 
the roots 
solutions 

equation 
are 2 and 3, so {e21, e31} is a basis 
to the given 
equation are of the 

form 

is A2 -SA + 6 = (A -2)(,\ -3) = 0. Thus, 
It follows 

for the solution space. 

that the 

The constants c1 and c2 can be determined 

if additional 

equations, 

called 

bound­

ary conditions

, are specified. 

Example 6 . 9 0  

Find the solution of y11 + 6y' + 9y = 0 that satisfies 
Solution The characteristic 
repeated root. 
solution is of the form 

{e-31, te-31} is a basis for 

Therefore, 

equation 

is A2 + 6A + 9 = (A + 3)2 = 0, so - 3  is a 
and the 
general 

the solution space, 

y(O) = 1, y'(O) = 0. 

The first boundary condition 

gives 

soy = e -3t + c2te -31. Differentia

1  = y(O) = c1e-3·o +  0 = c1 
we have 

ting, 

so the 

second 

boundary 

y '  = - 3e-3t +  c2(-3te-3t +  e-3t) 
condition 

gives 

0 = y'(O) = - 3e_3.0 +  c2(0 +  e_3.0) = - 3  +  c2 

or 
Therefore, 

the required 
solution is 
y = e-3t + 3te-31 = (1 + 3t)e-31 

Theorem 6.33 includes 

the case in which the 
roots 

of the characteristic 

equation 

If A = p 
are complex. 
its conjugate 
A = p 
space S of the 
differential 

+ qi is a complex 
-qi. (See Appendices 

A 2 + a,\ + b = 0, then so is 
y11 + ay' + by= 0 has {eA1, eX1} as a basis. 

root of the equation 

C and D.) By Theorem 6.33(a), the solution 

equation 

Now 

eAt = e(p+qi)t =  eP1ei(qt) = eP1(cos qt+ i sin qt) 
= ---

eAt - eAt 
eP1 cos qt =  and  eP1 sin qt 
2i 

eAt +  eAt 

2 

and 

so 

that {eP1 cos qt, eP1 sin 
qt are linear
qt} is also 
a basis 

It follows 
and eP1 sin 
eP1 sin 
root p + qi, the differential 

in span(eA1, ex1) = S. Since 
qt 
ly indepen
, 
equation 
has a 
for S. Thus, when its characteristic 
of the form 

y 11 + ay' + by = 0 has solutions 

dent (see Exercise 

S = 2, {eP1 cos qt
complex 

qt} is contained 

22) and dim 

equation 

eP1 cos 

5 2 4   Chapter 
6 Vector 

Spaces 

Example 6 . 9 1  

Find all solutions 

ofy" -2y' + 4 = 0. 

Solution The characteristic 
going 

us that the general 

discussion 

equation 

tells 

is,\ 2 -2,\ + 4 = 0 with roots 

1 ± i\13. The fore­
given differential 
is 

equation 

solution 
y = c1e1 cos V3t + c2e1 sin V3t 

to the 

Example 6 . 9 2  

A mass is attached to 
downward 
situation. 
x units, the force F needed 

and released, 
The first, 

the end 

of a vertical 

spring 

(Figure 6.21). If the mass is pulled 

up and down. Two laws of physics govern 
this 
that if the spring is 
stretched 

(or compressed) 
position is proportional to 

x: 

it will oscillate 
Hooke's law, states 
to restore 
it to its original 
F = -kx 
the spring 

(called 

constant). Newton's Second Law of 

mass times accelera

tion. 
at time t, x' gives 

Since x = x(t) represents 
its velocity and x" its ac­

where k is a positive constant 
Motion states 
distance, 
celera

or displacement, 
of the spring 
Thus, we have 

that force equals 

tion. 

mx" = -kx or x" + ( � )x = 0 

both k and m are positive, 
Since 
form x" + Kx = 0, where K is positive. 
The characteristic 
eral solution to the differential 

equation has the 
so is K = k/m, and our differential 
equation is ,\2 + K = 0 with roots ± ivK. Therefore, 
the gen­

oscillating 

spring 

is 

equation of the 
x = c1 cos vK t + c2 sin vK t 

-?-
> �  > 
-< & x 

0 

Figure 6 . 2 1  

Suppose the spring is at rest (x = O) at time t = 0 seconds and is stretched 

as far 

as possible, 

to a length 

before it is releas

of 20 cm, 
ed. Then 
0 = x(O) = c1 cos 0 + c2 sin 0 = c1 

x = 20 sin ,!Ki 

x 

20 

1 0  

- 10 

- 20 

Figure 6 . 2 2  

6.7 Applications 

5 2 5  

Section 
/2 VK), giving 

so x = c2 sin VKt. Since 
c2 = 20 (occurring 

the maximum 

value of the sine function 

is 1, we must have 

for the first time when 

us the solution 

t ='TT 

x = 20 sin VKt 

(See Figure 6.22.) 

Of course, this 

is an 

idealized 
will oscillate 

since 
forever. 

solution, 

that the spring 

and predicts 
(such as friction) 
portant application of differential 
equations 

into account, 

but this simple model has served to introduce an im­

and the techniques 

we have develop
ed. 

it neglects 

any form of 

resistance 

It is possible to take damping 

effects 

4 

(b) Repeat part (a), but use the data for the 

1970 
years 
and 1980 to solve for p(t). Does this approach 
give 
a better 
conclude about U.S. population 

(c) What can you 

approximation 

for the year 2000? 

..  1 Exercises 

6 . 1  

erential equa­

� Homogeneous 

given 

boundary condition(s). 

Linear 
Differential 
Equations 
1-12, find the solution 
of the diff
In Exercises 
tion that satisfies the 
I. y' -3y = O,y(l) = 2 
2. x' + x = 0, x(I) = 1 
3. y" -7y' + I2y = O,y(O) = y(l) = 1 
4. x" + x' -I2x = 0, x(O) = 0, x'(O) = 1 
5. f" -f' - f = O,f (O) = O,f(l) = I 
6. g" -2g = O,g(O) = 1,g(l) = 0 
7. y" -2y' + y = 0, y(O) = y(I) = 1 
8. x" + 4x' + 4x = 0, x(O) = 1, x'(O) = 1 
9. y" - k2y = 0, k -=F O,y(O) = y'(O) = 1 
10. y" -2ky' + k2y = 0, k -=F O,y(O) = 1 , y ( l )  
11.f" -2f' + Sf= O,f(O) 
12.h" -4h' + Sh =  O,h(O) = O,h'(O) = - 1  
13. A strain 

of bacteria 
the size of the population. 
Initially, 

has a growth 

= 1,f(7r/4) = 0 

= 0 

after 

there 
are 

t hours, 

3 hours, 
there 
the number 
find a formula for p(t). 

tional to 
100 bacteria; 
after 
(a) If p(t) denotes 
(b) How long does it take for the 
one million? 

cAs 14. Table 6.2 gives 
(c) When will the 

are 1600. 
of bacteria 

IO-year 
(a) Assuming an exponential 

at 
for the years 1900-2000. 
growth model, use the 
data for 1900 and 1910 to find a formula for p(t), 
the population 
Let t =  0 be 1900 
t = 1 be 1910.] How accurately 
and let 
formula calculate 
the U.S. population 

in year t. [Hint: 

intervals 

population 

population 

reach 

does your 
in 2000? 

rate that is propor­

growth? 

Table 6 . 2  

Year 
1900 
1910 
1920 
1930 
1940 
19SO 
1960 
1970 
1980 
1990 
2000 

Source: U.S. Bureau o f  the Census 
15. The half-lif

to double? 

the population of the United States 
t years 
remaining 

after 1000 years. 

with a 

e of radium-226 is 1S90 years. 
sample 

start 
(a) Find a formula for the mass m(t) remaining 

of radium-226 whose mass is SO mg. 
after 
the mass 

and use this formula to predict 

Suppose we 

(b) When will only 10 mg remain? 

dating 
16. Radiocarbon 
such as bone, 

to estimate the age of ancient objects that were once 
living matter, 

, wood, or paper. 

is a method 

leather

used by scientists 

Population 
(in millions) 

76 
92 
106 
123 
131 
ISO 
179 
203 
227 
2SO 
281 

5 2 6   Chapter 
6 Vector 

Spaces 

carbon 

carbon 

of which 

contain car

that is continuousl

bon, a proportion 
ve isotope 

take up radioactive 
atoms, the ratio 

is 
y 
formed in the upper atmosphere. 
Since 
liv­
along 

All of these 
carbon-14, a radioacti
being 
ing organisms 
other 
constant. However, 
remains 
the carbon -14 in its cells 
decays 
Carbon-14 has a known half-life 
the concentration 
by measuring 
object, scientists 
can determine 

with 
between the two forms 
when an organism 
dies, 

and is not replaced. 
of 5730 years, 
so 
of carbon-14 in an 
its approximate 
l applications 
the age of 
(Figure 6.23). 

age. 
of radio­
has been to determine 

One of the most successfu

monument in 

England 

dating 

the 

carbon 
Stonehenge 
Samples 
were found to have 
was 45% of that 
estimated 

age of these 

taken from the remains 

of wooden 
posts 

a concentration 

of carbon-14 that 
material. 

What is the 

found in living 

posts? 

{)  L 

/  I I I I 

Figure 6 . 2 4  

Let e = 8(t) be the 

angle 

of the 

pendulum from 

It can be shown that if there 

then when e is small it 

satisfies 
l 

is no resis­
the differentia

the vertical. 
tance, 
equation 

e "  + fe = o 

L 

due to gravity, 

9.7 m/s2. Suppose that L = 1 m and 
e = O) at time 
drawn to the right 
The bob is then 
at 
and released. 

where g is the constant of acceleration 
approximately 
that the pendulum is at rest (i.e., 
t = 0 second. 
an angle 
(a) Find the period 
of the pendulum. 
(b) Does the period depend 

ofe1 radians 

was 

1638. [Galileo 
as a student 

medicine 

( 1564-1642) studied 

but his real interest 

on the angle 81 at which 
ed? This question 

was 
by Galileo in 

cs. In 1592, Galileo 
was ap­
of mathematics 

niver­
primarily 

at the U 
where he taught 
y. He was the first to use 
and in 

at the stars and 
planets, 

the pendulum is releas
posed and answered 
Galilei 
at the University of Pisa, 
always 
mathemati
pointed professor 
sity of Padua in Venice, 
geometry and astronom
a telescope 
so doing, 
port of the 
volve around 
Galileo 
placed 
lish his resu
able to write 
pendulums. His notes 
and published 
in 1638.] 

to look 
he produced 
Copernican 

was summoned 
under house arrest, 

data in sup­
experimental 
view that the 
planets 
re­
For this, 
the sun and not the earth. 
before the Inquisi
tion, 
to pub­
and forbidden 
lts. While under house arrest, 
he was 
objects and 
up his research 
out of Italy 

were smuggled 

on falling 

as Discourses on Two New Sciences 

20. Show that the solution set S of the second-

order 

differential 
of2F. 

equation 

y" + ay' + by = 0 is a subspace 

21. Prove Theorem 
22. Show that eP1 cos qt and eP1 sin qt are linearly 

6.33(b). 

indepen
dent. 

Figure 6 . 2 3  Stonehenge 

17. A mass is attached to 

as in 
a spring, 
is stretched 
of 

Example 
At 

time t = 0 second, 
the spring 
10 cm below its 
position at rest. 
seconds 
and its length 10 
later 
Find a formula 
for the length 
t seconds. 

to a length 
The spring is 
released, 
is observed to be 5 cm. 
of the 

at time 

spring 

6.92. 

of oscillation is 10 seconds, 

a spring, 
as in Exam­

ple 6.92. 
find the 

18. A 50 g mass is attached to 
constant. 
of a mass, 
of a string 

19. A pendulum consists 

If the period 
spring 

to the end 

a bob, that 
called 
oflength 
L (see 

is affixed 
Figure 6.24). When the bob is moved from its rest 
position and 
back and forth. 
The 
released, 
time it takes the pendulum to swing from its farthest 
right 
its next farthest right 
the pendulum. 

t position and back to 
is called 
of 

position to its farthest lef

the period 

position 

it swings 

Review 
Chapter 
Kev Definitions 

and concepts 

493 

Theorem, 

combination 
transformation, 

isomorphism, 
basis, 
446 
of a linear 
kernel 
453 
Basis 
482 
transformation, 
change-of-basis 
matrix, 465 
composition oflinear 
433 
linear 
of vectors, 
linear 
477 
transformations, 
vector, 449 
coordinate 
linearly dependent vectors, 
diagonaliza
ble linear 
443,446 
transformation, 
509 
linearly 
453 
dimension, 
443,446 
matrix 
l Theorem of Invertible 
Fundamenta
512 
Matrices, 
transformation, 
identity 
nullity 
transformation, 
invertible 
linear 
transformation, 
478 
one-to-one, 488 

of a linear 
498 
of a linear 
484 

onto, 488 
range of a linear 
transformation, 
482 
rank of a linear 
484 
transformation, 
Rank Theorem, 
486 
span of a set of 
standard basis, 
447 
subspace, 
trivial subspace, 
vector, 429 
vector 
zero subspace, 
zero transformation, 

transformation, 

independent 

space,  429 

vectors, 

438 
vectors, 

4 7 4 

472 

434 

474 

43 7 

43 7 

Review Questions 
(a) If V = span(v1, .•. , vn), then every spanning 

statements 
set 

1. Mark each of the following 

true or false: 

for V contains 
w} is a linear

(b) If {u, v, 

at least n vectors. 

ly independent 

set of ve

ctors, 

then so is {u + v, v + w, u + w}. 
consisting 
consisting 

basis 
(c) M22 has a 
(d) M22 has a basis 
is zero

(e) The transformation 
T(x) = II x II is a linear 
(f) If T :  V---+ W is a linear 

. 

s. 

of invertible 
matrice
of matrices 
whose trace 
T :  !Rn ---+ IR defined 

by 

transformation. 
transformation 
and dim 
be both one-to-one 

V * dim W, then T cannot 
and onto. 

3. V = M22, W = { [: �]: a  + b = c + d 

=a + c=b + d} 
'!Jf, W ={fin '!Jf :f(x + n) = f(x) for allx} 
3 : x3p( 1 / x) = p(x)} 

4. V = IJ> 3, W = {p(x) in IJ> 
5.  V = 
6. Determine 

{1, cos 2x, 3 sin2x} is line

whether 

arly 

dependent or independent. 

7. Let A and B be nonzero n X n matrices 
and B is skew-symmetric. 

symmetric 
is linearly independent. 

A is 
Prove that {A, B} 

such that 

8 and 9, find a basis 

V = {p(x) in IJ>4: p(l) = O} is 

and 

nullit

transforma
tion and 

transformation 

ker(T) = V, then 

(g) If T :  V---+ W is a linear 
W = {O}. 
(h) If T :  M33 ---+ IJ> 4 is a linear 
y(T) = 4, then T is onto. 
(i) The vector space 
(j) If I :  V---+ V is the identi

isomorphic to rzf 3. 
W is a subspace of V 
2-5, determine 
2. v = IR2, w = { [;] : x2 + 3y2 = o} 

the matrix [IJc+-B is the identity matrix for any 
bases 

whether 

B and C of V. 

In Questions 

for W and state 
the 

In Questions 
dimension 

of W 

respect 

to the bases 

matrices 

9. W = {p(x) in IJ>5 : p(-x) = p(x)} 
10. Find the change-of-basis 

8. W = { [: �] : a + d = b + c} 
B = {1, 1 + x, 1 + x + x2} and 
C = {1 + x,x + x2, 1 + x2} ofrzf>2. 
11-13, determine whether 
T(x) = yxTy, where y = [21] 

In Questions 
transformation. 
11. T :  IR2---+ IR2 defined by 

Pc+-B and Ps+-C with 

T is a linear 

ty transformation, 

then 

5 2 1  

by T(A) = AT A 

12. T :  Mnn ---+ Mnn defined 
13. T :  <JP n---+ <JP n defined 
14. If T: <ZP2---+ M22 

5 2 8   Chapter 
6 Vector 
Spaces 
by T(p(x)) = p(2x - 1) 
T(l) = [� �l T(l + x) = [� �]and 
- 3x + 2x2). 
T(l + x + x2) = [� - 1] 0 , find T(S 

transformation such that 

is a linear 

15. Find the nullity 
T :  Mnn---+ IR defined 
16. Let W be the vector 

by T(A) = tr(A). 
space 

of the linear 

transforma
tion 

of upper triangular 

2 X 2 

. 

matrices
(a) Find a linear 
(b) Find a linear 
that range(T) = W. 

that ker(T) = W. 

transformation 

T :  M22 ---+ M22 such 
T :  M22---+ M22 such 

transformation 

matrix [ T] C+--B of the 

17. Find the 

T in Question 14 
B = {l, x, x2} of<ZP2 and C = {E11, E12, E21, Ed of M22• 

with respect 

ndard bases 

transformation 

linear 
to the sta

18. Let S 

= {v1, • • •  , vn} be a set 

of vectors 

in a vector 

space V with the property that every vector 
be written 
actly 

in V can 
tion ofv1, . • .  , vn in ex­

one way. Prove that S is a basis 
for V. 

as a linear 

combina

19. If T :  U---+ V and S :  V---+ W are linear 
such that range(T) � ker(S), what can be 

tions 
deduced abou

t S 0  T ?  

transforma­

20. Let T : V ---+ V b e  a linear 

{v1, • . •  , v"} be a basis 
T(v)} is also a basis for V. Prove that T is invertible. 

transformation, 

and let 
for V such that {T(v1), • .  ., 

D istance and 
Approximation 

1 . 0  Intro d u ction :  Taxicab G e o m etrv 

line may be the shortest 

A straight 
distance 
between 
is by no means the most interesting. 

two points, but it 

Although this may seem a paradox, 
all exact science 
by the 
is dominated 
idea of approximation. 

world, 

them to 

Euclidean 

street and 
asking 

dimensional 
govern 

geometry 
people on the 
"The shortest distance 
respond 
notions 

with "straight 
of distance. By allowing 
will open the door to the possibility 

We live in a three-
Euclidean 
stopping 
sentence: 
certainly 
intuitive 
flexible way, we 
polynomials, 

-Doctor 
Who 
Monster" 
In "The Time 
By Robert Sloman 
BBC, 
1972 
Russell 
-Bertrand 
In W. H. Auden and 
p. 263 
, eds. 
L. Kronenberger
1962, 
Viking, 

the straight-
consequence 
behaves in some familiar 

line distance 
of Pythagoras' 
ways. 

Suppose you are standing 

Theorem). As you'll 

In this section, 

functions, 

and many other 

matrices, 

you will 

at an intersection 
in a city, 

you are used to from Euclidea

ourselves 

line:' 

The Viking Book of Aphorisms 

discover a type of "distance" 

and, therefore, 

concepts 

our way of looking at the world. 

between two points is a ___  :' They will almost 

In particular, imagine 
fill in the blank in the following 

from 

There are, however, 
other 

sensible and 

equally 
to think of "distance" 
in a more 
between 
"distance" 
of having a 
algebra. 
that is every bit 
as real as 
(the one tha
of "distance" 

n geometry 
see, this new type 

t is a 
still 

objects that arise in linear 

B 

A 

Figure 1 . 1  Taxicab 

distance 

If you ask someone 

intersection. 
to measure 
). Instead, 

at another 
unlikely 
distance
is the way taxicab drivers 
as taxicab 

(i.e., 
like "It's 
distance, we will refer 

is 
of 
five blocks awaY:' Since 
this 
to this notion 

the response will be something 

"as the crow flies" 

how far it is to the 

distance 

using the Euclidean 

measure 

trying to get to 

a restaurant 
restaurant, 
that person 
version 

distance. 

of "distance" 

Figure 7.1 shows an example 

traversing 

the sides of 

requires 
than one route from A 
two vertical 
many shortest routes 
to B is 5. 

moves, where a "move" 

are there 

to B, all shortest routes 

corresponds 

of taxicab distance. The shortest 
five city blocks. Notice 

path from A to B 
more 

that although there is 
three 

require 
moves and 
to the side of one city block. 

horizontal 

from A to B?) Therefore, 

the taxicab distance 

(How 
from A 

Idealizing 

this situation, 

we will assume 

will use the 

notation 

dt(A, B) for the taxicab 
distance 

Problem 1 Find the taxicab 

between the following 

pairs of points: 

that all blocks are unit 
distance 
from A to B. 

squares, and we 

(a)  (1, 2) and (5, 5) 
(c) (0, 0) and ( -4, -3) 
(e)  (1, !)and ( L �) 

(b) (2, 4) and (3, -2) 
(d) (-2, 3) and (1, 3) 
(f) (2.5, 4.6) and (3.1, 1 .5) 

5 2 9  

5 3 0   Chapter 

7 Distance 

and Approximation 

is the correct 

formula 

for the taxicab 

distance 

Problem 2 Which of the following 
dt(A, B) between A = (a1, a2) and B = (b1, bi)? 

We can define the taxicab 

norm of a vector 

(a) dt(A, B) = (a1 - b1) + (a2 - b2) 
(b) d1(A, B) = (la1I -lb1I) + (la2I -lb2I) 
(c) d1(A, B) = la1 - b1I + la2 - bzl 

Problem 3 Find llvll

v as 
r for the following 
(a) v = [ _�] (b) v = [ _:J 
(c) v = [--36] (d) v = [21] 
Problem 4 Show that Theorem 1.3 

is true for the taxicab 

the Triangle 

Inequality 

(Theorem 

norm. 
1 .5), using 

vectors: 

Problem 5 Verify 
following 

and the 

pairs of vectors: 

(a) u = [�], v = [�] (b) u = [ _�], v = [-�] 

the taxicab 

norm 

Problem 6 Show that the 

Triangle 

Inequality is 

in general, for the 

true, 

taxicab 

norm. 

with the following 

radii: 

In Euclidean 

geometry, we can define 

a circle of radius 
sly, we can define a taxicab 

r, centered 
at the origin, as 
circle 

of radius 

the set of all x such that 
r, centered 

at the origin, 

Problem 1 Draw taxicab 

as the set 

II x II = r. Analogou
of all x such that II x II 1 = r. 
(a) r = 3 (b) r = 4 (c) r =  1 

of n is half the 

at the origin 

centered 

circles 

pi to be the 

Problem 8 In Euclidean 
geometry, the value 
1). Let's 

unit circle (a circle of radius 
the circumf

erence 

define taxicab 
circle. What is the 

In Euclidean 
the set 

of a taxicab unit 
geometry, the perpendicular 
of all points that are equidist
distance, it is reasona

value of 'TT t? 
bisector 
of a line segment AB can be 
ant from A and B. If we use taxicab 
ble to ask what the perpendicular 

defined as 
distance instead 
of Euclidean 
bisector 
bisector 

of a line segment now looks like. To be precise, 
of AB is the set of 

the taxicab 

perpendicular 

circumference 
number 'TT t that is half 

of a 

all points X such that 
d/X, A) =  d1(X, B) 

of points: 

Problem 9 Draw the taxicab perpendicular 

bisector 

of AB for the 

following 
pairs 

l), B  = (4, 1) 

(c) A  = (1, 1), B = (5, 3) (b) A  = (- 1, 3), 
B = (- 1, -2) 
(a) A =  (2, 

(d) A  = (1, 1), B = (5, 5) 
with 
shares 
illustrate,  taxicab 
ways. In this chapter, 
in some striking 
we will 

geometry, but it also differs 

some properties 

geometry 

As these problems 

Euclidean 

Section 

7.1 Inner 

Product Spaces 

5 3 1  

Ill 

other 

encounter 
several 
own way. We will try to 
properties 
in which the notion 

types of distances 
discover what they have in common and use these 

each of which is useful in its 

and norms, 

to our advantage. 

We will also explore 

a variety 

of approximation 

of "distance" 

plays 

an important role. 

common 
problems 

I n n e r  Pro d u ct Spaces 

dot product u · v of vectors 

u and v in !Rn, and we have 
In Chapter 1, we defined the 
we will use 
this book. In this section, 
made repeated use of this operation throughout 
the general notion 
of an inner 
the properties of the dot product as a means of defining 
products 
product. In the next section, 
than !Rn. 
of "length" and "distance" 
analogues 

we will show that inner 

spaces other 

can be used to define 

The following 

definition 

point; it is based on the properties 

of the 

in vector 
is our starting 

dot product proved in Theorem 1.2. 

Defi n i t i o n  An inner product on a vector 
to every 
properties 

pair of vectors 
hold for all vectors 

u and v in V a  real number (u, v) such that the following 

u, v, and w in V and all scalars c :  

V is an operation 

that assigns 

space 

3. (cu, v) = c(u, v) 

1. (u, v) = (v, u) 
2. (u, v + w) = (u, v) + (u, w) 
4. (u, u) 2:: 0 and (u, u) = 0 if and only if u = 0 
sumes that V is a real vector 

with an inner 

Remark Technica

A vector 

space 

space and since 

lly, this definition 

defines 

product is called 

an inner product space. 

number. There are complex inner product spaces too, 
different. 
with Complex Entries 
section.) 

(See Exploration: 

and Matrices 

Vectors 

but their 

at the end of this 

a real inner product space, 
the inner product 

since 
it as­
of two vectors 
definition 

is a real 

is somewhat 

Example 1 . 1  

!Rn is an inner 
verified 
as Theorem 1.2. 

product space 

with (u, v) = u · v. Properties (1) through (4) were 

The dot product is not the 

only inner 

on !Rn. 
product that can be defined 

Example 1 . 2  

Let u = [ ::J and v = [ ::J be two vectors 
(u, v) = 2u1v1 + 3u2v2 

in IR2. Show that 

defines 

an inner 

product. 

5 3 2   Chapter 

7 Distance 

and Approximation 

Solulion We must verify properties 

(1) through (4). Property (1) holds 

because 

(u, v) = 2U1V1 + 3U2V2 = 2V1U1 + 3V2U2 = (v, u) 

Next, let 

w = [ :J. We check that 

which proves 

property (2). 
If c is a scalar, then 

= 2U1V1 + 2U1W1 + 3U2V2 + 3U2W2 
= (2U1V1 + 3UzVz) + (2U1W1 + }UzWz) 

= (u, v) + (u, w) 

(cu, v) = 2(cu1)v1 + 3(cu2)v2 

= c(2u1v1 + 3u2vz) 
= c(u,v) 

which verifies 

property (3). 

Finally, 

is clear 

and it 
only if u = O). This verifies 
is an inner 

product. 

property ( 4), completing 

that (u, u) = 2uf + 3u� = 0 if and only if u1 = u2 = 0 (that is, 
that if w1, ••• , wn are positive 

7.2 can be generalized to show 

that (u, v), as defined, 

the proof 

if and 

scalars and 

Example 

are vectors 

in ll�r, then 

( 1 )  

an inner 

defines 
W; is negative 
cises 

product on 

!Rn, called 
or zero, then 
Equation 

13 and 14.) 
Recall that the dot product can be expressed 

as u · v = uTv. Observe 

(1) does not define an inner 

product. (See Exer­

a weighted 

dot product. If any of the weights 

that we can 

write 

the weighted 

dot product in Equation 
( 1) as 

Section 

7.1 Inner 

Product Spaces 

5 3 3  

where W is the n X n diagona

l matrix 

The next example 

further generalizes 

this type of inner 

product. 

Example 1 . 3  

Let A be a symmetric, 
be vectors 

in !Rn. Show that 

positive 

u and v 
definite n X n matrix (see Section 5.5) and let 

defines 

an inner 

product. 

Solution We check that 

(u, v) 

=  uTAv = u · Av = Av· u  
= Arv ·  u = (vrA)r · u = vrAu = (v, u) 

Also, 

and 

since A is positive definite, 

Finally, 
u r Au = 0 if and only if u = 0. This establishes 
the last 
property. 

(u, u) 

(cu, v) = (cuf Av= c(urAv) = c(u, v) 

=  u r Au >  0 for all u -=fa 0, so (u, u) = 
-2] 7 

. Then 

let A = [ 4 

-2 

To illustrate 
Example 

7.3, 

Example 1 . 4  

The matrix A is positive definite, 
product on IR2. 
Hence, 
products on vector 

an inner 
We now define some inner 

by Theorem 5.24, 

(u, v) defines 

since 

spaces 

other 

than !Rn. 

its eigenvalues 

are 3 and 8. 

6 + 2x -x2, then (p(x), q(x)) = 1 • 6 + (-5) • 2 + 3 • ( - 1) = - 7.) 

product on <;5}2• (For example, if p(x) = 1 -5x + 3x2 and q(x) = 

an inner 

defines 

Solution Since<;!} 2 is isomorphic 
is an inner product, 

which we have already 

established. 

to IR3, we need only show that the 

dot product in IR3 

5 3 4   Chapter 

and Approximation 

7 Distance 
Example 1 . 5  Let f and g be in C(6 [a, b] , the vector 
a 

interval 

[a, b] . Show that 

(f, g) = r f (x)g(x) dx 

defines 

an inner 

product on <-fi: [a, b]

. 

space of 

all continuous 

functions 

on the closed 

a 

Also, 

Solution We have 

if h is in <-fi: [a, b], then 

(f, g) = r f(x)g(x) dx = r g(x)f(x) dx = (g,f) 
a 
(f,g + h) = rf(x)(g(x) + h(x)) dx 
a 
= r (j(x)g(x) + j(x)h(x)) dx 
a 
= r f (x)g(x) dx + r f (x)h(x) dx 
a 
a 
(cf, g) = r cf (x)g(x) dx 
a 
= c r f (x)g(x) dx 
a 
Finally, (f, f) = r (j(x) )2 dx 2 0, and it follows 
a 
us, (f,f )  = J (j(x) )2 dx = 0 if and 

is continuo
(f, g) is an inner 

product on <-fi: [a, b] . 

If c is a scalar, then 

from a theorem 

= (f, g) + (f, h) 

= c(f,g) 

b 
a 

of calculus 

that, 

since f 
. Therefore, 

only if f is the zero function

Example 

we could restrict 
we consider 
using the 
inner 

7.5 also defines 
CZP [O, l], the vector 

an inner 
defined 
our attention 
on the 
to polynomials 
lynomials 
space 
on the 
product of Example 

product on any subspace of<-fi: [a, b]. For example, 
of all po
we have 

[O, l]. Then, 

interval 

interval 

[a, b] . Suppose 

7.5, 

x) = r x2(1 + x) dx = r (x2 + x3) dx 

0 

0 

(x2, 1  + 

Section 

7.1 Inner 

Product Spaces 

5 3 5  

Properties of  Inner 
theorem 
The following 
definition 
of inner 
product. 

Producls 
summarizes 

some additional 

properties 

that follow from the 

Theorem 1 . 1  

in an inner 

product space 

V and let c be a scalar. 

Let u, v, and w be vectors 

a. (u + v, w) = (u, w) + (v, w) 

b. (u, cv) = c(u, v) 
c. (u, 0) = (0, v) = 0 

Proof We  prove 
Exercises 

property (a), leaving 

the proofs of properties 
(b) and (c) as 

23 and 24. Referring to 

the definition 
of inner 

product, we have 

(u + v, w) = (w, u + v)  by (1) 
= (w, u) + (w, v) by (2) 
= (u, w) + (v, w) by (1) 

and Orlhogona

Lenglh, Dislance, 
In an inner 
tors, and 
every use of the 

dot product u · v by the more general inner 

we can define the length 
in Section 

product space, 
orthogonal 

just as we did 

vectors, 

lilv 

of a vector, distance 

between vec­
1.2. We simply have 

to replace 

product (u, v). 

Defi n ilion Let u and v be vectors 
1 .  The length 
2. Thedistance
3.  u and v 

(or norm) ofv is llvll = \l\V,V1. 

are orthogonal if (u, v) = 0. 

betweenu andv isd(u,v) =llu - vii· 

in an 

inner 

product space 

V. 

Note that llv II is always 

we can 
is called 

take the square root of this nonnega
a unit vector. The 

unit sphere in V is the 

(v, v) 2: 0 by the 

tive quantity. 

definition 
of inner 
As in !Rn, a vector 
1 
set S of all unit vectors 

product, so 
oflength 
in V. 

defined, 

since 

Example 1 . 6  

product on C{i; [O, l] given in Example 

7.5. Iff(x) = x and g(x) = 

the inner 

Consider 
3x - 2, find 

Ca) II! 11  (b) d(f,g) 

Solution (a) We find that 

(c) (f,g) 

JI JI 3]1 

(f,f )  = f2(x) dx = x2 dx = � 

1 
3 

3 0 

0 

0 

so 11111 =  vu:.n = l/v'3. 

5 3 6   Chapter 

7 Distance 

we have 

(b) Since 

f (x) - g(x) = x -(3x - 2) = 2 -2x = 2(1 - x) 

and Approximation 
d(f, g) = llJ -gll = v'(f -g,f -g) and 
(f -g,f -g) = r (j(x) - g(x))2 dx = r 4(1 -2x + x2) dx 
0 [ X3] I 
= 4 x - x2 + 3 o  4 
we see that d(f, g) = \/473  = 2/ v'3. 
(f,g) = rf(x)g(x) dx = r x(3x - 2) dx = f (3x2 -2x) dx = [x3 - x2]6 = 0 

Combining 
(c) We compute 

these 

facts, 

Thus, f and g are orthogona

0 
l. 

3 

0 

0 

0 

Example 1 . 1  

t related 

t the "distance" 

It is important to remember tha
to any measuremen

to the 
does not refer 
does the fact that f and g are orthogonal 
mean that their 
angles. We are simply 
the definition 
in doing so, 
we should be guided 
the inner 
us here, 

graphs 
of a particular 
inner 
by the corresponding 
notions 

between f and g in Example 
7.6 
functions. Neither 
intersect at 
right 
product. However, 
in IR2 and IR3, where 
guide 

product is the 
even though 

we cannot visualize 
things 

The geometry 

of Euclidean space 

dot product. 

graphs of these 

applying 

in the same way. 

can still 

draw a sketch 

of the unit 

7.2, 

in Example 

Using the inner product on IR2 defined 
sphere 
(circle). 

Solulion Ifx = [; l then (x, x) = 2x2 + 3y2• Since 
of all x such that II x II = 1, we have 

1 = llxll = v\X,X}  = v'2x2 + 3/ or 2x2 + 3y2 = 1 

This is the equation of an ellipse, and its graph is shown in Figure 7.2. 

the unit 

sphere 

(circle) 

consists 

y 

f--+�• x  

\ 2 

__, --+ -+--+--+-1-+-+--+--+-+-+-+--+-+-

1 \ 3 
that 
is an ellip
se 
A unit 

circle 

Figure 1 . 2  

Section 

7.1 Inner 

Product Spaces 

5 3 1  

We will discuss 

properties 

of length, 

distance, and orthogona

lity in the 

next sec­

tion and 
version 

in the exercises. 
of Pythagoras' 

Theorem, 

which extends 

Theorem 

1.6. 

One result that we will need in this section 

is the generalized 

Theorem 1 . 2  

Pythagoras' 

Theorem 

Let u and v be vectors 
in an 
if and 
only if 

inner 

product space V. Then u and v are orthogonal 

llu + vll2 = llull2 + llvll2 

Proof As you will be asked to prove in Exercise 

32, we have 

Projecti

immedia

It follows 

ons and the Gram-Schmidt 

v) = llull2 + 2(u, v) + llvll2 

112 = II u 112 + II v 112 if and only if (u, v) = 0. 

llu + vll2 = (u + v, u + 
tely that II u + v 
V; -=fa vj. An orthonormal set of vectors 

discussed 
inner product spaces. For example, 

space V is a set {v1, . . .  , vk} of vectors 

lity in ll�r. Most of this material 
from V such that (v;, vj) = 
is then an orthogona
l set 
for W that is an 
W of V is a basis 

An orthogonal basis for a subspace W of V is just a basis 
basis for a subspace 

Orthogonal 
In Chapter 5, we 
nicely to general 
in an inner 
0 whenever 
vectors. 
orthogonal 
that is an orthono
In IR:n, the Gra

rmal set. 
m-Schmidt Process 

(Theorem 5.15) shows that every 

ly, an orthonormal 

orthogona

set; similar

product 

Process 

l basis. We 

an orthogona
that every finite-dimensional 
to show 
thogonal 
basis-all we need to do 
is replace 
product. We illustrate 
those 

this approach 

in Example 
5.13.) 

can mimic the construction 
product space 

subspace 

of an inner 

has an or­

subspace 
of the Gram-Schmidt Process 

has 

the dot product by the more general 
here with 

ple. (Compare 

the steps 

inner 

with an exam

of unit 
for W 

an orthogonal set of vectors 

generalizes 

Example 1 . 8  

Construct 

an orthogonal 

to the inner 

product 

basis for <lP 2 with respect 

(f, g) = r f(x)g(x) dx 

-] 

the Gram-Schmidt Process 

by applying 
Solution Let x1 = 1, x2 = x, and x3 = x2. We begin by setting v1 = x1 = 1. Next we 
compute 

to the 

basis 

{ l ,  x, x2}. 

5 3 8   Chapter 

7 Distance 

and Approximation 

-I  4 -I 

= 0, 

-I 

-I 

I> 3/ 

2> 2/ 

compute 

3  3' 

Therefore, 

To find v3, we first 

(v1, x2)  0 
--, V1 = x --(1) = x 
V1, VI/  2 
( 

V2 = X2 --
(v x ' =  x2 dx = �  = -
3] I 2 
x4]1 
(v2, X3) = f i x3 dx = 
f I 
(v v ' =  x2 dx = -
f I  2 
Marie Legendre 
Adrien 
(1752-1833) 
was a French 
mathematician 
who 
y, number 
in astronom
worked 
, and elliptic 
theory
He 
functions. 
was involved 
in several 
heated 
disputes with 
Gauss. 
Legendre 
published 
gave 
statement 
the first 
of the law of quadra
ty 
tic reciproci
in number theory 
in 1765. Gauss, 
however, 
gave 
the first 
rigorous 
and 
of this 
proof
in 1801 
result 
claim
ed credit for the result, 
prompting 
outrage 
understandable 
from Legendre. 
in 1806, 
Then 
the first 
gave 
Legendre 
ed 
publish
application 
of the method 
of least 
squares 
in a book 
on the orbits of 
comets. 
Gauss 
published 
on the 
same 
topic 
in 1809 but claimed 
he had been 
the method 
using 
since 
again 
infuriating 
1795, once 
Legendre. 

are the first 
three 
its length 
relative 
mials (see Exercise 41). 

v onto  a 
of a vector 
orthogonal 
basis 

Legendre 
to the same inner 

2, v3} is an orthogonal 

1, x, x2 -t 

It follows 
polynomials 

Just as we did in Section 

If we divide 

product, 

polynomials. 

we obtain 

subspace 

that {v1, v

for W, then 

basis 

Then 

-I 

3 

for IJf 2 on the interval 
[ - 1, 1]. The 

each of these 
normalized 

polynomials by 
Legendre 
polyno­

5.2, we can define the orthogonal projection projw (v) 
If {u1, . . .  , uk} is an 

W of an inner product space. 

Then the 

component 

ofv orthogonal to W is the vector 

perpw(v) 

perpw(v) = v -projw(v) 

w 

Figure 7 . 3  

Decomposition Theorem (Theorem 5.11), projw(v)  and 
(see Exercise 

schematically, 

we have the situa­

As in the Orthogonal 
perpw(v) are orthogonal 
tion illustrated 

We will make use of 

in Figure 7.3. 
these 

43), and so, 
formulas in Sections 
lar, the problem 

approxima

tion problems-in particu

7.3 and 7.5 when we consider 
of how best to approximate 

a 

Section 

7.1 Inner 

Product Spaces 

5 3 9  

when they will make more sense. 

by "nice" functions. Consequentl
Our immedia

until 
y, we will defer any examples 
projection 

te use of orthogonal 

given function 
then, 
will be to prove 

an inequality 

that we first encountered 

in Chapter 1 .  

z and Triangle lnequalilies 
and inequalities 
involving 
general inner 
1 .2, we first encountered 

product in !Rn are easily 
product spaces. Some of these 
the Cauchy-Schwarz 

to give corresponding results in 
in Exerc

The Cauchv-Schwar
The proofs of identities 
adapted 
are given 
Inequality, 
proof 

ises 3 1-36. In Section 

which is important 

in many branches 

of mathematics. 

of this resu
lt for 

product spaces. 

We now give a 

the dot 

inner 

Theorem 1 . 3  

The Cauchy-Schwarz Inequality 

Let u and v be vectors 

in an inner 

product space 
V. Then 
l(u, v)I ::; llnll ll
vll 

with equality 

holding 

if and only if u and v are scalar 

multiples 

of each other. 

Proof If u = 0, then the 

inequality 

is actually 

an equality, since 

l(o, v)I = 0 = llOll ll
vll 

byu.Sinceprojw(v) = -( -,u and 

we can apply Pythagoras' 

Theorem to obtain 

(2) 

It follows 

(u, v) 
U, ll/ 

ofVspanned

= llprojw(v) 112 + llperpw(v) 112 

llvll2 = llprojw(v) + (v -projw(v)) 112 = llprojw(v) + perpw(v) 112 

Ifu -=!= O,thenletWbethesubspace
perpw v = v -projw(v) are orthogonal, 

This 
inequality 
was discovered 
by 
several 
different 
ticians, 
mathema
different 
in several 
contexts. 
It is 
no surprise 
that 
the name 
of the 
prolific 
Cauchy 
is attached 
to it. 
The second name 
associated 
with 
is that 
of Karl 
this result 
Herman 
Amandus 
Schwarz 
(1843-1921)
, 
a German 
mathematician 
who 
v) )  ( (u, v) )2  (u, v)
that llprojw(v) 112 ::; llvll2. Now 
taught 
at the University 
of Berlin. 
inequality that 
His version 
of the 
.  2 / (u, v)  (u, 
bears 
his name 
was published 
(u,u) = (u,u) = � 
that 
in 1885 in a paper 
used 
llproJ w(v)ll = \(u,u)"' (u,u)" = (u,u) 
integral 
equations 
to study 
surfaces 
of minimal 
area. 
A third 
(u v)2 M>::; llvll2 or, equivalently, (u, v)
name also 
associated 
with 
this 
important 
result 
is that 
of the 
mathematician 
Russian 
Viktor 
vll · 
Yakovlevitch 
Bunyakovsky 
(1804-
1889). Bunyakovsky 
the 
published 
inequality 
in 1859, a full quarter­
century 
before Schwarz's 
work 
on 
the same 
subject. 
it is more 
Hence, 
v = proJw(v) = -( -, u 
proper 
to the 
to refer 
result as 
the 
Cauchy-
y-Schwarz 
Bunyakovsk
Inequality. 

we obtain 
Clearly this last inequality 
if and 

(2) this is true 

.  (u, v) 
U, ll/ 

l(u, v)I ::; llull ll
is an equality 
only if perpw(v) = 0 or, equivalentl

if and only if 
y, 

square roots, 

2::; llnll2llvll2 

Equation 

Taking 

so we have 

2 

llprojw(v) 112 = llvll2. By 

2 (u, v)

5 4 0   Chapter 

7 Distance 

and Approximation 
-' u = 0 

perpw(v) = v -proJw(v) = cu --

of u. Conversely, if v = cu, then 
c(u, u) 
(
U,Uf 

-
-- ' u = cu --

If this is so, then v is a scalar multiple 

(u, cu) 
(
U,Uf 

. 

so equality 
holds 

in the Cauchy-Schwarz 

Inequali
proof of this inequality, 

ty. 
see Exercise 

For an alternative 
consequences 

some  interesting 
equali
follows 
Triangle 

ties in Exploration: Geometric 
this section. 
Inequality 

Inequalities and Optimization Problems, 
which 
of the 

For the moment, we use it to prove a generalized 
version 
(Theorem 1 .5). 

of the Cauchy-Schwarz 

Inequality 

and related in­

44. We will investigate 

Theorem 1 . 4  

The Triangle 

Inequality 

Let u and v be vectors 
in an 

inner 
product space V. Then 
llu + vii :s llull + llvll 

Proof Starting 

you will be asked to prove in Exercise 32, we have 

with the equality 
llu + vll2 = llull2 + 2(u, v) + llvll2 
:s llull2 + 2l(u, v)I + llvll2 
:s llull2 + 2 llull ll
vll + llvll2 
= (llull + llvll)2 
the result

yields 
. 

by Cauchy-

Schwarz 

1 . 1  

Taking 

square roots 

In Exercises 
1. (u, v) is the 

.. I Exercises 
<!/' 2 [ 0, l ]. Compute 
1-4, let u = [ _ �] and v = [:]. � 6. (p(x), q(x)) is the inner 
(a) (u, v) (b) llull (c) d(u, v) 
A = [� �]. Compute 
to p(x). 
� 8. In Exercise 
p(x). 
� In Exercises 
(a) (u, v) (b) llull (c) d(u, v) 
space C€, [O, 27T] with the inner 
7.5. 
5-8, let p(x) = 3  -2x and q(x) = 1 + x + x

9. Compute 
(a) (f,g)  (b) llJll 

(a) (p(x), q(x)) (b) llp(x) II (c) d(p(x), q(x)) 

9 and 10, let f(x) = sin x 

3. In Exercise l, find a nonzero vector 
4. In Exercise 2, find a 
nonzero vector 

orthogonal to 
orthogonal to 

5, find a nonzero vector 
6, find a 
nonzero vector 

cos x 
defined by Example 

2. (u, v) is the 

7. In Exercise 

product of Example 

product of Example 

product of Example 

in the vector 

7.2. Compute 

u. 
u. 

vector 

(c) d(f, g) 

7.3 with 

space 

inner 

inner 

2• 

In Exercises 
5. (p(x), q(x)) is the inner product 
ofExample 

(a) (p(x), q(x)) (b) llp(x) II (c) d(p(x), q(x)) 

7.4. Compute 

10. Find a nonzero vector 
to f 
11. Let a, b, and c be distinct 

orthogonal 
real numbers. Show that 
(p(x), q(x)) = p(a)q(a) + p(b)q(b) + p(c)q(c) 

7.5 on the 

orthogonal 
orthogonal to 

and g(x) = sin x + 

product 

Section 

7.1 Inner 

Product Spaces 

5 4 1  

an inner 

defines 
the fact that a polynomial of degree 
zeros. See Appendix D.] 

product on rzf 2. [Hint: 

n has at most n 

You will need 

12. Repeat Exercise 5 using 

the inner 

product of Exer­

cise 1 1  with a = 0, b = l, c = 2. 

example 

do not hold. Give a specific 

In Exercises 
uct axioms 

13-18, determine which of the four inner 
13. Let u = [�:]and v = [:Jin IR2. Define (u, v) = u1v1• 
= [�:] and v 
= [ :J in IR2. Define 
(u, v) = u1v1 - u2v2• 
= [�:] and v 
= [ :J in IR2. Define 

14. Let u 

15. Let u 

(u, v) = U1V2 + UzV1. 

prod­
in each 
case. 

duct space, 

there 

cannot 

be 

26. (2v -w, 3u + 2w) 
27. llu + vii 
28. ll2u -3v + wll 
29. Show that u + v = w. [Hint: How can 

you use 

product to verify that 

the properties 
of inner 
u + v - w = O?] 
in an 
inner pro
u and v with (u, v) < - 1. 
unit vectors 

30. Show that, 

31-36, (u, v) 

statement 

is an inner 

product. In Exer­
is an ide
ntity. 

cises 31-34, prove 
In Exercises 
that the given 
31. (u + v, u - v) = llull2 -llvll2 
32. llu + vll2 = llull2 + 2(u, v) + llvll2 
33. llull2 + llvll2 = t llu + vll2 + t llu - vll2 
34. (u, v) = i llu + vll2 -i llu - vll2 
35. Prove that llu + vii = llu - vii if and only ifu and v 
are orthogonal. 
36.Provethat d(u,v) = Vllull2 + llvll2ifandonlyifu 

and v are orthogonal. 

37-40, apply 

16. In rzf 2, define (p(x), q(x)/ = p(O)q(O). 
17. In rzf2, define (p(x), q(x)/ = p(l)q(l). 
18. In Mw define (A, B/ = det(AB). 

19 and 20, (u, v) defines an inner 
· 
[U1] [V1] · 

• Fm a symmetric 

product 

In Exercises 
on IR2, where u = 
and v = v2 
matrix A such that (u, v) = ur Av. 
19. (u, v) = 4U1V1 + U1V2 + UzV1 + 4U2V2 
20. (u, v) = U1V1 + 2U1V2 + 2U2V1 + 5U2V2 

for the 

to the given inner 

In Exercises 
basis 
space V relative 

l3 to obtain an orthogonal basis 
product. 

37. V = IR2, l3 = { [ � l [�]}, with the inner 
38. V = IR2, l3 = { [ �], [ �] } , with the inner 

Example 

7.2 

immedia

tely following 

Example 

7.3 

Uz 

d 

product 

product in 

the Gram-Schmidt Process 

to the 

inner 

product 

21 and 22, sketch the unit circle in IR2 for the 
product, where u = [ �:] and v = [ ::]. 

In Exercises 
given inner 
21. (u, v) = u1v1 + iu2V2 
22. (u, v) = 4U1V1 + U1V2 + UzV1 + 4U2V2 
23. Prove Theorem 7.l(b). 
24. Prove Theorem 
7.l(c). 

25-29, suppose that u, v, and w are vector

s in 

In Exercises 
product 
an inner 

space such that 

(u, v) = 1, (u, w) = 5, (v, w) = 0 
llull = 1, llvll = V3, llwll = 2 

25-28. 

Evaluate the expressions in Exercises 
25. (u + w, v -w) 

39. V = rzf 2, l3 = {1, 1 + x, 1 + x + x

2}, with the inner 

product in Example 
7.4 

�40. V = rzll2[0, 1 ] ,  l3 = {1, 1 + x, 1 + x + x

�41. (a) Compute the first three 

product in Example 
7.5 
normalized 

2}, with the 

inner 

Legendre 

polynomials

7.8.) 
(b) Use the Gram-Schmidt Process 

. (See Example 

to find the 

fourth 

n by 
of degree 
L"(x) 

a polynomial 

normalized 
42. If we multiply 

polynomial. 
e polynomial 

Legendre 

the Legendr
an appropriate 
scalar we can obtain 
such that Ln(l) = 1. 
(b) It can be 

(a) Find L0(x), L1 (x), L2(x), and L3(x). 

shown that L"(x) satisfies 
2n - 1 

relation 
Ln(x) =  xLn_1(x) -- Ln-z(X) 

n -1 
n 

n 

the recurrence 

5 4 2   Chapter 

7 Distance 

and Approximation 

43. Verify 

for all n 2 2. Verify this 
L3(x). Then 
space 
to all w in W. 

L4(x) and L5(x). 
use it to compute 
of an inner 

recurrence 
for L2(x) and 
product 

that if W is a subspace 
V and v is in V, then perpw(v) is orthogona

l 

44. Let u 

and v be vectors 

in an inner 

product space 

Inequality 

V. 
for u * 0 as 

Prove the Cauchy-Schwarz 
follows: 
(a) Lett be a 

for all values 

real scalar. Then (tu + v, tu + v) 2  0 
oft. Expand this ineq
uality 

to obtain 

a quadratic 

inequality 
of the form 
at2 +  bt  + c 2  0 

What are a, b, and 

(b) Use your knowledge 
to obtain 

their 
for which the inequality 

c in terms of u and 
of quadratic 
and 
equations 
a condition 
on a, b, and c 
in part (a) is true. 

graphs 

v? 

(c) Show that, 

in terms of u and v, your condition 

in part (b) is equivalent 
to the 
Inequality. 

Cauchy-Schwarz 

Vectors and Matrices with Complex Entries 

example 

applications. 

ed the theory 

In this book, we have develop
the most basic 

2� and their 
space, 
all hold for en, and concepts 
over from !Rn without 
difficulty. 
difference 
The first notable 

and applications 
of which is !Rn. We have also explored 
plex numbers IC as scalars. The vector 
of complex 
The set 
space 
basis, 
independence, 

en of n-tuples 

with the com

of real vector spaces, 
the finite vector 
numbers is 

spaces 

also a vector 
(Section 

axioms 
6.1) 
and dimension 
carry 

such as linear 
between !Rn and en is in the definition 

en as in !Rn, then for the nonzero vector v = [ �] we 

of dot product. 

If we define the dot product in 
have 

llvll = VV:V  = Vi2+l2  = v- 1  + 1 = v'o = o 

an undesirable 

situation 

This is clearly 
violates 
way that avoids 

Theorems 

this type of difficulty. 

l.2(d) and 1.3. We now generalize 

(a nonzero 

vector 

the real dot product to en in a 

whose length 

is zero) 
and 

com plex 
Ifu � [ J:l and v � [ n ace vedo" in C"' then the

Definition 

dot product of u and v 

is defined 

by 

The norm (or length) 

llvll = VV:V:  Likewise, the distance 

of  a complex vector 

between two complex vectors 

v is  defined  as 

in the  real 
case: 
u and v is still 

defined as

d(u,v) = llu-vll· 

5 4 3  

vector orthogonal 

to u 

fo n  � [ n in C", 11•11 � Vlvil + lvil + . . .  + lv;I. 

I .  Show th•t, 

2. Letu = [i]andv = [2  -3i]. Find: 

1  + Si 

l to v 

1 

(a) u · v  (b) llull (c) llvll (d) d(u,v) (e) a nonzero 
(f) a nonzero vector 

orthogona

The complex 

dot product is an example 

of the more general 

notion 

of a complex 

product, which satisfies 

the same conditions 

as a real inner product with two 

inner 
exceptions. 

Problem 3 provides a summary. 

3.  Prove 

that the 

complex dot product satisfies 

the following 

properties 

for all 

U, V, and W in en and all complex scalars. 

vectors 
(a) u · v=v · u  
(b) u · (v + w)  = 
(c) (cu) ·v = c(u · v) and u ·(cv) =c(u· v) 

(d) u · u  � 0 and u · u  = O if andonlyifu = 0. 

u · v  + u · w  

with complex 

entries, 

multiplication 
3.1, and the algebraic 

addition, 
are all defined 
properties 
of these 

multiplication 
by complex 
as we did for real ma­
hold. 

still 
(See 
operations 
of the inverse and determina
nt of a square 

exactly 

scalars, 

Likewise, we have the notion 

in Section 

For matrices 
transpose, and matrix 
trices 
Section 
complex matrix just as in 
complex case. 
over to the 

3.2.) 

the real case, 
(See Sections 

and the techniques 
3.3 and 4.2.) 

and properties all carry 

The notion 

of transpose is, however, 
provides 

definition 

The following 

real case. 

less useful in the 

complex case than in the 

an alternative. 

Definition 
matrix A* defined 

by 

If A is a complex matrix, 

then the conjugate transpose of A is the 

to the matrix 
of A; that is, 

whose entries 
if A  =  [ 

are the complex 

4. Find the conjugate transpose A *  of the given matrix: 

conjugates 

of the correspond

A refers 
In the preceding definition, 
ing entries 

(c) A =  4 �] 1  + 3i 
(a) A =  [2 - i 

0  - 2  ] 3 -4i 

problem 

shows. 

Properties of the complex conjugate (Appendix C )  extend 

. 

(b) A 

aij ] ,  then A  =  [ 
aiJ ] .  

_  [ 2  5 -2i] 
il 
5  + 2i - 1  [ 3i 
(d) A = 1  -1  4  1 
0 1  +
1  +  i 0  - i  
--

to matrices, 

scalar. Prove the 

complex 

as the next 

(b) A +  B = A +  B 
(d) AB= A B  

. 

-

5. Let A and B be 

complex matrices, 

and let c be a 

properties

following 
: 
(a) A = A  
(c) cA = c A 
(e) (A f = C:F) 

5 4 4  

Definition 
is, if it is equal to 

Hermitian 
matrices 
are named 
after the French 
mathema
tician 
Charles 
Hermi
. 
te (1822-1901)
for his 
is best known 
Hermite 
proof 
that 
the number e is tran­
scenden
but he also 
tal, 
was the 
use the term 
first to 
that 
matrices, and he proved 
sym­
metric 
tian) 
(and Hermi
matrices 
have 
real 
es. 
eigenvalu

(a) A  = 

orthogonal 

5 can be used to establish 

The properties 

in Problem 

jugate transpose, which are analogous 

to the 

the con
real matrices 

(Theorem 3.4). 

the following 

of 
of the transpose for 

properties 

properties 

6. Let A and B be complex matrices, 

and let c be a complex scalar. Prove the 

properties

following 
: 
(a) (A*)* = A  
(c) (cA)*  = cA* 

(b) (A + B)* =A* + B* 
(d) (AB)*  = B*A* 

u · v = u*v. (This 

7. Show that for vectors 
is why 
us the analogue 

u and v in en, the complex 
in 11;r.) 
of the formula u · v =  ur v for vectors 
we defined 

the com

result 

gives 

dot product satisfies 

plex dot product as 

we did. It 

For real matrices, 

we have seen the importance 

of symmetric 

matrices, 

especially 

in our study of diagonaliza
complex matrices, 
the following 

Recall 
definition 

that a real matrix A is symmetric 
is the correct generaliza
tion. 

tion. 

if Ar = A. For 

A square complex matrix A is called Hermitian 
its own conjugate 

transpose. 

if A* = A-that 

(c) A =  

+
1. 

8. Prove that the diagonal 
9. Which of the following 

entries 
matrices 

of a Hermitian matrix must be real. 
are Hermitian? 

i] 1 -i [l - 3  
[ 2  1 
-Si [-� � -�] -2 1 0 

-5 3i] 
[ -1  2 
(b) A  =  . 2 -31 [ 1 + 4i 
(d) A  = 1 -4i 2 
3  + i - i  [ 03 02 -21] -2 5 

Prove that the 
of Theorem 5.18 can be 

of a Hermitian matrix are real numbers. [Hint: 
eigenvalues 
by making use of the conjugate 
transpose 
adapted 

(f) A =  

(e) A =  

The proof 
operation

10. 
.] 11. Prove that 
u · v =  u *v instead 

eigenvalues 

distinct 
using 

of u · v =  u r v.] 

Recall that a square real matrix 

provides 

the complex 
. 

analogue

if A is a Hermitian matrix, 
of A are orthogonal. 

ctors 
[Hint: Adapt  the 
proof of Theorem 

then eigenve

corresponding 

5.19 

to 

Q is orthogonal 

if Q-1 = Qr. The next 

definition 

Definition 

A square complex 

matrix U is called unitary if u-1 =  U*. 

Just as for orthogonal 
You need only show that U* U = I to verify that U is unitary

matrices, 

in practice 

it is not necessary to compute u-1 directly. 

. 

5 4 5  

(a) 

their 

i/Vl 

For those 

matrices 

that are unitary

are unitary? 

12. Which of the following 

- i/Vl] i/Vl 
inverses. [i/Vl 
[ 3/5 -4/5]  [ (1  + ;)/Vii 0 2/:6] 

(b) [l + i 
1 - i 
(d) 0 
(- 1  - i)/V3 0 1/0 

1  + i] - 1  + i 

, give 

like orthogona
behave in most respects 
tions 
of unitary 

some alternative characteriza
statements 

l matrices
matrices. 

are equivalent 

13. Prove that the following 

3i/5 
matrices 

4i/5 
Unitary 
problem gives 

for a square complex 

(c) 

.  The following 

. 

of U form an orthonorma
l set 

matrix U: 
(a) U is unitary
(b) The columns 
dot product. 
(c) The rows of U form an orthonormal 
product. 
(d) llUx l l  = llx llforeveryx in cn. 
( e) Ux · Uy =  x 
[Hint: Adapt the 

· y for every x 
proofs of Theorems 5.4-5.7.] 

and y in en. 

in en with respect 

to the complex 

set in en with respect 

to the complex dot 

14. Repeat Problem 12, this time by applying 

the criterion 

in part (b) or part (c) 

of Problem 13. 

The next definition 

is the natural 

generalization 

of orthogonal 

diagonaliza
bility 

to complex matrices. 

Definition 
there 

exists 

A square complex 

a unitary 

matrix U and a diagonal 

matrix A is called 
matrix 

unitaril
D such that 

y diagonalizable 

if 

U*AU = D 

The process 

for diagonalizing 

a unitarily diagona

lizable n 
of U must form an orthonormal 
the eigenvalues 

The columns 
of A. Therefore, 
ace, (3) ensure 
matrix 

the real case. 
eigenve
ctors 
each eigensp
tors (using the Gram-Schmidt Process, 
( 4) form the 
Then U* AU will be a diagonal 
A, arranged 

we (1) compute 
that each eigenspace 
with the complex 
are the orthono

U whose columns 

in the same order 

basis 

X n matrix A mimics 
basis 
of A, (2) find a basis 
dot product, if necessa
just found. 

of 
for 
of orthonor
mal vec­
ry), 

ctors 

matrix D whose diagonal 
ing eigenve
as the 
matrix 

correspond
find a unitary 

are the eigenvalues 
in the columns 
matrix 

ctors 
U and a diagonal 

rmal eigenve
entries 

for en consisting 

15. In each of the following, 

consists 

of U. 
D 

of 

such that U* A U  = D. 

(a) A =  [ 2  i] - i  2 
(c) A =  [ - 1  

1 - i 

(b) A =  [� -�] 

1 ; i]  [i 0 I�;] 

(d) A =   2 

1  + i 

5 4 6  

The matrices 

in (a), (c), and (d) of the preceding 

It turns out that every Hermitian matrix is unitarily 
Complex Spectral Theorem, 
rem 5.20.) 
also be true-namely, that every unit
arily 
But unfortunately 
this is 
of Theorem 5.17 breaks down?) 

which can 

At this point you probably suspect that 

diagonaliza

be proved by adapting 

false! (Can you see where the complex analogue 

the proof of Theo­

the converse of this result must 
ble matrix must be Hermitian. 

of the proof 

problem are all Hermitian. 
diagonalizable. 

(This is the 

le, take the matrix in part (b) of Problem 15. It is 

not 

Linear 

Algebra 

See 
tions by S. J. Leon 
River, 

Saddle 
NJ: Prentice

(Upper 
-Hall, 2002). 

but it is unitarily 
It turns out that the 
theorem, 
the proof 

For a specific 
Hermitian, 

counterexamp
ble. 
correct 

following 

with Applica­

diagonaliza

characteriza

tion of unitary 

diagonali

zability 

is the 

of which can be found in more advanced textbooks. 

A square complex matrix A is unitaril

y diagonalizable 

if and only if 

A*A = AA* 

A matrix 

A for which A* A = AA* is called normal. 

16. Show that every Hermitian matri

x, every unitary 

Hermitian 
to symmetric, 

matrix (A* = -A) is normal. (Note that in the real case, 
respecti
vely.) 
17. Prove that if a square complex matrix is unitarily 
diagona

orthogonal, and 
skew-symmetric 

matrices, 

matrix, and every skew­
refers 

this result 

lizable, 

then it 

must be normal. 

G e o m etric I n e q ualities a n d  
Optimizati o n  Problems 

will introduce 

This exploration 
of various 
maximiza
calculus 

some powerful (and perhaps surprising) 
applications 
Inequal
see, certain 
ity. As you will 
problems) that typically 
at all! 

such as the Cauchy-Schwarz 

inequalities, 
problems 
tion/minimization 
(optimization 
using 
without 
course 
in !Rn states that for 
vll 

Cauchy-Schwarz 

lu· vl::; llull ll

Recall that the 

all vectors 
u and v, 

can be solved 

calculus 

Inequality 

arise 

in a 

with equality 

[x1 · · · xnf and v = [y1 · · · Ynf, the above 

if and only if u and v are scalar multiples 
inequality 
to 

Ix y + · · · + x y I < V x2 + · · · + x2 Vy2 + · · · + y2 

n n- I 

n I 

I I  

n 

of each other. If u 

is equivalent 

Squaring 

both sides 

and using 

summation 

notation, 

we have 

541 

D----� 
C 0 

A 

'-----y-----' �--�---�I 
y 

x 

B 

Figure 7 . 4  

holds 
if there is 
Equality 
begin by using Cauchy-Schwarz 

if and only 

Let's 

some scalar k such that 

y; = kx; for i = 1, . . .  , n. 
l case of one of the most 

to derive 

a specia

useful of all inequalities. 

1. Let x and y be nonnega

tive real numbers. Apply the Cauchy-Schwarz 

Inequality 

to u = [ �] and v = [ �] to show that 
vxy :s  2 
0 and diameter 

if and only if x = y. 

x + y  

with equality 

2. (a) Prove inequality (1) directly. [Hint: Square both sides.] (b) Figure 7.4 

with center 
shows a circle 
CD is perpendicular 
inequality 
The right-

triangles.] 
hand side of inequality 

(1). [Hint: Use similar 

to AB. Prove that CD = \/XY and  use 

AB = AC + CB = x + y. The segment 
this resu

( 1) is the familiar 

arithmetic mean (or average) 
of the numbers x and y. The left-hand side shows the less familiar 
geometric 
of x and y. Accordingl
Arithmetic Mean-Geometric 
for n nonnega
Mean Inequalit
Xi, . . .  , Xn, it states 

y (AMGM). It holds more generally; 

( 1 )  is known as the 

y, inequality 

mean 

lt to deduce 

tive variables 

(1) 

only if Xi = x2 = · · · = Xw 

with equality 

if and 

In words
tive numbers is always 
the same precise
Appendix B.) 

, the AMGM Inequality 

says that the geometric 

mean of a set of nonnega­

less than or equal 

to their 

arithmetic 

mean, and the two are 

ly when all of the 

numbers are the same. (For the gene

ral proof, see 

We now explore 

how such an inequality 

can be applied 

to optimization 

problems. 

Here is 

a typical 

calculus 
problem. 

Example 1 . 9  

Prove that among all rectangles 
largest 
area. 

whose perimeter 

is 100 units, 

the square has the 

Solulion If we let x and y be the dimensions 
the area we want to 

maximize is given by 

of the rectangle (see 

Figure 7.5), 
then 

x 

x 

y 

y 

Figure 7 . 5  

5 4 8  

We are given that the perimeter 

satisfies 

A =  xy 

2x + 2y = 100 

which is the same as x + y 
Inequality: 

= 50. We 

can relate 

xy and x + y 

using the AMGM 

-�  x+ y  
v xy ::::: --

2 

I 

or, equivalently, 

xy ::::: 4(x + y)2 

Since x + y 

= 50 is a constant (and this is 

of A = xy is 502/4 = 625 and it occurs when x = y = 25. 

the key), we see that the 

maximum 

value 

Not a derivative 

in sight! Isn't 

that in this maximiza­
the crucial step was showing 
that the right-hand side of the 
AMGM 
we may be able to apply the inequality 
to 

that impressive? Notice 

In a similar fashion, 

tion problem, 
Inequality 
a minimi

was constant. 
zation 

problem if 

we can arrange 

for the left-hand side to be constant. 

Example 1 . 1 0  

Prove that among all rectangular 
mum surface area. 

prisms 

8 m3, the 
with volume 

cube has the 

mini­

Solution As shown in Figure 7.6, if the dimensions 
then its volume is given 

of such a prism are x, y, and z, 

by 

V = xyz 
Thus, we are given that xyz = 8. The surface 

area to be minimized 

is 

Since 

this is a three-variable problem, 

the obvious 

thing 

to try is the 

version 
of the 

AMGM Inequality 

for n = 3-namely, 

S = 2xy + 2yz + 2zx 

v xyz ::::: 3 

,3� x + y + z 

Unfortunatel
Inequality 
that 

also implies 

y, the  expression 

for S does not appear here. 

However,  the 

AMGM 

Figure 1 . 6  

S 2xy + 2yz + 2zx 
3 
2: \Y(2xy)(2yz)(2zx) 

3 

= 2V(xYz)2 = 2V64 = 8 

which is equivalent 
occurs when 

to S 2: 24. Therefore, 

the minimum value of S is 24, and it 

(Why?) This implies 

the rectangular 

prism is a cube). 

2xy = 2yz = 2zx 
that x = y = z = 2 (i.e., 

3. Prove that among all rectangles 

with area 100 square units, the square has the 

smallest 

perimeter

. 

4. What is the minimum value off (x) = x + -for x > O? 

1 
x 

5 4 9  

Example 1 . 1 1  

be constructed 
at the corners 

to make 

value 

minimum 

tion might help.] 

up the sides. 

the dimensions 

6. Find the minimum 

What should 
as large as possible? 

5. A cardboard box with a square base and an open top 
is to 
out four squares 
of the box be in order 

from a square of cardboard 10 cm on a side by cutting 
and folding 
the enclosed volume 

of f(x, y, z) = (x + y) (y + z) (z + x) if x, y, and z are 

value of x + 

positive real numbers such that xyz = 1. 
7.  For x > y > 0, find the 
y(x -y) 
constraint x2 + y2 + z2 = 1. Where does the maximum value occur? 
tion 3x + y + 2z has the form of a dot product, so we let 

function j(x, y, z) = 3x + y + 2z subject to the 

S o lulion This sort of problem is usually 
variable 

covered in a multi­
use the Cauchy-Schwarz Inequality. The func­

The Cauchy-Schwarz 
illustra
tes. 

. [Hint: A substitu-

to similar problems, as the 

calculus course. Here's 

next example 

value of the 

Find the 

by techniques 

Inequality 

handled 

maximum 

itself 

can be 

how to 

applied 

8 

value 

gives 

and hence 

Inequality 

of our function 

Thus, the maximum 

Then the componentwise form of the Cauchy-Schwarz 

(3x + y + 2z)2 :s (32 + 12 + 22)(x2 + y2 + z2) = 14 
is v14, and it occurs when 
= k, and z = 2k, so 3(3k) + k + 2(2k) 
= v14. It follows 

Therefore, x = 3k, y 
k = 1 / vl4, 

[xl [3/vl4] y = l/vl4 
z 2/vl4 
to x2 + 2y2 + z2 = 1. 
of j(x, y, z) = x + 2y + 4z subject 
z2 
of f(x, y, z) = x2 + y2 + - subject to x + y + z = 10. 
2 
1 1 .  Find the point on the line x + 2y = 5 that is closest to the origin. 
lems. The quadratic mean of the numbers x1, ... , xn is defined 
I 
)x2 + . . .  + x2 

8. Find the maximum 
value 
9. Find the minimum value 
10. Find the maximum value 

that can be used to solve 

There are many other 

inequalities 

of sine + cos e. 

n 

that 

prob-

n 

optimization 
as 

5 5 0  

y 

-r�--,�-� 

2x 

Figure 1 . 1  

r 

quadratic, arithmetic, 

geometric, 

and harmonic 

means are all 

If x1, . . .  , Xn are nonzero, 
their 

harmonic 

mean is given by 
n 

It turns out that the 
related. 

12. Let x and y be positive 

real numbers. Show that 

R x+ y  .r---
>-- > vxy >  

- 2 -

2 

-l/x + l/y 

with equality 
only establish 

if and only if x = y. (The middle 
the first and third 

inequalities.) 
13. Find the area of the largest 

r (Figure 7.7). 

radius 

14. Find the 

minimum 

value 

of the function 

(x + y)2 
f(x, y) =  xy 

for x, y > 0. [Hint: (x + y)2 / xy = (x + y) (1/ x + l/y).] 

inequality 

is just AMGM, so you need 

rectangle 

that can be inscribed 

in a semicir
cle of 

15. Let x and y be positive real numbers with x + y 

= 1 .  Show that the mini­

mum value of 

f(x,y) = (x + �y + (y + t Y 

of x and y for which it occurs. 

is ¥-, and determine 

the values 

5 5 1  

5 5 2   Chapter 

7 Distance 

and Approximation 

Ill!!. 

Norms a n d  Distance Func

tions 

In the last section, 
product space. 
cepts 

that are not defined 

you saw that it is possible to define length 

and distance 

in an inner 

As you will see shortly, there are also some versions 

of these 

two con­

in terms of an inner 

product. 

To begin, 
to have.  The 
Triangle 

Inequality. 

we need to specify 
following 

the properties 
does this, 

definition 

that we want a "length 
" 

function

using as its basis Theorem 1.3 and the 

space V is a mapping 
that associates with 
the norm of v, such that the following 

prop­

u and v and all scalars c: 

are satisfied 

for all vectors 

2. llcvll = lclllvll 

Defi n ition A norm on a vector 
v a real number llvll, called 
each vector 
erties 

1 .  llvll 2 0, and llvll = 0 if and only if v = 0. 
3. llu + vii s; llull + llvll 
S o lulion Clearly, v'(V,VJ 2 0. Moreover, 

with a norm is called 

Show that in an inner 

product space, 

A vector 

space 

a normed linear 

space. 

llvll = v'(V,V/  defines 
v1i:V) =  0 � (v, v) =  0 � v = 0 

a norm. 

by the definition 
of inner 

product. This proves property (1 ). 

For property (2), we only need to note that 

llcvll = Y(cv, cv) =WM

= 0-vfM  = lclllvll 

Property (3) is just the Triangle 

Inequality, which we verified 

in Theorem 7.4. 

Example 1 . 1 2  

Example 1 . 1 3  

We now look at some 

examples 

of norms that are not defined 

in terms of an inner 

product. 
that we explored in 

7.13 is the mathematical 
the Introduction 

to this chapter. 

Example 

generalization 

to !Rn of the taxicab 
norm 

of the absolute 

values 

of its compo­

v in !Rn is the sum 

The sum norm llvlls of a vector 

nents. That is, if v = [ v1 · · · v n] T, then 
Solulion Clearly, llvlls = I v1 I + · · · + Iv n I 2 0, and 
is if I v1 I = · · · = Iv n I = 0. But this is so if and 

Show that the sum norm is a norm. 

v = 0, proving property (1). 

only if v1 = · · · = v n = 0 or, equivale
For property (2), we see that cv = [cv1 · · · cvn]T, so 

the only way to achieve equality 

ntly, 

Section 
7.2 Norms 
and Di
because if u = [u1 · · · unl r, then 

stance 

Functions 

5 5 3  

Finally, the Triangle 

holds, 

Inequality 

llu + vll,= lu1+ v1I +· · ·+ lun+ vnl 

:S (lu1I + lv1I) + · · · 

+ (lunl + lvnl) 

= (lu1I + · · · 

+ lunl) + (lv1I + · · · 

+ lvnl) = llull, + llvll, 

The sum 

norm is also 

known as the I -norm and is often 
b norm. As 
7.13 shows, 

Example 

same as the 

is the 
norms on the 

taxica

same vector 

space. 

Example 

7.14 illustrates another 

by llvll 1. On IR2, it 
denoted 
norm on !Rn. 

it is possible to 

have several 

The max norm llvllm of a vector 
values 

of its componen

ts. That is, if v = [ v1 · · · v n] r, then 

v in !Rn is the 

largest 

number among the absolute 

Example J . 1 4  

of I v1 I, . . .  , I vn I 

property (1). 

Show that the max norm is a norm. 

that llvllm 2: 0. If llvllm = 0, then the largest 

v1 = · · · = vn = 0, so v = 0. This verifies 

is clear 
Solution Again, it 
and so they 
is zero, 
all are. Hence, 
Next, 
we observe that for any scalar c, 
llcvllm = max{lcv1I, . . .  , lcvnl} = lclmax{lv1I, . . .  , lvnl} = lclllvllm 

Finally, for u = [u1 · · · unl r, we have 

:S max{lu1I + lv1I, . .  ·, lunl + lvnl} 
:S max{lu1I, . . .  , lunl} + max{lv1I, · · ·, lvnl} = llullm + llvllm 

llu + vllm = max{lu1 +Vil, . . .  , lun + vnl} 

inequality 

true?) This verifies 

the Triangle 

Inequality. 

..._...  (Why is the second 

denoted 

known as the 
The max norm is also 
by llvlloo· In general, 
it is possible 
llvllp = (lv1IP + · · · + 
number p 2: 1. For p = 1, llvll1 = llvll,, justif

lvnlP)11P 

for any real 

oo-norm or uniform norm and is often 
to define a norm llvllp on !Rn by 

ying the term 1 -norm. For 

p = 2, 

llvll2 = (lv11 2  + · · · 

norm on !Rn obtained 

+ lvnl 2)112 = Yvi + · · · 

from  the  dot 

+ v� 
Called the 
product. 
it can be 
the max norm llvllm· This justifies 
the use 

by llvk As p gets large, 

norm, it is often denoted 
that llvllp approaches 

which is just the familiar 
2-norm or Euclidean 
shown using calculus 
of the alternative 

notation 

llvll00 for this norm. 

Example J . 1 5  

vector 

v in Z�, define llvllH to be w(v), the weight 

of v. Show that it defines a 

For a 
norm. 

5 5 4   Chapter 

7 Distance 

and Approximation 

vector. Therefore, 

Solution Certainly, llvllH = w(v)  2: 0 ,  and the only vector 
the zero 
care 0 and 1, property (2) is immediate. 
ity, first observe that 

To verify the Triangle 

property ( 1) is true. 

Inequal

Since 

the only candida

then w(u + v) counts 

the number of places 
u = [ l  0 1 0] r and v = [ 0 

in which u and v differ. 

whose weight 

is zero is 
tes for a scalar 

[For example, if 

if u and v are vectors 

in Z'.�, 
in n0 
in n01 positions, 
n0 = 0, n1 = 2, n01 = 2, and 

u and v have zeros 

1 l f  

= 3, in agreement with the fact 
that 

three 

above, 

u has a 

0 and v has a 1 

positions.] Suppose that both 

n10 = 1.) Now 

then u + v = [ l  0  0 l f, so w(u + v) 
u and v differ in exactly 
positions 
1 and v has a 

and ls in n1 positions, 
0 in n10 positions. (In the example 
w (u) = n1 + n10, w (v) = n1 + n01, and w(u + v) 
= n10 + n01 
= n10 + n01 
(n1 + n10) + (n1 + no1) -2n1 
:s  (n1 + n10) + (n1 + no1) 

llu + vllH = w (u + v) 

Therefore, 

= w (u) + w (v) = llullH + llvllH 

The norm llvllH is called 

the Hamming norm. 

and u has a 

Functions 

Distance 
For any norm, we can define a distance 
section-

namely, 

function 

just as we did in the  last 

Example 1 . 1 6  

to (a) the Euclidean 
norm, 

d(u, v) = llu -vii 

Let u = [ _ �] and v = [ -�]. Compute d( u, v) relative 
that u - v = [ _:]. 

Solution Each calculation 
(a) As is by now quite 

(b) the sum norm, and (c) the max norm. 

requires 

familiar, 

knowing 

dE(u, v) = llu - vllE = V42 + ( - 3)2 = V25 = 5 

(b) d,(u, v) = llu -vii,= 141 + l- 31 = 7 
(c) dm(u,v) = llu - vllm = max{l4I, l- 31} = 4 

on Z'.� determined 

function 

The distance 
Hamming distance. 
Example 

by the 
We will explore 
tion of the Hamming 

7 .17 provides an illustra

its use in error-

Hamming 
correcting 

norm is called the 
codes in Section 

8.5. 

distance. 

Section 

7.2 Norms 

and Di

stance 

Functions 

5 5 5  

Example 1 . 11 

Find the Hamming 

distance 

between 

u =  [ 1 0  of and  v = 

[ O  

Solution Since 

we are 

over Z2, u - v = u 

working 
dH(u, v)  = llu + vllH = w (u + v) 

+ v. But 

As we noted in Example 7.
differ. 
therefore 

exactly the 

The given 

vectors 

same. 

15, this is just 

the number of positions 

in which u and v 

are the same ones used in that example; the calculation 
Hence, 

dH(u, v) = 3. 

is 

Theorem 7.5 summarizes 

the most important properties 

of a distance 

function. 

Theorem 1 . 5  

defined on 
u, v, and w in V: 

a normed 

linear 

space V. The 

following 

only ifu = v. 

function 

hold for all vectors 

Let d be a distance 
properties 

b. d ( u, v) =d(v,u) 
c. d(u, w) :s d ( u, v) + d(v, w) 

a. d(u, v) 2 0, and d ( u, v) = 0 if and 
d( u, v) = llu -vii 2 0, with equality 

u = v. 
(b) You are asked 
(c) We apply the Triangle 

Inequality 

to prove property (b) in Exercise 

19. 

d(u, v) + d(v, w) = llu -vii + llv -wll 

to obtain 

2 ll(u -v) + (v -w)ll 

= llu - wll = d(u, w) 

Proof (a) Using property (1) from the definition 

of a norm, it is easy to check that 
y, 

if u - v = 0 or, equivalentl

if and only 

holding 

A function 

d satisfying the three 

space tha

and a vector 
very important in many branches 
advanced courses. 

properties 

of Theorem 
is called 
such a 
t possesses 
of mathematics 
and are studied 

7.5 is also called 
a metric, 
a metric space. These are 

function 

in detail 
in more 

M alrix Norms 
We can define norms for matrices 
vector 
After all, the 
space 
course, properties 
not difficult 
to do. Of 
the setting 
. It turns 
of matrices
property. (We will 
satisfy an additional 
it is possible 
everything 

norms for vectors 
as we defined 
is isomorphic to !Rmn, so this is 
Mmn of all m X n matrices 
(1), (2), and (3) of a norm will also hold in 
for matrices, 
restrict 
.) 

our attention 

to arbitrary 

the norms that are most 
useful 

to generalize 

matrices

to square matrices, 

exactly 

out that, 

in !Rn. 

but 

5 5 6   Chapter 

7 Distance 

and Approximation 

Defi n ition A matrix norm on Mnn is a mapping 
n X n matrix 
properties are satisfied 

A a real number llAll, called 

1. llAll 2: 0 and llAll = 0 if and only if A = 0. 

for all n X n matrices 

the norm of A, such that the following 
A and B and all scalars c. 

that associa

tes with each 

2. llcAll = lclllAll 
3. llA + Bii :::; llAll + llBll 
4. llABll :::; llAll llBll 
A matrix 
all n X n matrices 

norm on Mnn is said to be compatible 
x in !Rn, we have 
llAxll :::; llAll llxll 

A and all vectors 

with a vector 

norm llxll on !Rn if, for 

Example 1 . 1 8  

The Frobenius 
matrix into 
a vector 
the square root of the sum of 

norm llAllF of a matrix A is obtained 
the Euclidean 
of the entries 

and then taking 

by stringing 
norm. In other 

out the entries 
words, llAllF is just 

of A. So, if A = [a;j] ,  then 

the squares 

of the 

(a) Find the Frobenius 

norm of 

A =  [� -:J 

Frobenius 

norm is compatible 

with the 

Euclidean 
norm. 

(b) Show that the 
( c) Show that the Frobeni

us norm is a matrix norm. 

vectors 

Solulion (a) llAllF = y32 + (- 1)2 + 22 + 42 = v30 

Before we continue, 

observe that if A1 = [ 3 - 1] and A2 = [ 2 4 ]  are the row 

of A, then II A1 II E = Y 32 + (- 1)2 and II A2 II E = V 22 + 42. Thus, 
if a1 = [ �] and a2 = [ -: ] are the column vectors 

of A, then 

Similarly, 

It is easy to see that these 
observations 

to solve 

parts (b) and (c). 

facts extend to 

n X n matrices 

in general. We will 

use these 

(b) Write 

Section 

7.2 Norms 

and Distance 

Functions 

5 5 1  

Then 

= VllA1xll� + · · · 
+ llAnxll� 
:s VllA1 ll�llxll� + · · · 
+ AJ �llxll� 
= (VllA1ll� + . . .  + 11An11DllxllE 
= llAllFllxllE 
from  the 

where the inequality 
arises 
dot products of the 
row vectors 
Cauchy-Schwarz 
the Euclidean 
norm. 
(c) Let h; 
denote 
product AB, we have 

the ith column 

has been applied?) 

Cauchy-Schwarz Inequality 

applied to  the 

A; with the col

umn vector 
the Frobenius 

x. (Do you see how 
norm is compatible 

Hence, 

with 

of B. Using the matrix-column representation 

of the 

II AB llF = II [Ahl· · · 
:S VllAll�llb1ll� + · · · 
Abn J llF 

= VllAb1ll� + · · · 
+ llAbnll� 

+ llAll�llbnll� 
llAllF Vllb1ll� + · · · 
+ llbnll� 
llAllFllBllF 

by part 
(b) 

which proves property ( 4) of the 
(3) are true, since 
satisfies these 

properties. Therefore, 

the Frobenius 

norm. Properties ( 1) through 

of a matrix 

definition 
norm is derived from the Euclidean 

us norm is a matrix norm. 4 

the Frobeni

norm, which 

For many applications, 

the Frobenius 

matrix norm is not the 

best (or the easiest) 

from considering 

the effect 

ing to the square matrix 

A. This transfor­

measure the "size" of A is to 
think ahead. 

norm. Let's 

compare 
Whatever 

to want it to be compatible 
with 

norms arise 

maps a vector 

norm we are using; 

(vector) 
at, we know we are going 

tion correspond
x into Ax. One way to 

one to use. The most useful types of matrix 
of the matrix transforma
mation 
llxll and llAxll using any convenient 
definition 
the vector 

of II A II we arrive 
II A xii :S llA 11 llxll or II A xii <  llA II for x * 0 
IGll measures 

Th  · llAxll 
e express10n 

that is, we will need 

llxll -

h  "  h.  b·1· " fA If  1.  h 
t  e stretc 

mg capa 1 ity  o  . 

we norma ize eac 

nonzero vector 

x by dividing 

it by its norm, we get unit vectors 

x = M x and thus 

1 

5 5 8   Chapter 

7 Distance 

and Approximation 

- 4  

Figure 1 . 8  

all vectors 

and the set of 

over all nonzero vectors 

in !Rn, then x ranges 
Ax determines 

7.8 shows how the matrix A  = [� �] affects the 

If x ranges 
unit sphere) 
Figure 
into an ellipse. With the Euclidean 
half the length 
max llAx l l  = 4. 
llxll=

of the principal 

norm, the maximum 

axis-in this case, 

IR2-it maps it 

unit circle in 

value of II Ax II is clearly 

just 

4 units. We express 

this by writing 

over all unit vectors 
the 

(i.e., 

some curve in !Rn. For example, 

1 
In Section 

not an isolated 

phenomenon. 

That is, 

7.4, we will see that this is 

II A x i i   , 
that l l A  II = max II Ax II defines 

max-
-
-
x;e o  x  llx lH 
11
1
1 
unit vector 
a particular 
and there is 

= max ll A x l l  

llxll=

! 

a matrix norm. 

y for which II Ar II is maximum. 

exists, 

always 
Now we prove 

that is compatible 

with the vector 

norm that induces 

it. 

llxll=

l 

2: 0 for all vectors 
l l A  II =max II Ax II 2: 0 also. If l l A  II = 0, then we must have 
� must have A = 0. Conversely, if A = 0, it is clear 

Proof (1) Certainly, llAxll 
is true if l l x l l  
= 1. Hence, 
l l Axll = 0-and, hence, 
each of the 
standard 
basis 

l 
= 1. In particu
!Rn. But Ae; is just the ith 

Ax= 0-for all x with l l x l l  
vectors 

lar, Ae; = 0 for 
of A, so we 

x, so, in particu

column 

e; in 

llxll=

(2) Let c be a scalar. Then 

lar, this inequality 

that II A II = 0. (Why?) 
I cl max II A x i i  

lcl llA ll 

llxll=

llcA ll  = max l lcA x l l  = maxlcl l l A x l l  
l 

llxll=

llxll=l 

l 

norm onMnn -----------

is a vectornorm on !Rn, then l l A  II = max II Ax II defines 

Theorem 1 . 6  If llxll 

a matrix 

Section 

7.2 Norms 

and Di

stance 

Functions 

5 5 9  

(3) Let B be an n X n matrix and let 

y be a unit vector 

for which 

llA + B II = max II (A + B)xll = II (A + B)yll 

llxll

= l  

Then 

�  (Where 

llA + Bii II (A+ B)yll 
llAy + Byll 
::; llAyll + llByll 
::; llAll + llBll 

norm [property ( 5)] and then use this fact to complete 

show that our 

definition 

is 
the 

come from?) Next, we 

inequality 

does the second 

with the vector 

that we have a matrix norm. 

compatible 
proof 
(5) Ifx = 0, then the 
If x * 0, then from the comments preceding 

II Ax II ::; llA 11 llxll is true, 
IGil ::; �;; IGil  = llA II 
II A xii II A xii 

inequality 

since 
this theorem, 

Hence, II A xii ::; llA 11 llxll. 

(4) Let z be a unit 
vector 

both sides 

are zero. 

such that 
= l  
llxll

llABll =max II (AB) xii= llABzll. Then 
llABll  ll
ABzll 
llA(Bz) II 
::; llAll ll
Bzll 
::; llAll ll
Bll ll
zll 
= llAll ll
Bll 

by proper
ty (5) 
by proper
ty (5) 

This completes 
compatible 

the proof 

with the vector 

that II A II = max II Ax II defines 

llxll=l 

norm that induces 

it. 

a matrix norm on Mnn that is 

Defi n ition The matrix 
induced 

vector 

by the 

norm II xii. 

norm II A II in Theorem 7.6 is 

called 

the operator 

norm 

the fact 

The term 
square matrix is also 
stretching 

operator norm reflects 
called 
of a linear 
capability 
commonly used operator norms are those induced 
norm, and the max norm-namely, 

The three most 
norm, the Euclidean 

that a matrix transforma

operator. 

a linear 

This norm 

is therefore 

operator. 

by the sum 

a measure of 

the 

tion arising 

from a 

llAll1 = max llAxll,, llAll2 = max llAxllE, llAlloo = max llAxllrn 
llxll.= 1 
lly nice formulas 
ively. 

llxllE= 1 
turn out to have especia

The first and last 

llxll.= 1 

that 

respect
make them very easy 

of these 
to compute. 

5 6 0   Chapter 

7 Distance 

Theorem 1 . 1  Let A be an n X n matrix with column 

A; for i = 1, . . .  , n. 

and Approximation 
a. llAll1 = J=�.ax_}lla1llJ = J=�.a�J�laiJI} 
b. llAll,, = i=�ax_}llA;ll,} = i=��ax,J�laiJI} 

a; and row vectors 

vectors 

In other 

column 
row sum. Before we prove the theorem, 
let's 

words, llA 111 is the largest 
use. 

absolute 

absolute 
easy it is to 

sum, and llA lloo is the largest 

look at an example 

to see how 

Example 1 . 1 9  Let 

A = [ : =� -�] 

- 5   3 

Solulion Clearly, the largest abs

olute 

column 

sum is in the first column, so 

The third 

row has the 

row sum, so 

absolute 

largest 

llAll,, = llAJ, =I- SI+ Ill+ 131= 9  

With reference to 
value of 10 is actually 

the definition 
l 
achieved 

llxll.=
= e1, for then 
when we take x 

11 A 111 = max 11Ax11 ,, we see that the maximum 

llAe1ll, = lla1ll, = 10 = llAll1 

For llAll,, = max llAxllm, if wetake 

llxllm=

l 

we obtain 

llAxllm � u =: -�r:J m [ =�] m 

= max{l- 21, l-71, 191} = 9 = llAll,, 

We will use these 

observations 

in proving Theorem 
7.7. 

Section 

7.2 Norms and Di

Functions 

5 6 1  

stance 
we show that II Ax II :s M 

occurs. 

sum. If M represents 

Proof Of Theorem 1 . 1  The strategy is the same in the case of both the column 
sum 
and the row 
for all unit vectors 
It is im
whereas 

portant to remember 
for property (b) it is the max norm. 

the maximum value, 
unit vector 

x. Then we find a specific 

that for property (a) the vector 

x for which equality 

norm is the 
sum norm 

absolute 

column 

sum, and 

let 

; = I ,  ... ' n 

(a) To prove (a), let M = . max {II a1 II J, the maximum 
llxll, = 1. Then lx1I + ··· + lxnl = 1, so 
:S lx1l lla1ll, + ·  ·  · + lxnl llanll, 
:S lx1IM + ·  ·  · + lxnlM 

llAxll, = llx1a1 + ·  ·  · + Xnanll, 

(lx1I + ·  ·  · + lxnl)M = l ·M = M 

If the maximum 

Therefore, 
llxll,=1 
(b) The proof 

column 

absolute 

sum occurs in column k, then with x = ek we obtain 
llAekll, = llakll, = M 

llA 111 = max llAxll, = M  = . max {II a1ll,}, as required. 
of property (b) is left as Exercise 32. 
formula for llA 111 or llA IL,,. 

will discover a formula 
as the 

norm llA 112, although 

; = l, ... , n 

tionally feasible 

for the operator 

7.4, we 

In Section 

it is not as 
computa

Number of  a Matrix 

The  Condition 
In Explora
tion: 
of an ill-cond
matrices

itioned system 

Lies My Computer 

of linear 

Told Me in Chapter 2, we encountered 

equations. Here is the 

definition 

. 

the notion 

as it applies to 

Defi n ition A matrix A is ill-conditioned if small changes 
produce 
of A produce 
well-condition

changes 
in the solutions 
only small changes 
ed. 

to Ax = b. If small changes 

in the entries 
to Ax = b, then A is called 

in its entries 

in the solutions 

large 

can 

Although 

the definition 
applies 

to arbitrary 

matrices, 

we will restrict 
our atten­

tion to 

square matrice

s. 

Show that A = [ � 1 ] is ill-conditioned. 
1.0005 
= [3 ] , then the 
3.0010 
A' = [� �.0010] 

Solution If we take b 
if A changes 

to 

solution to Ax= b is x = [21]. However, 

Example 1 . 2 0  

5 6 2   Chapter 

7 Distance 

and Approximation 
�  then the solution changes 
to x'  = [�].(Check these asser
1.0005 =  0.0005, 

change 
or 100%, in x1 and (1 -2)/2 = -0.5, or - 50%, in x2• Hence, 

or about 0.05%, 

of 0.0005/

causes 

a relative 

tions.) Therefore, 
a change 
of (2 - 1) / 1 
= 1, 

A is ill-conditioned4 

We can use matrix norms to give a more precise 

way of determining 

when 

a matrix is ill-conditioned. 
in turn, 
introduces 
an error 
x' = x +Ax. In Example 

Think of the cha
Ax in the solution x to Ax= b. Then A' = A  + AA and 

nge from A to A' as an error 

AA that, 

7.20, 

AA = [0 O ] and Ax = [ 1] 

0 0.0005 - 1  

Then, since 
canceling 

Ax= b and A'x' = b, we have (A + AA) (x + Ax) = b. Expanding 

and 

off Ax= b, we obtain 
A(Ax) + (AA)x + (AA)(Ax) = 0 or A(Ax) =  -AA(x +  Ax) 

Since 
can rewrite 

that Ax = b has a 
we are assuming 
tion as 

the last equa

solution, 

A must be 

invertible

. Therefore, 

we 

(using a matrix norm that is compatible 

with a vector 

-A-'(AA)x' II = l l A-'(AA)x' II 

II Axil  11

of both sides 

Taking norms 
norm), we have 

s; l l A-'(AA) 11 ll
s; l l A-'ll ll
�  (What is the justification 
we define cond(A) = oo. 

The expression 
cond(A). If A is not 

x' II 
AA ll llx'll 

llA-' 11 l l A  II is called 

invertible, 

What are we to make of the inequality 

for each step?) Therefore, 

the condition number of A and is denoted 

by 

just above? The  ratio 

II M II/ llA II is a 

measure of the relative change in the matrix A, which we are assuming to 
be small. 
Ax = b (although, 
Similar
created in the solution to 

ly, II Ax II/ II x' II is a measure of the 
one, x). Thus, the inequality 
original 

relative 
error 
relative 
is measured 

new solution, 
x', 

the error 

in this case, 

not the 

to the 

� s; cond(A)lfAll 

II Axil  llAA ll 

(1) 

an upper bound on 

gives 
of the relative 
error 
more ill-conditioned 
relative 

solution. 

to the 

how large 

the relative 

in the coefficient 
the matrix, 

matrix. 
there is 

since 

in the solution can be 

error 
The larger 
more "room" for the error 

the condition 

number, the 
to be large 

in terms 

Section 

7.2 Norms 

and Di

stance 

Functions 

5 6 3  

Remarks 

•  The condition 
•  For any 

commonly used norms are the operator norms l l A  111 and llA II,,,. 

norm, cond(A) 2: 1. (See Exercise 45.) 

number of a matrix depends 

on the choice 

of norm. The most 

Example 1 . 2 1  

Find the condition 

to the oo-norm. 

Solution We first compute 

1.0005 

number of A  = [ � 1 ] relative 
-2000] 2000 
=  8004. 

A-I = [ 2001 

row sum), 

-2000 

Therefore, 

in the oo -norm (maximum absolute 

llA lloo  =  1 + 1.0005 = 2.0005 and l l A-111,,,  = 2001  + l - 20001 =4001 

socond,,,(A) = llA-1ll"'llA ll"' = 4001(2.0005) 

It turns out that if the condition 

7.20 and 7.21, cond1 (A) =  8004, 
cond2 (A) =  8002 (relative to the 2-norm), and condp (A) =  8002 (relative 

matrix norm, it will be large 
it can be shown that for matrix A in Examples 

relative 
matrix norm. For example, 

number is large 
to any compatible 

relative 

to one compatible 

to the 

Frobenius 

norm). 

of lteralive Melhods 

method 

2.5, we explored 

The convergence 
In Section 
tions: Jacobi's 
out proof 
methods 
theorem. 
vergence 

that if A is a strict
converge 
Indeed, 
properties of various 

two iterative 

methods 

for solving 

equa­
oflinear 
method. In Theorem 2.9, we stated 
with­

a system 

and the Gauss-Seidel 

ly diagonally dominant n X n matrix, then both of these 
to the solution of 
establish 
one of the 
important 

Ax = b. We are now in a position to prove this 

norms is to 

the con­

uses of matrix 
iterative 
methods. 

We will deal only with Jacobi's 

method here. 

using similar techniq
in terms of matrices

handled 
write 
mind. The system 

the iterative 
process 
of equations 
is 

ues, but it 

requires 

(The Gauss-Seidel 
a bit more care.) The key is to 

method can 

be 
re­

. Let's 

revisit Example 

2.3 7 with this in 

so 

A  = [7  -l] and b = [ 5] 

3 - 5  

- 7  

(2) 

5 6 4   Chapter 

7 Distance 

and Approximation 

We rewrote 

Equation 
(2) as 

x =---1 7 

5  + Xz 

Xz = 

(3) 

which is equivalent 

to 

Xz +  5 

(4) 

or, in terms 

Study Equation 
entries 
entries 

of matrices, 

(5) carefully: The matrix on the 

[� -�J [;:] [ _� �J [;:] + [ _�J 
[7 -1J [o oJ [7 oJ [o -1J 
3 - 5  3 0  0 - 5  0  0 

of A, while on the right-hand side we see the negative 
of A and the 

left-hand side contains 
l 

b. So, if we decompose A as 

+  = L + D +U 

the diagona
of the off-diagonal 

A =   = 

vector 

(5) 

+ 

then Equation 

(5) can be 

written 

as 

or, equivalentl

y, 

Dx =  -(L  +  U)x + b 

since 
the matrix 
It is easy to see 
A = L + D + 
portions 
ten in the form 
of Equation 
c = D-1b so that Equation 

D is invertible. 
that we can do this in general: 
U, where D is the diagona
of A below and above the 
(6), provided 
(Why?) To simplify the 
(6) becomes 

�  diagonally dominant. 

(6) is the matrix version 

(6) 
(3). 
of Equation 
as 
A can be written 
l part of A and L and U are, respectively, 
the 

An n X n matrix 
The system 
Ax = b can then be writ­
D is invertible-which 
notation,let'sletM = -D-1(L + U) and 

it is if A is strictly 

diagonal. 

Equation 

(7) 

x = Mx + c 

1 into the 

right-hand side 

Recall how we use 

this equation in Jacobi's method. 

1 = Mx0 + c. Then we plug x

x2 = Mx1 + c. In general, 
we have 

is, x 
second iterate 

x0 and plug it into the right-hand side of Equation (7) 
fork 2 0. For Example 
M = -D-1(L +  U) =  -

[7 oJ-1[0 lJ 0 - 5  3 0 

2.37, 

we have 

with an initial 

We start 
vector 
to get the first iterate 

of Equation (7) 

x1-that 
to get the 

(8) 

and 

so 

Functions 

5 6 5  

7.2 Norms 
Section 
and Di
stance 
Xi= [1 �][�] + [tJ = [tJ = [�::��] 
5 0 1.400 5  1.829 
+ [;] = [0.914
Xz = [� �] [0.714] 
] 
we did in Example 2.37, but writ­

same calculations 

are exactly the 

and so 
on. (These 
ten in matrix 

form.) 

To show that Jacobi's 

method 

will converge, we need to show that the iterates 

approach the actual 
xk - x approach 

solution x of Ax= b. It is enough 
the zero vector. From our calculations 

to x = Mx + c. Using Equation (8), 

to show 
above, 

we then have 

that the 
error vectors 
Ax = b is equivalent 

xk 

xk+1 - x = Mxk + c - (

Mx + c) 

= M(xk - x) 

norm of both sides 

Now we take the 
tant which norm we use as long as we choose 
a vector 

norm.) We have 

of this equation. 

(At this point, it is not impor­

a matrix norm that is compatible 
with 

Ifwecanshowthat llMll < 1, then we will have llxk+i - xii < llxk - xii forallk 2'. 0, 
xk - xii  (9) 

llxk+J - xii = llM(xk - x)ll:::; llMll ll

zero, 

strict 

vectors 

so the error 

diagonal 

xk - x approach 
values 

The fact that 

to choose. If A = [aij] ,  then 

and it 
follows 
the zero vector. 

that 11 xk - x 11 approaches 

dominance is defined 
in terms of the absolute 

of the entries 
tor norm induced 

in the rows of a matrix suggests 
by the max norm) is the one 

[-a,�/a,, 

that the oo- norm of a matrix (the opera­
-a12f a11 0 

-a,,/a., l -a2nf a22 0 
�  (verif
y this), so, by Theorem 7.7, 
Suppose it occurs in the kth row. Then l- ak,k+11 1-aknl 
+· · ·+--
< 1 

llMll"' is the maximum abs

lakil + · · · 

row sum of M. 

-an2/ann 

M =  . 

-an1/ann 

olute 

+ 

Example 1 . 2 2  

Compute llM 1100 in Example 2.37 

to approximate 

and use this value 

to find the 

decimal-place 
round­

accuracy (after 

number of itera
tions 

required 
ing) if the initial 

the solution to three-
is x0 = 0. 

vector 

akk 

+ lak,k-11 + lak,k+1I + · · · 

+ laknl �����������������
diagonally dominant. Thus, llMll"' < 1, so llxk - xii --+ 0, as we 

I akkl 

akk 

since A is strictly 
wished to 
show. 

5 6 6   Chapter 

7 Distance 

property 

that Jacobi's 

computed 
Example 
2.37, 
converges in 
decimal places 

Solulion We have already 
(implying 
method 
mate solution xk will be accurate to three 
has  the 

and Approximation 
M = [ � �], so II M IL,, = t = 0.6 < 1 
�  (Why?) Thus, we need only guarantee 
llxk -x l lm :S l l M l l"'llxk-1 -x l lm :S l l M l l;llxk-2 -x l lm :S ·  ·  · 
:S llM ll�llxo -x l lm 
Now llM ll"' = 0.6 and llxo -x l lm =  llxo - X111m = l l x111m =II[�::��] t = 1 .4, 

approxi­
xk - x 
vector 
in absolute 
value. 
that the maximum absolute component of 

xk -x is less than 0.0005. 
that 

that each of its components is less than 0.0005 

as we saw). The 
if the error 

words, we need to find the 

II xk -x i i  m < 0.0005 

Using Equation 

value of k such 

(9) above, 

smallest 

In other 

we see that 

so 

(If we knew the exact 
this is not 
Therefore, 

the case, 
we need to find 

k such that 

so we use an approximation 
to the 

solution in advance, we could use it instead 

solution, 

as we have done here.) 

of x1. In practice, 

We can solve 

log10((0.6)k(l.4)) < log10(5 X 10-4) ==> klog10(0.6) + log10(1.4) < log105  - 4 

(0.6)k(l.4) < 0.0005 

logarithms 

10) of both sides. 

this inequality 

by taking 

We have 

(base 

::::? -0.222k 
::::? k > 15.5 

+ 0.146 < - 3.301 

k must be an integer
of Jacobi'

Since 
that 16 itera
tions 
this example. (In fact, it appears from our calcula
get this degree 
estimate.) 

conclude that k = 16 will work and 
in 
will give us three-decimal-place 
accuracy 
2.37 that we 
was only to come up with an 

, we can therefore 
s method 

sooner, but our goal here 

of accuracy 

in Example 

tions 

I Exercises 
.. 
In Emdm 1-3, /,tu� [ �:l 

andv � [-H 4. (a) What does d,(u, v) measure? 

1 . 2  

(b) What does dm(u, v) measure? 

1. Compute the Euclidean 

norm, the sum norm, and the 

max norm of u. 

max norm of v. 

2. Compute the Euclidean 

norm, the sum norm, and the 

3. Computed( u, v) relative 
sum norm, and the max norm. 

to the Euclidean 
norm, the 

1  0  0 l ]  rand 

u = [ 1  0 
5 and 6, let 
In Exercises 
v = [ O  1  1  0  1  1 
l ] �  
5. Compute the Hamming 
norms of 
u and v. 
6. Compute the Hamming distance 
between u and v. 
7. (a) Forwhichvectors
m? Explain 

E = l l v l l

v is l l v l l

your answer. 

7.2 Norms and Di

Functions 

5 6 1  

Explain 

your answer. 

answer. 

llu + vii,= llu ll,+ ll v l l,? Explainyouranswer. 

v is llvll, = llvll m? Explain 
your 
v is llvll, = llvllm = llvllE? 

on u and v is II u + v i i  E = 

conditions 

(b) For which vectors 

(b) Under what 

your answer. 
on u and v is 

8. (a) Under what conditions 

(c) For which vectors 
II u II E + II v II E? Explain 
(c) Under what conditions 
II u + v i i  m = llull m + llvll m? Explain 
9. Show that for all v in IKr, II v II m ::::; II vii E· 
10.Showthatforallvinll�r, llvllE s llvll,. 
1 1 .  Show that for all v in !Rn, llvll, ::::; nllvllm· 
12. Show that for all v in !Rn, llvllE::::; Vn llvll m· 
in IR2 relative 
13. Draw the unit circles 

to the sum norm 

answer. 

on u and v is 

your 

and the max norm. 

0 

33 in 

37.A  = 

duct. 

the identity 

defines a norm on the 

15-18, prove 

from any inner pro

14. By showing that 

of Exercise 
Section 7.1 fails, show that the sum norm does not 
arise 

16. V = Mmn' llAll = max{laijl} 

In Exercises 
vector 

space V 
that II II 
15. V =  IR2, 11[�]11 = max{l2al, l3bl} 
1,j 
� 17. v = C{b[O, IL llJll = r IJ(x)I dx 
O:sx:s 1 
18. II! II = max If (x) I 
s 20-25, compute llA II p, llA II 1, and llA lloo-
20.A  = [� �] 21. A =  [_� -�] 
22.A  = [_� -�J 23.A  = [: 3 �] 
u - 5  -:] [: -2 -�] 

25.A  = - 1  
-3 

19. Prove Theorem 
7.5(b). 

In Exercise

24.A  = 

-4 

26-31,findvectors

Section 

stance 
x andywith llxll, = 1 and 
InExercises
llrllm = 1 such that llAll1 = llAxll,and llAlloo = llArllm> 
exercise. 
where A is the matrix in the given 
21 28. Exercise 
26. Exercise 
22 
29. Exercise 
24 31. Exercise 
25 
32. Prove Theorem 7.7(b). 
33. (a) If II A II is an operator 

norm, prove that II Ill = 1, 

20 27. Exercise 
23 30. Exercise 

(b) Is there 

matrix. 

where I is an identity 
a vector 
Frobenius 
or why not? 

norm that induces 
norm as an operator 
norm? Why 

the 

with a 

,\ of A. 

itioned. 

the given 

matrix is ill-cond

. Prove that 

35-40,find cond1(A) and condoo(A). State 

vector 
norm 11x11
eigenvalue 

In Exercises 
whether 

34. Let llA II be a matrix norm that is compatible 

11 A 11 2: I,\ I for every 
35.A  = [! �] 36. A =  [_� -!] 
[� �.99] [ 150 200] 
[i 1 �] [i 1 !l 
41. LetA  = [1

(a) Find a formula for cond00(A) in terms of k. 
(b) What happens 
1 ?  

to cond00(A) ask approaches 
Ax = b, where A is invert­
the linear 
system 

39.A  = 5  40. A =  1 3 

2 
1 4 

l]. 

3001 4002 

42. Consider 

0 
k
1 

38. A =  

Suppose an error Lib changes 

ible. 
that is, 
Let x' be the solution to 
Ax' = b'. Let x' = x + Ax so that Ax represents 
the resulting 
Show that 

in the solution of the system. 

the new system; 

error 

b to b' = b + Lib. 

norm. 

for any 

matrix 

compatible 

lliixll  lliibll 

43.LetA = [10  10] andb  = [100]. 

W ::::; cond(A)lfbll 
10 9 99 
to A '  = [ 1 O 
(1) 

(a) Compute condoo(A). 
(b) Suppose A is changed 
change 
Ax = b? [Hint: Use inequality 

large 
the solution to 
from this section.] 

10] . How 

1 1  
produce in 

can this change 

a relative 

10 

a 

IA1I 

Show that 

42.] 

using 

matrices, 

band b' and 

then 
to any 

and smallest 

determine 

can this change 

matrix and let 

the actual 

(e) Solve the systems 

47. Let A be an invertible 

relative 
solution 

A and A '  and determine 

using 
relative 
error. 

46. Show that if A and B are invertible 

(d) Suppose b is changed to 
101 

cond (A) 2:: -IAnl 

the actual relative error. 

(a) Compute cond1(A). 

change 
produce in the 
to Ax = b? [Hint: Use Exercise 

cond(AB) :s cond(A) cond( B) with respect 
matrix norm. 

A1 and An be the 
with the largest 
eigenvalues 
absolute 
val­
ues, respecti
vely. 

5 6 8   Chapter 
(c) Solve the systems 

7 Distance 
and Approximation 
b' = [ 100]. How large 
44. Let A � [ � _ � � l '"d b � m 
to A' = [ l� 1 
- 1  nHow 
to b' � []How l"g" 

of Equation 
method that 
estimate the number of iterations 
will be needed to approximate the solution 
to three-decimal­
place 
exercise 
the solution 
from Section 
computed in the given 
49. Exercise 3, Section 
2.5 
1, Section 
48. Exercise 
2.5 
50. Exercise 
4, Section 
2.5 
51. Exercise 5, Section 
2.5 

48-51, write the given system in the form 
(7). Then use the method of Example 
7.22 to 
Xo = 0.) Compare your answer with 
2.5. 
Exercise 52( c) refers to the Leontief model of an open econ­
2.4 and 3.7. 
(a) Prove that An---+ 0 as n---+ oo. 
(I -A)-1 = I+ A + A2 + A3 + · · · 
( c) Show that (b) can be used to prove Corollaries 

cAs In Exercises 

(c) Solve the systems using 

that llAll < 1, where 
the norm is either the sum norm or the max norm. 

omy, as discussed in Sections 
52. Let A be 

change 
to Ax = b? [Hint: Use Exercise 

large 
the solution 
from this section

(b) Deduce from (a) that I - A is invertible 

to Ax = b? [Hint: Use inequality 

45. Show that if A is an invertible 
matrix, 
then 

(e) Solve the systems using 

band b '  and determine 

relative 
solution 

[Hint: 
Section4.3.] 

A and A' and determine 

an n X n matrix such 

(b) Suppose A is changed 

(d) Suppose b iS<h<mg<d 

produce in 
(1) 

the actual relative 

[Hint: See the 

See Exercise 34 and 

accuracy. (Use 

can this change 

can this change 

of Jacobi's 

a relative 

relative 

Theorem 4.18(b) in 

of Theorem 

the actual 

produce in the 

3.34.] 

error. 

error. 

change 

proof 

and 

42.] 

.] 

5 

cond(A) 2:: 1 with respect 

to any matrix norm. 

3.35 and 3.36. 

II  Least S q u a res Approximation 

experimental 
being 

data are used to infer 
For example, 

measured. 
in time and try to deduce 

a function 

we might measure 

that expresses 

the height 
the tree's 

a mathematical 

rela­

among the variables 

of science, 

In many branches 
tionship 
of a tree at various 
height 
try to find a rule that relates 
for example, 
business; 
relationship 

points 

h in terms of time t. Or, we might measure 
producing 

the size 
p to t. Relationships 
between 
costs c and the number n of widgets produced. 

p of a population 
variables are 
also of interest in 
may be interested 
in knowing 
the 

a company 
its total 

over time and 

between 

widgets 

variable 

In each of these examples, the 

one for the independent 
we have a set of 
Thus, 
approximates 
variable 
along 

data point
7.9 shows examples 
in which experimental 
"fits" 

and one for the 
s (x;, y;), and we are 

with a curve that approximately 

e. 
for a function 
x and the dependent 
data points are 

data come in the form of two measurements: 
(supposedly) dependent 
variabl
that best 

p between the independent variable 

the relationshi

y. Figure 

looking 

the data. 

plotted, 

Section 

7.3 Least 

Squares 

Approximation 

5 6 9  

y 

y 

y 

figure 1 . 9  Curves of"bes

t fit" 

of least 

squares, 

The method 

which we are about to consider

to 
Ceres, was discovered on New Year's Day, 1801, but it disap­
observed. Astronomers 
shortly after 
calcula

( 1682-1716) was an 
Roger 
Cotes 
who, while 
English mathematician 
edited 
a fellow 
at Cambridge, 
the 
second 
edition 
of Newton's 
he published 
cipia. Although 
little, 
impor
he made 
tant 
in 
discoveries 
the theor
hms, 
integral 
y oflogarit
calculus, 
and numerical 
methods. 

Gauss. A new asteroid, 
peared behind 
it was 
and where Ceres would reappear, but their 
tions 
done, independently, by Gauss. Ceres reappeared on 
actly 
it would be. Although 
ods at the time, 
descri
anticipated the method 
on it in 1806. Neverthele
squares 

bed in a paper in 1809. The same method 
in the early 18th century, and Legendr
ss, Gauss is generally given credit 

almost 
disclose his meth­
approximation method, 
which he 
was actually known earlier; Cotes 

December 7, 1801, 
he did not 

where Gauss had predicted 

e published 
a paper 
of least 

Gauss had used his least 

greatly from those 

, is attributed 

for the method 

predicted 
when 

the sun 

differed 

mation. 

approxi

squares 

ex­

Prin­

We begin our exploration of approximation with a more general 

result. 

510  Chapter 

7 Distance 

and Approximation 

are many problems 

T h e  Best Approximation Theorem 
In the sciences, 
there 
to X of type Y?" X might be a set of 
the best approximation 
things, 
a vector, or many other 
tor belonging 
to a certain vector 
finding 
is closest 

space, 
W of a vector 
problem 

the vector 
to) a given 

space 
gives 

w in a subspace 

v in V. This 

while Y might be a particular 

data points, a function, 
type of function, 
etc. A typical example 
of such a problem is 

V that best approximates 
rise to the following 

definition. 

that can be phrased 

generally as "What is 

(i.e., 

vector 

a vec­

Defi n ition If W is a subspace 
in V, then 

the best approximation to v in W is the vector v in W such that 

of a normed 

linear 

space V and ifv is a vector 

for every 

vector w in W different 

llv -vii < llv - wll 
from v. 

In IR2 or IR3, we are used to thinking 

"perpendicular 
notion 
we expect pro

distance
distance:' 
In algebraic terminolo
of !Rn and v is a vector 
jection: 
to v (Figure 7.10). 

of orthogonal pro
jw(v) to be the vector 

gy, "shortest 

If W is a subspace 

distance" as correspond
in !Rn, then 

ing to 

of "shortest 

in W that is closest 
in any inner 

can be defined 

product space, 

we have 

Since 

orthogonal 

projection 

the following 

theorem. 

" relates to the 

l lv -vii 

w 

w 
Ifv = projw(v), then 
llv - vii< llv -wll forallw =fa v 

Figure 1 . 1 0  

Theorem 1 . 8  

The Best Approximation 

Theorem 

product space 
If W is a finite-dimensiona
tor in V, then projw(v) is the best approximation 
to v in W. 

l subspace 

of an inner 

V and if v is a vec­

from projw(v). Then projw(v)  - w is also 
Proof Let w be a vector in W different 
in W, so v -projw(v) = perpw(v) is orthogona
l to projw(v) -w, by Exercise 
that 
Section 
Theorem now implies 

7.1 .  Pythagoras' 

43 in 

llv -projw(v)ll2 + llprojw(v) - wll2 = ll(v -projw(v)) + (projw(v) - w)ll2 

= llv - wll2 

7.3 Least 

Squares 

Approximation 

5 1 1  

Section 
llprojw(v) - wll2 > 0, since 

Example 1 . 2 3  

as Figure 

7.10 illustrates. However, 
llv -projw(v)ll2 < llv -projw(v)ll2 + llprojw(v) - wll2 = llv - wll2 

w * projw(v), so 

or, equivalentl

y, 

llv -projw(v)ll < llv - wll 

Let u, � u J u, � [ -n and F  m Hnd th< b"t 
(u·v)  (u·v) 

plane W = span(u1, u2) and find the Euclidean 
Solution The vector 
ates v is projw(v). Since 
orthogona

projw(v) = _I_ U1 + _2_ U2 

in W that best approxim

U1 • U1  U2 • U2 

distance 
from v to W. 

l, 

u1 and u2 are 

apprnrimation 

to v in th< 

The distance 
distance 

is just llperpw(v)ll = llv -projw(v)ll· We compute 

from v to W is the 

distance 

from v to the point in W closest 

to v. But 
this 

so  llv -projw(v)ll = \/02 + (lf-)2 + es1)2 =Vilt- = 12Vs/5 

which is the distance 
from v to W. 

when we explore 

In Section 

7 .5, we will look at other 

examples 
the problem of approximating 
s. 
of a vector 

projection 

Remark The orthogonal 

of the Best Approximation 
function

Theorem 

v onto a subspace 

W is defined 

in terms of an orthogonal 
alternative 
can be only one vector 

that projw(v) does not depend 
in W that is closest 

basis for W. The Best Approximation 
Theorem 
of this basis, 

on the choice 
to v-namely, projw(v). 

us an 
gives 
since 
there 

proof 

a curve that "best 

problem of finding 
however, 

Leasl Squares Approximalion 
We now turn to the 
fore we can proceed, 
the data points (1, 2), (2, 2), and (3, 4) have arisen 
some experiment. Also 
related by a linear 
that is, we expect the 
tion y = a + 
equation and we would have 

suppose we have reason 
to believe 
points to 
were accurate, all three 

bx. If our measurements 

function; 

fit:' Suppose 
taken during 

from measurements 

that the x and y values 

are 
lie on some line with equa­

points would satisfy this 

we need to define what we mean by "best 

fits" a set of data points. Be­

2 =a + b· l  2 =a +b · 2  4 =a + b· 3  

512  Chapter 

7 Distance 

and Approximation 

This is a system 

of three linear 

equations 

in two variables: 

a +  b = 2 
a + 2b = 2 or 
a +  3b = 4 
this system 

is incons
tle for a line that 

not  lie 
comes "as close as possible" to passing 

(since the three 

points  do 

istent 

Unfortunately, 
straight line). So we 
through 
point to the line 
choose the line 

(representing 
that minimizes 

our points. For any line, 

will set

we will measure 
the errors 
the total 

in they-direction
error. 

Figure 

the vertical distance 

from each data 

7 . 1 1  illustrates 

the situation. 

), and then we will try to 

on a 

y 6 5  y = a +  bx 
4 3 2 �-1-�+-�+--�1-----i�--+�--+�--+---+
X 
- 1   2 3 4 5 6 
ei + e� + e� 
Figure 1 . 1 1  Finding 
that 
the line 
minimizes 

- 1  

If the errors 

are denoted 

by 81, 82, and 83, then we can form the 

error vector 

We want e to be as small 

as possible, so llell 

to zero as possible. 
Which 
n norm is the best choice. 

errors 

must be as close 

Euclidea

sum of the 

we use? It turns 

since II ells = I 81 I + I 82 I + I 83 I is the 
norm should 
out that the familiar 
(The sum norm would also be a sensible choice, 
actual 
in Figure 7 . 1 1 .  
However, 
work with, and, as you will soon 
very nice formulas.) So we are going to minimize 
2 = 8i + 8� + 8� 
llell 
ly, llell
= V 8i + 8� + 8� or, equivalent
The number llell 

This is where the term "least 
squares" 
of squares, in the sense 
of the foregoing 
squares error of the approxima

the absolute 
are hard to 
of the Euclidean 

comes from: We need to find the 

see, the choice 

smallest 
the least 

value signs 

is called 

equation. 

tion. 

sum 

norm leads to some 

Section 

7.3 Least 

Squares 

Approximation 

513 

From Figure 7.1 1 ,  we also obtain 

the following 

formulas 

for 81, 8z, and 83 in our 

example: 

81 =  2  -(a + b · 1) 8

=  2  -(a + b · 2) 83 = 4 -(a + b · 3) 

z 

Example 1 . 2 4  

gives the 

smallest 

least 

squares 

error 

for the data 

points 

Which of the following 
lines 
( 1 ,  2), (2, 2), and (3, 4)? 
(a) y = 1  + x 
(b) y = - 2  + 2x 
(c) y = � +  x 

Solulion Table 7.1 shows the necessary calcula

tions. 

y =I +x 

y = - 2  + 2x 

Table 1 . 1  

2 - (1+2)= -l 

81 
8z 
83 

2 -( - 2  + 2) = 2 

2 - (1+1 )= 0 
4 -( 1  + 3) = 0 

8i + d + 8� oz + ( -1 )  z + oz = 1  2z +oz+ oz= 4 (t)Z + ( -t)Z + (t)Z 

2 -( - 2  + 4) = 0 
4 -( - 2  + 6) = 0 

llell 

v1=0.816 

2 -(t + 1) = t 2 -(t + 2) = -t 
4 -(t + 3) = t 

= t 

2 

y 

Figure 1 . 1 2  

We see that the line y = t + x produces the smallest 

7.1 2  shows the data points and 

least 
all three 

lines. Figure 

three 

these 

error 

among 

squares 
lines. 

514  Chapter 

7 Distance 

and Approximation 

It turns 

error of 
this section 

any line, 

out that the line y = � + x 

in Example 
through 
to illustrating 
why this is 

7 .24 gives 
none of the 
so. 

it passes 

even though 

is devoted 

the smallest 
given points. The rest 
of 

least squares 

In general, 

suppose we have

n data poi

nts (x1, y1), . . .  , (xn, Yn) and a line y = a + 

bx. 

Our error 

vector 

is 

[�l l e= : 

Sn 

;). The line y = a + bx that 
minimizes 

wheres;= y; - (a + bx
the least squares approximating 
(xn,Yn). As noted 
to Example 
points were actually 
the given 

prior 

on the 

line (or the line of best fit) for the 
7.24, 

points (x1, y1 ), . . .  , 
in matrix 

this problem 

we can 

express 

line y = a + 

bx, then the n linear 

form. If 
equations 

sf + · · · + s� is called 

a +  bxn = Yn 

would all be true (i.e., 
where the points are 
form, we have 

not colline

the system 

would be consistent). Our interest 
is inconsistent. 

ar, in which case 

the system 

is in the case 

In matrix 

Ax = b, where 

�  The error 

which is of the form 

A = [ : :: l' x = [: l 
h = [;: l 
Defi n ition If A is an m x n matrix and b is in !Rm, a least squares solution of 

e is just b -Ax (check this), and we want 

y, llell· We can therefore 

llell2 or, equiv­
as follows. 

in terms of matrices 

Ax = b is a vector x in !Rn such that 

to minimize 

our problem 

rephrase 

vector 

alentl

1 Xn 

Yn 

llb -Axil :::; llb -Axil 

for all x in !Rn. 

Section 

7.3 Least 

Squares 

Approximation 

515 

Solulion 
Any vector 

of lhe Leasl Squares Problem 
of the form Ax is in 
space 

the column 

tors in IJ�r, Ax varies 

over all vectors 
to a vector 

y in col(A) 

therefore 

equivalent 

such that 

in col(A). A least 

of A, and as x varies 

over all vec­
solution of Ax = b is 

squares 

lib -Yll ::; lib -Yll 

for ally in col(A). In other 
Best Approximation 
onto col(A). Thus, ifx is a least 

Theorem, 

the vector 
squares 

words, we need the closest vector 

in col(A) 
we want is the orthogonal 

to b. By the 
projection 

solution of Ax= b, we have 

of b 

(1) 

In order to find 
solve 

x, it would appear that we need to first compute projcol(A)(b) and then 
( 1). However, 
there 

is a better way to proceed. 

the system 
We know that 

b -Ax =  b -projcol(A)(b) = perpcol(A)(b) 

is orthogona
AT (b -Ax) = O, which, 

in turn, is 

equivalent 

to A Tb - AT Ax = O or 

l to col(A). So  b  - Ax 

is in (col(A)) _j_ = null(AT). Therefore 

This represents 
a system 

of equations 

We have just established that 

known as the 
of the 

normal equations for x. 
normal equations 

the solutions 
of Ax = b. This proves the 

cisely the least 
squares 
theorem. 

solutions 

for x are pre­

first part of the following 

Theorem 1 . 9  

Theorem 

The Least Squares 
Let A be an m X n matrix and let b be in !Rm. Then Ax = b always 
least squares 
solution 
squares 
a. x is a least 
AT Ax = A Tb. 
equations 
ly independent 
b. A has linear
columns 

x. Moreover: 
solution of Ax = b if and only ifx is a solution of the 
normal 

if and only if AT A is invertible. In this case, 

has at least 
one 

squares solution 

of Ax = b is unique 

and is 

given by 

the least 

Proof We have already establ
n columns 
and only if AT A is inverti
solution 

ly independent 
ble, by Theorem 3.28. 
x = (AT A)-1 A Tb. 

of AT Ax = A Tb is clearly 

of A are linear

ished 

property (a). For property (b), we note that the 
true if 

if and only if rank(A) = n. But this is 

If AT A is inverti

ble, then the unique 

516  Chapter 

7 Distance 

and Approximation 

Example 1 . 2 5  Find a least 

squares 

solution 

to the inconsistent 

system 

Ax = b, where 

Solulion We compute 

ATA  = [� 2 
Arb =  [� 

-:i[ � -�: 
[� 3�] 
-1 1 
2 -:i[�] 
[1!] 

- 2  

-2 

and 

The normal equations 

Ar Ax = A Tb are just 

which yield  x = [ 1]. The  fact  that 

this solution  is 

unique 

was guaranteed by 

Theorem 7.9(b), 
since 

the columns 

of A are clearly 

linearly independent. 

�  whose least 

mation to b in the 

squares 

Remark We could 

have phrased Example 

7.25 as follows: Find the best approxi­
give the system 

equations 

resulting 

A. The 

column space of 
solution we just found. (Verif
s of that linear 

combination 

y this.) In this case, 
columns 
of the 

Ax = b 
the components 
of A that produces 

of x are the coefficient
the best approximation to b-namely, 

Example 1 . 2 6  

This is exactly 

the result 

of Example 

7.23. 

Compare the two approaches. 

Find the least 
from Example 

squares 
7.24. 

approximating 

line for the data points (1, 

2), (2, 2), and 

(3, 4) 

Solulion We have alread

y seen that the correspond

ing system 

Ax = b is 

Section 

7.3 Least 

Squares 

Approximation 

511 

where y = a + bx is the line we 
pendent, 
squares 
Theorem. We 

will be a unique 
compute 

seek. Since the 
least 
solution, 

there 

columns 

of A are clearly 
linearly inde­
Least 

by part (b) of the 

Squares 

2 

2 

we can solve the 

normal equations AT Ax = A Tb, using 

Gaussian elimina
tion 

Hence, 
to obtain 

Sox =  [[],from which 

squares 

we see that a = t, b  = 
line: y = t + x. 

approximating 

1 are the coefficients 

of the least 

The line we just found is the line in Example 
squares 

produces 

the smallest 
that ifx is a least 

least 
squares 

claim 
that this line 
(2, 2), and (3, 4). Notice 
pute the least 
error 
as 

squares 

7.24(c), 

so we have justified 
for the data 
solution of Ax = b, we may com­

our 
points (1, 2), 

error 

llell = llb -Axil 

Since 
from b to the 

Ax = projcol(A)(b ), this is 
column space of 

just the length 
A. In Example 

of perpcol(A)(b )-that is, the 
7.26, 

we had 

distance 

of llell = v1 =  0.816. 

so, as in 

Example 

7.24(c), 

we have a least 

squares 

error 

Remark Note that the 

columns 

of A in Example 7.26 

are linearly independent, 

so (AT A)-1 exists, 
always 
to solve 
CAS do it for you!). 

easier 

and we could 

x as x = (AT A)-1A Tb. However, 
calculate 
using 

Gaussian 

elimination 
(or to let your 

it is almost 

the normal equations 

It is interesting 

view. On the one 
with corresponding 
Equivalentl
Figure 7.13(b). Here, 

y, we have the projection 

line y = t + x, 

points of 

of b onto the 
column 

7.26 from two different 

geometric 
squares approximating 

to look at Example 
hand, we have the least 
errors 

81 = L 82 = -t, and 83 =Las shown in Figure 7.13(a). 
P = projcol(A)(b) = Ax = [l

� �l][tl] [L!:ll 

space of 

A, as shown in 

518  Chapter 

7 Distance 

and Approximation 

y 

6 

5 

4 

3 

2 

-I 

- 1  

2 

3  4  5 

(a) 

Figure 1 . 1 3  

�  and the 

if the data points were collinear?]  0 

squares 

error 

least 

6 

(b) 

W =col( A) 

e  5 3 8 
p = 3 II 3 
vectorise = [;,31]. 
l [1  1]  [1] 

a+ b =
a + 2b  =  2 or 1  2 [a] 2 
a + 3b  =  2 1  3  b 
a + 4b  =  3 1  4  3 

approximating 

of the line we seek. 

Then, substi

we obtain 

[What would Figure 7.13(b) look like 

tuting 

the 

2 

So we want the least 

squares 

solution of Ax = b, where 

Since 

the columns of A are linearly independent, 

the solution we want is 

.-.... (Check this 
calcula

squares 

approximating 

x = (ATA)-IATb = [i] 
we take a = t and b  = 
line y = t + �x, as shown in Figure 7.14. 

tion.) Therefore, 

t producing the least 

Example 1 . 2 1  Find the least 

squares 
(1, 1), (2, 2), (3, 2), and (4, 3). 

Solulion Let y = a + bx be the equation 

four points into this equation, 

line and the least squares 

for the points 

error 

Section 

7.3 Least 

Squares 

Approximation 

519 

y 

5 

4 

3 

2 

2  3  4  5 

Figure 1 . 1 4  

Since 

the least 

squares 

error 

is llell = Vs/5 =  0.447. 

We can use the method 
than straigh
t lines. 

other 

of least squares to approximate 

data points by curves 

Example 1 . 2 8  

Find the parabola 
(O, - 1), (1, 

O), and (2, 2). 

that gives 

the best least 

squares 

approximation 

to the points ( -1, 1), 

Solution The equation 
the given points into this quadratic, we obtain 

of a parabola 

the linear 
system 

2
is a quadratic 
. Substi

tuting 

y = a  + bx + c:

a - b + c 

a 
a +  b + c 
a +  2b + 4c 

Thus, 

we want the least 

squares 

approximation 

of Ax = b, where 

5 8 0   Chapter 

7 Distance 

and Approximation 

We compute 

2 
6 
8 

so the 

normal equations 

are given by 

whose solution is 

Thus, the least 
squares 

approximating 

parabola 

has the 

equation 

as shown in Figure 

7.15. 

y = - fa - �x + x2 

y 

- 3  - 2  

3 

- 1  (0, - 1) 

- 2  

Figure 1 . 1 5  

A least 

squares 

approximating 

parabola 

squares 

uses of least 

One of the important 
with various 
of population 

process
growth. Recall from Section 

associated 
context 
ing (or decaying) exponentially satisfies 
p(t) is the size of the 
but k is not so easy to determine. 

It is easy to see 

es. The next example 

population at time t and c and k are constan

an equation of the form p(t)  = cit, where 
ts. Clearly, c = p( 0 ), 

is to 
approximation 
this application in the 

illustrates 
6.7 that a population 

estimate constants 

that is grow­

that 

which explains 
lation: 
It is the ratio 

of the growth 

why k is sometimes 

p'(t) 
p(t) 
to as the relative 
of the popu­
p(t). 

referred 
rate p' (t) to the size of the population 

growth rate 

k =­

Section 

7.3 Least 

Squares 

Approximation 

5 8 1  

CAS  Example 1 . 2 9  

the population 

Table 7.2 gives 
the 20th century. Assuming an exponentia
rate and predict 

the world's 

population 

of the world at 10-year 

intervals 
l growth model, find the 

for the second 
half of 
growth 

relative 

in 2010. 

Table 1 . 2  

Since 

is 1960, and so on. 

Solution Let's agree 
t =  1 
the population 
Population 
Year  (in billions) 
1950 
1960 
1970 
1980 
1990 
2000 

2.56 
3.04 
3.71 
4.46 
5.28 
6.08 

we use the 
method 
of both sides, 

How can 
logarithm 

is 

to measure 

time t in 10-year 

intervals 

c = p(O) = 2.56, 

the equation 

so that t =  0 
is 1950, 
for the growth rate of 

p = 2.56ekt 
squares 
the equation into a linear 
one: 

ofleast 
we convert 

on this equation? Ifwe take the 

natural 

Source: U.S. Bureau of the Census, Inter­

national 

Data Base 

Plugging 
have rounded 

in the values 

oft and p from Table 7 .2 yields the following system 
tions 

(where we 

to three 

calcula

ln p = ln(2.56ekt) 

= ln 2.56 + ln(it) 

= 0.94 + kt 

decimal places): 
0.94 = 0.94 

k = 0.172 
2k  = 0.371 
3k = 0.555 
4k = 0.724 
5k = 0.865 

1 
2 

0.172 
0.371 
A= 3 and b =  0.555 

4 
5 

0.724 
0.865 

We can ignore 
p(O) = 2.56). 

the first equation (it just corresponds 

The remaining 

equations 

correspond 

to a system 

to the initial 
Ax= b, with 

condition 

c = 

Since AT A = 55 and A Tb = 9.80, the 
single 

equation 

corresponding normal equations 

are just the 

55x = 9.8o 

9.80/55 = 0.178. Consequently, the least 

squares solution has 

Therefore, 

the form p = 2.56e0·173t (see Figure 7.16). 

k = x = 

The world's 

population 

in 2010 corresponds 
p(6) = 2.56e0·178(6J = 7.448 

_....  Earth in the year 

2010. 

will be "only" 

6.82 billion 

Thus, if our model is accurate, 
there 

(The U.S. Census Bureau estimates that the 

people on 
7.45 billion 
will be approximately 
global 
population 
er?) 

our estimate is high

Why do you think 

in 2010. 

tot =  6, from which 

we obtain 

and Approximation 

5 8 2   Chapter 
7 Distance 
VJ c::: .Sl �  5 
� 
:.s 3 4 
c::: 0 ·.=  3 
c:j 3 a. 0  2 
0... 

6 

7 

p(t) 

2  3  4  5  6 

Decades 

since 1950 

Figure 1 . 1 6  

o n  

Least Squares via the OB Factorizati
It is often the case that the normal 
equations 
ill-conditioned. 
Therefore, 
in a large 
tion will result 
error 
are usually 
other 
methods 
of A yields 
that the QR factorization 
It turns out 
approximation 

squares 
used to compute least 

in the least 

the least 

squares 

of Ax = b. 

a small numerical error in 

performing 

Gaussian elim
Consequently, in practice, 
approxima

solution. 
squares 

a more reliable 

tions. 
way of computing 

squares problem are 
ina­

for a least 

Theorem 1 . 1 0  

Let A be an m X n matrix with linearly independent 
columns 
If A = QR is a QR factorization 
least squares solution 
Ax= bis 

and let b be in !Rm. 
x of 

of A, then the 

unique 

Proof Recall from Theorem 5.16 that the QR factorization 
m X n matrix 
matrix R. From the Least Squares 

Q with orthono

and an  invertible 

A  = QR involves 

upper triangular 

an 

rmal columns 
Theorem, 
we have 
ArAx = Arb 

==> (QRfQRx = (QRfb 
==> RrQrQRx = RrQrb 
==> RrRx. = RrQrb 

...,.. since 

Qr Q = I. (Why?) 
Since 

R is invertible, 

so is Rr, and hence we 

have 

Rx= Qrb or, equivalently, 

x = R-1Qrb 

Remark Since 

directly than to invert R

R is upper triangular
and compute R-1Qrb. 

, in practice 

it is easier 

to solve 

Rx = Qrb 

Section 

7.3 Least 

Squares 

Approximation 

5 8 3  

Example 1 . 3 0  

Use the QR 

factorization 
to find a least 

Solution From Example 
5.15, 

A =  QR= 

We have [ 1/2 

QTb = 3 Vs/10 

- V6/6 

so we require 

the solution to 

0 

Vs 

1 

V6/6 

1  2 

Vs/10 

3Vs/10 

-2 
0 

- 1  0  1 

squares 

solution 

of Ax= b, where 

- 1/2 3Vs/10 
- 1/2 
1/2 Vs/10 V6/3 0 

A  = [-� � �] and b = [-�] 
- V6/�][: 
1/2 l 3Vs/2 
[ 1/2 
V6/2 [ 7 /2] - Vs/2 
Rx = Qrb, or [2  1 
1/2 l  [ 7 /2 l 
V6/2  - 2V6/3 [ 4/3] x =  3/2 

0  Vs 
0 0 
yields 

- 1/2 
Vs/10 
V6/6 

- 1/2 
3Vs/10 

3 Vs/2  x = - Vs/2 

-2V6/3 

0 

-4/3 

Back substi

tution quickly 

O rthogonal Proiecli
One of the nice byproducts 
thogonal 
of a vector 

projection 

of the least 

squares 
method 
of !Rm. 
onto a subspace 

o n  Revisiled 

is a new formula for the 

or­

Theorem 1 . 1 1  

Let W be a subspace 
basis 
is the vector 

for W. If v is any vector 

of !Rm and let A be 

in !Rm, then the orthogona

an m X n matrix whose columns 
of v onto W 

l projection 

form a 

The linear 
as its standard 

matrix. 

transformation 

projw(v) = A(ATA)-'Arv 
P :  !Rm� !Rm that projects !Rm onto W has A(ATA)-1Ar 

Proof Given the way we have constructed A, its column 
columns 
there 

space is W. Since the 
the Least Squares 
to Ax= v given 

ly independent, 
by 

of A are linear
least 

is a unique 

squares 

Theorem guarantees 

that 

solution 
x = (ATA)-1Arv 

5 8 4   Chapter 

7 Distance 

and Approximation 

By Equation 

(1), 

Example 1 . 3 1  

Therefore, 
Since 
as required. 
follows 
immedia

this equation holds 
tely. 

for all v in !Rm, the last statement of the theorem 

W in Ul' with 'q oation 

transfor­

mation onto W. 

We will 

l projection 

orthogona

l prnj,dion 

Find th, orthogona

illustrate 

5.1 1, we will take as a basis 

Solulion As in Example 

Example 
Theorem 7 . 1 1  by revisiting 
5.1 1 .  

ofv � [ -�] onto th, plan, 
x -y + 2z = 0, and give the standard matrix of the 
mirrn 
matrix  [ 1 - 1] 
A =  �  � 
1  [� �] 

with these basis 
vectors 

as its colum

We form the 

for W the set 

ns. Then 

1 

so 

By Theorem 
onto W is 

7.1 1, the 

standard 

matrix of the orthogonal 

projection 

transformation 

so the 

orthogonal 

projection 

of v onto 

W is 

1 

1 �] 

which agrees 

with our solution to 

Example 
5.1 1 .  

Section 

7.3 Least 

Squares 

Approximation 

5 8 5  

R e m a r k  Since 

the projection 

of a vector 

onto a subspace 

W is unique, 

the stan­

dard matrix 
on the 

of this linear 
of basis 
for W. In other 

tion (as given 
words, 

transforma

choice 
matrix A, but the matrix A(A T A)-1A Twill be the 
......,.. different 

with a different 

basis for W, we have a 
same! (You are asked to 

by Theorem 7 . 1 1) cannot depend 

verify this in Exercise 

43.) 

T h e  Pseudoinverse o f  a M alrix 
If A is an n X n matrix with linearly indepen
solution to 
the unique 
independent 
columns, then Ax = b has no 
is given 
by the unique 
therefore 
plays 

least 
of an "inverse of 
the role 

squares 

Ax = b is x = A -lb. If m > n and A is m  X n with linear

dent columns, then it is invertible, 

and 
ly 

exact 

solution x = (ATA)-1ATb. The matrix (ATA)-1AT 

but the best approxima
tion 

solution, 

A" in this situation. 

Example 1 . 3 2  

Defi n ition If A is a matrix with linear
ly independent 
matrix A+ defined 
pseudoinverse of A is the 
by 
A+= (ATA)-1Ar 

columns, then the 

Observe that if A ism X n, then A+ is n X m. 

Hnd the p<eudoinvem of A � [ : n 
have [ t -�][l 
2 �] 

Solution We have already done most of the 
previous work, we 

- 1  2  1 

calcula

tions 

1 

in Example 

7 .26. Using our 

The pseudoinve

rse is a convenient shorthand 

cepts 
columns, 

we have been exploring. 
solution 

the least 

squares 

For example, 

of Ax = b is given by 

notation for some of the con­
if A is m  X n with linearly indepen

dent 

and the standard 

matrix of the orthogonal 

projection 

P from !Rm onto col(A) 

is 

[P] = AA+ 

If A is actually 

a square matrix, then it is easy to show  that A+  =  A 

-l (see 
solution of Ax = b is the exact solution, 

Exercise 53). In this case, 
the least 
since 

squares 

5 8 6   Chapter 
7 Distance 
and Approximation 
�  The projection 

matrix becomes 
interpretation 
of this equality?) 
7.12 summarizes 

Theorem 

[P] = AA+ = AA-1 = I. (What is the geometric 
the key properties 
verify these 

of the pseudoinverse of a matrix. 
matrix in 

properties for the 

the proof of this theorem, 

.......... (Before reading 
7.32.) 

Example 

Theorem 1 . 1 2  Let A be a matrix with linearly independent 

properties, 
called 

columns. Then the pseudoinverse 
the Penrose conditions for A: 

the following 

A+ of A satisfies 
a. AA+A = A  
b. A+AA+ = A+ 
c. AA+ and A+ A are symmetric. 

Proof We prove condition 
remaining 
(a) We compute 

conditions 

as Exercises 
54 and 55. 

(a) and half of condition 

( c) and leave 

the proofs of the 

( c) By Theorem 3 .4, AT A is symmetric. 
Exercise 

46 in Section 

3.3. Taking 

the transpose of AA+, we have 

Therefore, (AT A )-1 is also 

symmetric, 

by 

AA+A = A((ATA) -1AT)A 
= A(ATA)-1(ATA) 
= AI =  A 

(AA+)T = (A(ATA)-1ATf 

= (ATf((ATA)-1fAT 
= A(ATA)-IAT 
= AA+ 

Exercise 

56 explores 

properties 

of the pseudoinverse. In the next section, 

we will see how to extend 
the columns of A are linear

further 
the definition 
ly indepen
dent. 

of A +  to handle 

all matrices, 

whether 
or not 

1 . 3  

..  I Exercises 
GAS 
1. y = -2  + 2x 2. y = x  3. y = -3  + & x 
4-6, consider the data points (-5, 3), (0, 3), 
(5, 2), and (10, 0). Compute the 
4. y =  3 -kx 
6. y =  2 -kx 

In Exercises 1-3, consider the data 
(3, 5). Compute the least 
In each case, plot the points and the line. 

squares error 
In each case, plot the points and the line. 

points (1, O), (2, 1), and 

In Exercises 

line. 

least 

given 

squares error for the given line. 

for the 

7-14, find the least 

In Exercises 
for the given points and compute the 
squares error. 

squares approximating line 
g least 

correspondin

7. (1, O), (2, 1), (3, 5) 
8. (1, 6), (2, 3), (3, 1) 
9. (O, 4), (1, 1), (2, O) 
10. (O, 3), (1, 3), (2, 5) 
1 1 .  (- 5, - 1), (O, 1), (5, 2), (10, 4) 

Section 

7.3 Least 

Squares 

Approximation 

5 8 1  

5), (4, 3), (5, 

15-18,find the least 

12. ( -5, 3), (0, 3), (5, 2), ( 10, 0) 
13. ( 1 ,  1), (2, 3), (3, 4), (4, 5), (5, 7) 
O) 
14. ( 1 ,  10), (2, 8), (3, 
In Exercises 
squares approximating 
parabola for the 
given 
15. ( 1 ,  1), (2, -2), (3, 3), (4, 4) 
16. (1, 6), (2, O), (3, O), 
17. (-2, 4), ( - 1, 7), (O, 
18. (-2, 0), ( -1, - 1 1), (0, - 10), (1, - 9), (2, 8) 
In Exercises 
by constructin

19-22, find a least 
g and solving the 

(4, 2) 
3), (1, O), (2, - 1) 

points. 

squares solution 
normal 

equations. 

1 , 

27.A = 

28.A = 

- 1  'Q =  2/ V6  
1  - 1/ V6  

In Exercises 27 and 28, a QR factorization of A is given. 
Use it to find a least 

squares solution of Ax = b. 
1] b = 
OJ [ 1/ V6  
1/ \/2] 0 , 
Ul 
R =[V6 -V6/2Jb=[�] 
of Ax = b 
h (cm ) 
b (cm ) 

height of the ball on the first 
the data in 
mating line for bounce 
of initial 
height h. 

and the 
heights, 
is measured. 
Use 
squares 
approxi­

Table 7.3 to find the least 

Table 1 . 3  
20 
14.5 

height b as a linear 

ball is dropped 

100 
73.5 

from various 

60 
45.5 

29. A tennis 

function 

bounce 

0  1/ \/2  

40 
31 

48 
36 

80 
59 

1/ \/2  

, 

1 

that the length 

30. Hooke's 
Law states 
a linear 
function 
Figure 
7.17 and Example 
are constants a and b such that 

6.92.) Accordingly, 

L of a spring 
there 

of the force F applied 

to it. (See 

is 

L = a + bF 

Table 7.4 shows the results of attaching 
weights 

to a spring. 

various 

equations to 

and 24, 

find all the least 

In Exercises 23 

squares solutions. 

squares solution 

show that the least 
and solve the normal 

of Ax = b is not unique 
� [-; l 
23. A � [ � = � O ] b 
24.A � [� -� -�Jb � [-�] 
F (oz) 

In Exercises 25 
solution 
25. x + y - z = 2 26. 2x +  3y + z = 21 
-y + 2z =  6  x + y + z =  7 
3x + 2y - z = 1 1   - x  + y -z = 14 
-x +  z =  0 
2y + z =  0 

and 26,find the best approximation to a 

system of equations. 

of the given 

F 

Fioure 1 . 1 1  

Table 1 . 4  
2 
7.4 
L (in.) 

4 
9.6 

6 
1 1 .5 

8 
13.6 

5 8 8   Chapter 

7 Distance 

and Approximation 

1930 
59.7 

1940 
62.9 

1950 
68.2 

1960 
69.7 

1970 
70.8 

1980 
73.7 

1990 
75.4 

Table 1 . 5  
Year of Birth 
Life Expectanc
Source: World Almanac 

y (years) 
and Book of Facts. 

1920 
54.1 

New York: World Almanac 

Books, 1 999 

the con

(a) Determine 

stants 
least squares 
What does a represent? 

the 
a and b by finding 
approximating 
line for these 
data. 

(b) Estimate 

of the spring when a weight 

of 

5 ounces 

the length 
is attached. 

years. 

3 1 .  Table 7.5 gives 

States 

United 
(a) Determine 

for people born in the 

life expectancies 
in the given 
the least squares 
line 
the life 
expec­

data and use it to predict 

for these 
tancy of someone 

(h) How good 

is this 
32. When an object is thrown 

born in 2000. 
model? Explain. 
straight 
up into the air, 
states 

that its height 

Law of Motion 

approximating 

Newton's 
s (t) at time t is given by 

Second 

s (t) = s0 +  v0t + tgt2 

where v0 is its initial 
acceleration 
surements 

due to gravity. 
shown in Table 7.6. 

velocity and g is the constant of 
Suppose we take the mea­

Table 1 . 6  
Time (s) 
Height 
(m) 

0.5 
1 1   17 

1.5 
21 

2 
23 

3 
18 

the equation 

to estimate the U.S. population 

(b) Use 
in 2010. 

Table 1.1 

Population 
Year  (in millions) 
1950 
1960 
1970 
1980 
1990 
2000 

150 
179 
203 
227 
250 
281 

Source: U.S. Bureau o f  the Census 

34. Table 7.8 shows average major league 

baseball salaries 

1970-2005. 
for the years 
(a) Find the 
least 
squares 
(b) Find the least 

squares 

these 

data. 

for these 

data. 

approximating 

quadratic 

for 

approximating 
exponential 

(c) Which equation 
(d) What do you estimate 

gives the 
better 

Why? 

baseball salary will be in 2010 and 2015? 

the average major league 

approxima

tion? 

(a) Find the least squares 
approximating 
for 

quadratic 

T a b l e  1 . 8  

(h) Estimate 

these 

data. 

the height 

at which the object was 

released (in 
acceleration 

m), its initial veloc
2). 
due to gravity 

ity (in mis), and its 

(in m/s

(c) Approximately 

when will the object hit 

the 

ground? 
33. Table 7.7 gives 

the population 

an exponential 

of the United 
States 

10-year 
(a) Assuming 

at 
intervals for the years 1950-2000. 
growth model of the 
form p(t) = clt, where p(t) is the 
at 
time t, use least 
for 
the growth rate of the population. 
[Hint: Lett = 0 
be 1950.] 

population 
squares to find 
the equation 

Year 
1970 
1975 
1980 
1985 
1990 
1995 
2000 
2005 

Average 

Salary 
(thousands 
of dollars) 
29.3 
44.7 
143.8 
371.6 
597.5 
1 110.8 
1895.6 
2476.6 

Source: Major League Baseball Players Association 

Section 

7.3 Least 

Squares 

Approximation 

5 8 9  

35. A 200 mg sample 

ofradioact

ive poloniu

m-210 is ob­

served as it decays. Table 7.9 shows the mass remain­
ing at various 
Assuming 

times. 
an exponential decay model, use least 

to find the 

half-life ofpoloniu

m-210. (See 

squares 
Section 6.7.) 

projection 

to show 
same. 

for W and repeat the calcula

tions 
matrix is the 

as a basis 
that the resulting 
44. Let A be a matrix with linearly independent 
columns 
and let P = A (Ar A)-1 Ar be the matrix of orthogona
projection 
(a) Show that P is symmetric. 
(b) Show that P is idempotent. 

onto col(A). 

l 

Table 1 . 9  

Time (days) 
Mass (mg)  0 

200 

30 
172 

60 
148 

90 
128 

36. Find the plane 

z = a + bx + cy that best fits the data 

points (O, -4, O), (5, 
( - 1,- 5,- 2). 

0, O), (4, - 1, 1), (1, - 3, 1), and 

In Exercises 37-42,find the standard matrix of the 
orthogonal projection 
of v onto W. 
find the orthogonal projection 
matrix to 

onto the subspace W. Then use 
this 

of A. 

pseudoinverse 

45-52, compute the 

In Exercises 

47.A � H �] 46.  A�[-�: 
1] 
45.A  = [2
48.A�[� �] 
1] 50. A =  
2] 
49.A  = [0
[3
SJ.A�[���] 52. A =  
[� � -�1 

53. (a) Show that if A is a square 

columns, then A+ = A -1• 

(b) If A is an m X n matrix with orthonormal 

independent 

1  1 - 2  
0  0 2 

matrix with linear

1 
4

1  1

ly 

columns, what is A+? 
54. Prove Theorem 7.12(b). 
55. Pr
56. Let A be a 

ove the remaining 

part of Theorem 7.12(c). 

matrix with linear

ly independent 

columns. 

Prove the following: 
(a) (cA)+  = (l/c)A+forallscalars
(b) (A+)+  = A if A is a square 
(c) (AT)+  = (A +f if A is a square matrix. 

matrix. 

c :;i: O. 

57. Let 

n data points (x1, y1), . . .  , (xn, Yn) be given. Show 
then they have a unique 

that if the points do not all lie on the same vertical 
line, 
mating 

squares 

approxi­

line. 

least 

58. Let n data points (x1, y1), . . .  , (xn, Yn) be given. 

Exercise 

Generalize 
57 to show that if at least 
of x1, . . .  , xn are distinct, then 
the given 
points have 
a unique least squares 
polynomial of 
approximating 
degree 

at most k. 

k +  1 

43. Verify that the standard 

matrix of the projection onto 

W in Example 
does not depend 

7.31 (as constructed 

by Theorem 7.1 1 )  

on the choice 

of basis. 
Take 

mH�ll 

5 9 0   Chapter 

7 Distance 

and Approximation 

• 

The Singular 

Value Deco m p o siti o n  

A is not 

symmetric, 

5 ,  we saw that every sy

In Chapter 
where P is an orthogona
ues of A. If 
in Chapter 4, we may still 
D is as before but P is now simply 
is diagonalizable, 
(symmetric 
P and Q are orthogonal 
gular 
factoriza

decomposition 

value 

or not, square or not) has a 

and D is a diagona
(SVD), and it 

may surprise 

tions. 

so  it 

mmetric 

matrix A can b e  factored 

l matrix and D is a diagonal 
be able to factor a square matrix A as A = PDP-1, where 

as A  = PDP T' 
matrix displaying 
the eigenval­
is not possible, but as we learned 

such a factorization 

an invertible 
matrix. 

However, 
you that we will now  show 

not every matrix 
that every matrix 
PDQ T' where 
tion of the form A = 
l matrix! This remarkable 
result 
is the sin­
is one of the most important of all matrix 

factoriza

In this section, 
some of its many 

sider 
answering 

we will show how to compute the SVD 

applications. Along the way, we will tie up some loose 

of a matrix and then con­
ends by 

a few questions 

that were left open in previous 
sections. 

Values of a M alrix 

The Singular 
For any m X n matrix A, the n X n matrix AT A is symmetric 
by the Spectral Theorem. 
thogona
AT A all real (Theorem 5.18), they are all nonnegative. 
value of AT A with corresponding 
ctor v. Then 

lly diagonalized, 

unit eigenve

and hence can be 

Not only are the eigenvalues 
To show this, 

or­
of 
let A be an eigen­

0:::::: 11Avll2 = (Av)· (Av)= (AvfAv = vTATAv 

It therefore 

makes sense 

= vT Av =  A(v · v) =  Allvll2 =  A 
to take (positive) square roots 

If A is an m x n matrix, the singular 
so that u 1 2:: u 2 2:: · · · 2:: u n-

of AT A and are 
values 

denoted 

of these 

eigenval
ues. 

by u 1, . . .  , u n-It is conventional 

A are the square 

values of 

Definition 
roots 
to arrange 

the singular 

of the eigenvalues 

Example 1 . 3 3  

Find the singular 

values 

of 

Solulion The matrix 

0  �] 

has eigenvalues 
VAi = v'3 and u2 =  VA;" = 1 .  

A1 =  3 and A2 = 1. Consequent

ly, the singular 

values 

of A are u1 = 

Section 

7.4 The Singular 

sition 

5 9 1  

Value 
Decompo
of AT A. Let {v1, ..• , vn} be such 
so that ,\1 2: ,\2 2: · · · 2: Aw 

of an m X n matrix A, 
we know that there 
is an 

To understand 
the eigenvectors 

consider 
orthonormal basis 
a basis correspond
From our calculations 

of AT A. Since 

the significance 
for !Rn that consists 
ing to the 

ctors 
eigenvalues 

of eigenve

just before the definition, 

of the singular 

of AT A, ordered 

AT A is symmetric, 

values 

A;= llAv;ll2 

Therefore,  O"; = '\IA; = llAv;ll 
of A are the lengths 
nice interpreta
tion. 
llxll = 1), then 

values 
lt has a 
in IR2 (i.e., 

words, the singular 
lly, this resu

If x lies on the unit 

Geometrica

circle 

In other 

Av1, .•• , Avn­

of the vectors 
Consider 

Example 

7.33 again. 

11Axll2 = (Ax)· (Ax) = (Axf(Ax) = xT A TAx 

the maximum 
and mini­
llxll = 1, are A1 = 3 and 

is a quadratic 

of this quadratic 

= 2xi + 2x1x2 + 2x� 

form. By Theorem 5.25, 
form, subject to the constraint 

which we recognize 
mum values 
,\2 = 1, respecti
vely, 

eigenve
and they occur at the corresponding 
vely. 

= [x1 x2] [� �] [::] 
is, when x = v1 = [ � � �] and x = v2 = [ - � � �l respecti
x - y - z = 0 (verif
see Figure 7 .18.) So O' 1 and O' 2 are the lengths 

with equation 
y this), and the 
transformation is an ellipse that lies in this plane. 
shortly; 
axes of this ellipse, as shown in Figure 7.19. 

for i = 1, 2, we 
and minimum 

11Av;ll2 = vTATAv; =A; 

see that 0'1 = llAv1 ll = V3 and 0'2 = 11Av2ll = 1 are the maximum 
values 
Now, the linear 

llAxll as x traverses the unit circle in 

IR2• 
in IR3 
to A maps IR2 onto the 
plane 
image of the unit 
(We will 

of half of the major and minor 

verify this fact in general 

of the lengths 

circle under this 

of AT A-that 

transforma

tion corresp

onding 

Since 

ctors 

We can now 

describe 

the singular 

value decomposition of a matrix. 

y 

2 

z 

multiplication 

by A 

.......------. 

2 

- 2  

r  y 
in !ffi2 into an ellip
se in !ffi3 
A transforms 
circle 

the unit 

The matrix 

Figure 1 . 1 9  

Fioure 1 . 1 8  

- 2  

- 2  

5 9 2   Chapter 

7 Distance 

and Approximation 

Valu e  Decomposition 

The Singular 
We want to show that an m X  n matrix A can be factored 

as 

A =  u�v r  

l matrix, and L is 

of A are 

If the nonzero singular values 

where U is an m X m orthogonal matrix, V is an n X  n orthogona
an m X  n "diagonal" 
matrix. 

and u ,.+ 1 = u r+ 2 = · · · = u n = 0, then L will have the block 
r n-r 
[D i 0 ] Jr 
0 i 0 ) m-r  [�I � l 0  u,. 
� = ----j-----
and each matrix 0 is a zero matrix 

,  where D = 

of the appropria

te size. 

form 

these 

will not appear.) Some examples 

of such a matrix L with r = 2 are 

(If r = m or r = n, some of 

(1) 

0 
3 

�  (What is D in each ca

se?) 

0 
3 
0 

To construct the orthogona
{v1, . . .  , vn} for !Rn consisting 

l matrix V, we first  find  an 

of eigenvectors 

of then X n symmetric 

orthonorma
matrixA r A. Then 

l basis 

U, we first note 
suppose that v; is the eige

that {Av1, . . .  , Avn} is an orthogonal 

set 
nvector of Ar A corresponding 

is an orthogonal 
For the orthogona

n X  n matrix. 

l matrix 

eigenvalue 

of vectors 
to the 

in !Rm. To see this, 
A;. Then, for i * j, we have 

(Av;)· (Av) = (Av;VAvj 
= vTATAv 

I  ) 
) I ) 
= ,\.(v .. v.) = 0 
<I; = llAv;ll and that the first r of these 
ai ' 

Av1, . . .  , Av,. by setting 

the eigenve

since 

ctors 

z 

1 

u  = -Av for i = 1, . . .  , r 

V; are orthogonal. 

Now recall 
are nonzero. 

that the 
singular 
Therefore, 

we can normalize 

values 
satisfy 

for !Rm. In this case, 

This guarantees that {u1, . . .  , u,.} is an orthonormal 
be a basis 
{u1, . . .  , um} for !Rm. (This 
techniques 

is the only tri
for carrying it out in the examples 

we extend the 

below and in 

the exercis

set in !Rm, but if r < m 
set {u1, . . .  , u,.} to an orthonor

it will 
not 
mal basis 

cky part of the construction; 

we will describe 
es.) Then we set 

Value 

Section 
7.4 The Singular 
Decomposi
tion 5 9 3  
A = ULVT. Since VT= v-1, this is equivalent 

need to verify that with 

that is, we 

to 

All that remains 

to be shown is that this works; 

U, V, and L as described, 
we have 
showing that 

AV= U!i 

We know that 

Av; = CT;U; for i = 1, . . .  , r 

and llAv;ll = CT; =  0 for i = r + 1, . . .  , n. 

Hence, 

Av; = 0 for i = r + 1, . . .  , n 

Therefore, 

AV= A[v1 
[Av1 
[Av1 

O J 

u  J : ·.  : ! 0 
, 
m 
0 · · · CT i 
---------0·-------�-1--0-

O J  [CTI  Q ! l 
CT 1 2 CT 2 2 · · · 2 CT r > 0 and 

an m X  m orthogonal 
U, 
matrix L of the form shown in 

extremely imp
theorem. 

ortant 

matrix 

as required. 

We have just proved the following 

Theorem 1 . 1 3  

The Singular 

Value Decomposition 

CT,+1 = CT,.+2 = ··· = CTn = 0. Then there 

values 
exist 
matrix V, and an m X  n 

Let A be an m  X  n 
an n X  n 
Equation 

orthogonal 
( 1) such that 

matrix with singular 

A factorization 

of A as in Theorem 7.13 is called a singular 

of U are called 

(SVD) of A. The columns 
of V are called 
determined 
Exercise 

25.) 

by A, but L must contain 

right singular 

vectors 

left singular 

vectors 
of A. The matrices 
the singular 

values 

of A, as in 

value decomposition 
of A, and the columns 

U and V are not uniquely 

Equation 

(1). 

(See 

Example 1 . 3 4  

value decomposition for the following 

Find a singular 

(a)A �[� � �] (b)A �[i �] 

: 

matrices

5 9 4   Chapter 

7 Distance 

and Approximation 

Solution (a)  We compute 

are ,\1 = 2, ,\2 = 1, and ,\3 = 0, with corresponding 

and find that its eigenvalues 
eigenve

ctors  m. [�]f il 

are orthogona

y this.) These vectors 

�  (Verif

l, so we normalize them to obtain 

The singular 

of A are <T1 =  Vl, <T2 = VI =  1, and <T3 = Vo =  0. Thus, 

and 

To find 

values 

U, we compute 

v =  1/:2 0 
[ l/Vl 0 
u, � :,Av, � �[� � �] [:;�] � [�] 
u, �:,Av, �  H� � �l[�] � [�] 
u = [� �] 
-[o  o  1] -[o 1][ o  1 o] _110 110 0 

� r  
[ l/Vl  l/Vl OJ 

1 10  l OVl O O  

form an orthonor

(the standard 

already 

mal basis 

basis) 

0  1 - U.'"'V 

0 

-

-

A-

These vectors 
have 

This yields 
the SVD 

for IR2, so we 

.,-.. which can be 
singular 

value <T3 does not appear in 2:.) 

easily checked. (Note that V had 

to be transposed. 

Also note that the 

(b) This is the matrix 
u1 = V3 and u2 = l, correspondmg 

in Example 

7.33, 

values 

the singular 
are 
. So 

Section 

7.4 The Singular 
Decomposi
tion 5 9 5  

Value 

so we already 
know that 
to v1 = l/Vl and v2 =  l/Vl 

.  [ l/Vl] [- 1/Vl] 
[\13 OJ 
}2 =  � � and V = [l/Vl - 1/Vl] 
l l [l/Vl] -[l/V6] 
;::;-[� 
l/V6 [l  ll [ 0 l 
u2 = �Av2 = l 1 0 [ l/Vl] = - 1/ Vl  

0 l/Vl - l/V6 

l
<T1  v 3 0 
Av1 = , 

2  0 1 

l/Vl  l/Vl 

1  - 1/Vl 

u1 = _!_

l/ Vl  

1 

1 

For U, we compute 

and 

for IR3. There are 

{u1, u2} to an orthonorma
l basis 

one method 
is to use the  Gra
5.14. We first need 
to find a linearly independent 
third standard 
basis 

This time, 

we need to extend 
several ways 
to proceed; 
Example 
tains u1 and u2. If e3 is the 
is linear
a reliable 
to use in 
its columns 
normalization) 

ly independent. 

method 

and use the Fundamental 
to {u1, u2, e3} (only the last step is needed)

vectors 

m-Schmidt Process, 
as  in 
set of three 
that con­
vector in IR3, it is clear that 
{u1, u2, e3} 
but 
inspection, 
this by 
with these 
vectors 
as 
Gram-Schmidt (with 

the matrix 

(Here, you should be able to determine 

general is to 

, we find 

U3 =  1/\13 

row reduce 
Theorem.) Applying 

[- 1Ml 
1/\13 [2/V6  0  - 1/v'3] 
- l/\13][\13 °][ l/\/2 
l/Vl] = U)2VT 
l/Vl -+ 

1/\13  0  1 
1/\13  0  0 

l/V6 l/Vl 1/\13 

- 1  v 2  

/' ;::;-

so 

u = l/V6 - 1/Vl  1/\13 

and we have the SVD [l  ll [2/V6 

0 1  l/V6 

A =  1 0 = 1/\/6 

0 

- 1/Vl 

l/Vl 

There is 

another 

spectral decomposition 
an outer 
version 

product expansion 
of the SVD 

form of the singular 

value decomposition, 
analogous 
from the  SVD 
by 
and is very useful in applications. We can obtain 
this 

matrix. It is obtained 

of a symmetric 

to the 

by imitating what we did to 

obtain 

the spectral decomposition. 

5 9 6   Chapter 

7 Distance 

and Approximation 

Accordingl

vf 

0  i 0 n 

y, we have i[7 ·. � i o][�r] 
Um -�-----:_:_: ____ �_�_;______ �T 
nm+ [u,,, rnr 
u,t u,t lf,u,] [-
= u1u1vf + · · · + O"ru,v'! 
O" 1 2: O" 2 2: · · · 
O" n = 0. Let u1, . . .  , Ur be left singular 
A = u1u1vf + · · · + u,u,v'! 

for obtaining 

of A corresponding 

with singular 

the process 

vectors 

column-row representation of 

values. Then 

singular 

matrix 

values 

to these 

Form of 

the SVD 

2: O" r > 0 and O" r+ 1 = 

using block multiplication 
and the 
following 
theorem summarizes 
theSVD. 

the product. The 

this outer product form of 

Theorem 1 . 1 4  

The Outer Product 

O" r+z = · · · = 
Let A be an m X n 
vectors 
singular 

and let v1, . . .  , Vr be right 

Remark If A is a positive definite, 

to results 

that we alread

7.14 both reduce 
that the SVD 
spectral decomposition. 

generalizes 

the Spectral Theorem and 
(See Exercise 27.) 

symmetric 
matrix, 
y know. In this case, 

then Theorems 7.13 and 
it is not hard to show 
the 

that Theorem 7 .14 generalizes 

The SVD 

of a matrix A contains 

much important information about A, as out­

lined 

in the crucial 

Theorem 7.15. 

Section 

7.4 The Singular 

Value 
Decompo

sition 

5 9 1  

Theorem 1 . 1 5  

decomposition of an m X n 
of A. Then: 

matrix 

A. Let u1, . . .  , 

Let A =  U�VTbe a singular 
value 
u,. be all the nonzero 
singular 
values 
a. The rank of A is r. 
b. {u1, . . .  , u,.} is an orthonor
c. {u,.+ 1, . . .  , um} is an 
d. {v1, . . .  , v,.} is an orthonorma
e. {v,+1, . . .  , vn} is an orthonor

orthonor

mal basis for null(A T). 
l basis for row(A). 
mal basis 

for null(A). 

mal basis 

for col(A). 

Proof (a) By Exerc

ise 61 in Section 

3.5, we 

have 

rank(A) = rank( U!i VT) 
= rank(!, VT) 
= rank(!,) = r 

�  column 

(b) We already know that 
independent, 
by Theorem 5.1. Since 

{u1, . . .  , u,.} is an orthonor

mal set. Therefore, 
it is linearly 
ui = (1/ u)Av; for i = 1, . . .  , r, each ui is in the 

space of 

A. (Why?) Furthermore, 
r = rank(A) = dim(col(A)) 
orthonormal 
basis 
orthonormal basis 

{u1, . . .  , u,.} 
is an 
{u1, . . .  , um} is an 
l complement 

Therefore, 
(c) Since 
col(A), by property (b ), it follows 
orthogona
(e) Since 

for col(A), by Theorem 6.lO(c). 
for !Rm and {u1, . . .  , u,.} is a basis 
for 
that {u,+ 1, . . .  , um} is an orthonormal basis 
for the 
T), by Theorem 5.10. 

of col(A). But (col(A)) = null(A 

Av,+1 = · · · = Avn = 0 
dim(null(A)) = n - r 

set of n - r vectors 

set contained 

the set {v,.
{v,.+ 1, . . .  , vn} is a linear

+1, . . .  , vn} is an orthonormal 

ly independent 

in null(A). But 

in the null space of 

A. Therefore, 

Theorem, 

by the Rank 
rem 6.lO(c). 
(d) Property (d) follows 
prove this in Exercise 
32.) 

so {v,.+ 1, . . .  , vn} is an orthonor

mal basis 

for null(A), by Theo­

from property (e) and Theorem 5.10. 

(You are asked to 

new geometric 

insight 

into the effect of matrix transforma­

in !Rn into an ellipsoid in !Rm. This point arose, for example, in our 

(without proof) 

that an m X n 

matrix transforms 

The SVD provides 
tions. We have noted several 
the unit sphere 
of Perron's 
discussions 
to singular 
values 

Theorem and of 
in this section. 

times 

We now prove this result. 

operator norms, 

as well as in the introduction 

5 9 8   Chapter 

7 Distance 

and Approximation 

Theorem 1 . 1 6  Let A be an m X n matrix 

with rank r. Then the 
tion that maps x to Ax is 

image of the unit sphere 

in !Rn 

under the matrix transforma
a.  the 
b. a solid 

ellipsoid in !Rm if r < n. 

surface 

of an ellipsoid in !Rm if r = n. 

Proof Let A = U2. VT be a singular value 
the left and right 
singular 
rank(A) = r, the singular 

decomposition of the m X n matrix A. Let 
of A be u1, . . .  , um and v1, . . .  , vn, respecti
vely. 
of A satisfy 

vectors 
values 

Since 

a). Let x � [ JJ he a unH vedodn 

vT x is a unit 

vector, 

fll". Now, ''"" V ;, an orthogona

l 

matrix, 

so is vT, and hence 

by Theorem 5.6. Now 

by Theocem 7 .15( 

so (vf x)2 + .. · + (v�x)2 

Therefore, 

= I .  

By the outer product form of the SVD, we have A 

the scalar <.T;vTx. 

where we are letting
(a) If r = n, then we must haven :s m and 

y; denote 

= Uy 

Ax = Y1 U1 + . . .  + YnUn 

whe<e y � [;J Thecef°''· agaill by The°'em 5.6, llAxll �  llUrll � llrll
But (;1J2 + · · · + (;:)2 = (vfx)2 + · · · + 
�  which shows that the 

orthogonal. 

vectors 
(b) If r < n, the only 
difference 

(v�x)2 
(;1J2 + . . .  + (�J2 :s 1 

surface 
in the above 
steps 

of an ellipsoid in !Rm. (Why?) 
is that the 

Ax form the 

we are missing some terms. This ineq

uality 

corresponds 

= 1 

. ''"" U" 

equation 

becomes 

to a solid 

ellipsoid 

since 
in !Rm. 

Example 1 . 3 5  

Describe the image of the unit sphere 

Solution In Example 

of the matrix 

7.34(a), we 

in IR3 under the action 

Value 

Section 
7.4 The Singular 
Decomposi
tion 5 9 9  
A = [ 1  1  OJ 0  0  1 
SVD of A: [ 1/v'2 
1] 1/Vl 
1/:2 0
[ 1  1  OJ = [ 1  OJ [ Vl  0  OJ O 
/' h - 1  v 2  
Y1 + y� ::::; 1 

r = rank(A) = 2 < 3 = n, the second 
will satisfy the inequality 
unit sphere 
2 
2 
u1 
ding to the left singular 

part of Theorem 7.16 applies. The image 

axes in IR2 (correspon

0  1  0  1  0 

found the following 

to y1y2 coordinate 

vectors 

0  0  1 

0

u1 = e1 and u2 = e2, the image is as sh

own in Figure 7.20. 

Since 
of the 

relative 
and u2). Since 

z 

Y2 

-1 

x 

Figure 1 . 2 0  

matrix A on the 

effect of each factor in its SVD, A = u� VT, from right 

m X  n 

effect of an 

we can describe the 

VT is an orthogonal 

In general, 
in !Rn in terms of the 
Since 
� does two things: The diagonal 
of the dimensions 
nonzero diagonal 
matrix 
U then aligns 
u1, . . .  , ur in !Rm. (See Figure 7.21.) 

u r+ 1 = u r+z = · · · 
u 1, . . .  , u r then distor

of the unit 
entries 

matrix, it maps the unit sphere 

the axes of this ellipsoid 

sphere, 

entries 

leaving 

to itself. The 
m X n 
= u n = 0 collapse n - r 
unit sphere, 

which the 

an r-dimensional 

unit sphere 
to left. 
matrix 

t into an ellipsoid. 
orthonor
with the 

The orthogonal 
mal basis vectors 

Of l h e  SUD 

APPliCalions 
The singular 
theoretica

value decomposition is an 

extremely 
lly. We will look at just a few of its many appl

ications. 

useful tool, 

both practica
lly and 

6 0 0   Chapter 

7 Distance 

and Approximation 

J 

Figure 1 . 2 1  

Rank Until 

now, we have not worried 
point of view. We compute 

about calculating 

the rank 

of a matrix 
from 
by row reducing 

the  rank 
the number of nonzero 

of a matrix 

rows. However, 
lly if the matrix 

is ill-conditioned. 

as we have 

can affect this process, 

especia

form and counting 
errors 

that should 
to accurately determine 

a computational 
it to echelon 
seen, 
roundoff 
Entries 
ability 
trix. 
more reliable 
is that the orthogona
introduce 

additional 

In practice, 

errors 
l matrices 
errors; 

when roundoff 

the SVD 

be zero may end up as very small nonzero 

quantities 

numbers, affecting 
associated 

with the ma­

our 

the rank and other 

is often used to find the rank of a matrix, since 

it is much 

are present. The basic 

this approach 

idea behind 
ve lengths 

and thus do not 

U and V in the SVD preser

any errors 

that occur 

will tend to show up in the matrix 

2:. 

cAs Example 1 . 3 6  Let [8.1650 

-0.0041 
- 3.9960 

A =  4.0825 
4.0825 

4.0042 

- 0.0041 l 4.0042 
- 3.9960  [8.17 

0 
-4 
and B = 4.08 
4.08 
4 

The matrix B  has 
places. If we compute 
that rank(A) = 3 but rank(B) = 2. By the Fundamental 
other 

by rounding 
been obtained 
the ranks 
of these 

things, 

The explanation 

that A is invertible 
but B is not. 
for this critical 

off the entries 

Theorem, 

two approximately 

equal matrices, 
we find 
this implies, 
among 

in A to two decimal 

difference 
SVDs. The singular 

between two matrices that 
values 

are approxi­
of A are 10, 8, and 0.01, so A has 

mately 
equal 
rank 3. The 

lies in their 
singular 

values 

In practical 

of Bare 10, 8, and 0, so B has rank 2. 
applications, 
it is 
roundoff 
" can be filtered 

often 
be close to zero, then 
error 
In this way, "noise
replace 

assumed 
has crept 

out. In this example, 

that if a singular 
value 
value 
in and 

to 
is computed 
the actual 
be zero. 
should 
if we compute A =  UL:VT and 

� =  [10 0  0 l � � �.01 

by � I= 

..-.. then UL:' VT = B. (Try it

!) 

Matrix Norms  and 

for certain expressions 
norm of a matrix. 
the singular 
values 

The following 
of the 
matrix. 

lhe Condilion Number The SVD can 
involving 

matrix norms. Cons
theorem shows that it is completely determined 

for example, the 

simple formulas 

provide 

ider, 

by 

Frobenius 

Theorem 1 . 1 1  

Let A be an m X n 
A. Then 

matrix 

and let 

Value 
Decompo

Section 
7.4 The Singular 
u 1, . . .  , u r be all the nonzero 

sition 

singular values 

of 

6 0 1  

The proof 

of this result 

depends 

on the following 

analogue 

of Theorem 5.6: 

If A is an m X n 

matrix and Q is an m X m orthogona

l matrix, 

then 

(2) 

To show 

that this is 
true, 

we compute 

llOAll� = II [ Qa1 ·  ·  · Qa" l II� 

= llOa1 ll� + ·  ·  · + llQanll� 
= lla1 ll� + ·  ·  · + llanll� 
= llAll� 
U2. VT be a singular 

value decomposition of A. 

Then, 

Proof of Theorem 1 . 1 1  Let A = 
using 

Equation 

(2) twice, we have 
llAll� = li UkVTll� 

which establishes 

the result. 

= llkVTll� = ll(kVT)Tll� 

= llVk Tll� = Ilk Tll� =  uf + ·  ·  · + u� 
-1] 4 has singular 
V 4.51502 + 3.10082 = V3Q  = llAllF 

A in Example 

7 .18. 

Verify 

Theorem 7 .17 for the matrix 

Solution The matrix A  = [� 

check that 

which agrees 

with Example 

7.18. 

values 

4.5150 and 3.1008. We 

In Section 7.2, we commented that there is no 

easy formula for  the 

operator 

that is 
true, 

the SVD of A provides 

us with a very nice 

2-norm of a matrix A. Although 
expression 

for llAll2. Recall that 

llAll2 = maxllAxll 

llxll�l 

where the vector 
the set 

norm is the ordinary 
llAxll lies on or 
inside 

of vectors 

Euclidean 

norm. By Theorem 7.16, for llxll = 1, 

an ellipsoid whose semi-axes have lengths 

cAs  Example 1 . 31 

6 0 2   Chapter 

7 Distance 

and Approximation 

equal to the nonzero 
these 

is u 1, so 

singular 

values 

of A. It follows 

immedia

tely that the largest 

of 

This pr

ovides 

us with a neat way to express the 

condition 

number of a (square) 

matrix with respect 
respect 
to the operator 

to the 

operator 2-norm. Recall that the condition 

number (with 

2-norm) of an invertible 

matrix A is defined 

as 

�  Therefore, 

the singular 

values 

that llA -l 112 = 1 I (J" n> so 

It follows 

As you will be asked to show in Exerc

ise 28, if A  = UL:VT, then A-1 = V2:-1UT. 

of A -l are 1/ u1, . . .  , 1/ u n (why?), and 
l/un 2: · · · 
2: l/u1 

Example 1 . 3 8  

Find the 

2-condition 

number of the matrix A in Example 

7.36. 

Solulion Since 

u1 = 10 and 

u3 = 0.01, 

U3 0.01 
cond2(A) =  - =  - = 1000 

U1 10 

be 

and we should 

errors

is large 

enough 

to suggest 

se and leasl Squares Approximalion 

In Section 7.3, we pro­

as we noted at 

only if AT A is invertible, 

This value 
wary of the effect of roundoff 

A+ =  (AT A) -1A T for the pseudoinverse of a matrix 

that A may be ill-conditioned 
. 

The Pseudoinver
duced the formula 
formula is valid 
SVD, we can now define the pseudoinverse of any matrix, generalizing 
formula. 

4 
Defi n ition Let A = UL: VT be an SVD for an m x n matrix A, where 2: = 
E. H. Moore 
(1862-1932) 
was an 
[ � �] and D is an r X r diagonal 
mathematician 
who 
American 
worked 
in group 
theory
, number 
theory
, and geome
try. 
He was 
u1 2: u2 2: · · · 2: ur > 0 of A. The pseudoinv
the first 
head 
of the 
mathemat­
ics departmen
t at 
the University 
when 
it opened 
of Chicago 
in 
1892. In 1920, he intro
a 
duced 
generalized 
inverse 
that 
matrix 
included 
matric
es. 
rectangular 
did not 
His work 
recei
ve much 
attention 
because of his obscure 
writing 
style. 

�+ = [v-1 OJ 0  0 

the nonzero singular 
erse (or Moore-Penrose 
of A 

is the n X m matrix A +  defined 

where 2:+ is then X m matrix 

matrix containing 

Equipped with the 

the time. 

A+ = v�+ur 

inverse) 

A. Clearly, this 

values 

by 

our previous 

Section 

7.4 The Singular 
Decomposi
tion 6 0 3  

Value 

0 

0 

0 

0 

0 

A =  

1/2 

1/\/2 

1/\/2 

0 
1 

we form 

0  0 

7 .34. 

1/\/2  0 

Example 1 . 3 9  

�+ =  0  1 

in Example 

(b) We have the SVD 

of the matrices 

Find the pseudoinverses 

1/\/3 0  1 
1  1/\/6  1/ \/2   1/\/3 0  0 

�] 
�][� 0 �l[ l�V2 
Solulion (a) From the SVD [� 1 �] = [� 
1  - 1/\/2 ['M "] 
�][� �] 
Then  [1/ V2  
0 - 1/v'l1/V2 
[ 1/2 
0 �] 
A+ = v�+ ur  = l/[2 
A � [i '] [2/v'6 
0 = 1/\/6  - 1/ \/2  -iM] [v'3 "][ 1M 
1/\/2] 1/\/2 
� +  = [l/[3 0 �] 1 
1/\/6] 1/\/2 
1/\/2  0  �J[ 2/:6 
- 1/\/2] [1/\/3 0 
[1/3 
-1/3] 2/3 
One of those 
of 
who was unaware 
Moore
on matrix 
's work 
inverses 
was 
(b.1 93 1 ), who 
his own notion 
introduced 
of 
inverse 
a genera
lized matrix 
in 1955. Penrose 
has made 
many 
contributions 
to geometry 
and 
physics. 
He is also 
theoretical 
the 
of a type 
inventor 
of nonperiodic 
tiling that 
covers 
the plane 
with 
only 
two different 
shapes of tile, 
yet has no 
repeating 
He has 
pattern. 
including 
recei
the 
ved many awards, 
in Physics, 
Prize 
1988 Wolf 
which 
he shared 
with 
Stephen 
Hawking. In 
1994, he was knighted 
vices 
for ser
to science. 
Sir Roger 
Penrose 
is 
tly the Emeri
curren
tus Rouse 
Ball 
Professor 
of Mathematics 
at the 
University 
of Oxford. 

to check 
izes the old one, for if the m X n 
substi
then direct 
in Exercise 
50.) Other properties 
We have seen that when A has linear
squares 
solution 

When the 
mal equations 
x of minimum length 
simply 

of the pseudoinverse general­
dent colum
to verify this 

ly indepen
ows that (AT A)- IAT = v�+ uT. (You are asked 

then AT A is not invertible, 
nor­
so the 
we will ask for the solution 
In this case, 

that this new 
matrix 

solution x to Ax = b; that is, the normal 

of A are linearly dependent, 

of the pseudoinverse are explored 

ly independent columns, there 

It is straightforward 

AT Ax = A Tb have the 

use the general version 

least 
unique 

out that this time 

1/\/6 
- 1/\/2 
1/\/3 

of the pseudoinverse. 

have infinitely 

to the origin). It turns 

A = U� VT has linear

the one closest 

many solutions. 

x = (ATA)-1Arb  = A +b 

definition 

in the exercis
es. 

is a unique 

columns 

equations 

tution sh

(i.e., 

/\/2 - 1  2 

Roger Penrose 

- 1/\/3 

1/3 

- 1/3 

ns, 

and 

1/\/3 

u�vr 

so 

we 

2/3 

6 0 4   Chapter 

7 Distance 

and Approximation 

Theorem 1 . 1 8  

squares 

problem Ax = b has a unique 

least 

squares 

solution x of minimal 

The least 
length 

that is given by 

Proof Let A be an m X  n 
v:2:+ uT). Let y =  v TX and let c =  uTb. Write 

y and c in block form as 

A  = U:2:VT (so that A+  = 

matrix of rank r with SVD 
y  = [;:] and c  = [::] 

where y1 and c1 are in !Pt. 

We wish 

to minimize 

llb  - Axil or, equivalentl

y, llb  - Axll2• Using Theorem 5.6 

and the fact that UT is orthogona

l (because U is), we have 

The only part of this expression 
mum value occurs when c1 -Dy1 = 0 or, equivale
squares 

x are of the 

solutions 

that we have any control 

ntly, 

form 

over is y1, so  the 
mini­

when y1 = D-1c1. So all least 

[D-1 ] 

C1 

Y2

x = Vy =  v 

Set 

We claim that 
suppose that 

this x is the 

least 

squares 

solution of minimal length. To show this, 

let's 

y2 -=fa 0). Then 

is a different least 

squares 

solution (hence, 
= llVYll = llrll < llY'll = llVY'll = llx' ll 

llxll 

as claimed. 

We still must 

show that x is equal 

to A+ b. To do so, we simply 

compute 

Example 1 . 4 0  

Find the 

minimum 

length 

solution of Ax = b, where 

least 

squares 

A  = [ � �] and b  = [ �] 

Section 

7.4 The Singular 
Decomposi
tion 6 0 5  

Value 

Solution The correspond

ing equations 
x + y =O 
x + y =l 

be infinitely 

hope. Moreover, the 
many least 
squares 

by 

our only 

An SVD of A is given 

are clearly 
so there will 
columns 
solutions-among which we want the one with minimal 
length. 

inconsistent, 
of A are linearly dependent, 

squares solution is 

so a least 

�  (Verif

A =  [� lJ = [l/V2 
A +  = v�+ ur = [1/V2 

1  l/V2 

y this.) It follows 

0  l/V2 

- 1/V2 0 

l/V2J [2 
OJ [l/Vl 
l/V2J [1/2 
OJ [l/Vl 

l/V2J - 1/V2 
l/V2J - l/V2 [1/4 

0  l/V2 

- 1/V2 0 

that 

l/V2 

1/4 

l/4J 1/4 

so 

You can see that the 

minimum least 

squares solution in 

7.40 satisfies 

�. In a sense, this is 

x + y = 
with. In Exerc
direct

ise 49, you are asked to solve 

a compromise between the two 
the normal equations 
is the one closest 

we started 
for this problem 
to the origin. 

ly and to verify that this solution really 

Example 
equations 

The Fundamenlal Theorem of Invertible Malrices It is appropria
Matrices 

of Invertible 

the Fundamental 
gly, the singular 

one more time. 
of a square matrix tell us when the matrix 

te to conclude 

Theorem 

values 

is 

by revisiting 
Not surprisin
invertible. 

Theorem 1 . 1 9  

The Fundamental 

Matrices: Final Version 

Theorem of Invertible 

matrix and let T :  V ---+ W be a linear 

[TJc,_8 with respect 
Band C of V and W, respectively, 
is A. The 
statements 
: 

are equivalent

to bases 

transformation whose 

b in !Rn. 
solution for every 

Let A be an n X  n 
matrix 
following 
a.  A is invertible. 
b. Ax = b has a unique 

c. Ax = 0 has only the trivial 

(A) = 0 

solution. 
form of A is In-
. 

row echelon 

d. The reduced 
e.  A is a product of elementary 
matrices
f. rank(A) = n 
g. nullity
h.  The 
column 
i. The column 
j.  The 
column 
k. The row vectors 

vectors 
vectors 
vectors of 
A form a 

of A are linear
of A span !Rn. 

1. The row vectors 

for !Rn. 
basis 

ly indepen
dent. 

of A are linearly independent. 
of A span !Rn. 

7 Distance 

6 0 6   Chapter 
and Approximation 
n. detA of- 0 

of A form a basis for IJ�r. 

m. The row vectors 

of A. 
o.  0 is not an eigenvalue 
p. T is invertible. 
q. T is one-to-one. 
r. T is onto. 
s. ker(T) = {O} 
t. range(T) = W 
u.  0 is not a 

singular 

value of A. 

of singular 

values, 0 is a singular 
value 
of A if 

note that, 

Proof First 
by the definition 
and only if 0 is an eigenvalue of 

(a) => ( u) If A is invertible, 
(u) => (a) If 0 is not a singular 

eigenvalue 

that 0 is not an 

implies 

AT A. 

AT A is invertible, 
Therefore, 
rank(A) = n, by Theorem 3.28, 
and (f). 

by the 

so is AT, and hence AT A is as 

well. 

Therefore, 
property ( o) 

of AT A, so 0 is not a singular 
value of A. 
eigenvalue of AT A. 
value of A, then 0 is not an 
(a) and ( o ). But then 

ence of properties 

equival

so A is invertible, 

by the equivalence 

of properties (a) 

Vignette 

Digital Image Compression 

of the SVD, one of the most impressive is its use in 

tions 

images 

ng digital 

Among the many applica
compressi
(by satellite, 
detecting 
and correcting 
consider 
has to do 
ted, without 

so that they 
fax, Internet, 
or the 
errors 

losing any essentia

with reducing 

l information. 

like). We have alread

can be efficiently 

electronica

transmitted 

lly 
y discussed 
the problem of 
The problem we now wish to 

in such transmissions. 
the amount 

of information 

that has to be transmit­

pixel is one of 256 shades of gray, 

suppose we have a grayscale 

numbers 
is that some parts of the picture 

this information 
95,200 

that is 
picture 
which we can 
repre­
in a 340 X 280 
is very expensive. 
are less interest­
of someone standing 
outside, 
there 
a lot of detail. 

the person's face 
contains 
every 

second 

or third 

pixel in the back­

tting 

In the case 

size. Each 

image compression 

l images, let's 

of digita
340 X 280 pixels in 
sent by a number between 0 and 255. We can store 
matrix A, but transmi
and manipulating 
these 
The idea behind 
ing than others. For example, in a photograph 
may be a lot of sky in the background, 
We can probably get away with 
ground, 
It turns 
the "boring" 
we have the SVD of 

out that the small singular 
parts of the image, 

and we can ignore 
product form 

A = u1u1vf + · · · 
+ u,u,v; 
Let k ::::: r and define Ak = u1u1vf + · · · 
+  ukukv[ 

Then Ak is an  approximation 

transmitting 

A in outer 

values 

while 

in the SVD 

of the matrix A come from 
many of them. Suppose, then, 

that 

but we would like to keep all the pixels in the region 

of the face. 

to A that corresponds 

to keeping 

only the first k singu­

lar values 
discover that 
values. 
plus the 20 vectors 

it is enough 
Then, instead 

and the corresponding singular 

vectors. 
to transmit only 
the data corresponding to the 
numbers, 

For our 340 X 280 example, 
we may 
we need only send 20 singular values 

first 20 singular 

95,200 

v1, . . .  , v20 in IR280, for a total 

of 

of transmitting 
u1, . . .  , u20 in IR340 and the 20 vectors 
20 + 20. 340 + 20. 280 = 12,420 
a substantial 

saving! 
Gauss in Figure 

of the mathematician 

of gray, 

so the correspond

ing mat

numbers. This represents 

The picture 
It has 256 shades 
between 0 and 255. 

7 .22 is a 340 X 280 pixel image. 
rix A is 340 X 280, with entries 

It turns out that the matrix A has rank 280. If we approximate 
first k singular 

A by Ak, as de­
we get an image that corresponds 
of A. 
values 
the 
of k from 2 to 256. At first, 

to the 

of these 

scribed above, 
images 
Figure 
7 .23 shows 
several 
image is very blurry, 
but fairly quickly 
a pretty good approximation 
shown in the upper left-hand 
values 

to the 
corner 

actual 
of Figure 

for values 
it takes 

shape. 

Notice that 
image (which comes from A = A280, as 

A32 already gives 

Some of the singular 

<T64 = 484,u128 = 182,u256 = 5,andu280 = 0.5. Thesmaller
little 

u16 = 22,589, 
singular
which is why the approximations 

u32 = 10,187, 
very 
to the original. 

of A are u1 = 49,096, 

valuescontribute

to the image, 

look so close 

quickly 

7.23). 

6 0 1  

Figure 1 . 2 2  

k  = r = 280 

Original, 

k = 2  

k = 4  

k = 8  

k  = 1 6  

k  = 32 

k = 64 

k = 128 

k = 256 

6 0 8  

Figure 7 . 2 3  

Section 

7.4 The Singular 
Decomposi
tion 6 0 9  

Value 

values of the given matrix. 

1 . 4  

In Exercises 

1-10, find the singular 

..  1 Exercises 
1. A =  [� �] 2. A =  [� �] 
3.A = [� �]  [Y2 4. A =  O �] 
5.A = [!] 6. A =  [3 4 ]  
7.A = u �] 8. A =  [: -�l 
9. A = [� �] [1 0 :J 
0 10. A =  -3 
2  0 
12. A =  [11 11] 
11. A in Exercise 3 
13.A = [ 0 -2] -3 0 
14. A =  [11 -11] 
20. A =  [� �] 
19. A in Exercise 9 
21. Exercises 3 and 11 22. Exercise 14 

16. A in Exercise 6 
18. A in Exercise 8 

15. A in Exercise 5 
17. A in Exercise 7 

21-24,find the outer product form of the SVD 

1 1-20, find an SVD of the indicat

In Exercises 
for the 

In Exercises 

matrix in the given 

exercises. 

ed matrix. 

7 and 17 24. Exercises 9 and 19 

23. Exercises 
25. Show that the matrices 

U and V in the SVD are not 
uniquely determined. 
Find an example 
in which 
[Hint: 
to make different 
it would be possible 
choices 
in the 
.] 
matrices
construction 
of these 
matrix. 
Show that the singular 
26. Let A be a symmetric 
olute 
values 
envalues 

(a) the abs
(b) 
27. (a) Show that, for 
matrix A, Theorem 7 .13 

of the eigenvalues 
of A if A is positive definite. 
a positive definite, 

of A are: 
the eig

of A, as guaranteed by the Spectral 

diagonalization 
Theorem. 

symmetric 

values 

gives 

of A. 

(b) 

Show that, 
matrix A, Theorem 
decomposition of A. 

7 .14 

symmetric 

the spectra

for a positive definite, 

gives 

l 

invertible 

28. If A is an 
show that !, is invertible 
an SVD of A-1• 

matrix with SVD A = U!i vr, 

and that A -I = V!i -I uT is 

29. Show that if A = U!i VT is an SVD of A, then the left 

singular 

vectors 

are eigenvectors 
of AA r. 

30. Show that A and AT have the same singular 
31. Let Q be an 
l matrix such that QA makes 
Show that A and QA have the same singular 

orthogona

values. 

sense. 
values. 

of the 

action 

action 

sphere 

the action 

in IR3 under 

in Exercise 

matrix in Exerc

the action 
of the 

circle in IR2 under the 

35. What is the 

37-40, compute (a) llAllz and (b) cond2(A) for 

of the matrix 
image of the unit sphere 

36. What is the image of the unit 
matrix in Exerc

In Exercises 
the indicated matrix. 
37. A in Exerc

32. Prove Theorem 7.lS(d). 
33. What is the image of the unit 
34. What is the image of the unit circle in IR2 under 
7? 
in IR3 under the 

ise 3? 
of the matrix in Exercise 9? 
ise 3 [ 1 0.9] 

ise 10? 
8 [ 10 
40. A =  100 10 100 �] 
39.A = 1 1 38. A in Exercise 
41. Exercise 3 42. Exercise 
43. Exercise 9 44. Exercise 10 
to Ax = b. 
45.A = [l 2] b -[3] 2 4 ' - 5 
46.A = [� � �],b = [�] 
47.A = [: } � [�: 

45-48, find A+ and use it to compute the mini­
least 

In Exercises 
mal length 

In Exercises 
the given 

41-44, compute the pseudoin

squares solution 

verse A+ of A in 

exercise. 

8 

the orthogonal 

6 1 0   Chapter 

7 Distance 

and Approximation 

48. A � [ � � + � m 

49. (a) Set up 
system 

and solve the 
of equations 

normal equations 
for the 
in Example 
expression 

for the length of a 

(b) Find a parametric 

(c) Find the solution 

solution 

vector in part (a). 
vector 
one produced by the method 

of minimal 

length 
and 

7.40. 

verify that it is the 
of Example 
coordinates 

7.40. [Hint: 
of the vertex 

Recall how to find the 
of a parabola.] 
ly independent 

that when A has linear

50. Verify 

56. Let Q be an orthogonal 

matrix such that QA makes 

sense. 

Show that (QA)+ = A+ QT. 

57. Prove that if A is a positive definite 

matrix 

with SVD 

A =  u2 vT, then u = v. 

58. Prove that for a diagonal 

matrix, 

the 1-, 2-, and 

ClO-norms are the same. 

59. Prove that for any square matrix A, llAll� :::::: llAll1 llAll00· 
value of 
of AT A. 

[Hint: 
A and hence is equal to the largest 
Now use Exercise 
7.2.] 

llA II� is the square of the largest 

eigenvalue 

singular 

34 in Section 

z has been 

in polar 
form as 
and e is its argument, with k01 = 1. (See Appendix C.) 
real number 
into a stretching factor r and 
a rotation 
A = RQfor square matrices, called 
the polar 
decompositio
60. Show that every 

decomposed 
factor e;0• There 

square matrix A can be factored 

as 

n. 

is an analogous decomposition 

positive semidefinite 

umns, the definitions 
and in Section 

7.3 are the same. 

written 
of pseudoinverse in this section z = re;8, where r = lzl is a nonnegative 

col-� Every complex number can be 

51. Verify 

that the pseudoinverse (as defined 
the Penrose conditions 

in this  Thus, 
for A 

satisfies 

section) 
(Theorem 7.12 in Section 

7.3). 

52. Show that A+ is the 

only matrix that satisfies the 
for A. To do 
this, 

assume 

that 
: 

Penrose conditions 
A' is a matrix satisfying the Penrose conditions
(a) AA'A = A, (b) A'AA' =A', and (c) AA' and A' A 
Prove that A' = A+. [Hint: 
are symmetric. 
Penrose conditions 
A+= A'AA +and A'= A'AA +. It is helpful 
that R = U2 UT and Q = UVT have the 
Then show 
that condition 
A' A = AT (A') T, with similar versions 
es.] 
right 
properti

A  = RQ, where R is symmetric, 
and Q is orthogonal. 
rewritten 

A  = U2 VT = l/2(UTU)VT = (U2, UT)(UVT) 

to note 
as AA' = (A') TAT and 

for A+ and A' to show that 

( c) can be written 

Show that the SVD can be 

Use the 

[Hint: 

for A+.] 

to give 

53. Show that (A+ t = A. [Hint: Show that A satisfies 

the 

for A+. By Exercise 52, 

A must 

Penrose conditions 
therefore 
be (A + t.J 

54. Show that (A +)T = (AT)+. [Hint: 
the Penrose conditions 

satisfies 
(A+ ) T must therefore 
be (AT)+.] 

Show that (A+ )T 
for AT. By Exercise 

52, 

55. Show that if A is a symmetric, 

idempotent 

matrix, then 

61-64. 

Find a polar decom
Exercises 
61. A in Exercise 3 

63. A  = [ 1  2] - 3  - 1  

position 

of the matrices in 

A+ = A. 

14 

62. A in Exercise 

64. A =  H -� -:

l 

APPiications 

or Funclions 

Annroximalion 
In many applications, 
function. 
tion g(x) = c + dx on some interval 
function

f, and we want to approximate 

a given function 
f (x) = ex by a linear 
[a, b] 

[a, b] .  In this case, 
it as closely as possible on the 

For example, we might want to approximate 

necessary to approximate 

it is 

func­

interval 

by a "nicer" 

we have a continuous 

Section 

7.5 Applications 

6 1 1  

by a function 
follows: 

g in the subspace 

<;JP1. The general problem can be phrased 

as 

Given a 
find the 

continuous 
function 

function 

"closest" 
to f in W. 

f on an interval 

[a, b] and a subspace 

W of� [a, b], 

The problem 

is analogous 

to the least 

squares 

fitting of data points, except now we 

have infinitely 
What should 
tion Theorem holds 
the answer. 

many data points-namely, the 
"approximate" 

mean in this context

points on the graph of the function 

f 

? Once again, 

the Best Approxima­

The given function 

f lives 

in the vector 

space� [a, b] of continuous 
on 

functions 

the interval 

[a, b]. This is an inner 

product space, 

with inner 

product 

(f, g) = r f(x)g(x) dx 

a 

If W is a finite-dimensiona
projection 
is given by the 
an orthogona
l basis for W, then 

l subspace of� [a, b], then the best approximation 
to f in W 
off onto W, by Theorem 7 .8. Furthermore, 
if { u1, . . .  , uk} is 

.  (u1,f)  (uk>f )  
proJw(f) = -( --)u1 + · · · 
+ -( --)uk 
Uk, Uk 

U1, U1 

[ -1, l ]. 

tion to f(x) = ex on the interval 
are polynomials of degree 

1, so we use  the 

subspace 

Example 1 . 4 1  

Find the best linear 

approxima

product 

A basis 

functions 

Solution Linear 

W =<;If i[ -1, l ]  of� [-1, l ]  with the inner 
(f,g) = r f(x)g(x)dx 
-] 
for <;If i[-1, l ]  is given by {l, x}. Since 
(1, x) = r x dx = 0 
-] 
r ( 1 . ex) dx r xex dx 
-] 
-] 
1  + 1 x 
J (l · l)dx J x2dx 
-] 
-] 
- -2- +- z-x 3 

an orthogonal 

this is 

basis, 

2e-1 

so the best approximation 

to f in W is 

6 1 2   Chapter 

7 Distance 

and Approximation 
�  ':here we ha�e used integra
tion by parts to evaluate r xex dx. (Check these 

tions.) See Figure 7.24. 

-I 

calcula-

y 

f(x) = eX 

- 1  

Figure J . 2 4  

llJ -gll 

The error 

in approximating 
the distance 

by the 
f by g is the one specified 
llJ - gll between f and g relative 
to the inner 

Best Approxima­
product on 

tion Theorem: 

CtS [ - 1, l] . This error 

is just 

and is often 
root mean 

the root mean square error. With the aid of a CAS, we find that the 

called 
square error 

II ex -(t(e - e-1) + 3e-1x) II = � f 1 (ex -t(e - e-1) -3e-1x)2 dx =  0.23 

in Example 

7.41 is 

Remark The root mean square error 

between the graphs off and g on the 
the graphs 

off and g on the interval 

can be 
specified 
[a, b] is given 

thought of as analogous 
that the area between 

interval. 
Recall 

to the area 

by 

r IJ (x) - g(x) I dx 
a 

(See Figure 7.25.) 

above Remark 
value sign 

Although 

the equation 

in the 

is a sensible measure of the "error" 

absolute 
to use and therefore preferable. 

between f and g, the 
square error 
is easier 
"compensate'' 
for the squaring 
would be for the area between the curves. For comparison 
the graphs off andg in Example 
7.41 is 

and to keep the unit of measurement 

the same as 

The square root is necessary to 
it 

makes it hard to work with. The root mean 

the area between 

purposes, 

r lex -t(e - e-1) -3e-1xl dx =  0.28 
-] 

a 

b 

Figure J . 2 5  

Section 

7.5 Applications 

6 1 3  

Example 4 . 3 0  

to f(x) = ex on the 

interval 
[ - 1, l ] .  

approximation 
function 

Find the best quadratic 
Solution A quadratic 
W = <!J> 2 [ - 1, l]. This time, 
we can construct 
Example 
7.8. The result 

the standard basis 

is a polynomial of the 

form g(x) = a + bx + cx2 in 
orthogona
l. However, 
basis using the Gram-Schmidt Process, 
as we did in 

{ 1, x, x2} is not 

an orthogonal 

is the 

set of Legendre 

polynomials 

{1,x, x2- H  
we compute 
terms in this calcula

Using this set as our basis, 
projw(ex). The linear 
only require 

the additional 
calcula

tions 

the best approximation 

to f in W as g(x) = 

tion are exactly 

as in Example 

7.41, so we 

Then the 

best quadratic 

and (x2 -t, x2 -ti= r (x2 -t)2dx = r (x4 -tx2 + !)dx = !s 
-I 
(1, ex/ (x, ex/  (x2 - t, ex/ 2 I 
1, 11 X,Xf  (x -3,X -3l 
( -, 1  + -
g(x) -proJw(e ) --
( -, x + 2 1 2 1 (x -3) 
I 
I  t(e -7e-l) 2 I 
I 
= 2(e - e- ) + 3e-x + 8  (x -3) 
-----

-I 
45 

approximation 

3(1 le-1 - e) 

lS(e -7e-1) 

- . x -

+ 3e-1x + 

4 

4 

to f(x) = ex on the interval 
[ - 1, l ]  is 

x2 =  1.00 + l.lOx + 0.54x2 

(See Figure 7.26.) 

f(x) = ex 

g(x) =  1.00 + l.l Ox  + 0.54x2 

y 

5 

4 

3 

2 

- 2  - 1  

2 

Figure 1 . 2 6  

6 1 4   Chapter 

7 Distance 

and Approximation 

Notice 

how much better the quadratic 

approxima

tion in Example 

7.41. It turns 

out that, 

tion in Example 
in the quadratic case, 

7.42 is than the 

the root 

linear 
mean square error 

approxima
is 

llex -g(x)ll 

the higher 

the degree 

of the approximating 

polynomial, the smaller the 

In general, 
error 

In many applications, 

and the better the approximation. 
functions 
is particul
periodic 
of a vibrating 

cosine function
mated displa
electrica

or almost 
the motion 

s. This method 

are approximated 
by combina

tions 
arly useful if the function 
behavior (such as that of a sound wave, an 

system). A function 

l impulse, or 

ys periodic 

of sine and 

of the form 

being approxi­

p (x) = a0 + a, cos x + a2 cos 2x + · · · 

+  an cos nx + b, sin x 

(1) 

+ b2 sin 2x + · · · 

+ bn sin nx 
if an and bn are not both zero, then 

p(x) is said 

is called 
to have 

a trigonometric polynomial; 
order n. For example, 

p(x) = 3  -cos x + sin 2x 

+  4 sin 3x 

is a trigonometric 

polynomial of order 
3. 
our attention 
to the 

restrict 

Let's 

vector 

space� [ - 7f, 7f] with 

the inner 

product 

(f, g/ = r j(x)g(x)dx 

-rr 
The trigonometric 
polynomials 
of the set 

B = {l, cos x, . . .  , cos 

nx, sin x, . . .  , sin 
nx} 

fin c€ [ -7f, 7f] by a trigonometric 

be projw(f), where W = span(B). It turns 

polynomial 

out that B is an 

for W. Verification of this fact involves 

showing 

7.43 presents some of the necessary calcula

you are asked to 

in B are orthogonal 

to the 

given inner 

with respect 
tions; 

The best approximation to a function 
of order n will therefore 
hence, 
orthogonal 
a basis 
set and, 
that any two distinct 
functions 
product. Example 
provide 

the remaining 

ones in Exercises 

17-19. 

of the form 

in Equation 

(1) are linear 

combinations 

Example 1 . 4 3  

Show that sin 

Solulion Using a trigonometric 

jx is orthogonal 

to cos kx in C{i; [ - 7f, 7f] for j, k 2: 1 .  
identity, we compute as follows: If j * k, then 
r sin jx cos kx dx = tr [ sin(j +  k)x + sin(j - k)x l dx 
cos() - k)x]rr 
-rr 

-TT  -TT = _1[cos(j +  k)x 
=O 

+ 
2  · + k  

is periodic 

with period 27f. 

function 

· - k  

} 

} 

since 

the cosine 

Section 

7.5 Applications 

6 1 5  

Ifj = k, then f 1T 

-1T 

sin kx cos kx dx = -[ sin2 kx] rr_1T = 0 

1 
2k 

sin br = 0 for any integer 

k. 

since 

In order 

to find 

the orthogonal 

projection 

subspace W spanned by the orthogona
norms of 

l basis 
vectors. For example, using 

the basis 

of a function 
B, we need to know the squares 
a half-angle 

formula, 

we have 

of the 

f in Cfb [ -n, 1T] onto the 

-rr 

(sin kx, sin kx) = r sin2 kx dx 
= tf (1 -cos 2kx)dx 
_  sin 2kx]rr 
= t[x 
that (cos kx, cos kx) = 1T and (1, 1) = 2n . 

2k -1T 

-1T 

= 1T 

+ a" cos nx + b, sin x + · · · 

+ b" sin nx (2) 

In Exercise 20, you are asked to show 

ak = 

bk = 

f(x)dx 

where 

We now have 
projw(f) = a0 + a1 cos x + · · · 

(  k  k 

1, 11 21T -rr 

X1 1T -rr 
sm  x, sm 
by Equations 

(l,f) 1 f 1T 
a0 = -(-1 = -
(cos kx,f) 1 J" 
cos X, COS X /  1T -rr 
1 = - f (x) cos kx dx 
(sin kx,f) 1 J"  . 
( . k . k 1 = - f (x) sm kx dx 
fork 2 1. The approximation to f given 
Fourier approximation to f on [ -n, 1T]. The coefficients 
tion to f(x) = x on [ -n, 1T]. 
1 Jrr 
a0 = -
x dx = -
-
1 [x2]1T 
and fork 2 1, integration 
1  ]rr 
x cos kx dx = 1T k sin kx + kl cos kx 
l[x 

Find the fourth-order Fourier 

21T -rr  21T  2 -rr 

Solution Using formulas 

are called the Fourier coe

lf1T 
ak = 1T _1T 

approxima

(3), we 

by parts 

obtain 

fficients off 

yields 

= 0 

= 0 

_1T 

(3) 

(2) and (3) is 

a0,  a1 ,  ••• , an, b1, . . .  , b" 

called 

the nth-order 

Example 1 . 44 

k 

and 

7T k 

Jean-Baptiste 

if k is odd 

2(-l)k+l 

6 1 6   Chapter 
7 Distance 
and Approximation 
1 J"  1 [ x 
kx dx = - --cos kx + 2 sin kx 
1 ]" 
7T -rr 
k -rr 
bk = - x sin 
{1 if k is even 

approximation to f(x) = x on [ -7T, 7T] is 
to f(x) = x on [ -7T, 7T]. 
( 1 768-1 830) was a French 
tician 
and physicist 
mathema
who 
through 
prominence 
gained 
his 
into 
investigation 
the theory 
of 
In his landm
heat. 
ark solution 
of 
the so-called 
heat equation, 
he 
techniques related 
introduced 
to 
what 
are now 
as Fourier 
known 
in many 
a tool 
series, 
widely 
used 
branches 
of mathematics, 
physics, 
and engineering. 
was a 
Fourier 
political 
activist 
during the 
French 
and became 
revolution 
a favorite 
of Napoleon, acc
him 
ompanying 
on his Egyptian 
in 1798. 
campaign 
Later 
appointed 
Napoleon 
Fourier 
Prefect 
ofisere, 
where 
he oversaw 
nt enginee
importa
many 
ring 
In 1808, Fourier 
projects. 
was made 
n  = l 
a baron. 
orated 
He is commem
by 
a plaque 
on the Eiffel 
Tower. 

2(sin x -� sin 2x + t sin 3x - i sin 4x) 
tions 
approxima

Figure 7.27 shows the first four 
Fourier 

fourth-order Fourier 

It follows 

that the 

Joseph Fourier 

y= x  

n = 2  

y 

y 

y 

y 

y= x  

y= x  

n = 3  

n = 4  

Figure 1 . 2 1  

Section 

7.5 Applications 

6 1 1  

You can clearly 

see the 

by computing 

be confirmed 
the Fourier 
The trigonometric 

approximation 

polynomial then becomes 

tions 

approxima
the root mean square error 
increases, 

in Figure 7.27 improving, 
a fact that can 
As the order 
of 
approaches 

it can be shown that this error 

in each case. 

zero. 

an infinite series, and we write 

f (x) = a0 + 2: (ak cos kx + bk sin kx) 

k=l 

the Fourier series off on [ - n, n] . 

This is called 
Mariner 

9 used the Reed-Muller 
9 were so sharp! 

is 24 = 16. 
code Rs, whose minimum distance 
By Theorem 1, this code can correct 
k errors, 
the 
of k for which this inequality 
is true is k = 7. Thus, Rs not only contains 
for transmi
right number of code vectors 
of correcting 
making 
transmi

where 2k + 1 :::::: 16. The largest 
value 
exactly 
64 shades 
reliable. This explains 

tted by Mariner 

up to 7 errors, 

why the images 

of gray but also is capable 

it quite 

tting 

1 Exercises 

1 . 5  

basis 

construct 

to the basis {l, x} to 

approximation to f on 

8. Apply the Gra

an orthogonal 

linear 
2. f (x) = x2 + 2x 
4. f(x) = sin( nx/2) 

of Functions 
1-4, find the best 

Approximation 
In Exercises 
[ - 1, l ] .  
the interval 
1 .  f(x) = x2 
3. f(x) = x3 
In Exercises 
[ - 1, l ] .  
to f on the interval 
5. f(x) = lxl 
6. f(x) = cos(nx/2) 
7. Apply the Gram-Schmidt Process 

5 and 6, find the best quadratic approximation 
for <!/' 2 [ 0, l ]  . 

for<!/' 1 [ 0, l ] .  
9-12, find the best linear 

m-Schmidt Process 
{ 1, x, x 2} to construct 

basis 
an orthogonal 

13-16, find the best quadratic 

In Exercises 
[O, l]. 
the interval 
9 .  f(x) = x2 
11. f(x) = ex  12. f(x) = sin( nx/2) 
In Exercises 
[ 0, l ] .  
to f on the interval 
13. f(x) = x3 
15. f(x) = ex 
Cf6 [- n, 7T] fork 2: 1. 
17. Show that 
forj * k,j, k 2: 1. 
18. Show that cos jx is orthogonal 

1 is orthogonal to 
to cos kx in Cf6 [ - n, n] 

14. f(x) = Vx 

10. f (x) = Vx 

to the 
basis 

approximation to f on 

16. f(x) = sin( nx/2) 

cos kx and sin 

kx in 

l to sin kx in Cf6 [ -n, n] 

forj * k,j, k 2: 1. 
19. Show that sin jx is orthogona
20. Show that 111112 = 2n and II cos kxll 2 = 7T in Cf6 [ - n, n] . 
In Exercises 
order Fourier 
approxi­
mation to f on [ -n, n] . 
21. f(x) = lxl  22. f(x) = x2 
In Exercises 

21 and 22, find the third-
23-26, find the Fourier 

coefficient

s a0, ak> and 

if- n :::::: x < 0 
ifO ::::: x ::=::: n 

1 

23" f (x) = 

1 if 0 :::::: x :::::: n 

25. f (x) =  n  -x 
Recall 
all x;f is called 

bk off on [- n, n]. {O if -n ::=:::x <O 
24. f(x) = {-1
-rr 
-rr 

f is odd. 

function. 

26. f(x) = lxl 
that a function f is an even function 
if f ( -x

an odd function 

if f( -x) = f(x) for 
) = -f (x) for all x. 

27. (a) Prove that r f(x) dx = 0 if f is an odd function. 
28. (a) Prove that r f(x) dx =  2 rf(x) dx if f is an even 

(b) Prove that the Fourier 

ak are all zero if 

coefficients 

0 

(b) Prove that the Fourier 

coefficients 

bk are all zero if 

f is even. 

approximation 

Chapter Review 
Kev Definitions 

and Concepts 

norm (2-norm), 553 
norm, 556 

Inequality, 
539 
number of a matrix, 
562 

Best Approximation 
Theorem, 
570 
Cauchy-Schwarz 
condition 
distance, 535 
Euclidean 
Frobenius 
l Theorem of Invertible 
Fundamenta
Matrices, 
605 
Hamming 
Hamming 
ill-conditioned 
inner product, 
inner 

matrix, 
561 
531 

distance, 554 
norm, 554 

product space, 

531 

error, 572 
solution, 574, 604 
Theorem, 

squares 
least 
least 
squares 
Least Squares 
matrix norm, 556 

max norm ( oo -norm, 

575 

l set of 
orthonorma
537 
vectors, 
pseudoinve
rse of a matrix, 
585,602 

value 

space, 
552 

decomposition 

linear 
norm, 559 

norm), 553 
norm, 535, 552 
normed 
operator 
basis, 
orthogonal 
537 
538, 583 
orthogonal 
projection, 
orthogona
l (set of) vectors, 537 
orthonormal 
basis, 
537 

uniform singular 
(SVD), 593 
singular 
singular 
sum norm (1-norm),  552 
Triangle 
unit sphere, 
535 
unit vector, 
535 
well-conditioned 

values, 590 
vectors, 
593 

Inequality, 

matrix, 
561 

540 

Review Questions 
(a) Ifu = [�:] and v = [ :J then <u, v) 

1 .  Mark each of the following 

statements 
true or false: 

= u1v1 + 

'1Tu2v2 defines 

an inner 

product on IR2. 

inner 

tr(A) 

product on 

product 

+ tr(B) 

an inner 

an inner 

norm, max norm, and Euclidean 

vectors in an 

product space 
= 2, then 

(d) If u and v are 

2u1v2 -2u2v1 + 4u2v2 defines 
on IR2. 
defines 
M22· 

(c) <A, B) = 
with llu ll = 4, l l v l l  
= Vs, and <u, v) 
(e) The sum 
(f) If a matrix A is well-conditioned, 
(g) If cond(A) 
(i) If A is a matrix with orthonormal columns, then 

llu +vll= 5. 
on !Rn are all equal to the absolute 
when n = 1. 
small. 

well-conditioned. 

(h) Every linear 

is small, then the 
matrix A is 

has a unique 

solution. 

system 

least 

squares 

then cond(A) 

value function 

matrix of an orthogona

l projection 

norm 

is 

the standard 
column 
onto the 

space 
is a symmetric 
of A are the 

values 

(j)  If A 

of A is P = AA T. 

then the singular 

matrix, 
same as the eigenvalues 

of A. 

6 1 8  

the definition 
gives 

an 

2-4, determine 

3. <A, B) = 

whether 

tr(A TB) for A, B in M22 

In Questions 
inner 
product. 

2. (p(x), q(x)) = p(O)q(l) + p(l)q(O) for p(x), q(x) in <!/'1 
4. <J, g) = (0��\aI/ (x))(01Jl::/(x)) for f, g in C(b [O, 1 ]  
5 and 6, compute the indicated 
In Questions 
quantity using 
the specified inner 
product. 
5. 111  + x + x2ll if<a0 + a1x + a2x2, b0 + b1x + b2x2) = 
6. d(x, x2) if (p(x), q(x)) = f �p(x)q(x) dx 
a0b0 + a1b1 + a2b2 

Gram-Schmidt Process 

s using the specified inner 
product. 

s 
to the given 
set of 

In Questions 
by applying the 
vector

7 and 8, construct an orthogonal set of vector
7. { [�l [�]} if<u, v) = uT Av, where A = [: !] 
8. {1, X, x2} if (p(x), q(x)) = rp(x)q(x) dx 
9 and 10, determine 

0 
whether 

the definition 

a norm. 

In Questions 
gives 
= vTv for v in !Rn 
9. l l v l l  
10. llp(x) II = lp(o) I + lp(l) -p(o) I for p(x) in <!/'1 

7.5 Applications 

6 1 9  

1 1 .  Show that the 

12. Prove that if Q is an orthogonal n X n 

13. Find the line of best fit through the points (1, 2), (2, 3), 

0.1 1  l 0.1 1 1  is 

0.1 1  

0.11 1 1  

then its 

matrix, 

Frobeni

us norm is 

ill-conditioned. 

0.1 
0.1 1  
0.1 1 1  

matrix A = [ �. l 
II Q II F = Vn. 
solution of [� -�][::] [-!l 
of x  � [ �] onto the 
column sp�e of A � [ � iJ 

! pmjection 

squares 

(3, 5), and ( 4, 7). 
14. Find the least 

15. flnd the mthogona

Section 
onto span (u, v). [Hint: Show that 

l vectors, 
ndard matrix of an or­

show that 

16. If u and v are orthonorma

P = uur + vvr is the sta
thogona
P = A(ATA)-1AT for some matrix A.] 

l projection 

In Questions 
singular value 
the matrix A. 

18,find (a) the singular 

17 and 
decomposition, 

values, (b) a 
of 

and (c) the pseudoin

verse 

18. A = [� - 1] - 1  

17.A = [�  �i 1 - 1  
20. If A is a square matrix for which A2 = 0, prove that 
(A +)2 = 0. 

for which PAQ is 
prove that PAQ has the same singular 

19. If P and Qare orthogona

defined, 
as A. 

l matrices 

values 

Appendix A* 

Mathematical Notation 
and Methods of Proof 

Please, 

sir, I want some 

Charles 

Dickens, 

more. -Oliver 

algebraic 

Twist 

Oliver 
Anyone who understands 
notation reads at a glance in an 
equation results 

reached 

effort has been made to use "mathema
thematical 

tical English
to a minimum. However, 

notation 

" as much as pos­

mathematical 

nota­

keeping ma

In this book, an 
sible, 
tion is a convenient shorthand 
have to do. Moreover, 
ability 
matical 
"obvious" 

to read and 
write 
understanding
. Finally, 
t notation 
if the righ

is used. 

that can greatly 

we 
it is commonly used in every branch of mathematics, so 
the 
nt of mathe­

is an essential 

mathematical 

y the amount 

notation 

ingredie

simplif

of writing 

there are 

some theorems whose proofs become 

in mathematics 

theorems 
Proving 
it is often 
hard to 
approaches, 
any one of 
it is important to 
study 
basic 

beginner, 
are many 
a theorem; there 
which might turn out to be the best. To become 
proficient 
at proofs, 
as possible and to get 
of practice. 
as many examples 
to sets. 

is as much an art as a science. 

know what approach 

to use in proving 

mathematical 

For the 

plenty 

notation applied 
sums, is also discussed. 

Summa­
Finally, 

a useful shorthand for dealing with 
examples. 

to proofs are illustra

ted with generic 

arithmetically only 
labour 

and pai
ns. 

with great 

This appendix summarizes 

into the Mathematical 
Researches 
Principles of the Theory of Wealth 

tion nota
tion, 
some approaches 

-Augustin 
Cournot 
p. 4 
T. Bacon 
by Nathaniel 
Translated 
Macmillan, 
1897, 
tion is 2x + 3y -z = 0. 
{l, 2, 3}, {a, t, x, z}, {2, 4, 6, 

Set Notation 
A set is a collection 
of sets include 
the set of 
library

It is often possible 
enclose the list within 

, the set of positive integers, 

of objects, called 

the elements (or members) 

of the set. Examples 

all words in 

this text, 

the set of 
and the set of all vectors 

all books in your college 
in the plane whose equa­

to list the elements 
braces. For example, we have 

to 

ional 

of a set, in which case it is convent

. . .  , 100}, { n 2n n 4n Sn} 

- - - -
4'5'2'7'" " "'6 
by N or z+, so 

sets are often ex
denoted 

is usually 

pressed 

omitted 

when a 

pattern 

using 

is present. 

(What is 
ellipses. 

-

.-.  Note that ellipses ( . . .  ) denote 
in the last 
the set of 

the pattern 
For example, 

two examp
positive integers 

elements 

les?) Infinite 

The set of all integers 

is denoted 

N = z+ = {l, 2, 3, . . .  } 
by "ll._, so 
"1l._ = { . . .  ,-2,- 1, 0,1,2, . . .  } 

Two sets 
in which 

are considered 
elements 

to be equal if they contain exactly 
Thus, 

the same elements. The 
are not counted. 

and repetitions 

are listed does 

not matter, 

order 

*Exercises and selected 
website. 

odd-numbered answers for this appendix can be found on the student companion 

A 1  

12 

Appendix 

Notation 
of" or "is in;' and the symbol fl_ denotes 

A Mathematical 
and Methods 
of Proof 
s E z+ but o fi- z+ 

not an element of" or "is not in:' For example, 

{l,2,3} = {2, 1,3} = {l,3, 2, l} 

the 

The symbol E means "is an element 

negation-tha

t is, "is 

It is often more convenient to describe 

a set in terms of a rule satisfied 

by all of its 

elements. In such cases, 

set builder notation is appropriate. The 

format is 

where P represents 
satisfy. The colon is pronounced 

a property or a collection 
For example, 

that:' 

{x: x satisfies 

P} 
of properties 

that the element 

x must 

"such 
{n: nE Z, n> O} 

is read as "the set of all 
This is just another 
write 

z+ = {n E Z :  n > O}.) 
The empty set is the set 

n such that n is an integer 

way of describing 

the positive integers 

and n is greater than zero:' 
z + .  (We could also 
0 or { } . 

by either 

with no elements. It is denoted 

Example A . 1  

Describe in words the following 
sets: 
(a) A = {n : n = 2k, k E Z }  

(c) C = {x E IR :  4x2 -4x - 3 = O} 

(b) B = {m/n: m, n E Z, n * O} 
(d) D = {x E Z :  4x2 -4x - 3 = O} 

A is 

of2. Therefore, 

of all even integers. 

of numbers n that are integer multiples 

set of all expressions 
This is the set 

Solution (a) A is the set 
the set 
(b) B is the 
is nonzero. 
way of describing 
above, is 
precis
(c) C is the set of all real solutions 
using the quadratic 
y this.) Therefore, 

Q produces 

of rational numbers, usually 
only one occurrence 
ely describes the set 
of all rational 
numbers.) 

ion, as noted 
of each element. Thus, this expression 

by Q. (Note that this 
of the form m / n, where m and n are integers 
of this equation are - � and �. 

of the equation 4x2 -4x -3 = 0. By factoring 

we find that the roots 

that we include 

however, 

our convent

many repeti

denoted 

formula, 

tions; 

and n 

or 

� (Verif

c =  {-t.n 

(d) From the solution to (c) we see that there 
in IR that are inte
D= 0. 

gers. Therefore, 

D is the empty 

are no solutions 
set, which we can express by writing 

to 4x2 -4x - 3 = 0 

(1834-1923) was an 
John 
Venn 
English 
mathematician 
who studied 
at Cambridge 
University 
and later 
lectured 
He worked 
there. 
primarily 
in mathematical 
best 
logic 
and is 
known 
for inventing 
Venn 
diagra
ms. 

subset of B, 
a Venn diagram, 
set, a set large enough 
A and B.) 

If every element 
denoted 

of a set A is also an element 

A <: B. We can represent 

this si

as shown in Figure A. l .  (The rectangle 

of a set B, then A is called 
tuation schematically using 
the universal 

represents 
question-in 

this case, 

sets in 

a 

to contain 

all of the 

other 

A3  Appendix 

A Mathematical 
and Methods 
of Proof 

Notation 

Figure A . 1  
A C:  B 

Example A . 2  

(a) {1, 2, 3} (;::; {1, 2, 3, 4, 

5} 

(b) z+ (;::; Z  (;::; IR 

Example A . 3  

(c) Let A be the set 
the set of all posi
the form 

tive integers 

of all positive integers 

that are evenly divi

whose last two digits 

are 24 and 
let B be 
sible by 4. Then if n is in A, it is of 

for some integer 

k. (For example, 

n = lOOk + 24 
36,524 

= 100 · 365 + 24.) But then 

n = lOOk + 24 = 4(25k 
+ 6) 

so n/ 4 = 25k + 6, which is an integer. 
Therefore, 

Hence, 

A (;::; B. 

n is evenly 

divisible 

by 4, so it 

is in B. 

4 

We can show that two sets A and B 

are equal by showing that each is a subset 

the other. This strategy is particularly 
not easy 

to list and 

compare their 

elements. 

useful if the sets are defined 

of 
abstractly 

or if it is 

of all positive integers 

whose last two digits 

form a number that is evenly 

by 4. In the case of a one-digit 
of all positive integers 

Let A be the set 
divisible 
the set 
Solution As in 
split 

off the 

Example 

number, 

we take its tens digit 
Let B be 

to be 0. 

that are evenly 

Show that A = B. 
A.2( c ), it is easy to see that A (;::; B. If n is in 

divisible 

by 4. 

A, then we can 

number m formed by its last two digits 

by writing 

n = lOOk + m 

have m  = 4r for some integer 

r. 

for some integer 
Therefore, 

k. But, since 

m is divisible 

by 4, we 

n = lOOk + m = lOOk + 4r = 4(25k 
+ r) 

A (;::; B. 

so n is also evenly 

divisible 

To show that B (;::; A, let 
n = 4s, where s is an integer
then, 

as above, 

by 4. Hence, 
n be in B. That is, n is evenly 
. If m is the 

say that 
number formed by the last two digits 

divisible 

by 4. Let's 

of n, 

n = lOOk + m for some integer 

k. But now 

m = n  -lOOk = 4s -lOOk = 4(s -25k) 

which implies 
is in A, and we have shown that B (;::;A. 

that m is evenly 

Since 

A (;::; B and B (;::; A, we must have A = B. 

divisible 

by 4, since 

s -25k is 

an integer. 

Therefore, n 

Appendix 

A Mathematical 
and Methods 
of Proof 

Notation 

14 

The intersection 

of sets A and B is denoted 

by A n B and consists 

of the elements 

that A and B have in common. 

That is, 

A nB = {x : xE A  and x E B} 

Figure A.2 shows a Venn 
A U Band consists 

of the elements 

of this case. 
that are in 

The union of A and B is denoted 
either 

A or B (or both). That is, 

by 

diagram 

A U B= {x : xE A  or x E B} 

See Figure 
A.3. 

Figure A . 2  
A n B  

Let A =  {n2: n E z+, 1 ::s n ::s 4} and let 

B = {n E z+: n ::s 10 and n is odd}. Find 

Figure A . 3  
A UB 

A n  B andA U B. 

Solution We see that 

A =  {12, 22, 32, 42} = {l, 4, 9, 
A n  B = {l, 9} andA U B = { l ,  3, 4, 5, 7, 9, 16}. 

Therefore, 

16} and  B =  {l, 3, 5, 7, 

9} 

If A n B = 0, then A and Bare called 

the set of 

even integers 

and the set of odd integers 
are disjoint. 

disjoint sets. (See Figure A.4.) 

For example, 

Example A . 4  

G 

notation 

summalion Nolalion 
Summation 

Fiuure A . 4  Disjoint 
sets 
L: is the capital 
letter 
Greek 
correspond
ing to 
S (for "sum"). 
Summation 
notation 
was 
by Fourier in 
intro
duced 
1820 and 
was quickly 
adopted 
by the 
mathematical 
community. 

Is this the sum 
4 +  8 + 
(or sigma notation) .  

16 +  32  + 

sigma, 

is a convenient shorthand to use to write 

out a sum such as 

1  +  2  +  3  + 

. . .  + 100 
a few terms. 
As in 
and have simply 

where we want to leave out all but 
that we have established 
In the above example, 
the positive integers 
what would one make of the following 

set nota
left out some intermediate 
terms. 
summing 
all of 
. For example, 

to recognize 
ellipses can be ambiguous

a pattern 
readers 

from 1 to100. However, 

ellipses( . . .  ) convey 

that we are 

are expected 

tion, 

sum? 

1  +  2  + 

. . .  +  64 

of all positive integers 

from 1 to 64 or just the powers of two, 1  + 2 + 

64? It is often clearer 

(and shor

ter) to use summation notation 

15  Appendix 

A Mathematical 
and Methods 
of Proof 

Notation 

We can abbreviate 

a sum of the form 

al + a2 + . . .  + an 

as 
us to 
which tells 
tive version 
of this expression 

sum the 
is 

terms ak over all integers 

k ranging 

from 1 ton. An alterna­

(1) 

(2) 

k is called 

The subscript 
that it 
letter 
else in the expressions 

does not appear in the 
we like as the index of summation 

the index of summation. It is a "dummy variable" 
we can use any 

sum in expression 
(1). 

Therefore, 

actual 

in the sense 

we are summing). Thus, expression 

(2) can also be written as 

(as long 

as it doesn't already 

appear somewhere 

The index of summation 

need not start 

at 1. The sum a3 + a4 + · · · + a99 becomes 

99 
2: ak 
k=3 

although 

we can 

arrange 

index to begin at 1 by rewriting 

for the 
97 
2: ak+2· 
k=l 
effectively is being able to recognize 
summation 

the expression 

notation 

patterns. 

as 

The key to using 

Write the following sums using 
(a) 1  + 2  + 
Solution (a) We recognize 

4 + .. · + 64 (b) 1  + 3  +  5 + 
1  + 2 +  4 + . . .  + 64 =  2° + 21 + 22 + . . .  + 26 

. .. + 99 (c) 3  +  8 + 
as a sum of powers of 2: 

summation nota
tion. 

this expression 

15 + . .  · + 99 

Therefore, 

the index of summation 

appears as 

the exp

onent, 

6 
and we have 2: 2k. 
k= O  

is the sum of all the odd integers 

from 1 to 99. Every odd integer 

is of 

(b) This expression 
the form 2k + 1, so the sum is 

1  + 3 +  5  + . . .  + 99 

49 
k=O 
(c) The pattern 
here is less clear, 
a perfect 

=  (2. 0  + 1) + (2. 1  + 1) + (2. 2 + 1) + . . .  + (2. 49 + 1) 
=  2: C2k + 1) 
=  (22 -1) + (32 -1) + (42 -1) + . . .  + (102 -1) 

3 +  8  + 15 + . . .  + 99 

reflection 

but a little 

reveals 

square: 

that each term 

isl less than 

10 
2: Ck2 -1) 
k=2 

Example A . 5  

Example A . 6  

Rewrite 

Solution (a) If we use the change 
i goes from 1 to 7. Since k = i -1, we 

A6 

each of the sums 

starts 

in Example 

of variable i = k + 1, then, 

A.5 so that the index of summation 
at 1 .  

Appendix 

Notation 
7 

A Mathematical 
and Methods 
ofrroof 
6 
fi 
� 
tion i = k -2 will work (try it), but it is easier 
to k = l, since 

2: (2k + 1) = 2: (2(i -1) + 1) = 2: (2i -1) 
k=O 

same substitution 

:L 2k = :Li-I 
k=O  i=l 

just to add a term 

as k goes from 0 to 6, 

obtain 

part (a), we get 

corre­

as in 

12 - 1 

� 

i=l 

i=l 

(c) The substitu
sponding 

(b) Using the 

= 0. Therefore, 
IO 
2: (k2 -1) = 2: (k2 -1) 
k=2 

k=I 

IO 

� 

Multiple 

summations 
arise 

when there 

is more than one index of summation, 

as 

there 

is with a matrix. The notation 

(3) 

means to sum the 
in expression 

terms 

(3) is equivalent 

to either 

a;1 as i and j each 

range independently from 1 to n. The sum 

where we sum first 

over j and then over i (we always 

work from the 
t), or 

inside ou

i,j=I 

n :Lau 
n n :L :Lai) 
n n :L :LaiJ 

i=l j=I 

j=I i=l 

Example A . 1  

both possible 

orders of summation. 

where the order 

of summation 

is reversed. 

Write out :L ji using 

3 

i,j = I  

Solution 

n :L (ii + i2 + i3) 

3  3 
:L :L ij 
i=l j=l 

i=l 

= (1 +  1  + 1) + (2 +  4  +  8)  +  (3  +  9  + 

27) = 56 

A1  Appendix 

A Mathematical 
and Methods 
of Proof 

Notation 

and 

3  3 n 
2: 2:ij = 2: 0j +  2j +  3j) 
j�li�I j�I 

( 11 +  21 +  31)  + ( 12 +  22 +  32)  + ( 13 +  23 +  33) 
(1 +  8  + 27) = 56 
( 1  +  2  +  3)  + 

( 1  +  4  +  9)  + 

Remark Of course, 

of summation 

the value of the sum 
because 

in Example 
the sum is finite. 
in calculus
have a value and great care must be taken when rearranging 

infinite sums (known as infinite series 

we choose, 

A.7 is the sa

It is also possible 
), but such sums 

to 
do not 

me no matter 

or manipulating 

which order 
consider 
always 
their 

terms. For example, 

suppose we let 

. .  ·) 

Then 

How to Solve It 

is the error?) 

gative 

S is a sum of 

terms! (Where 

nonsense, 
since 

S =l+ 2 + 4 + 8 +  

that S  = - 1. This is clearly 

from which it follows 
non-ne

1  + 2(1 +  2  +  4  + 
1  + 2S 

is the title 
of a book 
hematician 
by the mat
George 
Since 
its publi­
(1887-1985). 
P6lya 
How to Solve It has 
in 1945, 
cation 
a million 
copies 
and has 
sold 
over 
s. � 
language
translated into 17 
been 
P6lya 
in Hungary
, but 
was born 
because of the political situation 
in 
Europ
e, he moved 
to the United 
States in 
1940. He subse
quently 
taught 
at Brown 
and Stanford 
Unive
rsitie
s, where 
he did mathe­
and devel
matical 
a 
research 
oped 
well-deser
ved reputation 
as an 
outstanding 
. The P6lya 
teacher
annually 
awarded 
Prize is 
by the 
Society 
for Industrial 
and Applied 
Mathematics 
for major contribu-
tions 
to areas 
of mathematics 
close 
to those 
P6lya 
on which 
worke
d. 
The Mathematical 
Association 
of America annually awards 
P6lya 
Lectureships 
to math-
demons
ematicians 
trating 
the high-
quality 
exposition 
for which 
was known. 
P6lya 

Melhods of Proof 
The notion 
is true; 
by means of a logica
you how to do proofs; you will become 
try to teach 
and by practicin
examples 
this text. The intention 
amples 
ther illustrations 

speaking, 
proofs. Many theorems have the structure 
or premise) and Q (the conclusion
such an implication 
of implica

P => P1 => P2 => · · · => Pn => Q 

by P => Q. A direct 

proofs 
P (the 
"if P, then Q;' where 
that are either 
true or 
proceeds 

and indirect 
hypothesis, 
false. We denote 
ing a chain 

of some types of proofs. The proofs of theorems 

is simply to provide a few elementary 

g-something 
section 

better at doing proofs by studying 

of mathematics. It is one thing 

proofs fall into two categories: 

) are statements 

to know why it is true 

do often as you work through 

mathematical 

and to be able to demonst

ts. The intention 

sequence of statemen

in the text will provide fur­

of "how to solve 

lly connected 

is at the very 
heart 

you should 

of this brief 

another 

Roughly 

by establish­

it is quite 

of proof 

proof 

tions 

leading 

directly from P to Q. 

it:' 

direct 

ex­

to know what 
rate its truth 
here is not to 

Example A . 8  

Prove that any 
tion can be rephras

a -b is odd:' Hence, 

it has the form P => Q, with P being "a and bare consecutive 

ed as "Prove that if a and b are consecutive 

by an odd number. This instruc­

squares, then 

perfect 

squares 

perfect 

two consecutive 

differ 

perfect 

squares" 

and Q being "a -b is odd:' 

Appendix 

A Mathematical 
and Methods 
of Proof 

Notation 
tive perfect squares, with a > b. Then 

that a and bare consecu

AB 

Solulion Assume 

a = (n + 1)2 and b = n2 

a - b = (n + 1)2 - n2 
= n2 + 2n +  1 - n2 

= 2n +  1 

for some integer 

n. But now 

so a - b is odd. 

There are two 

types of indirect proofs that can be used to establish 
l 

a conditiona

statement 
of the form P =;. Q. A 
just as in 
P is true, 
a direct 
strategy 
show that this is not 
then is to 
the conclusion 
that Q must be true. 

is false) by finding 

proof by contradiction 

assumes 

that the hy

pothesis 

proof, but then supposes that the conclusion 

possible (i.e., 

to rule out 

the possibility 

a contradiction 

to the truth of P. It then follows 

Q is false. The 
that 

Example A . 9  

Let n be a positive integer. 
to find a direct 
that follows.) 

proof 

Prove tha

of this assertion; 

t if n2 is even, so is n. (Take a few minutes 
to try 
it will help you 
proof 

to appreciate the indirect 

Solulion Assume 
n is not 

even. Then n is odd, so 

that n is a positive integer such 

that n2 is even. Now suppose that 

for some integer 

k. But if so, we have 

n = 2k +  1 

n2 = (2k + 1)2 = 4k2 + 4k +  1 

so n2 is odd, since 
hypothesis 
have been false; in other 

words, 

that n2 is even. We conclude that our 

it is 1 more than the 

even number 4k2 + 4k. This contradicts 

our 
supposition that n was not even must 

n must be even. 

of proof 

Closely related 

method 
to the 
tive. The negative 
of a statemen
ated symbolically as -,p and pronounced 
-in other 
-,pis "it is not 
P =;. Q is the statement 

"not P:' 
For example, 
words, 

the case that n is even" 

t P is the statement 

of the statement 

"it is not 

by contradiction 
­

is proof by contraposi­
the case that P;' abbrevi

if P is "n is even;' 
then 
•Q =;. •P. A conditional 

�  they are either both 

The contrapositive 

statement 

P =;. Q and its contrapositive •Q =;.-,pare logica
(For example, 

true or both false. 

note that if the hypothesis 

•Q is true, 

"n is odd:' 
lly equivalent 
if P =;. Q is a theorem, 
then so is •Q 
-,p 

then Q is false. 

in the sense 

that 

The conclusion 

=;. •P. To see this, 
be false, 
cannot 
imply the truth 
proved 

for if it were, 
of Q, giving 
•Q =;. •P.) Here is 

then P would be true and our known theorem 
us a contradiction. 

that -,p is true and we have 

It follows 
of the assertion 

in Example 
A.9. 

a contrapositive proof 

P =;. Q would 

19  Appendix 

A Mathematical 
and Methods 
of Proof 

Notation 

Example A . 1 0  

Let n be a positive integer. 
Solulion The contrapositive of the given statement is 

Prove that if n2 is even, so is n. 

"If n is not even, 

then n2 is not even" or "If n is odd, so is n2" 

To prove this contrapositive, 
ger k. 
completes 
original 

As before, this means that n2 = (2k + 1)2 = 4k2 + 4k +  1 is odd, which 
the proof of the 
statement. 

that n is odd. Then n = 2k  +  1 for 
some inte­

contrapositive. Since 

the contra

positive 

assume 

is true, 

so is the 

Although 

we do not require 

a new method 

of proof 

consider 
only if Q" signals 
statement, 
described 

P � Q is "P 

how to prove an "if and only if" 
a double implication, 
te. It is important 

we must prove P =;. Q and Q =;. 
earlier, where appropria

if" part of P � Q is "P 

theorem. A statement of the form 

if Q;' which is Q =;. P; the "only 

which we denote 

to notice 

read as 
ry for P"; Q =;. P is read "Q is sufficient 

P =;. Q is sometimes 

for P" or 

ing P =;. Q. The implication 
is necessa
Taken together
versa. 

are P � Q, or 

, they 

it, we will briefly 

to handle 

by P � Q. To prove such a 
that the "if" part of 

only if Q;' 
mean­
"Pis sufficient 
for Q" or "Q 
"P is necessary for Q:' 

"P is necessary and sufficient 

for Q" and vice 

"P if and 

P. To do so, we can use the 

techniques 

on a chessbo

ard and is allowed 
tour of a chessbo
exactly 
once, 
is a pawn's tour of an n X n chessbo

A pawn is placed 
or vertic
horizontally 
ally. 
that visits 
as described, 
Prove that there 

Solulion [ {:::: ] ("if") Assume 

A pawn's 
each square 

to move one square 
ard is a path taken 
and ending 

starting 

by a pawn, moving 
on the 

same square. 

at a time, 

either 

ard if and only if n is even. 

trated 

in Figure 

that n is even. It is easy to see that the strategy illus­
A.5 for a 6 X 6 
chessbo

ard will always 

give a pawn's tour. 

that n must be even. To this end, 

let's 
to a square of a different 

a pawn's tour of an n X n chessbo
ard. We 
assume 
that n 
color. The total 
num -
in 

an odd number, according 

[ =;. ] ("only if") Suppose that there is 
will give 
a proof 
by contradiction 
is odd. At each move, the pawn moves 
ber of moves in its tour is n2, which is also 
Example 
from that of 
the square on which it started. 
ends where it started, 
hence, 
n is even and the proof 

so we have a contradict

is complete. 

(Why?) This is impossible, since the 
that n cannot be 

pawn 
odd; 

ion. It follows 

A.10. Therefore, the pawn must end up on a square of the opposite color 

to the proof 

statements 

assert that several 

Some theorems 

true if and only if all of the others 

(n) n! n2 - n "  ,, 
= (  ) = ---if and 

each is 
equivalent 
however, 
statemen
an excellent example 

2  2! n - 2 !  2 
of the 
of this approach. 

requires 
it is often easier 
ts. The proof 

Fundamental 

to establish 

are true. Sho

a "ring" 

are equivalent. This means that 

that links 
of n implications 
all of the 
provides 
Matrices 

Theorem of Invertible 

wing that n statements 
only if proofs. In practice, 

are 

Example A . 1 1  

I I 
I I 
I I 

Figure A . 5  

I I 
I I 

Appendix B* 0 

Mathematical Induction 

The ability 
solving. Consider 

is one 
the following 
pattern: 

to spot patterns 

of the keys to success 

in mathematica

l problem 

1  =  1 

1  +  3  =  4 

1 +3 + 5 = 9  

Great fleas have little 

fleas 

upon their backs to bite 'em, 
fleas have lesser 

fleas, 

And little 

and so ad infinitum. 
-Augustus 
De Morgan 
........... 
Green, 
Longmans, 
and Company, 
1872,p.377 

The sums are all perfect squares: 12, 22, 32, 42, 
52• It seems reasona
that is, the sum 
this pattern 
ing at 1, will 
be more precis
then the last odd number in the sum is 2n -1. (Check this in the five cases 
symbols, our conjecture 

will continue 
always 

1  +  3  +  5  +  7  = 

1  +  3  +  5  +  7  +  9  = 

e. If the sum is n2, 
above.) In 

be a perfect square. 

becomes 

of consecu

A Budget of Paradoxes 

try to 

to hold; 

16 
25 

Let's 

ble to conjecture 

tive odd numbers, start­

that 

· · · + (2n -1) =  n2 for all n 2 1 

1  +  3  +  5  + 

(1) 

(1) is really 
our conjecture 

an infinite collection 
seems reasonable, 
need to prove it. This is where mathematical 

of statements, 
one for each 
that the pat­
comes in. 
induction 

we cannot assume 

Notice 

First Principle 

that Equation 

value 
tern continues-we 

of n 2 1. Although 
2. for all k 2 1, the truth 
then S(n) is true for all n 2 1 .  
for some k 2 1 is called 

Let S(n) be a statement 
1. S (1) is true and 

Verifying 

of Mathematical 

Induction 

about the positive integer 
n. If 

of S (k) implies 

the truth 

of S ( k  + 1) 

that S ( l )  is true is 

called 

the induction hypothesis

the basis step. The assumption 
. Using the induction 
to 

that S (k) is true 

hypothesis 

prove that S ( k  + 1) is then true is 
has been referred 
line of dominoes 
step) 
the next domino (the induction 

ple because 
to as the domino princi
will fall down if ( 1) the first 
domino 

and ( 2) knocking down any 

(the induction 

it is analogous 
can be 

knocked 

to showing that 
down (the basis 
will knock 

hypothesis) 

over 

a 

We now use the princi

ple of 

induction 

to prove Equation 

( 1 ). 

domino 
step). See Figure B.l .  
mathematical 

called the induction step. Mathematical induction 

*Exercises 
website. 

and selected 

odd-numbered answers for this appendix can be found on the student companion 

8 1  

82  Appendix 

Induction 

B Mathematical 
-� ..... 
I I '7,, 
I I  / I\ 
I I / / \ 
I I / / ). 
// /_,,..Y .,,..,,.... \ 
A ?-,,,,Y 
\ 
//// ,/' ) 
;.-4 / \ 
\ 
�:-_-_-_-� _-_-_-�J 
1-t""j''xL ;::--:;··'\ 
• 
• 
• 
• 
� 

� '.A '�� • 
•  • 
• 
• 
• 
!!Iii • • 
• • 
� 
• 
• 
• 
• • 
-

• • 
•  • 
• 

If the first domino falls, and ,  ,  , each domino 

• 

then all the dominoes can be made to fall by pushing over the first one, 

Figure B . 1  

Example B . 1  

Use mathematical induction 
to prove that 

· · · + (2n -1) = n2 

1  +  3  +  5  + 

that falls knocks down the next  one,  ,  ,  , 

for all n 2: 1. 
Solulion For n = 1, the sum on the left-
is 12• Since 
the basis 
that the formula is true 
1 +  3  +  5  + 
hypothesis

1 = 12, this completes 

the induction 

Now assume 

hand side is just 1, while 

the righ

t-hand side 

step. 

for some integer k 2: 1. That is, assume 
. . .  + (2k -1) = k2 

that 

(This is 
of proving that the 
formula is true when n = k + 1. We see that when n = k + 1, the left-hand side of 
formula (1) is 

.) The induction 

step consists 

1  + 3 + 5 + 

. . .  + (2(k +  1) - 1) = 1  + 3 + 5 + 
= 1  + 3 + 5 + 
= (k +  1)2 
(1) when n = k + 1 .  

. . .  + (2k + 1) 

. . .  + (2k - 1) + (2k +  1) 

k2 

which is the right-hand side ofEquation 

This completes 

all n 2: 1, by the princi

the induction step, and 

we conclude that Equation 
induction. 

ple of mathematical 

+ 2 k  + l� by the induction 
hypothesis 
(1) is true 

for 

Appendix 

B Mathematical 

Induction 

83 

The next example 

gives 

positive integers. The formula 
solution to 

Exercise 51 in Section 

2.4. 

a proof of a useful formula for the sum of the first n 
for example, 
see the 

in the text; 

appears several 

times 

Example B . 2  

Prove that 

for all n 2 1 .  

1 +  2  +  · · ·  +  n  = 

n (n  + 1) 

2 

Solution The formula is true for n = 1, since 

Assume 

that the 

formula is true for n = k; that 

1 ( 1  + 1) 

l =---2 

1 +2 +  . .  · + k =---

is, 
k (k + 1) 

2 

We need to show 

that the 

formula is 

1 +  2  + . . .  + (k + 1) = -------

l; that is, we must prove 

true when n = k + 

(k + l)[ (k + 1) + l ]  

that 

But we see that 
1 +  2  + . . .  +  (k  + 

2 

1) = ( 1  +  2  + . . .  + k) +  (k  + 

1) 

k (k + 1) 

= 

2 

+  (k  + 

1)  by the induction 
hypothesis 

k (k + 1) + 2(k + 1) 

2 

k2 +  3k  +  2 

(k  + l)(k + 2) 

2 

2 

(k+l)[ (k +l) + l] 

2 
to show. 

the induction 
step, 

of mathematical 

and we conclude that the 
induction. 

formula 

is true 
for all 

In a similar vein, 

which is what we needed 
This completes 

n 2 1, by the principle 
for all n 2 1. (Verif

integers 

satisfies the 

we can prove that the sum of the 
formula 
2  2  2  2 n (n + 1)(2n + 1) 

1 + 2  + 3  + . .  · + n  =------

squares 

6 

y this for yourself.

) 

� 

of the 

first n positive 

84  Appendix 

B Mathematical 

Induction 

The basis 

step need not be for n = 1, as the next two examples 
illust

rate. 

Example B . 3  

Prove that n ! > 2" for all integers 
S o lution The basis step here is when n = 4. The inequality 
since 

n 2: 4. 

is clearly 

true in this case, 

Assume 

k 2: 4. Then 

(k + l ) !  = (k + l )k! 

4! = 24 > 16 = 24 
that k l  > 2k for some integer 
> (k +  1)2k 
> 2 • 2k = 2k+I 
2:  5. 2k 
n ! > 2" for all integers 
for n = k +  1 and 

which verifies 

the inequality 
We conclude that 

induction. 

by the induction 
hypothesis 
since 

k 2:: 4 

completes 
n 2: 4, by the princi

the induction 
step. 

ple of mathematical 

If a is a nonzero real number and n 2:  0 is an integer, 

of the power a" that is compatible 

with mathematical 

we can give a recursi
We define 

induction. 

ve 

definition 
a0 = 1 and, for n 2: 0, 

(This form avoids 
mathematical 

the ellipses used in the version 
property 

to verify a familiar 

induction 

an = a� .) We can now use 

of exponents. 

n times 

Solution At first glance, 
m and n. But we simply 
using the 

it is not clear 
need to keep one of 
other. So, let m 2: 0 be a fixed integer

how to proceed, 
since 

there 

are two variabl
es, 

them fixed and perform our induction 
. When n = 0, we have 

using 

the definition 

Now assume 

For n = k + 1, using 

the basis 
that the formula 
holds 
ve definition 

cation are associa

tive, 

a0 = 1. Hence, 
amak+I =  am(aka) 
= (amak)a 

our recursi
we see that 

step is true. 

and multipl

and the fact that 
addition 
i­

k 2: 0. Then amak = am+k. 

when n = k, where 
by definition 
by the induction 
hypothesis 
by definition 

= a(m+k)+I 
= am+(k+I) 

Example B . 4  

Let a be a nonzero real number. Prove that ama" = am+n for all integers 
m, n 2: 0. 

Therefore, 

the formula is true for n = k + 1, and the induction 

We conclude that a'"a" = am+n for all integers 

mathematical 

induction. 

Appendix 

Induction 

B Mathematical 
m, n 2 0, by the principle of 

step is complete. 

85 

In Examples 

B. l through B.4, 
the use of 

straightforward. 

However, 

the induction 
this is not 

hypothesis 
during 
the 
the case. 
An 

always 

induction step 
alternative 
of the princi
version 

is relatively 

ple of mathematical 

induction 

is often more useful. 

Second Principle 

of Mathematical 

Induction 

Let 5(n) be a statement about the positive integer 
1 .  5 (1) is true 
2. the truth 
then 5(n) is true 

2), . . .  , 5(k) implies 

of 5(1), 5(

n. If 

and 

the truth of 5(k + 1) 

for all n 2 1. 
�  other. (Can you see why?) 

two principles 

between the 
: The first version 
2), . . .  , 5(k) are 

The only difference 
induction 
hypothesis
version assumes that all 
ciple 
(although, 
paradoxic
In fact, however, 

makes the second 
prin­
more in ordertoprove5(k + 1) 
). 
strong 
the 

called 
is sometimes 
lly equivalent
: Each one implies 

of mathematical 
that 5(k) is true, 

induction 
whereas 

than the first, since we 
need to assume 

principle 
are logica

the two principles 

true. This 

of 5(1), 5(

seem weaker 

the second 

assumes 

ally, 

the second 

induction

is in the 

The next example 
induction 

is easier 

presents an instance 
to use than the first. 
factors 

ple of mathe­
Recall that a prime number is a posi­
are 1 and itself. 

in which the second 

princi

whose only positive integer 

matical 
tive integer 

Example B . 5  Prove that every 
uct of primes. 

positive integer 

is prime or can be factored 

into a prod­

n 2 2 either 

is clearly 

Solution The result 
all integers n 
primes. 
product of two smaller integers-say, 

true when n = 2, since 
is prime or can be factored 
between 2 and k, n either 
we are done. 
is prime, 

Let n = k + 1. If k +  1 

that for 
into a product of 
it must factor into a 

Otherwise, 

Now assume 

2 is prime. 

�  Since 

2 ::; a, b ::;  k (why?), the induction 

applies to 

a and b. Therefore, 

k +  1 

= ab 

a = p1 • • • Pr and b = q1 • • • q, 

hypothesis 

where the p's and q's are all prime. 

Then 

ab = P1 ·  ·  · 
of ab into primes, 
is true 

gives a 

factorization 
that the result 
induction. 

We conclude 
of mathematical 

p,q1 ·  ·  · 

completing 
for all integers 

q, 
the induction 
step. 

n 2 2, by the second 

princi
ple 

86  Appendix 

B Mathematical 

Induction 

� 

Example B . 6  

Do you see why the first 
princi

ple of mathematical 

induction 

would have been 

difficult 

to use here? 

and 

tion of in­

proof 

example 

discussed 

real numbers. Prove that 

in Example B.6 

We conclude with a highly nontrivial 

combina
that involves a 
Arithmetic 
Mean-Geometric 
Geometric 
Inequalities 
is due to Cauchy. 

duction  and 
Mean Inequality, 
Optimization Problems. The clever 

backward induction. The result is  the 
in Chapter 7 in Exploration: 

Let xI, ... , xn be nonnegative 
V X1X2'' 'Xn :S 
�nl X1 + Xz + ... + Xn 
n 2 2. 
Solulion For n = 2, the inequality 
1 and 2 of the Exploration 
tive real numbers xI, ... , xk. Let 

above. 
mentioned 
ted inequality, we will prove that S(k) implies 

\/XY :s (x + y)/2. 

verify this in Problems 

for all integers 

If S(n) is the sta

S (2k). Assume 

for all nonnega

You are asked to 

S(k) is true; 

becomes 

that is, 

that 

n 

Then 

by S (2) 
by S (k) 

:s k e1:Y2> .. e2k-12+Y2k) 
= '\fl X1 ... Xk X1 + Xz + ... + xk 
:s �������-
(YI + Y2) (Y2k-1 + Y2k) 
2 + ... + 2 
Y1 + ·  ·  · + Y2k 2k 

k 

k 

Mean-Geometric 

Mean Inequality 
the proof, we need to "fill in the gaps:' 
S(k) is 

is true for n = 2, 4, 
S(k -1). Assuming 

to complete 
to prove that S(k) implies 

which verifies 

S(2k). 

Thus, the Arithmetic 

8, . . .  -the powers of 2. In order 
k -1 

We will use backward 
true, let 

induction 

BJ 

B Mathematical 

Appendix 
Induction 
(X1 + Xz + . . .  + Xk-1) 
< ----------------

x + x + . . .  + 

k - 1 

Then 

1 (xi + Xz + . . .  + Xk-1) I 2 

1 2 k-1  k - 1  -

k x x  . . .  x 

k 
kxk-i 

kx1 + kx2 +  · · ·  + 

k (k -1) 

Equivale

ntly, 

X1 + Xz +  · · ·  + 

xk-1 

k - 1 

. . . (Xi + Xz + . . .  + Xk-1) < (Xi + Xz + . . .  + Xk-l)k 

< (Xi + Xz + . . .  + Xk-l)k-I 

X1X2 . . .  Xk-1 -

- 1 

- 1 

k 

-

X1X2 Xk-1  k 

k - 1 
S(k -1). 

or 

Taking 

the (k -l)th root of both sides 

The two inductions, 

taken together

yields 
, show that the Arithmetic 

Mean-Geometric 

Mean Inequality 

is true 

for all n 2 2. 

Remark Although 

mathematical 

induction 

is a powerful and indispensable tool, 

it cannot work miracles. That is, it cannot prove tha
does not. Consider 
regions 

or formula 
holds if it 
Figure B.2, which show the 
maximum number of 
lines. 
be subdivided 

R(n) into which a circle can 

the diagrams in 

by n straight 

t a pattern 

R(O) =  1  =  2

° 

R( l )  =  2  =  21 

R(2) =  4  =  22 

Figure B.2 

Based on the evidence 
and try to prove this conjecture 
since 
discovered that R(3) = 7 *  8 
correct 

this formula is 

formula 

turns 

out to be 

in Figure 
using 

B.2, we might conjecture 
mathematical 
If we had considered 

not correct! 

= 23, thereby 

one more 
case, 
demolishing 

we would have 
In fact, the 

our conjecture. 

induction. We would not 

succeed, 

that R(n) = 2" for n 2 0 

�  which can be verified 

n2 + n +  2 

R(n) = ---- 2 

by induction. (Can you do it?) 
in which a pattern 

For other 
cases 

examples 
are considered, 

when 
"The Strong 
Law of 
American Mathematical Monthly, Vol. 95 (1988), pp. 697-712. 

K. Guy's delightful 

see Richard 

only to disappear 

appears 

article 

to be true, 

in the 

enough 
Small 

Numbers" 

Appendix C* 

Complex Numbers 

l type of complex 

number is a number 

of the form 
a + bi, where a and b are real numbers 
that i2 = - 1. The real number a is considered 
since a = a + Oi. If z = a + bi is a complex 
by Re z, is a, and the imaginary part of z, denoted 
a + bi and c + di are equal if their 
real parts are equal 

A complex 
with the property 
i is a symbol 
number, 
specia
then the real part of z, denoted 
Im z, is b. Two complex 
numbers 
parts 
and their imaginary 
bi can be identified 
with the point (a, b) and plotted in 
plane, or the Argand plane), as shown in Figure 
the real axis and the vertical 
tal axis is called 

number a + 
the complex 
the horizon­

plane, 
the imaginary axis. 

are equal-that is, 

if a = c and b = d. A complex 

to be a 
number, 

C.l. In the complex 

the plane (called 

axis is called 

and 

by 

[The J extension 

of the number 
concept to include 
nal, and 
we will at once add, the imaginary, is 
the greatest 

the irratio

Re 

2i 

4i 

6i 

3  + 2i 

4  6 

- 4  + 3 i  

ysteme 

Theorie 

forward step which pure 

mathematics has  ever 
taken. 

�-+--+--+--+--+�+--+--+--+---+--<,__.,_-+--+
-6  -4  -2  2 

der Complexen 
Zahlens

Im 
-Herma
nn Hankel 
• 
• 
Leipzig, 
1867, p. 60 
"imaginary" 
There 
is nothing 
•  - 2i 
about 
complex 
numbers-t
hey are 
-3 - 2i 
numbers. 
as "re
just 
al" as the real 
- 4i • 1  -4i 
The term 
imaginary arose 
from the 
equations 
of polynomial 
study 
-6i 
such 
as x2 + 1 = 0, whose solu­
al" (i.e., 
are not "re
tions 
real num­
bers)
. It 
is worth 
ing that 
remember
at one time 
negative 
numbers were 
thought 
of as "imaginary" 
too. 
plane 
Figure C . 1  The complex 
Argand 
Jean-Robert 
(1768-1822) 
was a French 
and ama­
accountant 
mathematician. 
teur 
His geometric 
numbers 
of complex 
interpretation 
in 1806 in a book 
appeared 
that he 
privatel
y. He was not, 
published 
however, 
to give 
such 
an 
the first 
interpreta
tion. 
The Norwegian­
Wessel 
Danish 
surveyor 
Caspar 
(1745-1818) gave 
the same 
version 
of the complex 
plane 
in 1787, but his 
paper was 
not notic
ed by the 
until 
mathematical 
community 
after 
his death. 

Notice 
(a + c) + (b + d)i with (a + c, b + d), addition 
as vector 

The product of a + bi and c + di is 
(a + bi)(c + di) = a (c + di) + bi(c + di) 
= ac + adi + bci + bdi2 

Operations 
The sum of the complex numb

with the identification 

(a + bi) + (c + di) = (a + c) + (b + d)i 

on Complex Numbers 

addition. 

*Exercises 
website. 

that, 

and selected 

C 1  

ers a + bi and c + di is defined 

as 

of a + bi with (a,  b), c + di with (c, d), and 

of complex numbers is 

the same 

odd-numbered answers for this appendix can be found on the student companion 

Appendix 
C Complex 

Numbers 

C2 

Since 

i2 = - 1, this expression 

simplifies 

to (ac -bd) + (ad + bc)i. 

Thus, we have 

(a + bi)(c + di) = (ac -bd) + (ad + bc)i 

Observe that, 
- ( c + 
di) =  ( 
ofa + bi and c + di as 

as a special 
- 1) ( c + 

a(c + di)  =  ac  + 
di. This fact allows 

di) =  - c -

case, 

adi, so the 

us to compute the difference 

negative 

of c  + di is 

(a + bi) -(c + di) = (a + bi) + ( -l)(c + di) 

= (a +  ( - c)) + (b + ( - d))i 
= (a -c) + (b -d)i 

Example C . 1  

Find the sum, difference, 

and product of 

3 - 4i and - 1  + 

2i. 

Solution The sum is 

(3 -4i) + ( - 1  + 2i)  = (3 -1) + ( - 4  + 2)i =  2  -2i 
is 

The difference 

(3 -4i) -( - 1  + 2i)  = (3 -( -1)) + ( - 4  -2)i =  4  -6i 

(3 -4i)( - 1  + 2i) = -3 + 6i + 4i -8i2 

- 3  +  1 Oi  -

8( - 1)  =  5  + 

lOi 

The product is 

bi 

-1-----+----+

z = a +  bi 

Im 
T I ..L 
la I T I 
z = a  - bi 
(z is pronounced 
� 
conjugates 

Figure C . 2  Complex 

tor by the con

To find the 

denomina

conjugate. 

Re 

- bi 

The conjugate of z = a + bi is the complex number 

z =  a  -bi 

"z bar:') Figure C.2  gives 

the geometric 

interpretation 

of the 

quotient 

of two complex numbers, we multiply 
jugate of the 

denominator. 

the numerator and 

the 

Example C . 2  

Express in the form a + bi. 

- 1  + 2i 
3  + 4i 

the numerator 

and denominator 

by 3  + 4i =  3  -4i. Using 

Solution We multiply 
Example 

C.l, we obtain 
- 1  + 2i 
3  + 4i 

·---

---

- 1  + 2i 3  -4i 5  + lOi 
3  +  4i  3  - 4i  3

5  + lOi 1  2 
25 5  5 

= - + -i 

2 +  42 

= ---

---

......,.. On the 

following 

proofs follow from the 

page is 
a summary 
definition 

of some of the properties 

of conjugates. The 
you should verify them for yourself. 

of conjugate; 

C3  Appendix 
C Complex 

Numbers 

1. z = z 
2. z +  w = z + w 
3. zw = zw 
4. Ifz * 0, then (w/z) = w/z. 
5. z is real if and 
only ifz = z. 

Im z = a +  bi 

The absolut

tance 

from the 

e value (or modulus) lzl of a complex numb
origin. 

As Figure C.3 shows, Pythagoras' 

Theorem gives 

er z = a + 

bi is its dis­

lzl = la  + bil = Va2 +  b2 

Observe that 

zz =  (a  + 

bi)(a -bi) =  a2 -abi + bai - b2i2 =  a2 +  b2 

a 

Hence, 

zz = lzl2 

Figure C . 3  

us an alternative 

This gives 
way of describing 
two complex numbers. If w and z *  0 
wz 

the division 

process 
are two complex numbers, then 

-

-
-
-zz I zl 2 

w  w  z  wz 
z 

z z 

for the 

quotient 

of 

........... Below is a summary 

of some of the properties 
of absolute 
value and other 

the definition 

of absolute 

value. (You should 
try 
properties 
of complex 

to prove these 
numbers.) 

using 

1. lzl = O if andonlyif z= O. 
2. lzl = lzl 
3. lzwl = lzl lwl 

zl + lwl 
5. lz +  wl  :::;  l

4. Ifz * O,then l�I lzl' 
(r, 8), where r 2': 0, as shown in Figure C.4. We have 
r cos 8  + (r sin 8)i 

Polar Form 
As you have seen, 
by the point (a, b). This point can also be 

number z = a + 

the complex 

so 

Thus, any complex number can be written 

in the polar form 

z = r (cos 8  +  i sin 

8) 

bi can be 

represented 
geometri

cally 

expressed 

in terms of polar coordinates 

a  = r cos 8 and  b  =  r sin 8 
z =  a  +  bi  = 

Im a +  bi 

r 
e 

b 

Figure C . 4  

Appendix 
C Complex 

Numbers 

C4 

where r = I zl = V a2 +  b2 and tan (J = b /a. The angle (J 
by arg z. Observe that arg z is not unique: 
and is denoted 
integer 
of 27T gives 

is called 
Adding 
of z. However, 

an argument of z 
or subtracting 
there is 

any 
only one argu­

argument 

another 

multiple 

ment (J that satisfies -7T < (J :s 7T 

This is called 

the principal argument of z and is denoted 

by Arg z. 

Example C . 3  

Write the following 
(a) z = 1  + i (b) w = 1  - VJi 

complex 

numbers in polar form using 

their 

princi

pal argumen
ts: 

Solution (a) We compute 

r = I z I = V 12 +  12 = v'2 and tan (J = _1_ = 1 
Arg z = (J = - ( = 45°), 

and we have 

7T 

Therefore, 

4 

Im 

, 2  

z = l  +  i } 

Re 

- -l'--- r-'---+----

- 1  2 

-

ce3 

w =  -,3 i  

- 2  

Since w lies in the fourth quadrant, 
Therefore, 

Figure C . 5  

as shown in Figure C.5. 
(b) We have 

r = lwl = \/12 + ( -V3)2 = V4 = 2 and tanfJ = --

= - V3  

Arg  z = (J = -- ( = -60°). 

-V3 

1 7T 
w = 2( cos( -�) +  i sin( -�)) 

we  must  have 
3 

See Figure 

C.5. 

of multiplica

ers can be used to give geometric 

The polar form of complex numb
tion and division. 
z1 = r1(cos fJ1 +  i sin fJ1) and  z2 = r2(cos fJ2 +  i sin fJ
Multiplying, 
we obtain 

Let 

2) 

interpreta

tions 

z1z2 = r1r2(cos fJ1 +  i sin fJ1)(cos fJ2 +  i sin fJ2) 

= r1r2 [ (cos fJ1 cos fJ2 - sin fJ1 sin fJ2)  + i (sin fJ1 cos fJ2 + cos fJ1 sin fJ2) ]  

Using the trigonometric 

identities 

cos(fJ1 +  fJ2) = cos fJ1 cos fJ2 -sin fJ1 sin fJ2 
Sin(fJI +  fJ2) = Sin (JI COS (J2 + COS (Jl Sin (J2 

C5  Appendix 
C Complex 

Numbers 

Im 

we obtain 

which is the 
el + ez. This shows that 

polar form of a complex number with absolute 

value r1r2 and argument 

(1) 

figure C . 6  

Equation 
values and 
Similar

(1) says that to multiply two complex numbers, we multiply their absolut
add their arguments. See Figure C.6. 
ly, using 
the subtraction 

for sine and cosine, 

identities 

we can show that 

e 

�  (Verify this.) Therefore, 

Im 
r 

z 

-r 

l 
z 

figure C . 1  

and we see that to divide 
subtract their arguments. 

two complex numbers, we divide their 

absolute 
and 

values 

As a specia

l case of the 

of a com­
plex number in polar form. Setting Z1 = 1 (and therefore el = O) and Zz = z (and 
therefore 

a formula for the reciprocal 

last result, 

we obtain 

e2 = e), we obtain 

the following: 

If z = r (cos e  + i sine) is nonzero, 

then 

- = -(cos e - i sin e) 

1 1 
z  r 

See Figure 
C.7. 

- v3i in polar form. 

Example C . 4  

Find the product of 1  + i and 1 
Solulion From Example 

C.3, we have 

Therefore, 

See Figure 
C.8. 

Appendix 
C Complex 

Numbers 

C6 

Im 

1  +  i 

2 

(1  +  i)(l  -,3 i) = 
-
(l +,3)+ i(l -,3) 

-

- 2i 

1  -,3 i  

Figure C . 8  

Remark Since (1 + i) (1 -\13i) = (1 + \13 ) + i(l -\13 ) (check this

), we 

must have 

�  (Why?) This implies 

and 

1 +  V3 = 2 V2  cos(-�) = -2 V2  cos(�) 
1  -V3 = 2 V2  sin(-�) = -2 V2  sin(�) 
('TT) 1 + \/3   ('TT) \/3 - 1 
cos ii = lW  and sin ii = lW 

that 

have a method 

We therefore 
that is not a special angle 
angles. 

for finding 

the sine and cosine 

of an angle 

such as n/12 

but that can be obtained 

as a sum or difference 

of specia

l 

De Moivre's Theorem 
If n is a positive integer and 
yields 

formulas for the powers of z: 

z = r (cos 8 + i sin 8), then repeated use of Equation 
z2 = r2(cos 28 + i sin 28) 
z3 = zz2 = r3( cos 38 + i sin 38) 
z4 = zz3 = r4(cos 48 + i sin 48) 

(1) 

In general, 

we have the following 

result, 

known as De Moivre's 

Theorem. 

Abraham De Moivre ( 1 667-1 754) was 
who made 
a French mathematician 
contributions 
important 
to trigonom­
etry, analytic 
and statistics. 

geometry, probability, 

CJ  Appendix 
C Complex 
Numbers 

Theorem C . 1  

De Moivre's 

Theorem 

If z = r(cos () + i sin()) and n is a positive integer, 

then 
z" = rn(cosn()  + isinn()) 

Stated 

differentl

y, we have 

lz"I = lzl" and arg(z") = n arg z 

In words, 
we take the nth power of its absolut

De Moivre's Theorem says that to take the nth power of a complex number, 

e value and multiply its argument by n. 

Example C . 5  

Find (1 + i) 6• 

Solution From Example 

Hence, 

De Moivre's 

Theorem gives 

C.3(a), we have 

1  + i = v2 (cos : + i sin : ) 
(1 + i)6 = (\/2)6( cos 6: + i sin 6:) 
( 317  317) 
= 8 cos 2 + i sin 2 

= 8(0 + i( -1)) = - Si 

Im 
.......... Re 

2i 

l  +  i 

- 2  + 2i 

-+-

-- -+---+--+--+--+-

- 4i 

See Figure 

C.9, which shows 1  + i, ( 1  + i)2, ( 1  + i)3, • . .  , ( 1  + i)6• 

We can 

also use De 

Moivre's Theor

em to find nth roots 

of complex 

numbers. An 

nth root of the 

complex number z is any complex number w such that 

- 4-4i 

In polar form, we have 

w" = z 

- 8i 

so, by De Moivre's 

Theorem, 

w = s (cos <p + i sin <p) and  z 

= r(cos ()  + i sin()) 

Figure C.9 Powers 

of 1  +  i 

Equating 

the absolute 

s"(cos n<p + i sin n<p) = r (cos ()  + i sin()) 
values, we see that 

s" = r or s = r11" = \(;: 

We must also have 

�  (Why?) Since the sine and cosine functions 

cos n<p = cos () and  sin 

n<p = sin () 

each have period 

imply that n<p and() differ by an integer 

multiple 

of 217; that is, 

217, these 

equations 

n<p = ()  + 2k17 or <p = ----n 

()  + 2k17 

Appendix 
C Complex 

Numbers 

CB 

where k is an integer

nth roots 

w  = r11" cos n  +  i sin n 
of z as k ranges 
integers. 
produce distinct 
of w, so there 

. Therefore, [ ((J + 2br) ((J + 2k1T)] 
of z = r(cos (J + i sin 
fJ). We summarize 
fJ) and let 
1/ [ ((J + 2k1T) .
 ((J + 2k1T)] 
r n cos  + l sm 
.

over the 
values 

n be a 

this resu

n 

n 

the possible 

describes 
show that k = 0, 1, 2, . . .  , n - 1 
different 

nth roots 

Let z = r (cos (J + i sin 

nth roots 

given 

tinct 

by 

fork = 0, 1, 2, . . .  , n -1. 

(2) 

It is not hard to 

are exactly 

n 
lt as follows: 

positive integer. Then z has exactly n 

dis­

Example C . 6  

Find the three 

cube roots 

of - 27. 

Im 

that the cube roots 

of 

3 

3 

fork = 0, 1, 2 

sin 7T). It follows 

Using formula (2) with n = 3, we obtain 

Solution In polar form, - 27 = 27(cos 1T + i 
-27 are given by 

( - 27)1/3 = 271/3[ cos( 1T +
2k1T) +  i sin( 1T +
271/3[ cos; +  i sin;] 
[ (7T  + 21T) (1T  +  27T)] 
[ (7T  +  47T) (7T  +  47T)] 

271/3 cos 3  +  i sin 3 
271/3 cos 3  +  i sin 3 

2k1T)] 
= 3(_1_ + V3 i) = i + 3V3j 
( 57T  57T) 

= 3  cos 3 +  i sin 

2  2 2  2 

3 

= - 3  

= 3(cos 1T  +  i sin 7T) 

Figure C . 1 0  The cube 
of -27 

roots 

As Figure 

C. l 0 shows, 

the three 

cube roots 

( 120°) apart around 

a circle 

of radius 

3 centered 

of - 27 are equally 
at the origin. 

spaced 

21T /3 

radians 

of z = r(cos (J + i sin fJ) will lie 

spaced 

In general, formula (2) implies 
r11" centered 
apart. (Verif

on a circle of radius 
21T!n radians 
(360/n°) 
nth roots 
remaining 
increments 
of21T/n radians. Had we known this in Example 
the fact that the real cube root of -27 is - 3 and then rotated 
angle 

that the nth roots 
at the origin. 
they will be equally 
y this.) Thus, if we can find one nth root of z, the 
successi
ve 
C.6, we could 
have used 

of z can be obtained 

by rotating the 

first root through 

of27T/3 radians 

the other 

Moreover, 

(120°) to get 

two cube roots. 

it twice through an 

C9  Appendix 
C Complex 

Numbers 

collected 

to his name, and his 

mathematician 

formula" or "Euler's 

of all time. He has over 900 pub­

works fill over 70 volumes. There are so many results 

lications 
uted to him that "Euler's 

Leonhard Euler ( 1707 - 1 783) was the most prolific 
of mathematics, 
to list 
it is difficult 
areas 
in so many 
on the context. Euler 
His con
all. 
worked 
them 
tribu­
� for 
7T, e, i, � for summation, 
to calculus 
equations, 
number theory
differential 
tions 
and analysis, 
ry, topolo
gy, me­
, geomet
chanics, 
and other 
areas 
of applied 
mathematics 
continue to 
He also 
intro­
be influential. 
duced 
much 
of the notation 
we curren
tly use, 
including 
as functions. 
and was the 
first 
f(x) for a function, 
and 
difference, 
and cosine 
to treat sine 

Theorem" can mean many different 

Euler was born in Switzerland 

life in Russia and Germany. 

but spent most of his 

mathematical 

things, 

depending 

attrib­

In 1 727, he joined the St. Petersburg 
I, the wife of Peter the Great. 
in 1 766 to St. Petersburg, 
returned 
in one eye as the result 
the vision 
and was 
Remarkabl
be productive until 

the day he died. 

totally 

blind. 

Academy of Sciences, 

which had been founded by Catherine 

He went to Berlin in 

1741 at the invitation 

of Frederick 

the Great, 
but 

where he remained 
of an illness, 
y, his mathematical 

and by 177 6 he had lost the vision 
output did not diminish, 

until his death. When he was young, he lost 

and he continued to 

in the other eye 

Formula 

Euler's 
In calculus, 

you learn 

e2 has a power series expansion 

e2 = 1 + z + - + - + 

that the function 
z2 z3 
2! 3! 

that converges 
works when z is a complex 

e2 obeys the usual rules 

series expansions: 

for every real number z. It can be shown that this expansion also 

number and that the complex exponential 
also have power 

for exponents. The sine and cosine 

functions 

function 

sin x = x - - + - - - + 

x3 xs x7 
3! 5! 7! 
x2 x4 x6 
cos x = 1  -- + --- + 
2! 4! 6! 

let z = ix, where x is a real number, then we have 

If we 

Using the fact that i2 = - 1, i3 = - i, i4 1, i5 = i, and so on, 
eix = 

x2 ix3 x4 ix5 x6 ix7 
2!  3! 4! 5! 6! 7! 

1 + ix +-- +-- + 
(ix)2 (ix)3 
2! 3! 
- -

length 

see that 

4, we 

.. · 

1 +  ix - - - - + - + - - - - - +  + 

repeating in a cycle of 

= cos x +  i 
sin x 
is known as Euler's formula. 

This remarkable 
result 

Appendix 
C Complex 

Numbers 

C 1 0  

Theorem C . 2  

Formula 

Euler's 

For any real number x, 

eix = cos x +  i sin x 

Using Euler's 
as 

more compactly 

written 

formula, we see that the 

polar form of a complex numb
er can be 

For example, from Example 

z = r(cose  + i sinO) = re;e 

C.3(a), we have 

1 +  i = \/2( cos : +  i sin : ) = \/2ei"/4 

We can also go 
form. 

into polar or standard 

in the other direction and 

a complex 

convert 

exponential 

back 

Example C . 1  

Write the following 
(b) ez+i7T/4 
(a) e;" 

in the form a + bi: 

Solution (a) Using Euler's 

e;" = cos n +  i sin n = - 1  +  i • 0  = - 1  

formula, 

we have 

this equation as ei" +  1  = 0, we obtain 
equations 

It contains 
the additive 
tion; 

what is surely 
the fundamenta
identity 

in mathematics. 
and exponentia
1; the two most important 

transcendental numbers, 

one of 
the most 
of 
l operations 
0 and the multipli­

n and e; and the 

multiplication, 

unit i-all in one equation

!) 

(If we write 
remarkable 
addition, 
cative 
complex 
(b) Using rules 

identity 

for exponents 

together 

with Euler's 

formula, 

we obtain 

=--+--i 

ez\/2  ez\/2 

2 

2 

If z = rei8 = r(cos e + i 

sine), then 

z = r(cose  -i sinO) 

The trigonometric 

identi

ties 

cos(-8) 

= cose and sin(-8) =- sine 

(3) 

C 1 1   Appendix 
C Complex 

Numbers 

allow 

us to 

rewrite 

Equation 
(3) as 

This gives the 
following 

z = r(cos(-e) 

+ i sin(- e)) = rei(-e) 

useful formula for the 
If z = rei0, then 

conjugate: 

Note Euler's 

formula 

a quick, 

one-line proof 

of De Moivre's 
gives 
Theorem: 
sine) in = (reier = rneinO = rn(COS ne + i sin ne) 

(r(COS e  +  i 

Appendix D* 0 

Polynomials 

A polynomial is a function 

p of a single variable 

x that can be written 

in the form 

(an * O), called the coefficients of p. With the con­

notation 
p as 

to write 

(1) 

of the 

ely to Gauss, who merely 

it as 
exclusiv

of the roots of 
I regard 

where a0, a1, . . .  , an are constants 
vention that x0 = 1, we can use summation 

Euler gave the most algebraic 
proofs  of the existence 
[a polynomial] equation . . . .  
unjust to ascribe 
this proof 

n 
Frobenius, 
1907 
-Georg 
Quoted on 
the MacTutor 
of 
History 
Mathematics 
archive, 
http://www-history
.mcs 
ac. uk/history/ 
.st-and.

The integer 
polynomial of degree zero is 
a constant 

p(x) = 2: akxk 
k=O 

n is called 

added the finishing touches. 

called 

Example D . 1  

Which of the following 

are polynomials

? 

the degree of p, which is denoted 

by writing 

deg p = n. A 

polynomial. 

(a) 2  -tx + v'2x2 

(d) ln(2e5x') e3x 

(g) cos(2 cos-1x) 

(c)� 
(f) Vx 

x - 2 

2 --3x2 
(b) 1 
(h) ex 
(e) x2 -5x +  6 
x�c 

is obviously 

Solution (a) This is the only one that 
(b) A polynomial of the 
x approaches 
x approaches 
(c)  We 
have 

value [lim p(x) * ±oo], whereas 

a finite 
zero. 

form shown in Equation 

it is not a polynomial. 

Hence, 

a polynomial. 

(1) cannot 
2 -1/3x2 approaches 
-oo as 

become 

infinite 

as 

which is equal to v'2x when x 2 0 and to - \/2x when x < 0. Therefore, 

together" 

two polynomials 

(a piecewise 

this expres­
polynomial), but it 

sion is formed by "splicing 
is not a 

polynomial itself. 

*Exercises 
website. 

and selected 

odd-numbered answers for this appendix can be found on the student companion 

0 1  

02  Appendix 

D Polynomials 

(d) Using properties 

of exponents 

and logarithms, we have 

(2e5x') , 
ln -- = ln(2e5x -3x) = ln 2  + ln(e5x -3x) 

= ln 2  + 5x3 -3x = ln 2  -3x + 5x3 

e3x 

, 

consists 

of all real numbers x * 2. For these 
values 

is a polynomial. 

so this expression 
(e) The domain of this function 
of x, the function 

to 

x - 2 

simplifies 
x2 -5x +  6 
x - 2 
so we can say that it 
(f) We see that this function 
since 

�  eventually 

(x -2)(x -3) ------= x - 3  
- 1  :S x :S  1. Let 8 = cos -i x so that cos 8 = x. Using 

on its domain. 
be a polynomial 

x 2 O), 

shown in Equation 

(even on its domain 

cannot 
of a polynomial of the form 
this property. (Verif

in zero and Vx does not have 
cos(2 cos-1 x) = cos 2() = 2 cos2 8  - 1 = 2x2 - 1 

of this expression is 
identity, we see that 

(g) The domain 
a trigonometric 

repeated differentiation 

is a polynomial 

results 

y this.) 

( 1) 

so this expression 
(h) Analyzing 
polynomial. 

is a polynomial 
this expression 

on its 

domain. 

as we did the one 

in (f), we conclude that it is not 

a 

Two polynomials are equal if the coefficients 

of corresponding 

powers of x are all 
must have the same degree. 
The sum of two 

together 

the coefficients 

of corresponding pow­

In particular

equal. 
polynomials is obtained 
ers of x. 

polynomials 
by adding 

, equal 

Example D . 2  Find the sum of2 -4x + x2 and 1  + 2x - x2 + 3x3. 

Solution We compute 

(2 -4x + x2)  + ( 1  + 2x - x2 + 3x3) = (2 + 1) + ( - 4  + 2)x 

+ (1 + ( -l))x2 + (O + 3)x3 

whm we have "padded

" the fast 

= 3  -2x + 3x3 

polynom;al by g;vmg ;t an x' coeffident of mo".-+ 

We define the difference of two polynomials 

analogousl

y, subtracting 

of adding them. 
the distri

butive law and then gathering 

The product of two polynomials 

is obtained 
correspond­

together 

instead 

coefficients 
by repeatedly using 
ing powers of x. 

Appendix

D Polynomials 

03 

Example D . 3  

Find the product of2 -4x + x2 and 1  + 2x - x2 + 3x3• 

Solution We obtain 

(2 -4x + x2)(1 + 2x - x2 + 3x3) 

= 2(1 + 2x - x2 + 3x3)  -4x(l + 2x - x2 + 3x3) 

+ x2( 1  + 2x - x2 + 3x3) 

= (2  + 4x -2x2 + 6x3)  + (-4x -8x2 + 4x3 -12x4) 

+ (x2 + 2x3 - x4 + 3x5) 

= 2  + (4x -4x) + (-2x2 -8x2 + x2) + (6x3 + 4x3 + 2x3) 

+ (-12x4 - x4)  + 3x5 

= 2  -9x2 + 12x3 -13x4 + 3x5 

Example D . 4  

Observe that for two polynomials p and q, we 

If p and q are polynomials with deg q ::s deg p, we can divide 

deg(pq) 

= deg p  + deg q 

have 

plq. The next example 
of one 

integer 
the quotient 

an integer, 

into another

illustrates 

q into p, using 

long 
the procedure, which 
. Just as the quotient 

of two 

of two polynomials is not, in gen­

the quotient 

same as for long division 
is not, in general, 

to obtain 

division 
is the 
integers 
eral, another 
Compute 1  + 2x - x2 + 3x3 

polynomial. 

2  -4x + x2 

Solution We will 
decreasing powers of x. Accordingly, we have 

perform long division. It is helpful 

to write each 

polynomial with 

x2 -4x + 2)3x3 - x2 + 2x +  1 

x2 into 3x3 to obtain 
We begin 
3x by the divisor x2 -4x + 2, subtract 
the dividend 

by dividing 
(3x3 - x2 + 2x + 1): 

the partial 
and bring 

the result, 

quotient 

3x. We then 

multiply 

down the next term from 

Then we repeat the 
the resu

lt from l lx2 -4x + 1. We obtain 

process 

with l lx2, multiplying 

1 1  by x2 -4x + 2 and subtracting 

3x 
3x3 -12x2 + 6x 

x2 -4x + 2hx3 - x2 + 2x +  1 
l lx2 -4x +  1 

3x + 1 1  
x2 -4x + 2hx3 - x2 + 2x +  1 

3x3 -12x2 + 6x 
l lx2 - 4x + 
l lx2 - 44x + 

22 
40x -2 1  

04  Appendix 

D Polynomials 

We now have a remainder 
x2 -4x + 2, so the process 

40x -21. Its degree 
stops, and we have found that 

is less than that of the divisor 

3x3 - x2 + 2x +  1 = (x2 -4x + 2)(3x + 1 1)  + (40x -21) 

or 

3x3 - x2 + 2x + 

x2 -4x +  2 

40x -21 
= 3x + 1 1  + 2 x - 4x +2 

Example 

D.4 can be 

generalized to 

give the following 

result, 

known as the division 

algorithm. 

Theorem D . 1  

The Division 

Algorithm 

If f and g are polynomials with deg g :s degf, then there are polynomials 
q and r 
such that 

where either 

r = 0 or deg r < deg g. 

f(x) = g(x)q(x) + r(x) 

In Example 
D.4, 

f(x) = 3x3 - x2 + 2x + 1, g(x) = x2 -4x + 2, q(x) = 3x + 1 1 ,  

and r(x) = 40x -21 

In the division algori

thm, if the remainder is zero, then 

f(x) = g(x)q(x) 

and we say that g is a factor off (Notice 
connection 

f is a number a such thatf(a) = 0. [The number a is also 

of a polynomial and its 

between the factors 

mial equation f(x) = O.] The following 
lishes 

the connection 

result, 
of a polynomial and 

between factors 

its zeros. 

zeros. A zero of a polynomial 

called a root of the 
known as the Factor Theorem, estab­

polyno­

that q is also a factor off) There is 
a close 

Theorem D . 2  

The Factor Theorem 

Let f be a polynomial and let a be a constant. Then a is a zero off if and only if 
x - a is a factor of f(x). 

Proof By the division 

algorithm, 

f(x) = (x -a)q(x) + r(x) 

where either 
constant. 

Now, 

r(x) = 0 or deg r < deg(x -a) = 1. Thus, in either 

case, 

r (x) = r is a 

j(a) = (a -a)q(a) +  r = r 

D 
if r = 0, which is equivalent 

to 
j(x) = (x -a)q(x) 

so f(a) = 0 if and only 
to prove. 
as we needed 

Appendix

Polynomials 

05 

There is no method 

that is guaranteed to find the zeros 

of a given polynomial. 

are some guidelines 

However, 
there 
nomial with integer coefficients 
known as the Rational Roots Theorem, gives 
to be a rational number. 

that are useful in specia
is particularly 

l cases. 

The case of a poly­

interesting. 

The following 

result, 

criteria 

for a zero of such a polynomial 

Theorem D . 3  

The Rational 

Roots Theorem 

Let 

j(x) = ao + a1X + · · · 

+ anXn 

be a polynomial with integer 
ten in lowest 
of b. 

terms. 

coefficients 

and let 

alb be a rational 

number writ­

If alb is a zero off, then a0 is a multiple 

of a and an is a multiple 

Proof If a/ b is a zero off, then 

ni b  nb 
(a)n-1 (a)n 
(a) 
a + a  - +· · ·+ a _  - + a  - = O  
O lb 
by bn, we have 
Multiplying 
through 
+ an-Ian-lb + ana =  0 
aobn + a1abn-I + · · · 

which implies 

that 

(1) 

The left-hand side of Equation 
(2) is 
also. Since a/b is in lowest 
an must be a multiple 
Therefore, 
of b. 
Equation 
( 1 )  as 

We can also write 

of b, so anan must be a multiple 

(2) 
of b 
greater 

than 1. 

terms, a and b have no common factors 

a multiple 

�  and a similar 

argument 

shows that a0 must be a 

of a. (Show this.) 

multiple 

Example D . 5  Find all the rational 
roots 

of the equation 

6x3 + 13x2 - 4  =  0 

(3) 

Solution If a/b is a root of this equation, 
of a, by the Rational Roots 

Theorem. Therefore, 

a E {:±::l,:±::2,:±::4} and b E {:±::l,:±::2,:±::3,:±::6} 

then 6 is a multiple 

of band -4 is a multiple 

06  Appendix 

D Polynomials 

Forming 
all possible 
the only possible rationa

rational 

numbers a/b with these choices 
of a and b, we see that 

l roots 

of the given 

equation 

are 

�  are the only values 

tuting 

these 

Substi

values 
from this list that 

into Equation (3) 
are actually 

one at a time, 

we find 

roots. (Check these.) As we will see 

that - 2, -t, and! 

shortly, 
these are not 

a polynomia

only all the rational roots 

l equation of degree 

3 cannot have more than three roots, 

so 

of Equation 

(3) but also its 

only roots. 

We can improve 

on the trial-and-error method 

of Example 

D.5 in various 

ways. 

once we find one root a of a given polynomial equation 

f(x) = 0, we 

x - a is a factor ofj(x)-say,f(x) = (x -a)g(x). We can therefore divide 

For example, 
know that 
f(x) by x - a (using long division) 
[which are also roots 
of f(x)  = OJ 
dratic 
Suppose 

to findg(x). Since 
may be easier 
to the quadratic 

we have access 

polynomial, 

formula. 

degg < degf, the roots 
of g(x) = 0 
to find. In particular, if g(x) is a qua­

(We may assume 
an equivalent 

is positive, 
equation otherwis

that a 

multiplying 
both sides 

by - 1  would produce 

e.) Then, completing the 

square, 

we have 

ax2 + bx +  c =  0 
since 

( b  b2) b2 

a  4a2  4a 

a  x2 + -x + - = - - c 

�  (Verif

y this.) Equivale

ntly, 

b2 -4ac 

4a2 

b +�2 -4ac = ±Vb2 -4ac 

x + -= 2a - 4a2  2a 
x= - b  ± Vb2 -4ac 

2a 

Therefore, 

or 

Let's 

revisit 

the equation from Example 

D.5 with the quadratic 
formula 

in mind. 

Example D . 6  Find the 
roots 

of 6x3 + 13x2 - 4 = 0. 

suppose we use the Rational 

Solution Let's 
root of 6x3 + 13x2 - 4 = 0. Then x + 2 
is a rational 
gives 
long division 

is a factor of 6x3 + 13x2 - 4, and 

Roots Theorem to discover that x =  - 2 

6x3 + 13x2 - 4  = (x + 2)(6x2 + x -2) 

Appendix

D Polynomials 

DJ 

�  (Check this.) We can now 

apply the quadratic formula 

to the second 

factor to find 

that its zeros 
are 

- 1  ± v12 -4(6) ( - 2) 

x = 

2 · 6  

- 1  ±  v'49 

-1 ± 7 

12 

12 

or, in lowest 
as we determined 

terms, 

in Example 

t and -t. Thus, the three roots 

(3) are -2, t, and -t, 

of Equation 

D.S. 

R e m a r k  The Factor 

Theorem establishes 

have factors 

polynomial and its 
still 
we need to know the number system 
posed to belong. 

linear factors. However, a 
Furthermore, 

degree. 

of higher 

a connection 
polynomial without 

between the zeros 
of a 
may 
factors 

linear 

when asked to factor a polynomial, 

to which the coefficients 

of the factors 
are sup­

For example, consider 

the polynomial 

p (x) = x4 + 1 
Over the rational numbers Q, the only 

possible 
Roots Theorem. A quick check shows that neither 
no linear factors 
still 
the method 

factor into a product of two quadratics. We will 

of undetermined coefficients. 

with rational 

coefficients, by the Factor 

p are 1 and - 1, by the 

zeros of 

Rational 

of these actually 

works, so p (x) has 
Theorem. However, 
p(x) may 
check for quadratic 
factors 

using 

Suppose that 

�  Expanding 

x4 + 1 = (x2 + ax +  b)(x2 + ex+ d) 

the right-hand side and comparing 

coefficients, 

we obtain 

the equations 

b + ac + d 

a+  c = 0 
= 0 
be+ ad= 0 

bd = 1 

we may assume 

If a = 0, then c = 0 and d = - b. This gives 
of which has solutions in Q. It follows 

Hence, 
that b 1  = 1, sob = 1 orb = - 1. This implies 
neither 
We say that it is 
However, 

that a =fa 0. Then c = - a, and we obtain 
over Q. 

-b1 = 1, which has no solutions in Q. 
over Q. 

over the real numbers IR, x4 + 1 does factor. The calcula

that a2 = 2 or a2 = - 2, respecti
vely, 
that x 4 + 1 cannot 

d = b. It now follows 

be factored 

irreducible 

we have 

tions 

just done show that 

�  (Why?) To see whether we can factor further, we apply the quadratic 
+ --i 

-----
x =  -vz ± V( v2)2 - 4 

---
= - ( - 1  ± i) = 

x4 + 1 = (x1 + v2x +  l)(x1 -v2x + 1) 

that the first factor has zeros 

- v2  ±  V=2  v2 

formula. 

1  1 
v2 -v2 

2 

2 

2 

We see 

DB 

D Polynomials 

Appendix 
Im  which are in IC but not in IR. Hence, 
complex numbers IC. The four zeros 

ly, x2 -Vlx +  1 cannot 
show that a complete 

tors over IR. Similar
tions 

Our calcula

of x4 +  1 are 

x2 + Vlx +  1 cannot 
be factored 

into linear 

fac­
be factored 
over IR. 
into linear factors 
of x4 +  1 is possible 
over the 

factorization 

1 
1 
()' = --- + -i 
V2  \12' 

1 
1 
()' = --- --- i 
V2  \12 ' 
1 
1 
V2  V2 

- a= ----i 

-a =- + -- i 

1 
V2  \12' 

- 1 

Figure D.1 

which, 
factoriza

as Figure D.l shows, all lie on the unit 
tion of x4 +  1 is 

circle 

in the complex plane. 

Thus, the 

x4 +  1 = (x -a)(x -a)(x + a)(x +  a) 

The preceding Remark illustrates 

important 
that the polynomial p (x) = x4 +  1 satisfies 

several 

its complex zeros 

deg p = 4 and has exactly 

four 
occur in conjugate pairs; that is, its com­

in IC. Furthermore, 

Notice 
zeros 
plex zeros 

can be paired up as 

properties 

of polynomials. 

{a, a} and { -a, -a} 
These last two facts are true in general. 
Theorem of Algebra 

(FTA), a result 

The first is an instance 

of the Fundamental 

that was first proved by Gauss in 1797. 

Theorem D . 4  Th e  Fundamental 

Theorem of Algebra 

Every polynomial of degree 
(counting 

multiplicities) 

in IC. 

n with real or complex coefficients 

has exactly 

n zeros 

This important 

theorem is sometimes 
as 
coefficients 
PTA'. Certainly, PTA implies 

stated 
"Every polynomial with real or complex 
call this statement 
then if we have a polynomial p of degree 
us that x -a is a factor of p(x), so 

Let's 
true, 
Theorem then tells 

has a zero in IC." 

PTA'. Conversely, if PTA' is 

n, it has a zero a in IC. The Factor 

where q is a polynomial of degree 

p (x) = (x -a)q(x) 
apply PTA' to q to get another 

�  ment can be 

made into a 

nice induction 

n - 1 (also with real or 
on, making 
.) 

zero, 
proof. (Try it
(along the lines 

It is not possible 

to give a 

can now 

formula 

and so 

complex 

coefficien
ts). We 

PTA true. 

This argu­

of the quadratic 
for 

formula) 

of polynomials 

of degree 

see page 311.) Consequently, other 
that Gauss gave uses topologica

5 or more. (The work of Abel and Galois 
used to prove PTA. The 

methods 

must be 

confirmed 

l methods 

and can 

be found in more advanced 

the zeros 
this; 
proof 
mathematics 
courses. 
Now suppose that 

is a polynomial with real coefficients. Let a be a complex zero of p so that 

Appendix

D Polynomials 

09 

Then, using 

properties of conjugates, we have 

= p(a) = 0 = 0 

Thus, a is also a zero of p. This proves 

the following 

result: 

The complex zeros 

of a polynomial with real coefficients 

occur in conjugate 

pairs. 

In some situations, 

we do not need to know what the zeros of 

a polynomial are­
we might only need to 

the zeros 

are positive or negative 

we only need to know where they are located. For example, 
know whether 
that is useful in this regard 
predictions 
based on the 

Descartes' sta
in his 
ted this rule 
La Geometrie, but did 
1637 book 
a proof. 
Several 
not give 
mathema­
ticians later furnished 
a proof, and 
a somewhat 
Gauss provided 
in 
er version 
sharp
of the theorem 
1828. 

Given a polynomial a0 + a1x + · · · + anxn, write 

is Descartes' 
about the number of positive zeros 
signs 

Replace 
minus sign. 
where the coefficients 
-7x5 has the 
sign pattern 

by a 
the polynomial 2 -3x + 4x3 + x4 

sign and each negative 
by a plus 
polynomial has k sign changes if there 

Rule of Signs. It allows 
of a polynomial 

coefficient 
are k places 

each positive coefficient 

its nonzero coefficients 
in order. 

(as in Theorem 4.35). 

with real coefficients 

We will say that the 

sign. For 

One theorem 

us to make certain 

example, 

coefficients. 

change 

of these 

._,_,._,_, 

._,_, 

+ - + + -

so it has 

three 

sign changes, as indica

ted. 

Theorem 0 . 5  

Descartes' 

Rule of Signs 

Let p be a polynomial with real coefficients 
number of positive zeros 

of p (counting 

multiplicities) 

is at most k. 

that has k sign changes. Then the 

In words
positive zeros 

Rule of Signs 
, Descartes' 
than it has 
sign changes. 

says that a real polynomial 

cannot 

have more 

Example 0 . 1  

Show that the polynomial p(x) = 4 + 2x2 -7x4 has exactly 

one positive zero. 

Solution The coefficients 
change. 
p (O) = 4 and p ( l) = - 1, so there is 
this is the 

So, by Descartes' 

only positive zero of p. 

of p have the sign pattern + + - , which has only one sign 
Rule of Signs, 

p has at most one positive zero. 
But 

a zero somewhere in the interval (O

, 1). Hence, 

0 1 0   Appendix

D Polynomials 

We can also use Descartes' 

Rule of Signs 

to give a bound on the number of nega­

tive zeros 

of a polynomial with real coefficients. Let 

and let b be a 

negative 

p(x) = a0 + a1x +  a2x2 + · · · + 
zero of p. Then b 

anx" 

= - c for c > 0, and we have 

0 = p(b) = a0 + a1b + a2b2 + ·  · · + 

anb" 

But 
so c is a positive zero of p ( -x). Therefore, 
p(-x) has 
yields 

positive zeros. Co
the following: 

mbined 

p (x) has exactly 
as many negative 
zeros 
this observation 

Rule of Signs, 

with Descartes' 

as 

Let p be a polynomial with real coefficients. Then the 
p is at most the number of sign changes 

of p(-x). 

number of negative zeros 

of 

Example D . 8  

Show that the zeros 

of p(x) = 1  + 3x + 2x2 + x5 cannot 

all be real. 

-x) = 1 -3x + 2x2 - x5 has three 

of p(x) have no sign changes, sop has no positive zeros. 
p has 
, sop has at most 

among its coefficients, 

zeros. We note that 0 is not 

sign changes 

Solulion The coefficients 
Since p( 
at most three 
three 

real zeros. Therefore, 

negative 

a zero of p either
two complex (nonreal) 
zeros. 

it has at least 

Answers to Selected 
Odd-Numbered Exercises 

Answers are easy. It's asking 

the right questions [that's J hard. -Doctor 
Who 
"The Face 
of Evil;' 
By Chris 
Boucher 
BBC, 
1977 

(d) 

z 

Chapter 
1 
1.1 

Exercises 
1 .  

y 

3. (a), 

(b) 

z 

3 

- 3  

x 

(c) 

z 

3 

x 

3 

y 

y 

5. (a) y 

3 

y 

3 

x 
3 � 2 
x 4 
x 3 
1 / 2 

2 3 

- 1  

2 

- 1 

- 2  

(b) y 

ANS1 

Exercises 
19. y 

y 

(c) 

to Selected 
umbered 
ANS2  Answers 
Odd-N
2 " 
--+----+----+-----'1----t--1r--.---X 
l 2 I 3 

(d) 

- 2  

y 

2 

21.w=-2u+4v 

y 

I 6 
I 6 
b = [5, 3] 

I 3 

7. a + 

y 

4 

2 

4 

6 

9. d -c = [5, -5] 

y 

[�] 
25. u + v = 
27.u+v= [0,1,0,0] 
0 2 3 
0 2 3 
29. + 
0  0 2 3 2 3 0 
0  0  0  0  0 
0 2 3 
2  2 3 0  1 
2 0 2 0 2 
3 3 0 2 
3 0 3 2 1 
31. 0 35.0 39.5 43. [O, 0, 2, 
33. 1 
37.2,0,3 
41. [1, 1, OJ 45. x = 2 
2], [2, 3, 1, 1] 
47. No solution 
49. x = 3 
53. x = 2 
51. 
55. x = 1, or x = 5 
57. (a) All a * 0 

(c)  a and m can 

No solution 

(b)a =l, 5 

have no common factors 
the greatest common divisor (gcd) 

other 
of a and 

than 1 

[i.e., 
m is l ] .  

11. [3, -2, 3] 
[ 1/2] [-v'3/2] 
13.u= v'3/2 ,v= _112 ,u+v= 
[(1 + V3)/2] 
[(1 -V3)/2] u -v = 
(V3 -1)/2 , (1 + V3)/2 
15.5a 17. 
x = 3a 

Exercises 

1.2 
1. -1 3. 11 
[-1/Vs] 
7. Vs, 2/Vs 

5. 2 [ 1/Vi4] 

9. Vi4, 
2/Vi4 
3/Vi4 

Exercises 

ANS3 

), no two 

vector. 

for the 

two lines 

the four 

vectors 

diagonals 

diagonals 

are given by 

are perpendicular. 

cube to be a unit 

are given by the vectors 

to Selected 

Odd-Numbered 

11. \16, [ 1 I \16, 1 I \13, 1 I \/2, o l 
13. vu  15. V6 17. (a) u · v is a scalar, not a 
19. Acute 21. Acute 23. Acute 
(c) v · w is a scalar and u is a vector. 
25. 60° 27. =88.10° 29. =14.34° 
31. s.ince AB · AC = [-�i · [ �1 = 0, LBAC is a 
nght angle. -1 -3 
cube (as in Figure 1.34), 
33. If we take the 
Since d; · dj of. 0 for all i of. j (six possibilities
35. D = (-2, 1, 1) 
of= 53.13° to the bank 
37. 5 mi/hat an angle 
39. 60° 
41. [ -u 43.u: 
47. A= \/45/2 49. k = -2, 3 
k [ _ � l where k is a scalar. 
51. v is of the form 
53. The Cauchy-Schwarz 
1.3 1. (a) [�] · [;] 
= 0 (b) 3x + 2y = 0 
3. (a) [;] [�] + t[-�] 
(b) x = 1 - t 
y = 3t 
5. (a) 
z = 4t 
7. (a) m · [�] � 2 (h) 3x + 2y + F  2 

Answers 
(b) x = 2s -3t y = s + 2t 
z = 2s + t 11. [;] 
l3. [ � 1 � rn + { -: 1 + { � � 1 
= [ -�] + t [ �] 
15. (a) x =  [;] [ _ �] + t[�] 
y = -1 + 3t 
17. Direction 
d1 = [ �J and dz = [ �J. The lines 
if and only if 1 + m1mz = 0 or, equivale
m1mz = -1. 
[-0.301] 45. 0.033 -0.252 
19. (a) Perpendicular 
(c) Perpendicular 
21. [;] 
= [ _�] + t[�] 
23. [ � l � [-� l + { � � l 
25. (a) x = O,x = 1,y = O,y = 1,z = O, z  = 1 
(b) x -y = 0 (c) x + y -z = 0 
27. 3\/2/2 29. 2\13/3 31. (t, t) 
33. (�, �, �) 35. 18VD/13 37. � 
43. =78.9° 45. =80.4° 
1.4 1. 13 N at approx 
N 67.38 E 3. 3\13 N at an angle of 30° to f1 
5. 4 N at an angle of 60° 
7. 5 N at an angle of 60° 
force, 5\13 N 
to the 5 N force 
9. 750\/2 
N 11. 980 
N 13. = 117.6 N in the 15 cm wire,= 88.2 
N in the 20 cm 

But d1 · dz = 0 

(b) Parallel 
(d) Perpendicular 

only if d1 and dz are orthogonal. 

to fz 
to the given 

x = t 
(b)y=- t  

are perpendicular 

perpendicular 

Inequality 

would be violate

Exercises 

Exercises 

if and 

ntly, 

wire 

d. 

Exercises 

Review 

Questions 

to Selected 
umbered 

ANS4  Answers 
Odd-N
1. (a) T  (c) F  (e) T (g) F  (i)  T 
3.x =  [1
9. 2x + 3y - z =  7 

1]  [- 2M l 

5. 120°  7. l/Vs 
0 
1 1 .  V6/2 
Schwarz 
Inequality 
17. x = 2 19. 3 

13. The Cauchy-
15. 2V6/3 

would be 

violated. 

8

3 1 .  y + z =  1 33. [l ,  l ]  

x- y  = l  
2x - y + z =  1 

35. [ 4, - 1] 

37. No solution 

39. (a) 2x + y = 3  (b) x = � -}s 
I 
J 
4 1 .  Let u = - and v = -. The solution is x = 3,y = -2. 

4x + 2y =  6  y = s 

1 
x  y 

1 

Chapter 
2 
s 2.1 

Exercise
1 .  Linear 3. Not linear 
5. Not linear 7. 2x + 4y = 7 
9. x + y = 4(x, y i= O) 

because of the x-1 term 

2.2 

43. Let u = tan x, v = sin 

y, w = cos z. One solution is 

x = n/4, y = - n/6, z = n/3. (There are infinitely 
many solutions.) 

0 

form 

Exercises 
1. No 
5. No 

row echelon 

3. Reduced 
7. No 

[: 1 :J 1 1 .  (b) [: �] 
9. (a)  1 
[i 0 - 1  i 
in the order 
R4 + 29R3, 8R3, R4 -3R2, R2 � R3, R4 - RP R3 + 2Rp 
and, finally, R2 + 2R1• 
on A in the order R2 -3Rp } R2, R1 + 2R2, 

0  0 
elementary 

perform elementary 

row operations 

17. One possibility 

13. (b) 1 - 1  

15. Perform 

is to 

operations 
R2 + 3Rp RI� R2. 

row 

19. Hint: Pick a random 

2 X 2 

matrix and try 

this­

carefully! 

combined: 

2 1 .  This is really 

two elementary 
row operations 
3R2 and R2 -2R1• 

3: 2; Exercise 
5: 2; 

23. Exercise 1: 3; Exercise 

25. [r 27.{J 29. [ _�] 

Exercise 7: 3 

24  6  0  12 
-10  -2  6  -6 
3 1 .  0 +  r 1 + s  0 +  t 0 
0 1  0 
0 0 

0 
0 

33. No solution 
35. Unique 

solution 

15. Unique 

x = 3, y = - 3  

solution, 
y 

17. No solution 

y 

25. [2, - 7, -32] 

19. [7, 3] 
2 3 .  [5, - 2, 1, l] 

27. [� -�I �J 2 1 .  [�, t, -t l 

29. [-� � =�] 2  4 4 

many solutions 

37. Infinitely 

39. Hint: Show that if ad -be * 0, the rank of [: �] 

[Further 
y = (a -b)/2.] 

that the given 

system 

1 1 .  We need to 'how 

a  =  0 and a  * 0.) Use 

is 2. (There are two cases: 
the Rank 
must have a unique 
4 1 .  (a) No solution if k = - 1  

Theorem to deduce 

solution. 

(b) A unique solution if k *  ± 1 
many solutions if k =  1 

(c) Infinitely 
( c) Infinitely 

43. (a) No solution if k =  1 

(b) A unique solution if k * - 2, 1 

many solutions if k =  -2 

Exercises 

ANS5 

to 

�dd' 

th'1 the 

0  2 b + c - a  

ve<tm equation 

yield x = (a+ b)/2, 

row operations 
Hence, 

of a, b, and c. This vector equation 
is equivalent 
the linear system 
whose augmented 
matrix is 

to Selected 

Answers 

Odd-Numbered 
�2 =span([�], [ _�]). 
x [ �] + 
{] +  z [:J � rn h� a rnlution focall valu" 
[ � � O n Row cedu<tion 
[i 1 � � ] , from which we can see 
Hence,�'� 'pan( [ H [J [:]) 
vector [-�] 
vedu" [H U] 
i' {-� l 

that there is 
operations 
y = (a +  b  -c)/2,z =  ( - a+  b  + c)/2.] 

(b) The line with general equation 
the origin 

equation 2x - y + 4z = 0 
system 

2x + y = 0 
with direction 

a (unique) 
solution. 
yield x = (a - b + c)/2, 

with general 
the linear 

a  + 3c =  0 
- a+  b  -3c =  0 

with direction 

many solutions, 

who" 'olution 

tution yields 

(b) The plane 

the simplest 

the origin 

[Further 

through 

perhaps being 

It folluw' 

that the;e 
are 

row 

infinitely 
a =  - 3, b = 0, c = 1 .  

13. (a) The line through 

15. (a) The plane 

17. Substi

19. u = u + O(u + v) + O(u + v + w) 

v = ( -l ) u  + (u + v) + O(u + v + w) 

w = Ou + ( -l ) (u + v) + (u + v + w) 

If 

5 1 .  Thnequfred 

49. No intersection 

matrix 

are still 

solutions 

solutions 
are given by 

many solutions. 

By Theorem 3, there 

with augmented 

the homogeneous 
system 

V1 V2 V3 0 
are infinitely 

But a direct 
check shows that these 
even if u1 = 0 and/or u1 v2 - u2v1 = 0. 

mtm x � [::J acethe rnlutionrnf 
[u,  U2 U3 I 0] 
u1 *  0 and u1 v2 - u2 v1 *- 0, the 
= �:::] U1V2 -UzV1 
t[�::: 
55. [:J 
57. [�] 
x[ �] + 
yields [ 1  1 I a ] , from which we can 

is [ 1  1 I a]. Row 

y [ _ �] = [ �] has a 

solution for all values 
equation is equivalent 

This vector 
whose augmented 

reduction 
see that there is 

of a and b. 
to the linear 

7. Yes 
vector 

equation 

3. No  5. Yes 

0 - 2  b  - a 

a (unique) 

solution. 

matrix 

that the 

system 

Exercises 
1. Yes 
9. We need to show 

53. [�] 

1 - 1  b 

2.3 

19. I1 = 3 amps, I2 = 5 amps, I3 = 2 amps 

2 1 .  (a) I =  10 amps, I1 = Is = 6 amps, I2 = I4 = 4 amps, 

I3 = 2 amps 
(b) Reff = � ohms 
(c) Yes; change it to 4 ohms. 

$54/hr. 

23. Farming 
25. The painter charges 
electrician 

27. (a) Coal should 
29. (a) Yes; push switches 

million. 
(b) Coal should 
should 

steel 

: Manufacturing 
= 2 : 3 

$39/hr, the plumber $42/hr, the 

produce $100 million 

and steel 

$160 

production 

reduce 
increase production 

by "' $4.2 million 
by"' $5.7 

million. 
1, 2, and 3 or switches 
3, 4, and 5. 

and 

31. The states 

that can be obtained are represented 

by 

those 

vectors 

X1 
X2 
X3 
X4 
Xs 

in.:£'.� for which x1 + x2 + x4 + Xs = 0. 
(There are 16 such possibilities.) 

33. If 0 = off, 1  = light 

blue, 
that arises 

and 2 = dark blue, 
has augmented 

then the 

matrix 

ANS&  Answers 
Odd-N

to Selected 
umbered 

Exercises 

25. Lmeedy 

2 1 .  (c) We must show that span(ep e2, e3) = span(ep e1 + 

e2, e1 + e2 + e3). We know 
that span(ep e1 + e2, 
e1 + e2 + e3) � IR3 = span(ep e2, e3). From 
Exercise 19, el' e2, and e3 all belong to 
span(ep e1 + e2' e1 + e2 + e3). Therefore, 
by 
Exercise 2 l (b), span(ep e2, e3) = span( el' e1 + 
e2, e1 + e2 + e3). 

23. Linearly independent 

27. Linearly dependent, 
29. Linearly independent 

since 

the zero vector 

the set contains 

dependent. -m + m m 
dependent. [ j] + u l + [ = il � rn 

(b) No 

2.4 

43. (a)  Yes  (b) No 

31. Lmeedy 

Exercises 
I. x1=160,x2=120,x3 = 160 
3. two small, 
5. 65 bags of house blend, 
blend 

45 bags of gourmet 

three 

medium, 

four large 

30 bags of special 

blend, 

2 -----+ 2NaCN + 3CO 

7. 4FeS2 + 1 102 -----+ 2Fe203 + 8502 
9. 2C4H10 + 1302 -----+ 8C02 + lOH20 
1 1 .  2CsH110H + 1502 -----+ 12H20  + 10C02 
13. Na2C03 +  4C  +  N
15. (a) 11 =  30 - t (b) 11 = 15, 13 = 15 

0 -::; 1
2 -::; 20 
10 -::; l 3 -::; 30 

f2=-10+ t  
f3 =  t 

(c) 0 -:s 11 -:s 20 
17. (a)  11 = - 200 + s +  t (b) 200-:s 13 -:s 300 
-:s 150 (from the 14 equation). This is 

flow would mean 
against 

12 =  300 - s - t 

(c) Ifl3 = s = 0, thenls = t 2  200 (from theJ; equa­

13 =  s 
14 = 150 - t 
ls= 

tion), but ls = t 
a contradiction. 

(d) Negative 

the direction 

backward, 

of the arrow. 

( d) 50 -::; 13 -::; 300 

that water was flowing 

linear system 
1 1 0 0  0 2 
1 1  1 0  0 1 
0 1  1 0 2 
0  0 1 
0 0  0  2 

which reduces 
over .l'.3 to 

1 0  0 0 
0 1 0  0 2 
0 0 1 0  0 2 
0  0  0 2 
0 0  0  0  0 
0 

This yields 

the solutions 

2 
X1 
1 
X2 
X3  2 +  t 0 
X4  2 
Xs  0  1 

2 

are exactly 

three 

solutions: 

there 

where t is in 23. Hence, 
0 2 1 2 0 2 , 2 , 2 2 0 0 2 

switch 

indicates 

the number of times 
the 

where each entry 
corresponding 
should 

35. (a) Push squares 3 and 7. 
(b) The 9 X 9 coefficient 
22, so for any b in 2�, Ax = b has a unique 

be pushed. 

matrix A is row equivalent 

to 

solution. 

Exercises 

ANS1 

to Selected 

Odd-Numbered 
Answers 
37. Grace is 15, and Hans is 5. 
39. 1200 and 600 square yards 
41. (a) a= 4 -d, b = 5 -d, c = -2 + d, d is arbitrary 
(b) [a,b,c,d,e,f] = [4,5,6, -3, -1,0]  + 
j [-1, -1, -1, 1, 1, 
45. (a) y = x2 -2x + 1 (b) y = x2 + 6x + 10 
49. A =  -LB= t, C = 0, D = -f5, E = -t 
51. a = t, b = t, c = o 

(b) No solution 
43. (a) No solution 

47. A= l, B= 2  

l] 

Exercises 

5. n  0  I 

2.5 
x, 0 0.8571 0.971
Xz 0 0.8000 0.9714 0.9943 0.9992 

4 0.9959 0.999

1. n 0  I  2  3  4  5 

Exact solution: 

3. n 0  I  2  3  4  5 

3  4  5 

Exact solution (to four decimal places

x1 = 1, x2 = 1 

1 0.9998 
0.9998 

6 0.2623 
0.2622 
x, 0 0.2222 
0.2539 0.2610 0.2620 
Xz 0 0.2857 0.3492 0.3582 0.3603 
0.3606 
0.3606 
): x1 = 0.2623, 
X2 = 0.3606 
7  8 0.300
6 0.2986 
0.2500 0.3055 0.2916 0.3009 
1 0.2997 
4 0.1250 0.0972 
0.0996 
0.1008 0.1000 
0.2500 0.3055 0.2916 0.3009 
1 0.2997 
0.2986 
0.300
x2 = 0.1, x3 = 0.3 
0 0 0.8571 0.9714 0.9959 0.9992 

4 0.2623 
2 0.2222 
3 0.2622 
0.3492 0.2610 
0.3606 
0.3606 
0.3603 

x, 0 0.3333 
Xz 0 0.2500 0.083
X3 0 0.3333 
x1 = 0.3, 

0.1042 

9. n  0 0 0 
0.9998 
1.0000 1.0000 
1.0000 
within 0.001 of the 

After three 

took four iterations 

iterations, 
exact 
to reach 

After three 

within 0.001 of the 

iterations, 
exact 
to reach 

took four iterations 

the Gauss-Seidel 
method 
Jacobi's 

method 
the same accuracy. 

method 
Jacobi's 

the Gauss-Seidel 

the same accuracy. 

Exact solution: 

7. n  0  I 

solution. 

solution. 

method 

3  4 

is 

is 

2 

2 

I 

to Selected 
umbered 

ANSB  Answers 
Odd-N
n 0 1 2 3 4 
11. 

Exercises 

5 6  13. 

X1 0 0.3333 0.2777 0.2962 0.2993 
Xz 0 0.1667 0.11 12 0.1020 0.1004 
X3 0 0.2777 0.2962 0.2993 0.2998 

0.2998 0.3000 
0.1000 0.1000 
0.3000 0.3000 

After four iterations, 
0.001 of the exact 
to reach 
iterations 

solution. Jacobi's 
the same accuracy. 

the Gauss-Seidel 

method 

method 

took seven 

is within 

15. n  0 1 2 3 4 

0 
0 

3 
-4 

-5 
8 

19 

- 53 

- 28 

80 

If the equations 
method 

are interchanged 
to the equivalent 
system 

is applied 

and the Ga

uss-Seidel 

3x1 + 2x2 = 1 
X1 -2x2 = 3 

we obtain 

0.5 

�t--r-+-+-+-+- ...--+--+-+oot-t--+--.
0.5 1 I 

Xj 

n  0 1  2  3 

4  5 

X1 0  0.3333 1.2222 0.9260 
Xz 0  - 1.3333 -0.8889 - 1.0370 -0.9876 - 1.0041 

1.0247 0.9918 

6  7 

1 .0027 0.9991 

8 
1 .0003 

- 0.9986 - 1.0004 

- 0.9998 

After seven iterations, 
within 
0.001 of the exact 

the process 

has converged 

to 

X2 
-
17. 
� 

10 20 

solution x1 = 1, x2 = - 1. 
X] 

-30 

19. n 0 1 2 3 

X1 0 - 1.6 14.97 8.550 
Xz 0  25.9 1 1 .408 14.051 
X3 0 - 10.35 - 9.3 1 1  - 11.200 

4  5 

6 
10.740 9.839 
10.120 
1 1.249 
1 1 .615 1 1.718 

- 11.322 - 11.721 

- 11.816 

n  7 8  9 

10 11 

12 

X1  9.989 10.022 10.002 
Xz 1 1. 187 1 1 .082 1 1.052 
X3 - 11.912 - 11 .948 - 1 1.973 

10.005 10.001 
1 1.026 1 1.015 
- 1 1.985 - 11.992 

10.001 
1 1 .008 
- 11 .996 

After 12 iterations, 
within 

the Gauss-Seidel 
solution 

0.01 of the exact 

has converged 

to 

method 

x1 = 10, x2 = 1 1, x3 = - 12. 

Answers 

to Selected 

Odd-Numbered 

Exercises 

ANS9 

6 

7 

8 

9 

24.805 24.951 24.988 24.997 24.999 
24.902 24.976 24.994 24.998 24.999 
74.902 74.976 74.994 74.998 74.999 
74.951  74.988 74.997 74.999 75.000 

2 1 .  n  1 3  

14 

1 5   16 

x, 10.0004 10.0003 10.0001 10.0001 
X2 1 1.0043 1 1.0023 1 1 .0014 1 1.0007 
X3 - 1 1.9976 - 11 .9986 - 1 1.9993 - 11.9996 

23. The Gauss-Seidel 
method 

produces 

2 

3 

n  0  1 
x, 0  0  12.5 21.875 24.219 
Xz 0  0  18.75 21.438 24.609 
X3 0  50  68.75 73.438 74.609 
X4  0  62.5 71.875 74.219 74.805 

5 

4 

The exact 

solution is x1 = 25, x2 = 25, x3 = 75, x4 = 75. 

25. The Gauss-Seidel 

method 

produces the following 

iterates: 

2 

1 

4 

3 

n  0 
t, 0  20  2 1 .25  22.8125 23.330
t2 0  5 
t3 0  21.25 24.6094 26.9873 27.7303 
t4 0  2.5  5.8594 8.2373 8.9804 
ts 0  7.1875 14.6289 16.2829 16.7578 
t6 0  23.0469 24.9072 25.3207 25.4394 

1 1 .25  13.3203 14.6386 

23.6596 23.7732 
15.0926 15.2732 
27.9626 28.0352 
9.2126 9.2852 
16.9036 16.9491 
25.4759 25.4873 

1 

6 

5 

7 

9 

8 

10 

n 
t,  23.8093 23.8206 23.8242 23.8252 
t2  15.2824 15.2966 15.3010 15.3024 
t3 28.0579 28.0650 28.0671  28.0678 
t4  9.3079 9.3150 9.3172  9.3178 
ts 16.9633 16.9677 16.9690 16.9695 
t6  25.4908 25.4919 25.4922 25.4924 

23.8256 23.8257 
15.3029 15.3029 
28.0681  28.0681 
9.3181 9.3181 
16.9696 16.9696 
25.4924 25.4924 

1 1  

1 2  

27. (a) n  0  1  2  3  4  5  6 

x,  0  0  l 
4 
l 
l 
2 
2 

X2 

l  5 
16 
4 
3 
3 
8 
8 

21 
5 
64 
16 
ll 
ll 
32  32 

X2 

1 .2 

0.8 

0.4 

Xj 

umbered 

Exercises 

to Selected 

ANS10  Answers 
Odd-N
(b) 2x1 + X2 =  1 
X1 + 2x2 =  1 
(c) n  0  1  2  3  4  5 

6  7 

0 

6 

6 

to 

1) 

of A) 

X1 0 0  0.25 0.3125 0.3281 0.3320 
X2  0.5 0.375 0.3438 0.3360 

0.3340 

Review 

Questions 

19. Their ranks must be equal. 

independence 
of u and v 
this 

ble are the 
1, 3, and 5 from the table 
are converging 

( c 1 - c 2)v = 0. Linear 
implies 
system, 
are linearly independent. 

[Columns 1, 2, and 3 of this ta
odd-numbered columns 
in part (a).] The iterates 
X1 = X2 = 0.3333. 
(d) X1 = Xz = t 
3.[J 5. [�] 7. k = - 1  9. (O, 3, 
1. (a) F  (c) F (e) T (g) T  (i)  F 
1 1 .  x  -2y + z = 0  13. (a) Yes 15. 1or2 
17. If c1(u + v) + c2(u - v) = 0, then (c1 + c2)u + 
c 1 + c2 = 0 andc1 - c2 = 0. Solving 
we get c1 = c2 = 0. Hence u + v and u - v 
Exercises 
3.1 
3 
1 .  [ 3 -6] - 5  7 [ 12 5. -4 
19 2�] 
7. [ 3 
-!] 
[-4 
13. [� 0 �] 0 
15. -49 
12�] 
[ 27 
17. [� �] 
1 .00 2.00] BA = [650.00 
[ 1.50 
462.50] 

Chapter 

19.B = 1.75 

3. Not possible 

1.50 1 .00 

1 1 .  8 

9 .  [ 10] 

,  675.00 
to warehouse i, row 1 contains 
Column i corresponds 
the costs of 
by truck, 
shipping 
costs of 

and row 2 contains 

shipping 

by train. 

406.25 

such 

- 6  -4 

column 

(where 

columns 

column 
of A) 

a; is the ith 

29. Ifb; is the ith 

0.3330 0.3332 
0.3335 0.3334 

of AB are linearly dependent. 

of B, then Ab; is the ith 
of B are linearly dependent, 

column 
of AB. If the columns 
then there are scalars Cp . . .  , en (not all zero) 

21. [� -2 _:J[::: [�] 
23. AB= [ 2a1 + a2 - a3 3a1 - a2 + 6a3 a2 + 4a3] 
25.H 3 �] [: 0 :J+H - 12 -:i 
-9 + - 1  
[ 2A1 + 3A2 l 
27. BA = A1 -A2 + A3 (where A; is the ith row 
-A1 + 6A2 + 4A3 
that c1 b1 + · · · + cnbn = 0. But then c1 (Ab1) + · · · + 
cn(Abn) = A(c1h1 + · · · + c"b") = AO =  0, SO the 
31. [-� � �i 0  0  5 
33. r� ! � 1 0 1 
l]A3=[- l  
35. (a)  A2 = [- 1  
- l]As=[l 
A4 = [� 
A7 = [_ � 
- 1  ,  1 �] 
(b)  [- 1  
A2001 = 
O -�J 
37.An = [� :J [ I - 1  1 -: l (c) [; 0  0 

-�l 
- l]A6=[1 

�l 

1  1 
4 8 
9 27 

1 - 1  1 - 1  

- 1  1 - 1  1 

0 ,  0 

- 1   - 1  

0 ,  0 

39. (a) 

0 1 0 

- 1  

the 

81 

0 

,�] 

Exercises 

3.2 
1.X =[! :J 

5. B = 2A1 + A2 

to Selected 

Odd-Numbered 

Exercises 

A N S 1 1  

Answers 
(c) The method 
17. (b) (AB)-1 = A-1B-1 if and 
21. X = A-1(BA)2B-1 

in part (b) uses fewer multiplica

tions. 

only if AB = BA 

0 

29.E = 

25.E = 

27.E = 1 
0 

23. X = (AB)-1BA + A  [: 0 il u 0 :: 
[i 0 :J 31. [� �] 
33. [� �] 35. [i � :J 37. [i l�c :: 
39. A=[-� �][� -�lA-1=[� -�J[� �] 

43. (a) If A is invertible, 

then BA = CA =} (BA)A-1 = 
(CA)A-1 ==? B(AA-1) = C(AA-1) ==? BI =  CI ==? 
A2 -2A + I =  0 as A(2I - A) = I. 
B = C. 

0 

then there 

exists 

a matrix X such 

45. Hint: Rewrite 
47. If AB is invertible, 
(with inverse BX). 

dent 

13. Linearly indepen
23. a = d, c = 0 
27.a = d,b = c = 0 
29. Let A = [ aiJ J and B = [ bij J be upper triangular 

15. Linearly independent 
25. 3b 

= 2c,a = d -

n X n 

c 

by the 

definition 

of an 

and let i > j. Then, 

matrices 
upper triangular 
a;1 = ai2 = · · · 
bij = bi+l,j = · · · 
Now let C = AB. Then 

matrix, 
= ai, i-i = 0 and 
= bnj = 0 

ciJ =  a;1 b1j + a;2b2j + . . .  ai, i-1 bi-1,J + a;;bij 

ew-symmetric. 

0 · bi-l,J + a;;· 0 

11 + 0 · b21 + · · · 
+ a;n· O  = 0 

+ ai, i+lbi+l,j +· . .  + ainbnj 
=  0 · b
+ ai,;+1 · 0  +· · ·
from which it follows 
35. (a) A, B symmetric 
A + B =} A + B 
37. Matrices 
(b) and (c) are sk
41. Either 

A or B (or both) 

that C is upper triangular. 
==?(A + B)T = AT + BT= 
is symmetric 

trace. 

47. Hint: Use the 

must be the zero matrix. 

J [: ; n + [� -� =�J 
43.(b) [; � :
1.[_� -:J 3. Not invertible 
9· - b/(a2 + b2) a/(a2 + b2) 11. [-!] 
[ a/(a2 + b2) b/(a2 + b2)] 

5. Not invertible 

3.3 

Exercises 

that (AB)X = I. But then A(BX) = I too, so A is 
invertible 

53. Not invertible 

-2 
57. 5  -2 

[ l/(a2 +  1) 

55. - l/a2 l/a 
l/a 

-a/(a2 +  l)] l/(a2 +  1) 

49. [1 !] 10 5 
51. 2 a/(a  +  1) qa* O  
[ 1/a 0 
l/a3 - l/a2 [-!� -2 5 
-�] 
9 2 -4 [ 1  0 
0 quo 
63. [� 6 �] 3 

61. Not invertible 

-�/d 

0 
59. 

- c/d l/d 

- b/d 

0 
1 

0 

1 

6 

0 

- 1  

ANS12  Answers 
Odd-N

to Selected 

Exercises 

umbered 
3.5 

in row(A). 

for null(A). 

1 ]} is a basis 

Exercises 

19. {[ 1  0  1 

0 
1 - 1  0 

0 3  1 0  0 0  3  1 

0 ] ,  [ 0  1 - 1  0 ] ,  [ 0  0  0 

8  3  1  0  0  3 

-2 - 3  1  0 
- 1  - 2  0  1 

- 1  0 - 2  1  0  0  0  1 

Exercises 
1. Subspace 
3. Subspace 
5. Subspace 7. Not a subspace 
1 1 .  b is in col(A), w is not 
15. No 
17.{ [ l  0 - 1 ] ,[ 0  1  2l} is abasisforrow(A); 

{[:]. [ �]} i" b"'i< fm wl(A ); { [ -m irn b.,i< 
fon ow(A); { [ n [ :l [ J) i< a b"'i' fm wl(A); 
{ [-J} i' a b"i' fornull(A). 
21. { [ 1  0 
{[ �], [ �]} is a basis 
is a basis for row(A); { [�], [�], [�]) is a basis for 
row(A)={ [a b -a+2bl}.Both{[�l[�]} 
and { [ �], [ �] } are linearly independent 

69. [ � � � � l 71. [-� � _: � l 
3.4 [- 3/2] 3. -2 
7. [ _� �][� �] 
9. [: � �][� -� -!] 
1 1 .[ � � � �][� � -� -�i 
1] 
13. [� � �][� � � -2
15. r-1 = [ � �], u-1 = [-t 1J. 
A -I = [-5/12 1/12] 1/6  1/6 rn� � �l 
2!. [� � ! m� � ! � m � t �l 
23. [� � �JU : �r� � _,!] 
� � m� J � �r� -� i �: 

29. { [ 1  0 
31. { [2 - 3  l ] ,  [ l  
35. rank(A) = 2, nullity
37. rank(A) =  3, nullity
39. If A is 3 X 5, then rank(A) :s  3, so 
be 
there 
dent columns. 

0 ] ,  [ 0  1 0 ] ,  [ 0  0  l 
- 1  O ] ,  [4 -4 l ] }  
(A)  =  1 
(A)  =  1 

25. Both { [  1  0 - 1  ] ,  [ 0  1  2 

are linearly independent spanning 

- 1 ] ,  [ 1 1 l l} is a basis for row(A); 

1 ] ,  [ 0  1 -1 1 ] ,  [ 0  1 -1  -1 

0  0  1  0  0  0 

more than three 

23. { [ 1  1  0 

[ 1  1  l ] } 

= IR2• 

y(A) = 2, 3, 4, or 

linearly indepen

sets for col(A) 

- 1  ] ,  
sets for 

spanning 

41. nullit

cannot 

O O 1 

for col(A) 

col(A) 

l} 

l} 

5 

5

l} and { [ 1  0 

ANS13 

with a reflection 

Answers 

Odd-Numbered 

to Selected 
Exercises 
k < O); [ � � J is a 
in the line y = x; [ � � J is a shear in the 
[� � J is a shear in they-direction. 

x-direction; 
example, 

in the x-axis if 

reflection 

For 

[BI w] is consistent, which 

then rank(A)  = l; if a = 2, then 
43. If a = -1, 
rank(A) = 2; for a * -1, 2, rank(A) = 3. 

49. No 

47. Yes 

45. Yes 
5 1 .  w is in 

row echelon 

span(B) if and 

only if the linear 

form, it is also clear 

From this reduced 

system 
with 
is true 

augmented 
matrix 
since 
in this case, 

IBlwl � [� _H]---7 [� � -�] 
that [w]8 = [ _�J. 
53. rank(A) = 2, nullity
(A)  = 1 
(A)  = 1 
57. Let A1, ... , Am be the 
row(A)  = span(A1, . . .  , Am). If x is in null(A), then, 
Ax= 0, we also have Ai· x = 0 for i = 1, . . .  , m, 
If r is in row(A), then r is of the form 
r = C1A1 + . . .  + cmAm. Therefore, 
r· x  =  (c1A1 + · · · + cmAm) ·x 
=  c1(A1 · x) + · · · + cm(Am·x) = 0 

55. rank(A) = 3, nullity

since 
by the 

of matrix multiplica

definition 

row vectors 

59. (a) If a set 

row-column 

of A so that 

tion. 

It follows 
that 

of B are linear

of columns 

of AB [i.e., 

to that needed 

29 in Section 3.1). 

ly 
similar 

k = rank(AB)] is not more than 
ent 

of AB is linearly independent, 
ding columns 
then the correspon
independent 
(by an argument 
to prove Exercise 
the maximum number k oflinearly independent 
columns 
the maximum number r oflinearly independ
columnsofB [i.e.,
rank(AB) 
:s rank(B). 
From Exerc
rank(UA) :s rank(A) and 
ise 59(a), 
rank(A) = rank(( U-1U)A) = rank(U-1( UA)) :s 
rank(UA). Hence, 
s 3.6 

r = rank(B)] .Inotherwords, 

rank(UA) = rank(A). 

61. (a) 

Exercise

1 .  T(u) = [1�], T(v)  = [�J 
1 1 .  [� -�J 13.[� -1 -�J 
15. [F] --[-01 o1J 17. [DJ --[20 03J 
19. [k 0J stretches 
0  1 

bined with a reflection 
stretches 

in the y -axis if 
in they-direction 

or contracts 

or contracts 

[l OJ 
k < O); 0 k 

(combined 

in the x-direction 

(com-

y 

y 

y 

y 

k) 

( 1 , k) 

x 

� (k > 0) 
(0, 1 )   ( 1,  1 ) [� �J (k, 1 )  ( k  +  1, 1 )  
x 
• x 
(0, 0) ( 1 , 0) (0, 0) ( 1 , 0) 
(0, 
1 )   ( 1,  1 ) [� �J � (k > O) 
(0, 
x 
(0, 
0)  ( 1 , 0) (0, 0)  ( 1 , 0) 
2 1 .  I -1 2 l/2J v3/2  23. [ _! -n 
[v3/2 
25. [ 0 -1 -�J  27. [-i fl 
3 1 .  [ S0T] [-: �J 
33. [So T] [� 6 -�J -2 H 0 -�] 

35. [So T] =  -1 
[-v3/2 
37. 1/2 l/2J v3/2  [-v3/2 
39. 1/2 -l/2J -v3/2 
p' + td. Their images 
x = p + td and x'  = 
be given by 
are 
T(x) = T(p + td) = T(p) + tT(d) and T(x') = 
T(p' + td) = T(p') + tT(d). Suppose T(d) of-0. If 
T(p') -T(p) is parallel 
rep­
represent 
resent the same line; 
distinct 
if T(d) = 0, 

otherwise 
On the other hand, 

form, let the 
lines 

to T(d), then the images 

45. In vector 

the images 

parallel 

parallel 

lines. 

to Selected 

ANS14  Answers 
Odd-N
47. y ' 

umbered 
Exercises 
(a) P = 0.07 0.1 1  0.05 
11. 

[ 0.08 0.09 0.1 1  l 

then the 
T(p') * T(p) and single 

points if 
point otherwise. 

represent two distinct 

images 

(- 2, 3) 

(2, 3) 

� 

x 

(2, - 3) 

(- 2, - 3) 

49. y 

(-t. -%) 
51. y 

Exercises 

3.7 [0.4] [0.38] 
1. Xi = 0.6 , Xz = 0.62 
3. 64% 
7. fs 
5. X1 = [ ���], Xz = [ ���i 
9. (a) p = [0.662 0.250] 

120  1 1 5  

0.338 0.750 
wet, 57.5% 
dry 

(c) 42.5% 

(b) 0.353 

No 

poor 

(b) 0.08, 
(c) 10.6% good, 

0.85 0.80 0.84 
0.1062, 0.1057, 0.1057, 0.1057 
5.5% fair, 83.9% 
vector 

of the 

jP are just the column 

50  35  175 

of the 
4 

matrix P. So P is stochastic 

sums 
if and only if jP = j. 

fo ,F rnl 

17. 9.375 
21. 
25. 
29. Not productive 

13. The entries 
15. 
Yes, x = [�] 
19. 
23. No 
27. Productive 
31. x = [��] 
37. X1 = [ 4!l Xz = [1��l x3 = [3��] 
39.x1 = [5��],Xz = [���],x3 = [l��:i 
(a) For LI' we have X1 = [5�l Xz = [ !�l 
41. 
X3 = [ 2�� l X4 = [ � :� l X5 = [ ��� l � = [ ::� l 
2560 , � = 
[3200] [2560] [12800] 
X7 = 512 'Xg = 
[10240] Xia = 10240 . 
population): If 0.1 < s ::::: 1, the 
43. The population oscillates 
45.A � [� � � �] 0  1 1  1 

states 
actual 
population goes through 
0 ::::: s 
< 0.1, the actual 
will eventually 

47. A= 1 1  0 1 0 

(b) The first population 
the second 

(for the relative 
population 

1 0 1 0 1 
1  0  0  1 
0 

oscillates 
between two 
approaches 

a cycle 
population 

the actual 
3; and if 

a steady state. 
a cycle 

is declining 

is growing; 

1 0  0  0 

states,while 

through 

of length 

ifs = 0.1, 

of three 

2048 

die out). 

(and 

, 

Exercises 

ANS15 

V1 

49. Vz  51. V1 
Vz 
V4 
V3 
V3 
[; 1  1 !] 1 0  0 1 0 
53.A = 
57. Vl 

V5 
V4 
0  0 0 
0  0 
55. A =  1 1 0 0 0 
1 0  1 0  0 0 
0  0  0  0 0  0 
vz 59. 

(i) T 

j. 

1 - 6  

vertex 

Review 

k if and 

to vertex 

i and vertex 

Questions 

connected 

j by a path of length 

(I-A)-1 = I+ A +  A2• 

the number of vertices adjacent to both 

Odd-Numbered 

Answers 
to Selected 
only if (A k)ij * 0. 
75. (AA T)ij counts 
77. Bipartite 79. Bipartite 
-33 I ] 
1. (a) T  (c) F  (e) T  (g) T 
1�6 [o -9] 9. 2 4 
3. Impossible 
[1 3] [ 4  10] 
7· 3 9 + 10 25 
Because (I -A)(I + A  + A2) = I  - A3 = I  -0 = I, 
11. 
13. A basis for row(A) is { [1, - 2, 0,-1, O], [O, 0, 1, 2, O], 
O, l ] ); a b"'i' fmrnl(A) i' \ m [H m) 
[O, 0, 0, 
(or the standard basis for R3); and a basis 
1  0 0 , -2 0 1 0  0 
15. 
A  = [� �]. 
17. 
0 0 0 1 0  0 0 0 1 0 0 0 
0 0 A =  0 1 0 
1 0 1 0 0 
take A = [�]. [-1/SVZ -3/SVZ] 
'h 
19. 'h 
Chapter 
4 
4.1 
1. Av = [ �] = 3v, A = 3 

Because 
rank(A) = n. Hence rank(A 
Because AT A is n X n, this implies 
ible, 
Matrices

matrix 
(zero) null space. 
then so is Ar, and so both A and AT have 
If A is not invertible, 
same null space. 

An invertible 
is invertible, 
null spaces. 
trivial 
need not have the 

. AA T need not be invertible. 

Theorem of Invertible 

by the Fundamental 

TA) = n by Theorem 3.28. 

check the (i, j) entry 

has a trivial 

Exercises 

that AT A is invert­

A has n linear

2/5 v2 6/5 v2 

dent columns, 

vertices. 

followed 

ly indepen

For example, 

for null(A) is 

then A and AT 

P3, P4, 

2  1 

If A 

i is not adjacent to any other 

61.2  63.3 65.0 67. 3 
69. (a) Vertex 
71. 
73. (a) Ann 

Ifwe use direct wins 
and P6 tie for 
place. 
ers rank as 
P3, P5, and P1. 

is in first place; 
and P1 and P5 tie for 
third 
and indirect wins, the play­
by P6, P4, 

only, P2 
place; 
direct 

second 
Ifwe combine 

follows: P2 in first place, 

Dana Carla 
(h) two steps; all of 

entries of the 
row of A  + A 2 are nonzero. 

second 
of 
the powers Ak fork = 1, . . .  ,n -1 .  Vertex 

( d) If the graph has n vertices, 

the off-diagonal 

i is 

For example, take 

umbered 

Exercises 
i,E1+; =span([�]); A =  1  -i,E1_;= 
span([_�]) 
4.2 

29.A  =  1  + 

33. A  =  4 

to Selected 

ANS16  Answers 
Odd-N
3. Av= [-!] = - 3v, A  = - 3  
5.AF Hl � 3v,A � 3 
7. [�] 9. [�] II. [ J 
13.A = l ,E1 =span([�]); A = - l, E_1 =span([�]) 
15. A = 0, E0 = span([�]); A = 1, E1 = span([�]) 
17.A = 2,E2 =span([�]); A= 3,E3 =span([�]) 
19.v = [1] A =  l·v = [0] A =  2 
23. A = 2, E2 = span([�]); A = 3, E3 = span([�]) 

[ 1 I \/2] [-1 I \/2] 

, A= 2;v = l/\/2 

21.v = 

, A=  0 

0 , 

1 , 

l/\/2 

, 

y 

4 

/ 

3y 

5. - 18 
13. 4 
27. - 24 
35. 8 
47. - 6  

7 .  6 
15. abdg 
29. 0 
37. -4 

31. A= l, 2  
Exercises 
1. 16 
9. - 12 
17.0 
31. 0 
39. - 8  
5 1 .  ( - 2)3n 
53. det(AB) = (det A) (det B) = (det B) (det A) = det(BA) 
55.0, 1 

3.0 
l l .a2b  + ab2 
25. 2 
33. - 24 
45.k * 0,2 

3  I 
49. -� 
61. [i -iJ 

[ I  I 
- 1] - 1  
4.3 
(c) E3 = span([�]); E4 = span([�]) 

Exercises 
1. (a)  A2 - 7 A  + 

59.x = - 1, y  = O,z =  1 

2 -2 
63. 0 1 

57.x = 2»Y = -2 

12 (b) A  = 3, 4 

0  0 

1 

(d) The algebraic 

and geometric 

multiplicities 

are all 1 .  

3 .  (a) -A3 + 2A2 + S A  - 6 

(b)  A  = 
- 2, 1, 3 

2 

4 

y 

25. A  =  2, E

2 = span([�]) 
t---+--B+ -- x 
-----
27. A =  1  + i,E1+; =span([�]} A =  1  -i,E1_; = 
span([ �J) 

x  2x 

2 

0 

and geometric 

5.(a)- A3+ A2  (b) A= O, l  

multiplicities 

(d) The algebraic 

(c) E0 � spon([-mE, � sp�([�]) 

(d) A  =  0 has algebraic multiplicity 

are all 1 .  

2 and geometric 
and geometric 

multiplicity 
multiplicity 

l; A  =  1 has algebraic 
1. 

7. (a) - A3 + 9A2 -27A + 27 

(b) A  =  3 

Odd-Numbered 

Exercises 

ANS11 

to Selected 

Answers 
(b) (i) ,\  = -LE-1;2 =span([_�]},\ =  k, E1;5 = 
span([�]) 
(iii) A  =  0, E
0 = span([_�]} A  = 7, 
E7 =span([�]) 
27. n � -
ln-A' -3A' + 4A  -12 

35. A2 =  4A  -5I,A3 = l lA  -20I 

A4 = 24A  -55I 

37. A-1 =  -�A + iI  A-2 = _ _i_A + _!_l_I 
5  5 '  25 25 

s 4.4 

polynomial 

9. Not diagonaliza

of A is ,\2 -5,\ + 1, but 

3. The eigenvalues 

of A are ,\  =  2 and ,\  = 4, but those 

Exercise
1. The characteristic 
that of B is ,\ 2 -2A + 1. 
of B are A  =  1 and A  = 4. 

7.A, � 6, E, � 'P""( [�]}A, � - 2, L, � 
5. A1 = 4,E4 = span([�]},\2 = 3,E3 =span([�]) 
'P••([ J. UJl 
ble [ 1  1 
- 1]  [2 � ,D = � 
15.P = [� ! � -�],D = [H -� �] 
[ 35839 - 69630] 
[(3k + 3(- l)k)/4 (3k+l -3(- l)k)/4] 

19' (3k -( - 1/)/4 (3k+l + ( - l)k)/4 

0  0  0  1 0  0  0 - 2  

1 1. P=  1 1 
1 - 2  

13. Not diagonaliza

- 11605 24234 

ble 

(d) A  =  3 has 

multiplicity 

algebraic 
2. 

multiplicity 

3 and geometric 

9. (a) ,\4 -6,\3 + 9,\2 + 4,\ -12 

(b) ,\  = - 1, 2, 3 

(b) ,\  = - 1, 1, 3 

and geometric 

l; A  =  2 has algebraic multiplicity 

multiplicity 
and geometric 
1. 
1 1 .  (a) ,\4 -4A3 + 2A2 + 4,\ - 3 

(d) ,\  = - 1  and,\  =  3 have algebraic 
2 
multiplicity 

(c) E_, � 'P""( [-rn' E, � 'P··( [-�]} 
E, � 'P""( [�]) 
(c) E_, � 'P""( [�]} 
E, � 'P�( [ _�]f�]} 
E, � 'P�( [�]) 
[ r 9 + 3 · 2'0] 
23. (a) ,\ =  -2,E_2 =span([ _�]}A =  5,E5 = 
span([�]) 

and geometric 
l; A  =  1 has algebraic and geometric 
2. 

multiplicity 
multiplicity 

(d) ,\  = -1 and,\  =  3 have algebraic 

15. - r 9 + 3 · 2'0 

17. 

10 

2.5 

3.2. 

P such that 

using 

0.618 

45 in Section 

Exercise 

Exercises 

an invertible 
matrix 
we have 

to Selected 

Odd-N
A N S 1 8   Answers 

Exercises 
umbered 
(b) dimE_1 = 1, dimE1 = 2, 
51. 
dimE2 = 3 
4.5 
1. (a) [1 ], 6.000 
(b) A, = 6 
3. (a) [1 ], 2.618 
(b) A, = (3 + Vs)/2 =  2.618 
5. (a) m5 = l l.001,y5 = [- 0.333] 1 .000 
y, � m 
7. (a) m, � 10.000, 

21. [� -� -�l 
23. [ (5 + 2k+2 + (-3)k)/10 
(-5 + 2k+2 + (-3)k)/10] 
(2k + 4(-3)k)/5 
(5 + 2k+2 + (-3)k)/10 
(-5 + 2k+2 + (-3l)/10 
(2k -(-3)k)/5 
(2k+l - 2(-3)k)/5 
(2k+l - 2(-3)k)/5 
(2k -(-3)k)/5 
27. k = 0 
25.k = 0 29. All real values 
37. If A �  B, then there is 
of k 
B = p-'AP. Therefore, 
tr(B) = tr(P-1AP) = tr(P-1(AP)) = tr((AP)P-1) 
= tr(APP-1) = tr(AI) = tr(A) 
p � [ =! I �: 
- 2] - 3  
-2 3 -2 3 -2 
39. p = [ 7 
41. 
4  5 [17.999] 6.000 [18.000] 6.000 
k  0  1  2 
3 [18.018] 6.004 [ �.333] 
9. 
xk  [�] [2:] [ 1 7.692] 5.923 
[ �.333] [ �.333] 
Yk [�] [ �.308] [ �.335] 
A1 =  1 8, v1 =  [ 1 ]. 0.333 
0  1  2 
3 [7.755] 3.132 [�.404] 

11. 
xk  [�] [�] [7.571] 2.857 
Yk  [�] [�.286] [�.377] 
v1 =  [1 ]. 0.414 

4  5  6 
[ 7.808] 3.212 [7.823] 3.234 [7.827] 3.240 
[�.411] [�.413] [ �.414] 

A1 =  7.827,

7.808 7.823 7.827 

26  1 7.692 

17.999 18.000 

Therefore, 

Therefore,

7.571 

7.755 

1 8.018 

mk 

mk 

7 

k 

Answers 

to Selected 

Odd-Numbered 

Exercises 

ANS19 

3 

3 

1 

4 

2 

1 

2 

xk 

13. k  0 

1 7.  k  0 

Thecefoce, 

[:] [::] [ 16809 l [17011] 
5 [ 17 000 l 12.363 10.818 
4 [ 16999 l 12.363 10.818 
12.238 12.371 
10.714 10.824 
Yk m [�714] [ �728 l [�727] 
[�727 l 0.636 16.999 
[�727 l 0.636 17.000 
0.619 0.637 0.636 
mk  21 16.809 17.011 
A, =  17, v, =  [ �.727 l 0.636 
15. A, =  5, v, =  [ � l 0.333 
[7.571] 2.857 [7.755] 3.132 [7.808] 3.212 
5 [7.823] 3.234 7.828 
6 [7.827] 3.240 
xk  [�] [�] 
7.823 7.828 7.828 
7.828 
R(xk) 7 7.755 
Yk [�] [ �.286] 
[�.413] 
[�.377] [ �.404] [�.411] 
[�.414] 
3 [16809] [ 17 011 l 
5 [ 17000 l 12.363 10.818 17.000 
4 [ 16999 l 12.363 10.818 17.000 
xk  m [m 
12.238 12.371 10.714 10.824 
[ � 728 l 0.637 [�727] 0.636 
17.000 17.000 
R(xk) 16.333 
16.998 
[�727-0.636 
Yk [:] [�714] 0.619 
[�727] 0.636 
] 1.600 [ 4.364] 1.455 
] 2.667 [ 4.571] 2.286 [ 4.500] 2.000 
[ 4.444] 1.778 [ 4.400
xk [�] [!] [ 4.8] 3.2 [ 4.667
Yk [�] [�.8] [�.667] [ �.571] [�.5oo] 
[ �.400] [�.364] [ �.333] 
mk  5 4.8 4.667 4.571 
4.444 4.400 4.364 
A1 = A2 = 4, v1 = [ 1], mk is converging 
answer. 0 

[ �.444] 4.500 

Since 
the exact 

slowly 

to 

19. k 

2 1 .  k  0 

8 

1 

2 

7 

1 

3 

5 

6 

4 

2 

0 

[ 4003 l [ 4001 l [ 4000] [ 4000] 
[�750 l 0.001 [�750 l [�750 l [ � 750 -

5  6  7  8 
3.003 3.001 3.000 3.000 
0.003 
4.003 

0.001 0.000 
4.001 4.000 

0.000 
4.000 

0.012 0.003 

xk 

Exercises 

umbered 

ANS20  Answers 
Odd-N
23. k  0 

to Selected 
1  2  3  4 

m m [ 42] [ 4048] [ 4012 l 
3.2 3.048 3.012 
Yk m [�· l [�762] [�753 l [ � 751 l 
0.2 0.048 
0.012 
0.2 0.048 
mk  5 4.2 4.048 
4.012 
lnthi'"''·A, � ,, � 4 �d E, � 'P�([H [m 
m 
to 4 and Yk is converging 
E,-namdy, [ �] + 0 75 
5 [�] 
25. k 
xk  [�] [�] [ =�J [�] [ = �] 
Yk [�] [�] [�] [�] [�] 
[�] 
-1  -1 

0  1  2 3 4 

Clearly, mk is converging 

wdodn th, dgmpooe 

mk 

to a 

eigenvalues 

cannot possibly converge 

The exact 
power method 
or the 
dominant eigenvalue 
start 
iterate. Instead, 
between two sets of real vectors. 
oscillates 

are complex (i and - i), so the 
the 
ctor if we 

with a real initial 

dominant eigenve

to either 

the power method 

xk 

27. k  0 

1  2 3  4 

m m [ 2500] [2250] ['125] 
Yk m [ �750] [ �625] [ �562] [ �531] 

5 
[2063] 4.000 
4.000 
4.000 
4.000 
2.500 2.250 2.125 
2.063 
[�516] 0.516 4 
0.750 0.625 0.562 0.531 

mk 1 4  4 4 4 

Answers 

to Selected 

Odd-Numbered 

Exercises 

A N S 2 1  

The eigenvalues 
responding 
eigenve

ctors 

are A1 = -12, A2 = 4, A3 = 2, with cor­

Since Xo = tv2 + tv3, the initial 

component in the direction 
so the power method 
eigenvalue
eigenvalue

/eigenvector. Instead, 
/eigenvector pair, as the calcula

vector 

cannot 

it converges to 

a second 
show. 

tions 

x0 has a zero 

of the dominant eigenvector, 
converge to 

the dominant 

2 

3 

29. Apply the power method 

k 

xk 

Yk 
mk 

2 

. 

3 

0  1 

eigenvalue 

k  0  1 

[-4 

to A -181 = 5 12] - 15 
[�] [-1�] [ 15.2] -19 [ 15.2] -19 
[�] [-�.8] [-�.8] [-�.8] 
-10 -19 -19 
of A. [-8  4 8] 
Thus, -19 is the dominant 
of A -181, and 
A2 = -19 + 18 = -1 is the 
8 -4 -8 
to A -171=  4 -2 -
4 .  
xk  [�] [ 0.5] -0.5  [ -0.833
] 1 .056 
Yk  [�] [ _�] [ -�.789] 
1/(-1) = -1. 

In this case, 
second eigenvalue 
could choose 
ever, 
converges 
value 
second 

Thus, the eigenvalue 

of A that is smallest 

in magnitude is 

1 .056 

0.5 

of A. 

mk 

1 

4 

3 

1 

2 

31. Applythepowermethod 

33. k  0 

(We 

there 

is no 

quotient 

method (E

the Rayleigh 

eigenvalue 

dominant eigenvalue. 

-18 

-18 -18 
-18 -18 
xercises 17-20) 

4 -0.667 
either 18 or -18 for mk> k 2 2.) How­
to -18. Thus, -18 is the dominant eigen­
of A -17l,andA2 = -18 + 17 = -1 is the 
[ 0.800] -1.000 [ 0.800] -1.000 
[ 0.798] -0.997 
[ -�.801] [-�.800] [-�.800] 
-1.000 -1.000 
-0.997 

5 

l [ 0500] 
[ 0500] [ 0500 
3 4  5 
[-1000 
l [-1000] [-1000] 
0.111 0.259 0.160 
-0.500 -0.500 -0.500 
-0.222 
-0.518 -0.321 
1.000 1.000 1.000 
-0.500 -0.500 -0.500 

k 

xk 

eigenvalue 

37. The calculations 

We apply the inverse power method 

49. Im 

Exercises 

umbered 

ANS22  Answers 
Odd-N
to Selected 
35. 
0  1 2 

u-[-0500] [ 0500] 
[J [-1000] [-1000] 
0.333 
0.000 
0.500 -0.500 
0.000 
-0.667 
Yk 
1.000 1.000 
mk  1 -0.500 -0.500 
Clearly, mk converges to -0.5, so the smallest 
of A is 1/(-0.5) = -2. 
[-1 -1 6 
� l · Taking 
are the same as for Exercise 33. 
39. 
to A -SI = 
0 -2 0 
x0 = -1 
1 2 [ 0200: [-0080] 
3 [ 0032 l -0.500 0.032 
[:] [:] 
[-�064] -0.064 -0.500 
[-�400-
[ �160] 0.160 
-0.500 -0.500 
0.200 
-0.400 
-0.500 -0.500 
Clearly, mk converges to -0.5, so the eigenvalue 
= 5 -2 = 3. 
to 5 is 5 + 1/(-0.5) 
41. 0.732 43. -0.619 
47. Im 
2 
-2 

-0.080 

closest 

of A 

k 

0 

all column 

Exercises 

in any Gerschgorin 

of the column sums 
for a stochastic 
matrix, 

that l,tl is less than or equal 
to all 
A. But 
of A for every eigenvalue 

51. 
Hint: Show that 0 is not contained 
disk and then apply Theorem 4.16. 
53. 
Exercise 52 implies 
sums are 1. Hence 
IAI :s 1. 
4.6 1. Not regular 3. Regular 
5. Not regular 7. L = [i iJ 
[ 0.304 
0.304 
0.304 -
9. L = 0.354 
0.354 0.354 
11. 1, [�] 13.2. [';] 
0.342 0.342 
0.342 
15. 

The population 
respecti
vely. 

is increasing, 

decreasing, 
and constant, 

ANS23 

(a) 

Answers 
to Selected 
Odd-Numbered 
Exercises 
65. (a) x(t) = -120e8t/s + 520e11tl10, y(t) = 240e8t/s + 
17. b1 b2S1 b3S1S2  bn- 1S1S2 ..• Sn-2 bns1s2 • · • s,, _ 1  
26011t/IO. Strain X dies 
out after 
approximately 
p-I1p =  0  0 
2.93 days; strain 
Y continues 
to grow. 
1 0 0  0 
0 
b = 20; x(t) = lOet(cos 
67. a = 10, 
t + sin t) + 10, y(t) 
0 0 
Y dies 
out when 
t -sin t) + 20. Species 
= lOet(cos 
t =  1.22. 
0 0 0  0  0 
71. x(t) = C1e2t + C2e3t 77. (a) 
0 
0 0  0 
The characteristic 
of Lis (A" - b,A"-1 -
polynomial 
b2s1An-l - b3s1s2A"-3 -· · • -b,,s1s2 • • ·s,,_,)(- lt. 
(c) Repeller 
[�l [�l [:l [��] 
[ 0.660 l 
A =  1.746, 
p =  0.264 
19. 
(c) Neither 
79. (a) 
[�l [�l [�l [�] 
0.076 0.535 
[ �], [ -�·5], [ -��ss l [ _3;.\2ss] 
81. 
(c) Saddle 
point 
0.147 0.094 
(c) Attractor 
A =  1.092, p =  0.078 0.064 
83. (a) [�l [�::l [�:�:l [�:��:] 
21. 
l repeller 
85. 
r = v2, e = 45°, spira
0.053 
87. 
r = 2, e = -60°, 
spiral repeller 
0.029 
89. P = [ -� -� l C = [ �:� -�:� l spiral attractor 
(a) h =  0.082 
29. 3, [ iJ 
25. 
-[1/2 -V3;2] -[ 1/2 -V3;2] 
91.P-1 O , C- \/3/2  1/2 ' 
3 1 .  3, [ll 33. Redu<ibk 
orbital 
center 
43. 1, 2, 4, 8, 16 
35. Irreducible 
= 4" -(-1)" 
1, 1, 0, -1 47. x,, 
45. 0, 
3. -18 (c) F  (e) F (g) T  (i)  F 
1. (a) F 
= (n -t)2" 51.b,, = ,1;;;-[(l + v3)" -(1-v3)"] 
49. y,, 
2 v3 57. (a) d, = 1, d2 = 2, d3 = 3, d4 = 5, d5 = 8 
Ar = -A, we 
5. Since 
<let A = det(A r) = 
have 
det(-A) = (-1)" <let A = -<let A by Theorem 4.7 
C  d = _l [(l + Vs)n+I _ (1 -y5)n+l] 
and the fact that 
n is odd. 
It follows 
that <let A = 0. 
7. Ax = [ S ] = Sx A = 5 10 , 
( )  n Vs  2 2 
59. The general 
x(t) = - 3C1e-t + C2e4t, 
solution is 
9. (a) 4 - 3A 2 - A3 
y(t) = 2C1e-t + C2e41• The specific 
solution is 
x(t) = -3e-t + 3e4t, y(t) = 2e-t + 3e4t. 
6 1 .  The general 
solution 
is x,(t) = (1 + V2 )C1e\/2t + 
(1 -V2 )C2e-V2t, x2(t) = C,eV2t + C2e-V2t. The 
specific 
x1(t) = (2 + V2 )e\/2t/4 + 
solution is
17.0,1,or-l  15. Not similar 
13. Not similar 
(2 -V2 )e-V2t/4, x2(t) = V2 eV2t/4 -V2 e-V2t/4. 
63. Thegeneral
solutionisx(t) = -C1 + C3e-t,y (t) = 
C1 + C2et - C3e-t, z(t) = C1 + C2et. The specific 
19. If Ax = Ax, then 
(A 2 -SA + 2I)x 
x(t) = 2 - e-t, y(t) = -2 + et+  e-t, 
solution is 
A2x -5Ax + 2x = 32x -5(3x) + 2x = -4x. 
z(t) = -2 + et. 

(b) d,, = d,,_, + d,,-2 

Questions 

Review 

= 

5 

Qx ·Qy x ·y 

Exercises 

Exercises 

umbered 
Odd-N
ANS24  Answers 
to Selected 
� o). B" � { [J [:] ) 
5. w" � { [;] x -y + 3z 
Chapter 
5.1 1. Orthogona
7, [w]3 � [ _;] 9. [w]B � m 11. Q,thono
l 3. Not orthogona
l 5. Orthogona
l 
cmaJ 
HI! O 1]. [O 1 -2]}, 
null(A)o \[ �]) 
7. ww(A 
9. wl(A) { [ j} UJ }.null(A
13. [���], [�:�], [ !���] 
') 
2/3 0  -5/3Vs 
15. Orthonormal 
[1/v2  - 1/v2] 
17. Orthogonal, 
1 / v2  1 / v2 [cos () sin () 
mJrrn 
si� () l cos() 
cos2 () 
-cos() sin() -sin2 () -cos () sin () 
thogonal, 
19. Or
11 { [-��l) 13. { [-]f]} 
21. Not orthogonal 
27. cos(L(Qx
, Qy)) = llQxll llQrll llxll llrll 
15. [ll  17. n i 
= cos(L(x, 
y)) by Theorem 5.6 
IB +AI= 
y = v3x 
45° 31. Reflection, 
n,() = 
29. Rotatio
= AArB + ABrB = 
33. 
(a) A(Ar + Br)B 
F  [-il + Ul 
B+A=A+B 
19. v � [ =ll + [ _;J 21. 
(b) From part (a), 
r + Br)B) 
+ B) = det (A(A 
det(A 
25.No 
= det A det(A 
r + Br)det B 
= det A det((A + B)r)det B 
5.3 
[ 1 ] [-1 I 2] [ 1 I v2] [-1 I v2] 
+ B)det B 
= det A det(A 
0 (so that det 
Assume 
that 
det A + det B = 
1. v, = 1 'Vz = 1/2 ; q, = 1/\/2 'qz = 1/\/2 
3.v1=[-�],v2 =[�],v3 =[-�];q,= [-���], 
B = -det A) but that A + 
Bis invertible. 
Then det(A 
+ B) * 0, so 1 = detA detB = 
detA(-detA) = -(detA)2. This 
is impossi
ble, 
be invertible. 
cannot 
so we conclude 
that A + B 
1. w� = {[;] : x  + 2y = o },B� = {[-�]} 
5.2 
[2/V6] [ 0 l 
qz = l/V6 , q3 = - 1/\/2 
3. w" � {[;]x � t,y � t,z � -1). 
B '�{[J) 

Exercises 

Exercises 

l/V6  l/v2 

1  - 1/v3 

- 1   1 

Answers 

to Selected 

Odd-Numbered 

Exercises 

ANS25 

19.A = AI 
21. A-I = (QR)-! = R-IQ-1 = R-IQT = 

1/\/2] 1/\/6 2/v'3 

f [3] [-?si [-�]) 
l l .  l � , �1 ' � 
D� [� � � �] 
[ 1/\/2 l/v'3 1/\/6 l 
ll .  TA = [1/\/2 1/\/2] [a b]. 
13. Q =  0 l/v'3 -2/\/2 
15. [1;°\/2 -�j� �j�][� �j� 
Q Q 1/\/2 -1/\/2 b a 
-1/\/2 l/v'3 1/\/6 
[ 1/\/2 1/\/2] [a + b 0 ] 
1/\/2 -1/\/2 = 0  a -b = D 
9 tl 6 I 3 0 z 3 
13. (a) If A and B
are orthogona
lly diagona
lizable, then 
1/\/2 1/\/6 -1/\/3 0  0 
is symmetric, 
each 
by the Spectral Theorem. 
35 in 
by Exercise 
A + B is symmetric, 
Therefore, 
17R � [� 
ble, 
3.2, 
Section 
and so is orthogona
lly diagonaliza
by the Spectra
l Theorem. 
lizable, 
then 
are orthogona
lly diagona
15. If A and B
each is 
symmetric, 
Since 
by the Spectral Theorem. 
36 in 
AB = BA, AB is also 
symmetric, 
by Exercise 
[1/\/2 -1/\/6 -1/2\/3] 
Hence, 
Section 
3.2. 
AB is orthogona
lly diagonalizable, 
0 2/\/6 -1/2\/3 . 
by the Spectral 
Theorem. 
17. A = [i n + [ _! -n 
[2/0\/6 -�j� �j�] 
0 0 3/2\/3 
19.A � [1 0 :J + [: 0 �] + [: 0 J 
l/v'3 l/v'3 -1/\/3 
0 2 -1 
21. [ -i -iJ  -3 � 3 
Ax= QRx = QO = 0. Since 
23. Let 
Rx =  0. Then 
0 2 
231! 2 -ii 
represents 
combination 
of the 
a linear 
of A 
columns 
t), we must 
have 
ly independen
are linear
(which 
I 3 
by the Fundamental 
Hence, 
R is invertible, 
Theorem. 
5.4 
1. = [1/\/2 Q 1/\/2 1/\/2] [ 5 -l/\/2'D= 0 �] 
s 5.5 
1. 2x2 + 6xy + 4/ 3. 123 5. -5 
7. [� �] 
9. [ �� =n 11. [ � -� -�i -2 2 2 
. Q -l/v'3 l/v'3] [2 -2/\/6'D= 0 -�J 
3 -[2/\/6 
0 -�l 4 0 0 r 0 
[' 0 0 l [' 
5. Q =  0 1 I v'2 -1 I v'2 , D 
l3. Q = l/Vs 
= 0 
l/Vs] 2 2 -2/Vs , yl + 6y2 !j�� -�j�], 9yi + 9A - 9y� 
[2/Vs 
0 1/\/2 1/\/2 0 
1/:2].D � [� 1/\/2 0 
[-1/Vi 0 
[2/Vs 
7. Q =  0 1/\/2 0 0 0 
15. Q = 0 l/Vs 
['M I/Vi 0 l 
-4/3Vs 2/3 1/\/2 1/\/6] 0 2/\/6 , 2(x')2 + 
[ l/v'3 
-1/\/2 0 
9 -1/\/2 .Q- 1/\/2 0 1/\/2 , 
17. Q = -l/v'3 -1/\/3 
0 0 1/\/2 0 -1/\/2 
1/\/2 -1/\/6 
(y')2 _ 
(z')z 

Exercises 

Exercise

x = 0. 

Ax 

+ t, (x')2 / 4 -(y')2 /9 = 1 

3 

- 3  

0. 

y,y' 

y  y' 

x' = x, y' = y 

ANS26  Answers 
Odd-N
to Selected 
umbered 
Exercises 
te 21. Negative 
definite 
defini
19. Positive 
te 
43. Hyperbola, 
23. Positive 
defini
25. Indefini
te 
xr Ax = xTBTBx = 
27. For any vector 
x, we have 
-+--+-+-1--+--+--+--+-+--+X 
= 11Bxll2 2: 0. IfxrAx 
11Bxll2 = 0, 
(Bxl(Bx) 
= 0, then 
so Bx= 0. Since 
B is invertible, 
implies 
this 
that x = 
TB 
xr Ax > 0 for all x * 0, and hence 
Therefore, 
A = B
•�3-- x' 
---=j----
definite. 
is positive 
form 
29. 
of cA is of the 
eigenvalue 
(a) Every 
CA for some 
eigenvalue 
A of A. By Theorem 5.24, 
A > 0, so 
CA > 0, since 
c is positive. 
Hence, 
cA is positive 
+ 2, x' = -t(y')2 
x' = x -2, y' = y 
45. Parabola, 
by Theorem 5.24. 
definite, 
(c) Let x * 0. Then xr 
> 0, since 
Ax > 0 and xrBx 
A 
and Bare positive 
te. But then 
defini
xr(A 
+ B)x = 
definite. 
+ B is positive 
> 0, so A 
xr Ax + xrBx 
31. Themaximum
value
ofj(x)is2whenx= 
is 0 when x = 
value 
the minimum 
ofj(x) 
value 
33. The maximum 
of f(x) 
x = 
is 4 when 
[ l/v2] [-l/v2] 
the minimum 
is 1 when 
value 
of f(x) 
x = 
::t:::  0 or ::t::: l/vl . 
-1/vl 0 
39. Hyperbola 
35. Ellip
se 37. Parabola 
x' = x -2, y' = y -2, (x')2 + (y')2 = 4 
41. Circle, 
y  y' r �--+----+---
49. Hyperbola, (x')2 -(y')2 = 1 

::t::: [-��:�} [1/v2] 
::t::: l/vl . [1/V3] 
::t::: 1/V3 ; 1/V3 

47.Ellipse,(x')2/4 + (y')2/12 = 1 

1---- x' 

y 

2 

5 

3 

y 

4 

1 3  4  5 
I 

y 

2 

y 

2 

- 2  

ANS21 

2 I ]} 

to Selected 

l set. 
{ [ 1 0 

57. Degener
ate (a point) 
�1--+--+--e--+--+--+-_.x - 2  
59. Degenerate 

Answers 
Odd-Numbered 
Exercises 
if V; • vj = 0, then 
7. Theorem 5.6(c) shows 
that 
(x")2/50 + (y")2/10 = 1 
5 1 .  Ellipse, 
Qv; · Qvj = 0. Theorem 5.6(b) 
shows that 
{ Qv1, .•. , Qvd consists 
(x")2 -(y")2 = 1 
53. Hyperbola, 
of unit ve
ctors, 
because 
{ v1, .•. , vk} does. 
{ Qv1, .•. , Qvk} is an 
(two lines) 
55. Degenerate 
Hence, 
orthonorma
9. { [ _�]} 
2  3 4 ] ,  [ 0 1 0 
13. row(A): 
rnl(A) { [-�} [t]} -2  -3  -4 0 -2 -1 
): 1 0 , 0 0 0 
null(A
0 0 
') { [ = � l [-�] } 
null(A 
lS. (a) { [] uirm 
17. { [-�l [-lJ [ _)]} 
19. n -! �l 

(x')2 -(y')2 + 3(z')2 = 1 
6 1 .  Hyperboloid 
of one sheet, 
z = -(x')2 + (y')2 
63. Hyperbolic 
paraboloid, 
x' = -V3(y')2 + V3(z')2 
paraboloid, 
65. Hyperbolic 
3(x")2 + (y")2 + 2(z")2 = 4 
67. Ellipsoid, 
6 
Chapter 
space 
1 .  Vector 
1. (a) T  (c) T  (e) F (g) F (i) F 
6. 1 
space
fails. 
3. Not a vector 
; axiom 1 
space
8 fails. 
5. Not a vector 
; axiom 
space 
7. Vector 
space 
9. Vector 
vector 
15. Complex 
space 
space 
1 1 .  Vector 

3. [ �j� l -11/6 5. Verify 

(two lines) 

that Qr Q = I. 

Questions 

Exercises 

Review 

y 

l -x, 1 - x2} 

W, then 

to Selected 
ANS28  Answers 
Odd-N
umbered 
Exercises 
vector 
space; 
17. Not a complex 
axiom 
6 fails. 
35. dim V = 2, B = {
space; 
axioms 
19. Not a vector 
1, 4, and 
6 fail. 
37. dim V = 3, B = { [ � � l [ � � l [ � �]} 
of addition 
2 1 .  Not a vector 
space; 
the operations 
and 
the same. 
multiplica
tion 
are not even 
39.dimV= 2,B= {[� �l[� �]} 
25. Subspace 27. Not a subspace 
41. (n2 -n)/2 43. (a) dim(U X V) =dim U +dim V 
29. Not a subspace 
31. Subspace 
that if {w1, ••• , wJ is a basis for 
33.Subspace 35. Subspace 
(b) Show 
{(w1, w1), ••• , (wn, wn)} is a basis for�. 
37. Not a subspace 
39. Subspace 
41. Subspace 
43. Not a subspace 
x + x2, l} 
45. {l + x, 1 + 
45. Not a subspace 
47·{[� �l[� �l[� -�l[� �]} 
le. 
U to be the x-axis and 
47. Take 
W the y-axis, 
for examp
49. {l, 1 + 
U U W, but [ �] = 
Then [ �] and [ �] are in 
x} 51. {l -x,x -x2} 53. { sin2 x, cos2 x} 59. 
[ �] + [ �] is not. 
(a) p0(x) = ix2 -�x + 3,p1(x) = -x2 + 4x-3, 
51. No 53. Yes; s(x) = (3 + 2t)p(x) + (1 + t)q(x) + tr(x) for 
P2(x) = ix2 -�x + 1 
any scalar 
x2 -4x + 5 
61. 
(c) (i) 
3x2 -16x + 19 (ii) 
(pn _ l)(pn _ p)(pn _ p2) . . .  (pn _ pn-1) 
63. 
t. 55. Yes;h(x) = j(x) + g(x) 
57.No 59.No 61. Yes 
6.3 
1. [x]s = [� l [x]c = [ _:J, Pc+-B = [t _tJ, Ps+-c = 
6.2 1. Linear
ly indepen
dent 
[� -�] 
3. [xJ,� Ul [xJ,� [=:J.Pc�u� H _: n 
0]=4[-1 l]+ 
dent; [-l -1 
depen
3. Linearly 
7  -2 2 
[� �]-2[_� �] 
Ps.--c= [� � �i 1 1 1 
ly indepen
5. Linear
dent 
dent; 3x + 2x2 = 7x - 2(2x - x2) 
depen
7. Linearly 
9. Linearly 
indep
endent 
= sin2 x + cos2 x 
5. [p(x)]8 =[-�l[p(x)]c=[-�lPc+-s= [-� �l 
1 1 .  Linearly 
dependen
t; 1 
t; ln(x2) = -2 ln 2 · 1 + 2 
dependen
13. Linearly 
17. 
(a) Linear
ly indep
endent 
Ps+-c = [� �] 
7. [p(x)]6 � Hl [p(x)], +J.Pc�B � [: 0 n 
(b) Linear
ly depen
dent 
19. Basis 
2 1 .  Not a basis 
23. Not a basis 
25. Not a basis 
Pn�c� H _: �-
[p(x) ]u � [-�] 
29. 

· ln(2x) 

Exercises 

Exercises 

-

ANS29 

17. T(4 - x + 

Odd-Numbered 

Exercises 

Answers 
to Selected 
r(k[:]) = r[�:J = (ka) + (ka + kb)x 
and 
k(a + (a +  b)x) = kT([:]) 
[-7]  [a] (a + 3b) 
Therefore, 
T is line
ar. 
15. T 9 = 5  -14x -8x2, T b = 4 
(a : 7b )x + (a� b )x2 
[-� [ 2] [-1/2] 
(3a - b -c) 
3x2) = 4 + 3x + 5x2, T(a +bx+ cx2) = 
, [j(x) le= 512 , 
1 1 .  [j(x) ls= _5 
a +  ex + 2 x2 
Pc�s=[� -���],Ps�c=[� -�J 
a= T(E11), b = T(E12), c = T(E21), 
19. Hint: Let 
d = T(E22). 
l3. (a) [ (3 -2\13)/2] =  [ 3.232] 
for rzf n. 
standard 
23. Hint: Consider 
the effect 
Don the 
of T and 
(-3V3+2)/2 -1.598 
basis 
(b) [2 + 2V3] =  [5.464] 
25. (S o T) [ �] = [ � -! l (S o T) [;] = [2; 
-y ] 2x + 2y · 
2V3 -2 1.464 
( T 0 S) [;] does 
15. B = {[ =�l [!]} 
not make 
sense. 
27. (S 0  T)(p (x)) = p'(x + 1), (T 0 S)(p(x)) = 
17. -2 - 8(x -1) -5(x -1)2 
(p(x + l))' = p'(x + 1) 
19. -1 +  3(x + 1) - 3(x + 1)2 +  (x + 1)3 
-/4Y]) = 
29. (S 0 T)[;J = s( r[;]) = s([ _;x
6.4 
[4(x -y) + ( - 3x + 4y)] = [x] 
transforma
3. Linear 
tion 
1 .  Linear 
transforma
tion 
transforma
5. Linear 
tion 
7. Not a linear 
transforma
tion 
(s[;]) = r([!:: �]) = 
(To s)[;J = r
transforma
9. Linear 
tion 
[ (4x + 
y) - (3x + y) ] [x] 
1 1 .  Not a linear 
tion 
transforma
13. We have 
S 0 T = I and T 0 S = I, so S and T are 
S (p(x) + q(x)) = S((p + q)(x)) = x((p + q)(x)) 
Therefore, 
= x(p(x) + q(x)) = xp(x)  + x
inverses. 
= S(p(x)) + S(q(x)) 
S(cp (x)) =  S((cp)(x)) = x((cp)(x)) 
and 
=  x(cp(x)) =  cxp (x) = cS(p(x)) 
6.5 
1 .  (a) Only (ii) is in ker(T). 
Therefore, 
. Similar
ly, 
S is linear
(b) Only (iii) is in 
range
(T). 
r([:] + [�]) = r[ a+ c] b + d  
(c) ker(T) = { [� �]}, range(T) = { [� �]} 
3. (a) Only (iii) is in ker
(T). 
(h) All of them 
are in range
(T). 
(c) ker(T) = {a +  bx +  cx
2: a = - c, b = - c} = 
{t + tx -tx2}, range
(T) = 

= r([:J) + r([�]) 

(a +  c) + ((a +  c) + (b + d))x 
d)x) 
(a +  (a  +  b

-3(4x+y)+ 4(3x+ y) - y 

3(x -y) + ( -3x + 4y) y 

)x) + (c +  (c + 

Exercises 

Exercises 

q(x) 

IR2 

Exercises 

3. [T]c<-B = [� � �], [ T]c<-s[ a  +  bx + cx2] 5  = 0 0 1 
[� � �][�] � m � [a +  b (x  +  2)  + 
[ 1 0 OJ 2 
,  [T ]c<-s[ a  + bx +  ex l s= 1 1 1 

y(T) = 1, 

c (x + 2)2) ] c  = [T (a + bx +  cx2) ] c  

basis for 

5. [T ]c<-B = 

and rank(

rank(T) + nullit

y(T) = 4 = 

2 1 .  lsomocphk

23. Not isomorphic 

7. A basis for 

y(T) = 2, and 

y(T) = 3 =dim <!/'2• 

nullit
dimM22• 

ker ( T )  is { l  +  x  - x2}, and a 

15. One-to-o
17. Neither 
19. One-to-o

ne and onto 
one-to-one nor onto 
ne but not onto 

umbered 

T) + nullit
y(T) = 2 
9. rank(T) = nullit
( T) = 2 
1 1 .rank(T) = nullity
y(T) = 2 

ANS30  Answers 
Odd-N
to Selected 
5. A basis for ker(T) is { [ � � l [ � �]},and a basis 
for range( T) is { [ � � l [ � �]}; rank(T) = 
range( T )  is { [�], [�] }; rank(T) = 2, nullit
13. rank(T) = 1, nullit
, r[ � � �] � rn 
3 1 .  Hint: Define T :  Cf;; [O, 
l ]  � Cf;; [O, 
, T(a +  bi) = [:] 
x in [O, 
0  T)(v1) = 
(S 0  T)(v2) .  Then S(T(v1)) = S(T(v2)), 
S 0  T is one-to-
dim V. If T is onto, then range( T) = W, so 
( T )  < 0, which is impossible. Therefore, 
= rank(T) + nullity
1 .  [ Tlc<-B = [ O 1], [ Tlc<-s[4 + 2x] s  = -1 0 [ _ � �] [ �] = [ _ �] =  [ 2  -4x] c =  [ T( 4 +  2x) ] c 

letting 
T(j) be 
whose value at x is ( T(j))(x) = j(x/2) for 

33. (a) Let v1 and v2 be in V and let (S 

so T(v1) = T (v2), since 
v1 = v2, since 
But now 

35. (a) By the Rank Theorem, rank(T) + nullity

S is one-to-one. 
T is one-to-one. Hence, 

rank(T) = dim(range( T)) = dim W. Therefore, 

so nullity 
T cannot 

( T) < dim W + nullit

dim V + nullity

Exercises 

25. Isomorphic

the function 

( T) = dim V 

be onto. 

2]  by 

6.6 

one. 

( T) = 

y(D 

2]. 

o o ol 0 1 0 
1 0 0 
, [ T ]c<-s[A]s = 0 0 1 
-1 [ 0 
1 1 .  [T J c<-B = � [-� -�  � �][�]=[�=�]= 
1 0 0 
-1 c  a -d 
0 -1 0 d b  - c 
[[c - b  d-a]] 

= [AB -BA] c =  [T(A)] c  

a - d  b - c  c 

Odd-Numbered 

Exercises 

A N S 3 1  

1] [_
5] __ 

0

3

(c) [ DJ s [ 3  sin x - 5 cos x J s  = 1 
= 

13. (b) [ D J s= [� -�] [o -
[:] = [ 3 cos x + 5 sin x J s 
15. (a) [ D Js= [� � �i 0 - 1  2 
17. [ S  0  TJv+-s = [-� =�J 

[ D (3sin x- 5cosx)J s  

19. Invertible
2 1 .  Invertible
23. Invertible

, r -1(a + bx) = - b  + ax 
, r-1(p(x)) = p(x -2) 
, r-1(a + bx + cx2) = (a  - b + 2c) + 

(b -2c)x + cx2 or Y-1(p(x)) = p(x) - p'(x) + p"(x) 

37

27. - 3  sin x -cos x +  C 

25. Not invertible 
29. te2x cos x -te2x sin x +  C 

[ T Jt. = 2d1d2/(dl + di) 
· 

Exercises 
1 .  y(t) = 2e31/e3 

3 1 .C = {[-�], [-�n 33. C = { 1  - x, 2 + x} 
35.C={ l, x} [(d� - di)j(d� + di) 
6. 7 
3. y (t) = ((1 _ e4)e3t + (e3 _  l)e4r)/(e3 _ e4) 
( (Vs-1)/2) ev's - 1 
[ e(1 +v'slr/2 _  e(1-v's)1/2 J 
1 1 .y(t) =  e1cos(2t) 
13. (a) p (t) = 100eln(16)1/3 =  100e0924t 
15. (a) m(t) = 50e-c1, where c = ln2/1590 =  4.36 x 10-4; 
VK  sin(VKt) + 10 cos(VKt) 

5. j(t) = 
7. y(t) = e1 -( 1  - e-1)te1 
9. y(t) = ((k + l)ekt + (k -l)e-k1)/2k 

mg remain 
(b) After 3691.9 years 

(b) 45 minutes (c) In 9.968 

after 1000 years. 

5  -10 cos(l O VK) 

sin ( l O  K) 

32.33 

17. x(t) = 

hours 

e 

19. (b) No 

Answers 
to Selected 
7. Let c1A + c2B = 0. Then c1A - c2B = c1A r + c2Br = 
(c1A + c2Bf = 0. Adding, we have 2c1A  = 0, so 
. Hence c2B = 0, and so 

3. Subspace 5. Subspace 

c1 = 0 because A is nonzero
c2 = 0. Thus, {A, B} is line

arly independent. 

15. n2 - 1 

zero transformation

. 

transformation 

9. { 1 ,  x2, x4}, dim W = 3 
1 1 .  Linear 
13. Linear 

transformation [1  0 - 1] 0  1 - 2  

17. 0  0 1 
1  0 - 1  

19. S 0  T is the 
Chapter 
1 
7.1 
Exercises 
1 .  (a) - 10  (b) Vi4 
3. Any nonzero 
5. (a) 1 
7. x2 is one possibility 
9. (a) 1T 
13. Axiom ( 4) fails: 

scalar multiple 

(c) \/93 

(c) Vi4 

of [ �] 
(h) v'i3 
(b) V7i 
(c) V7i 
u = [ �] i= 0, but (u, u) = 0. 
u = [ �] i= 0, but (u, u) = 0. 

15. Axiom ( 4) fails: 

17. Axiom (4) fails: 

p(x) = 1  -x is not the zero poly­

nomial, but (p(x), p(x)) = 0. 

19.A  = [� :J 

2 1 .   y 

___,f---+----+�-+--+-- x 
- 2  

2 

27. v'6 

25. - 8  
29. llu + v -wll2 = (u + v -w, u + v -w) 
= (u, u) + (v, v) + (w, w) 
+ 2(u, v) -2(u, w) -2(v, w) 
= 1 + 3 + 4 + 2 -10 - 0 = 0 

Review 

Questions 

1 .  (a) F (c) T (e) F (g) F (i) T 

2d1d2/(d� + di) ] 

(di - dl}j(dl + di) 

ANS32  Answers 
Odd-N

to Selected 

umbered 

Exercises 

Therefore, llu + v -wll = 0, so, by axiom (4), 
u + v - w = 0 or u + v = w. 

31. (u + v, u -v)  = (u, u) - (u, v) + (v, u) - (v, v) = 

llull2 - (u, v) + (u, v) -llvll2 = llull2 -llvll2 
similar 

33. Using Exercise 32 and a 

for llu -vll2, 
identity 

we have 

llu + vll2 + llu -vll2 = (u + v, u + v) + (u -v, u -v) 

+ llull2 -2(u, v) + llvll2 

= llull2 + 2(u, v) + llvll2 
= 2llull2 + 2 llvll2 
the identity 
35. llu + vii = llu -vii 9 llu + vll2 = llu -vll2 
9 llull2 + 2(u, v) + llvll2 

by 2 yields 

Dividing 

we want. 

= llull2 -2(u, v) + llvll2 

9 2(u, v) = - 2(u, v) 9(u, v)  = 0 

37. { [�l [�]} 39. { 1 ,  x, x2} 

41. (a) 1 /\/2 , V3 x/Vl ,Vs (3x2 - 1 )/2\/2 

(b) \/7 (5x3 -3x)/2 Vl  

7.2 

Exercises 

1. llullE = v'42, llull, = 1 0, llullm = 5 
3. dE(u, v) = v70, d,(u, v) = 1 4, dm(u, v) = 6 

5. llullH = 4, llvllH = 5 
7. (a) 
9. Suppose llvllm = I vk I. Then llvllE = 

At most one component of vis nonzero. 

Yv� + . . .  + v[ + . . .  + v� 2: � =lvkl = llvllm· 
+ lvnl :S lvkl + · · · 
11. Suppose llvllm = I vk I. Then I vi I :s I vk I for 

llvlls = lv1I + · · · 

i = 1, . . .  , n, so 

+ lvkl 

=  nlvkl =  nllvllm 

13. 

y 

y � 1 

- 1  

-] 

1 x 

21. llAllF = \fi9, llAll1 = 4, llAlloo = 6 
23. llA llF = v'31, llA ll1 = 6, llA lloo = 6 

25. llAllF = 2VU, llAll1 = 7, llAll,, = 7 

27.F [�].y � [-:i 29. F m.y � rn 
31.F [Hy� [ =:J 
llxH  llxH 

33. (a) By the definition 

of an operator norm, 11111 = 

maxlllxll = maxllxll = 1 . 

35. cond1(A) = condoo(A) = 2 1 ;  well-conditioned 
37. cond1(A) = condoo(A) = 400; ill-conditioned 
39. cond1(A) = 77, cond00(A) = 1 28; moderately 

ill-conditioned 

41. (a) condoo(A) =(max {  I kl + 1 ,  2}) · 

(max{l�I + l�I, 161}) 

43. (a) cond00(A) = 40 

(b) At most 400% relative 

change 

45. Using Exercise 33(a), we have cond(A) = llAllllA-1112 

llAA-111 = 11111 = 1. 

49.k 2: 6 

51. k 2: 1 0  

7.3 

Exercises 

3. llell = V6 /2 =  1.225 

1. llell = V2 =  1.414 
5. llell = \/7 =  2.646 
7. y = -3 + �x, llell =  1 .225 
9. y = ¥--2x, llell =  0.8 16 
11. y = to+ fsx, llell =  0.447 
13. y = -t + �x, llell =  0.632 
17. y = s -1i5X -zX 
18 17 I 2 
5 
[ I ] [ 4] 
- - 3 
-
15. y = 3  -¥-x + x2 
21. x --� 
19. x = ft 
- 5  - t  II 
[ 4 + t l [�] 
Tl 
- 5  -2t  42 
25. H-

23.x = 

t 

Answers 

to Selected 

Odd-Numbered 

Exercises 

ANS33 

1.04 

79.9 years 

tot = 0, then 

y = 56.6 + 2.9t; 

53. (a) If A is invertible

ellipse Yi + Yz :::::  1 

31. (a) Ifwe let the year 1920 correspond 
33. (a) p(t) = 150eQ.131t 

33. The line segment [ - 1, l ]  
2  2 
35. The solid 
5  4 
37. (a) llAll2 = Vl 
39. (a) llAllz = 1 .95 

23. (Emd" 7) A � 3 [:Ji 0 I ] + 2 [ _�Ji 1 O ]  
27.x = [ _l] 29. y = 0.92 + 0.73x 
l [2] 
35. 139 days [! !J [z_J [t t t
(b) cond2(A) = oo 
, � 
37. t t , i  39. i i i 
�] 
(h) cond2(A) = 38. 1 1  
41. A+ = [i �J 43. A+ � [! 
41. u 1 -n ui 45. A+� ii Ii 
45. A+ = [ � 25 � l x = [0.52J 
25 
-3 I 6 
6 ! 6 
47. A+ = [i 2 -i] 49. A+ = [� -�J 
47.A+ = u ! i],x = [�J 
51.A+ � u 0 I Jl -3 
[V2 61. 
0 OJ [ l/Vl 
l/ VlJ l/Vl 
- 1  ! 3 ! 3 
- 1  J [ 0 
�J 
63. [ 2 
1. g(x) = t 5. g(x) = f6 + fix2 
7.5 
13. g(x) = fa -�x + �x2 
(210e -570)x2 =  1 .01 + 0.85x + 0.84x2 
cos 3x) 
-i(cos x + 
9 _ I 
23. a0 -2, ak =  0, bk = ---­
1/VS: -2�Vs 
25. a0 = 1T, ak =  0, bk =  --
1. (a) T  (c) F 
(e) T 

1. 2, 3 3. V2,o 5. 5 
I I. A=[� OJ [Vl  OJ [l/Vl  l/VlJ 
13.A = [� 
-u[�J[ l ]  o '][3 o][ ] 
15. A = [i 
°  1 
17.A � [: 
0  1 0 2 �r:s l/Vs 
19.A = [l OJ[Vs 0 
21. A= Vl[�J [ l/Vl l/Vl] + o[�J 

0  0  0  2 
- 1  0  0 0 1  0 
0 
1 
0 

(ATA)-IAT = A-l(AT)-IAT = A-1. 
7.4 

1J[3 OJ[- 1  OJ 

9. g(x) = x -t 
11. g(x) = (4e -10) + (18 -6e)x 

=  0.87 + l .69x 

(g) T  (i)  T 

15. g(x) = 39e -105 + (588 -216e)x + 

3. g(x) = �x 
7. {1, x -H 

1  0  0 l/Vl - 1/Vl 

, so is Ar, and we 

0  0  2  0 

21. 1T 
2 1T 

k1T 
2( - l)k 

Exercises 

Exercises 

9. Vs , 2, 0 

Questions 

1  -(- l)k 

Review 

7. 2, 3 

have A+ = 

0 - 1/Vl 

3 - 1  

- 1  

3. Inner product 

5. V3 

-1 

k 

[ - 1/Vl 1/Vl] (Exercise 3) 

umbered 

Exercises 

9. Not a norm 

13. y = l .7x 

ANS34  Answers 
Odd-N

to Selected 
1 1 .  cond00(A) =  2432 
15. [!] 
(c) A+ = [! 0 -!J 0 

17. (a) \/2 , V2 ['M 1/\/2 m� �][: 

(b) A =  0  0 
1/\/2 - 1/\/2 

�] 

19. The singular 

of PAQ are the square roots 

values 
of(PAQ)r(PAQ) = QrArPrPAQ = 
eigenvalues 
Qr(A r A)Q. But Qr(A r A)Q is similar 
Qr = Q-1, and hence it has the same eigenvalues 
as 
A rA. Thus, PAQ and A have the same singular 

to Ar A because 

values. 

of the 

Index 

A Abel,  Niels 

Henrik, 3 1 1 ,  DS 

Absolute  value,  C3 
Addition 

closure under, 1 92, 429 
of complex numbers, Cl 
of matrices, 
140 
of polynomials, 
D2 
of vectors, 

5-6, 9, 42 9 

matrix, 242, 244 

Adjacency 
Adjoint (adjugate) 
Al-Khwarizmi, 
Musa,  85 

of a  matrix, 

276 

Abu Ja'far Muhammad ibn 

of vectors, 

1 0  

3 3  

24-26 

99-1 0 1  

multiplic
ity, 294 
properties 

Algebraic 
Algebraic 
8 5  
Algorithm, 
Allocation 
o f  resources, 
Altitude 
o f  a triangle, 
Angle between vectors, 
Argand, Jean-Robert, Cl 
Argand plane, Cl 
Argument of a complex number, C4 
Arithmetic 
Arithmetic 
Associativity, 
Attractor, 
Augmented matrix, 61, 64 
Axioms 

mean, 548 
Mean-Geometric 
10, 1 54, 158, 223, 429 

B inner product space, 5 3 1  

vector space, 429 

Back substitution, 
Balanced chemical 
Basis, 1 98, 446-448 

6 1  
equation, 

350 

1 98, 447 

372, 537 

507-509 

change of, 463-470, 
with respect 
coordinates 
orthogonal, 
370, 537 
orthonormal, 
standard, 
Basis step, Bl 
Basis Theorem, 202, 453 
Best Approximation 
Best approximation, 
Binary vector, 
1 4  
3 3 8  
Binet, Jacques, 
Binet's 
Bipartite 

formula, 3 3 9 ,  428 
graph, 2 5 1 ,  254 

Theorem, 570 
to a vector, 

1 0 1 -102 

570-57 1  

to, 208, 448-452 

Block, 145 
Block multiplication, 
Block triangular 
Bunyakovsky, Viktor Yakovlevitch, 

form, 283 

539 

148 

c '€, 435 
C2, 432 
C", 543 

Inequality, 22, 539-540 

273, 280 

Lewis, 1 4 1 ,  284 
Giovanni Domenico, 
362 
identity, 362 

Carroll, 
Cassini, 
Cassini's 
Cauchy, Augustin-Louis, 
Cauchy-Schwarz 
Cayley, Arthur, 300 
Cayley-Hamilton 
Centroid 
Change of basis, 463-470 
Characteristic 
Characteristic 
Circuit, 
242 
Circumcenter 
Closure 

equation, 
292 
polynomial, 

of a triangle, 

of a triangle, 

Theorem, 300 

292 

33 

32 

1 92, 429 

under addition, 
under linear combinations, 
under scalar multiplication, 

1 92 

Codomain, 2 1 2  
Coefficient(s) 
Fourier, 
6 1 5  
o f  a linear combination, 
1 2 ,  1 54 
of a linear equation, 
matrix, 64 
method of undetermined, 
of a polynomial, 

58 

DI 

D7 

Cofactor, 266 
Cofactor expansion, 
Column matrix, 138 
Column-row representation 

266-269 

of a matrix 

3, 138 
10, 19, 1 54, 429 

product, 
147 
Column space, 195 
Column vector, 
Commutativity, 
Companion matrix, 299 
Complete bipartite 
Complex dot product, 
543 
Complex numbers, CI-CU 

graph, 254 

value of, C3 
absolute 
of, C l  
addition 
argument of, C4 
conjugate 
of, C2 

Mean Inequality, 

548 

1 92, 429 

of, CI-C2, CS 

part of, Cl 

division 
of, C2, CS 
equality 
of, Cl 
imaginary 
modulus of, C3 
multiplication 
negative 
of, C2 
polar form of, C3-C6 
powers of, C6-C7 
principal 

real part of, C 1 

argument of, C4 

roots of, C7-CS 
Complex plane, Cl 
Complex vector space, 429, 543 
Component of a vector, 

3 

orthogonal 

to a subspace, 

Composition of linear 

transformati

382, 538 
ons, 2 1 9, 

476-478 

of complex numbers, C2-C3 
transpose of a matrix, 544-545 

4 15-4 1 6  

number, 562, 602 

Condensation method, 284-285 
Condition 
Conic sections, 
Conjugate 
Conjugate 
Connected graph, 361 
Conservation 
Consistent 
linear system, 60 
Constant polynomial, 
Constrained optimization, 

of flow, 102 

DI 

4 13-4 1 5 ,  

methods, 125, 3 16, 

208, 448-452 

547-551 

Consumption 
Contradiction, 
Contrapositive, 
Convergence of 

matrix, 236 
proof by, AS 
proof by, AS 
iterative 

563-566 

207-209 

Coordinate grid, 13 
Coordinate vector, 
Coordinates, 
Cotes, Roger, 569 
Cramer, Gabriel, 
274 
Cramer's 
Cross product, 
Crystallographic 
Curve fitting, 

Rule, 274-275 

290-291 

D 'd!, 435 

De Moivre, Abraham, C6 
De Moivre's 
Degenerate 
Degree of a polynomial, 
Demand vector, 
236 
Descartes, Rene, 3, D9 

Theorem, C6-C9 
conic, 415, 424 
D I  

48-49, 286-287 
5 1 7  

restriction, 

11 

of a matrix, 139 

3 1 8- 3 1 9  

12  Index 

Descartes' Rule of Signs, D9-D1 0  
Determinant(s), 1 6 5 ,  263-265 

applications 

of, 266-269 
matrices, 271-272 

cofactor expansion 
of elementary 
geometric 
history 
and matrix operations, 272-274 

of n X n matrices, 

of, 280-2 8 1  

265-269 

of, 286-291 

of, 269-2 7 4 

properties 
Vandermonde, 
291 
entries 
matrix, 139 

Diagonal 
Diagonal 
Diagonalizable 
Diagonalizable 

orthogonally, 400 
unitarily, 546-54 7 

linear transformation, 509 
matrix, 303 

Diagonalization, 
303-309 

orthogonal, 

400-407 
Diagonalization 
Diagonalizing 
Diagonally 
Difference 

Theorem, 307 
form, 4 1 1  

a quadratic 

dominant matrix, 128, 324 

of complex numbers, C2 
of matrices, 
140 
of polynomials, 
D2 
of vectors, 
8, 433 
Differential 

equation(s), 363, 436, 5 1 8  

for, 523 
436,5 18-525 

boundary conditions 
homogeneous,
initial 
344,363 

conditions 

for, 340, 343, 

of, 5 1 8  

solution 
system oflinear, 

340-348 

473 

607-608 

operator, 

image compression, 

Differential 
Digital 
Digraph, 
243 
Dimension, 203, 452-456 
Direct proof, A7 
Direction 
Disjoint 
sets, A4 
Distance 

vector, 

35, 39 

Hamming, 554 
from a point to a line, 4 1-43 
from a point to a plane, 43-44 
taxicab, 
between vectors, 
functions, 
vity, 10, 19, 1 54, 158, 429 

23-24, 535 
554-555 

529-531 

127 

algorithm, 

Distance 
Distributi
Divergence, 
Division 
Dodgson, Charles Lutwidge, 
Domain, 2 1 2  
Dominant eigenvalue, 
Dominant eigenvector, 
Dot product, 

3 1 1  
3 1 1  

1 8-20, 49 

D4 

281, 284 

complex, 543 
weighted, 
532 
Dual space, 5 1 4  
Dynamical system, 2 5 3 ,  348-355 

E Echelon form of a matrix 

trajectory 
of, 349 

reduced row, 73 
row, 65 

Edge of a graph, 242 
Eigenspace, 
256 
(s), 254 
Eigenvalue
algebraic 
dominant, 
3 1 1  
of, 294 
geometric 
multiplicity 
power method 
inverse 
for computing, 

multiplicity 
of, 294 

3 1 7- 3 1 8  

power method fo r  computing, 
shifted inverse power method fo r  computing, 

3 11 - 3 1 6  

shifted power method fo r  computing, 

3 1 6-31 7  

Eigenvector(s), 
254 
dominant, 
3 1 1  
402 
orthogonal, 

network, 1 04-107 

Electrical 
Elementary 
Elementary 
Elementary 
Elements of a matrix, 138 
Elimination 

matrix, 1 70 
reflector, 
row operations, 

397 

66 

6 1 7  
Fourier series, 
Free variable, 
7 1  
Frobenius, 
Frobenius 
Fundamental 
Fundamental 
Fundamental 

Georg, 204 
norm, 556 

subspaces of a matrix, 380 
Theorem of Algebra, 
Theorem of Invertible 

DS 

G  Matrices, 1 72, 206, 296, 

605-606 

5 1 2 ,  

69, 1 25, 538, 

569, DS 

elimination, 
68-72 

526 
Galileo, 
Evariste, 
3 1 1 ,  DS 

Galilei, 
Galois, 
Gauss, Carl Friedrich, 
Gauss-Jordan elimination, 
72-76 
Gauss-Jordan inverse method, 1 75-1 78 
Gauss-Seidel method, 1 24- 1 3 1  
Gaussian 
General form of the equation 
General form of the equation 
Geometric 
Geometric 
Gerschgorin disk, 
Gerschgorin 
Gerschgorin, 
Gerschgorin's theorem, 3 1 9-322 
Gibbs, Josiah Willard, 
Global Positioning 
Google, 358 
390 
Gram, Jiirgen Pedersen, 
Gram-Schmidt Process, 
388-392 
Graph,242,253-254 

mean, 548 
multiplic
ity, 294 

Disk Theorem, 3 2 1  
Semyon Aranovich, 
3 1 9  

3 1 9  

49 

System (GPS), 1 2 1 - 123 

of a line, 34, 36, 41 
of a plane, 38, 41 

Gauss-Jordan, 72-76 
Gaussian, 
Empty set, A2 
Equality 

68-72 

of complex numbers, C l  
o f  matrices, 
139 
of polynomials, 
of sets, Al-A2 
of vectors, 

D2 

4 
Equation(s) 
linear, 
normal, 575 
system oflinear, 

58 

C9 

302 

formula, C9-Cl l  

59 
Equilibrium, 
50, 107 
relation, 
Equivalence 
Error vector, 
565, 572 
Euclidean 
norm, 553 
Euler, Leonhard, 
Euler's 
Even function, 
6 1 7  
Exchange matrix, 235 
Expansion 
Exponential 

F S', 431 
LU, 1 80-1 86 
modified QR, 396-398 
QR, 392-394 

Factor Theorem, D4 
Factorization 

by cofactors, 
of a matrix, 346 

solution, 

Feasible 
336 
Fibonacci, 
Fibonacci 
numbers, 335, 

236 

338-339,427 

266-269 

vector space, 453 

Field, 429 
Finite-dimensional 
Finite linear games, 109- 1 1 3  
Floating-point form, 83 
Force vectors, 
50-53 
Fourier approximation, 
Fourier coefficients, 6 1 5  
Fourier, 

Jean-Baptiste Joseph, 
6 1 6  

6 1 5  

361 

254 

adjacency 
matrix of, 242, 244 
2 5 1  
bipartite, 
complete, 
253 
complete bipartite, 
connected, 
cycle, 254 
directed 
edges of, 242 
k-regular, 
361 
path in a, 242 
Petersen, 254 
vertices of, 242 

(digraph), 243 

Grassmann, 
Grassmann's Identity, 458, 496 

Hermann, 429 

H Half-life, 

520 
William Rowan, 2, 300 

554 

Hamilton, 
Hamming distance, 
Hamming norm, 554 
Harmonic mean, 5 5 1  
Head o f  a vector, 
Head-to-tail rule,  6 
Hermitian 
Hilbert, 
Hoene-Wronski, 
Homogeneous 

David, 403 

matrix, 545 

3 

5 18-525 

linear system, 76 

Homogeneous 
Hooke's Law, 524 
Householder, 
Householder 
Hyperplane, 

Alston Scott, 397 
matrix, 397 
40 

Josef Maria, 457 

linear differential 

equations, 

Index 1 3  

I i,C l  

matrix, 1 7 9  

matrix, 139 
transformation, 
221 ,  474 
linear system, 84 
matrix, S61 

Idempotent 
Identity 
Identity 
Ill-conditioned 
Ill-conditioned 
Image, 212 
Imaginary 
Imaginary 
Imaginary 
Inconsistent 
Indefinite 

matrix, 4 1 3  

axis, C I  
conic, 424 
part of a complex number, C I  

linear system, 60 

B l  

A S  

proof, A 7  

quadratic 

form of, 4 1 3  
Index o f  summation, 
Indirect 
Induction 
Induction 
Infinite-dimensional 
3 
Initial 
point of a vector, 
Inner product, 
Inner product 

hypothesis, 
step, B l  

S3 l 
space, S 3 1-S34 
and Cauchy-Schwarz 

Inequalities, 

S39-S40 

vector space, 4S3 

and Triangle 

in, S3S 

between vectors 

distance 
length of vectors in, S3S 
orthogonal 
vectors in, S3S 
of, S3S 
properties 

modulo m, 14-1 6  
of a matrix, 284 

Integers 
Interior 
Intersection 
Inverse 

of sets, A4 

Gauss-Jordan method of computing, 
of a linear transformation, 
of a matrix, 163 

221-222, 478-479 

l 7S-l 78 

Inverse power method, 3 1 7- 3 1 8  

linear transforma

tion, 221-222, 

shifted, 3 1 8- 3 1 9  

Invertible 
478-479 
Invertible 
Irreducible 
Irreducible 
Isometry, 3 7 S 
Isomorphism, 
Iterative 

method( s) 

matrix, 163- 1 70 

matrix, 33S 
polynomial, 

D7 

493-49S 

of,  12S, 

3 16, S63-S66 

convergence 
Gauss-Seidel method, 124- 1 3 1  
inverse power method, 3 1 7- 3 1 8  
Jacobi's 
method, 124- 1 3 1  
power method, 3 11 - 3 1 6  
shifted inverse power method, 3 1 8 -3 1 9  
shifted power method, 3 1 6-31 7  

Jacobi, 
Jacobi's 
Jordan, 

Carl Gustav, 1 24 
method, 124- 1 3 1  
Wilhelm, 72 

K Kernel, 482 
L Lagrange interpolation 

Kirchhoff's 

Laws, 1 04 

Lagrange, 

Joseph-Louis, 

formula, 4S9 
4S8 

Theorem, 266, 277-280 

4S8 

Lagrange polynomials, 
Laplace Expansion 
Laplace, 
Pierre Simon, 267 
Lattice, 
S l 6  
Leading entry, 6 S  
Leading 1 ,  7 3  
Leading variable, 
line, S74 
Least squares approximating 
Least squares approximation, 
S68-S69, 

via the QR factorization, 

Best Approximation 
and orthogonal 
projection, 
and the pseudoinverse 

S83-S8S 

S82-S83 

value decomposition, 

via the singular 

7 1  

S71-S82 
Theorem and, S70-S71 

of a matrix, S8S-S86 

603-60S 

of minimal length, 

Least squares error, S72 
Least squares solution, 
S 7 4 
603-604 
Least Squares Theorem, S7S 
Left singular 
Legendre, 
Legendre polynomials, 
Leibniz, 
Lemma, 271 
Length 

vectors, 
S93 

Adrien Marie, S38 

Gottfried Wilhelm von, 2 8 1  

S38 

of a binary vector, 
1 4  
o f  a n  m-ary vector, 
1 6  
o f  a path, 242 
of a vector, 

closed model, 108, 23S 
open model, 108, 236 

20, S3S 
Leonardo of Pisa, 336 
Leontief 
Leontief 
Leontief, Wassily, 
107 
Leslie matrix, 240 
Leslie model, 239-241 ,  330-332 
Line, 34-38 

of best fit, S74 
equation(s) of, 34, 36, 4 1  
least squares approximating, 

S74 

12, 1 S4, 433 
92-93, 1 S7,443,446 

Linear combination, 
Linear dependence,
Linear economic  models, 

Linear equation(s), S8, S9. See also Linear 

107-109, 23S-236 

system(s) 

Linear independence, 
92-97, 1 S7, 443-446 
Linear recurrence relations, 
Linear system(s), S8-62 

33S-336 

60 

60 

64-79 

augmented matrix of, 61, 64 
coefficient matrix of, 64 
consistent, 
direct method for solving, 
equivalent, 
homogeneous, 76 
ill-conditioned, 
inconsistent, 
60 
iterative 
solution 

over !RP, 77-79 

methods for solving, 
(set) of, S 9  

84 

of, 2 1 9, 476-478 

onto, 488 
composition 
diagonalizable, 
S09 
identity, 221, 474 
inverse of, 221 -222, 478-479 
invertible, 
221-222, 478-479 

124- 1 3 1  

Linear transformation

(s), 2 1 3-214, 472-474 

dependent matrices, 
dependent 

93, 443, 446 

of, 484 

of, 47S-476 

1S7 
vectors, 

kernel of, 482 
matrix of, 2 16, 497-S03 
nullity 
one-to-one, 488 
properties 
zero, 474 
Linearly 
Linearly 
Linearly independent 
matrices, 
1S7 
independent 
vectors, 
Linearly 
matrix, 329 
Long range transition 

LU factorization, 
M m-ary vector, 1 6  
Mmn• 430 

1 80-1 86 
Lucas, Edouard, 336, 428 

Maclaurin, 
Magic square, 

Colin, 274, 280 
460-462 

93, 443, 446 

classical, 
weight of a, 460 

460 

Mantissa, 83 
Markov, Andrei Andreyevich, 
32S-330 
Markov chain, 230-23S, 
induction, 
B l -B7 
Mathematical 

230 

first principle 
of, Bl 
second principle 
of, BS 

Matrix (matrices), 61,  138 

form, 409 

of, 292 

of, 292 

number of, S62 
transpose of, S44-S4S 

of, 16S, 264, 26S-269 

), 276 

of, 140 

equation 
polynomial 

addition 
adjacency, 242, 244 
adjoint (adjugate
associated with a quadratic 
augmented, 
6 1 ,  64 
change-of-basis, 
46S 
characteristic 
characteristic 
coefficient, 64 
column space of, 19S 
companion, 
299 
condition 
conjugate 
consumption, 
236 
determinant 
diagonal, 
139 
diagonalizable, 
303-309 
difference 
of, 140 
eigenspace 
of, 2S6 
of, 2S4 
eigenvalue 
of, 2S4 
eigenvector 
elementary, 
1 70 
elements 
of, 138 
entries 
equality 
of, 139 
exchange, 
23S 
of, 346 
exponential 
factorization 
of, 180 
fundamental subspaces of, 380 
Hermitian, 
S4S 
idempotent, 
179 
identity, 139 
ill-conditioned, 
indefinite, 
413 
interior, 
inverse of, 163 
invertible, 
163-1 70 

of, 138 

284 

S61 

14  Index 

Matrix (Continued) 

400 

282 

335 

187 

335 

4 1 3  

4 1 3  

of, 204 

602-603 

of, 1 4 1 -143 

of, 585-586, 

237-238 
2 1 8-21 9, 366, 586 

definite, 
4 1 3  
of, 140 
semidefinite, 

irreducible, 
Leslie, 
240 
of a linear transformation, 
multiplication 
negative 
negative 
negative 
nilpotent, 
norm of, 555-561 
normal, 547 
null space of, 197 
nullity 
373-376 
orthogonal, 
diagonalizable, 
orthogonally 
partitioned, 
145-149 
permutation, 
positive, 325 
positive definite, 
4 1 3  
positive semidefinite, 
powers of, 149-150 
primitive, 
productive, 
projection, 
pseudoinverse 
rank of, 72, 204 
reduced row echelon form of, 73 
reducible, 
regular, 
325 
row echelon form of, 65 
row equivalent, 
row space 
of, 1 9  5 
scalar, 
139 
scalar multiple 
similar, 
singular 
singular 
size of, 138 
skew-symmetric, 
square, 139 
standard, 
216 
stochastic, 
232 
strictly 
sum of, 140 
symmetric, 
trace of, 162 
transition, 
transpose of, 1 5 1 ,  1 59-160 
unit lower triangular, 
1 8 1  
unitarily 
unitary, 
upper triangular, 
zero, 1 4 1  

301-303 
values of, 590-591 
vectors of, 593 

diagonalizable, 
546-547 
545-546 

1 5 1 - 1 52, 1 60-1 6 1  

diagonally 

dominant, 
128 

of, 140 

162 

231 

334 

162 

68 

2 16, 497-503 

2 1 1 -2 16, 472 

Matrix transformation, 
projection, 
2 1 8-21 9, 509-51 0  
reflection, 
2 1 5  
rotation, 

2 1 6 -2 1 8  

Max norm, 553 
Mean 

arithmetic, 548 
548 
geometric, 
harmonic, 
5 5 1  
quadratic, 
550 

32 

Median of a triangle, 
Metric, 555 
Metric space, 555 
Minimum length least squares solution, 
Minor, 264 

Modified QR factorization, 

396-398 

603-604 

1 3 - 1 6  

Modular arithmetic, 
Modulus o f  a complex number, C 3  
Moore, Eliakim Hastings, 
602 
Moore-Penrose inverse, 
602 
Muir, Thomas, 2 8 1  
Multiplication 

of complex numbers, C I - C2, CS 
of matrices, 1 4 1 -143 
of polynomials, 
D2-D3 
scalar, 

7-8, 140, 429 

Multiplicity 
algebraic, 
geometric, 

of an eigenvalue 
294 
294 

N Negative 

of a  complex  number,  C2 
of a  matrix, 
140 
of a vector, 
8, 429 

Negative definite matrix, 4 1 3  

quadratic 

form of, 4 1 3  

Negative semidefinite 

matrix, 4 1 3  

quadratic 

form of, 4 1 3  
Net reproduction 
rate, 360 
Network, 102 
Network analysis, 
Newton's Second Law of Motion, 524 
Nilpotent 
Node,  102 
Nondegenerate 
conic, 4 1 5-41 6  
Norm o f  a matrix, 555-561 

102-103 

matrix, 282 

1-, 559 
2-, 559 
7-, 559 
556 
compatible, 
Frobenius, 556 
operator, 
559 

Norm of a vector, 

20, 535, 552 

representation 
product, 
146 

1 80. See also Singular 

of a matrix 

value 

and diagonalization, 
303-309 

Matrix-column 

Matrix factorization, 
decomposition 
(SVD) 

LU, 1 80-1 86 
modified QR, 396-398 
P1 LU, 1 86-187 
QR, 392-394 

and Schur's Triangularization 

Theorem, 408 

1-, 553 
2-, 553 
7-, 553 
Euclidean, 
553 
Hamming, 554 
max, 553 
sum,552 
taxicab, 
530 
uniform, 553 

of a line, 34, 36, 4 1  
of a plane, 38, 4 1  

21 

575 

Normal equations, 
Normal form of the equation 
Normal form o f  the equation 
Normal matrix, 547 
Normal vector, 
34, 38 
Normalizing 
a vector, 
Normed linear space, 552 
Null space, 197 
Nullity 

0 of a linear transformation, 

of a matrix, 204 

484 

inequalities 

and, 547-551 

Odd function, 
6 1 7  
Ohm's Law, 1 04 
One-to-one, 488 
Onto, 488 
Operator norm, 559 
Optimization 

4 13-4 1 5  

8 

constrained, 
geometric 
Orbital center, 
355 
Ordered n-tuple, 9 
Ordered pair,  3 
Ordered triple, 
Orthocenter 
Orthogonal 
Orthogonal 
Orthogonal 
Orthogonal 
Orthogonal 
Orthogonal 

378-382 

of a triangle, 
33 
basis, 370, 537 
complement, 
Decomposition Theorem, 384-385 
diagonalization, 
400-407 
matrix, 373-376 
projection, 

382-387, 
538 

583-585 

least squares approximation, 
369-373, 
537 

set of vectors, 
vectors, 

26, 535 
basis, 372, 537 
set of vectors, 

Orthogonal 
Orthogonal 
Orthonormal 
Orthonormal 
147 
Outer product, 
expansion, 147 
Outer product 
form of the SVD, 
Outer product 
596 

372, 537 

p 9/',431 
9J>n, 431 

8 

rule,  6 

Parallel vectors, 
Parallelogram 
Parameter, 
Parametric 
of a line, 36, 4 1  
o f  a plane, 3 9 ,  4 1  

36 
equation 

fractions, 
pivoting, 

Partial 
Partial 
Partitioned 

1 19 
84-85 

matrix, 145-149 

Path(s) k-, 243 

length of, 242 
number of, 242-245 
simple, 
242 

Peano, Giuseppe, 
429 
Penrose conditions, 
586 
Penrose, 
Permutation 
Perpendicular 

matrix, 187 
33 

Roger, 603 

bisector, 

Population 
Population 
Positive 

distribution 
growth, 239-241 ,  330-332 

vector, 
239 

definite matrix, 4 1 3  

of a linear transformation, 
of a matrix, 72, 204 
singular 

value decomposition, 

484 

600 

Theorem, 332-335 

graph, 254 

Perron eigenvector, 
335 
Perron-Frobenius 
Perron, Oskar, 332 
Perron root, 335 
Perron's Theorem, 333 
Petersen 
Pivot, 66 
Pivoting, 
partial, 
Plane, 38-41 
Argand, Cl 
complex, Cl 
equation 

66 
84-85 

of, 38, 39, 41 

6 1 0  

Polar decomposition, 
Polar form o f  a complex number, C3-C6 
P6lya, George, A 7 
Polynomial, 

Dl-DlO 

characteristic, 
constant, 

292 

D 1 
degree of, D 1 

irreducible, 
D7 
Lagrange, 
458 
Legendre, 
538 
Taylor, 472 
trigonometric, 
zero of, D4 

614 

quadratic 

form of, 4 1 3  

Positive 
Positive 

matrix, 325 
semidefinite 

matrix, 4 1 3  

quadratic 

form of, 4 1 3  
Power method, 3 11 - 3 1 6  

3 1 8- 3 1 9  

model, 343 

3 1 7- 3 1 8  

inverse, 
shifted, 3 1 6-31 7  
shifted inverse, 
Predator-prey 
Price vector(s), 235 
Primitive 
Principal 
Principal 
Probability 
Product 

vector, 
2 3 1  

of complex numbers, C l -C2 
of matrices, 
D2-D3 
of polynomials, 
vector, 
236 

141-143 

Production 
Projection 

382-387, 
538 

orthogonal, 
into a subspace, 382 
onto a vector, 

27-28 

matrix, 335 
argument of a complex number, C4 
Axes Theorem, 4 11 

Projection 
Projection 
Proof 

form of the Spectral 
matrix, 2 1 8-21 9, 366, 586 

Theorem, 405 

AS 
by contradiction, 
by contrapositive, AS 
direct, 
indirect, 
A7 
by mathematical 

A7 

induction, 
B l -B7 

Pseudoinverse 

of a matrix, 585-586, 

602-603 
Pythagoras' 

Theorem, 26, 537 

Q QR algorithm, 
QR factorization, 

398-399 
392-394 

least squares and, 582-583 
modified, 396-398 
equation(s), D6 

Quadratic 

graphing, 

4 15-423 

Quadratic 

form, 408-41 6  

4 1 3  

indefinite, 
matrix associated with, 409 
negative 
negative 
positive definite, 
4 1 3  
positive semidefinite, 

definite, 
4 1 3  
semidefinite, 

4 1 3  

4 1 3  

mean, 550 
Quadratic 
Quadric surface, 420 
Quotient of complex numbers, 

C2,C5 

R IR, 4 IR3, 8 IR", 9-1 1  

1-3 

Racetrack game, 
Range, 2 1 2 ,  482 
Rank 

356-358 
D5 

Rank Theorem, 72, 205, 386, 486 
Ranking vector, 
Rational 
Rayleigh, 
Rayleigh 
Real axis, C l  
Real part 

Roots Theorem, 
Baron, 3 1 6  
quotient, 
3 1 6  

o f  a complex 

number, C l  
Recurrence 
solution 

relation, 
of, 337 

336 

Reduced row echelon form, 73 
Reducible 
matrix, 334 
Reflection, 
2 1 5  
Regular graph, 361 
Regular matrix, 325 
Repeller, 
Resolving 
Resultants, 
Right singular 
Robotics, 
Root, for a polynomial 
Root mean 
square error, 6 1 2  
Rotation, 

352 
a vector, 
50 

vectors, 
593 

226-229 

2 16-21 8  

51 

center of, 5 1 6  
Rotational 
symmetry, 5 1 6  
Roundoff error, 6 2 ,  83-84 
Row echelon form, 65 
Row equivalent 
Row matrix, 138 
Row-matrix 

matrices, 

68 

product, 
146 
66 

Row reduction, 
Row space, 195 
Row vector, 

3, 138 

equation, 

D4 

representation 

of a matrix 

Index 1 5  

s Saddle point, 

352 

8 

Scalar, 
Scalar matrix, 139 
Scalar multiple, 
481 
Scalar multiplication, 

7 - 8 ,  9, 1 4 0 ,  429 

closure under, 1 92, 429 

Erhard, 390 

Scaling, 3 1 4  
Schmidt, 
Schur complement, 
283 
Schur, Issai, 283 
Schur's Triangularization 
Theorem, 408 
Schwarz, 
539 
Seidel, 
Seki Kowa, Takakazu, 
280 
Set(s), A l -A4 
A4 

Philipp Ludwig, 125 

of, A 1 

disjoint, 
elements 
empty,A2 
intersection 
subset of, A2 
union of, A4 

of, A4 

Karl Herman Amandus, 

power method, 3 1 8- 3 1 9  

Shifted inverse 
Similar matrices, 301-303, 508 
Simple path, 242 
Singular 

value decomposition 

(SVD), 590-599 

squares approximation, 

603-605 

of, 599-606 
number, 602 

applications 
and condition 
and least 
and matrix norms, 600-602 
outer product form of, 596 
and polar decomposition, 
6 1 0  
and pseudoinverse, 
602-603 
and rank, 600 

values, 
590-591 
593 
vectors, 
a matrix, 138 

Singular 
Singular 
Size of 
Skew lines, 
Skew-symmetric 
Solution 

matrix, 1 62 

76 

57 4-582 

equation, 
5 1 8  

of a differential 
least squares, 
of a linear system, 59 
minimum length least squares, 
of a recurrence 
of a system of differential 

337 
equations, 

relation, 

603 

340-342 

Span,90,1 56, 1 93,438 
Spanning set of vectors, 
Spanning sets, 438-441 
Spectral 
Spectral 

decomposition, 
405 
Theorem, 403 

88-92 

projection 

form of, 405 

355 
355 

Spectrum, 403 
Spiral attractor, 
Spiral repeller, 
Square matrix, 1 39, 374 
Square root of a matrix, 424 
Standard 
Standard 
Standard 
Standard 
State vector, 
2 3 1  
Steady state vector, 
2 3 3  

basis, 198, 447 
matrix, 2 1 6  
position, 
unit vectors, 
22 

4 

16  Index 

232 
dominant matrix, 324 

diagonally 
John William, 
3 1 6  

Stochastic matrix, 
Strictly 
Strutt, 
Submatrices, 
Subset, 
Subspace(s), 

1 92, 433-438 

145 

A2 

fundamental, 
380 
spanned by a set of vectors, 
sum of, 442 
43 7 
trivial, 
zero, 437 

1 92-1 93, 441 

Subtraction 

of complex numbers, C2 
of matrices, 
140 
of polynomials, 
D2 
8, 433 
of vectors, 

Sum 

ns, 481 

of complex numbers, C l  
oflinear transformatio
of matrices, 
140 
D2 
of polynomials, 
of subspaces, 
442 
of vectors, 

5-6, 9, 439 

A4-A7 
policy, 
harvesting 
360 
James Joseph, 
206, 280 

Sum norm, 552 
Summation notation, 
Sustainable 
Sylvester, 
Symmetric matrix, 1 5 1 - 1 52, 1 60-1 6 1  
System of linear 
System(s) of linear 

equations. See Linear 

T Tail of a vector,  3 

differential 

system(s) 

equations, 

340-348 

530 

529-5 3 1  

Olga, 320 
Taussky-Todd, 
Taxicab circle, 
530 
Taxicab distance, 
Taxicab norm, 530 
Taxicab perpendicular bisector, 
Taxicab pi, 530 
Taylor polynomial, 
Terminal point of a vector, 
Ternary vector, 
Theorem, 1 0  
Tiling, 
5 1 5  
Tournament, 
244 
Trace of 
Transformation, 

a matrix, 162 
2 1 2  

472 

1 6  

3 

2 13-214, 472-474 

linear, 
matrix, 2 1 1 -2 1 6, 472 
matrix, 2 3 1  
probabilities, 
symmetry, 5 1 6  

Transitional 
Transitional 
Translational 
Transpose 
Triangle 

o f  a matrix, 1 5 1 ,  159-160 

inequality, 22, 540, 552 

2 3 0  

matrix, 1 8 1  

Vector form of the equation 
o f  a 

6 1 4  

subspace, 
437 

Trigonometric 
polynomial, 
Triple scalar product identity, 2 8 7  
Trivial 
Turing, Alan Mathison, 1 8 1  

u Uniform norm, 553 

21 

21, 535 

Union of sets, A4 
Unit circle, 
Unit lower triangular 
Unit sphere, 535 
Unit vector, 
Unitarily 
Unitary matrix, 545-546 
Upper triangular 

v Vandermonde, 

block, 283 

matrix, 162 

Alexandre-Theophile, 
291 

Vandermonde determinant, 
Vector(s), 

3, 9, 429, 439 

291 

diagonalizable 

matrix, 546-547 

48-49, 286-287 

23-24, 535 

35, 39 

of, 5-6, 9, 439 
properties 
of, 10 

addition 
algebraic 
angle between, 24-26 
binary, 14 
column, 3, 138 
complex, 429, 432, 543-544 
complex dot product of, 543 
of, 3 
components 
coordinate, 
208, 448-452 
cross product of, 
demand, 236 
direction, 
distance 
between, 
dot product of, 1 8-20 
equality 
of, 3 
force, 50-53 
inner product 
of, 5 3 1  
length of, 2 0 ,  5 3 5  
linear combination 
linearly 
dependent, 
linearly 
independent, 
norm of, 20, 
normal, 34, 38 
orthogonal, 
orthonormal, 
parallel, 
population 
price, 235 
probabili
production, 
ranking, 
356-358 
resultant, 
50 
row, 3, 138 
scalar multiplication 

ty, 231 
236 

372, 537 

535, 552 

8 

distribution, 

240 

of, 7-8, 9, 429 

of, 12, 433 
92-93, 443, 446 

92-97, 443, 446 

26, 369-373, 535, 537 

span of, 438 
spanning sets of, 88-92 
state, 
2 3 1  
steady-state, 
ternary, 
unit, 2 1 ,  5 3 5  
zero, 4, 429 

2 3 3  

1 6  

Vector form of the equation 
of a 

line,36,4 1  

plane, 39, 4 1  

Vector space(s), 429 

446 

460 

of, 453 

A2-A3 

543-544 

493-495 
433-438 

basis for, 
complex, 429,  432, 
dimension 
finite-dimensional, 453 
infinite-dimensional, 453 
isomorphic, 
subspace of, 
over "ll.P, 429, 432 
Venn diagram, 
Venn, John, A2 
Vertex of a graph, 
242 

Weighted dot product, 532 
Well-conditioned 
Wessel, Caspar, Cl 
Wey!, Hermann, 429 
Wheatstone 
Wilson, Edwin B., 49 
Wronskian, 
457 

w Weight of a  magic square, 
x x-axis, 3 
y y-axis, 3 
z "ll., 14 

xy-plane, 8 
xz-plane, 

bridge circuit, 

yz-plane, 8 

matrix, 561 

8 

105-106 

"ll.,, 14 
"ll.�, 14 
"ll.m> 1 6  
"ll.�, 1 6  
Z-axis, 8 
Zero  matrix, 
Zero of a polynomial, 
Zero subspace, 
437 
Zero transformation, 
Zero vector, 

4, 429 

141 

474 

D4 

