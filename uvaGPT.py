import argparse
from dotenv import load_dotenv
import faiss
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import os
import pickle
from pdfminer.high_level import extract_text
from transformers import GPT2LMHeadModel, GPT2TokenizerFast


# Returns a list of all available models.
def get_all_models():
    return [m.removesuffix(".pdf") for m in os. listdir("PDFs/") if m != ".DS_Store"]


# Parser to handle the model argument.
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('-m', '--model',
                        type=str,
                        required=True,
                        choices= ALL_MODELS,
                        help=f'The model (course) you want to query. \
                        Model must be one of these values: {ALL_MODELS}.')

    return parser.parse_args().model


class UvaGPT:
    def __init__(self, model="arco"):
        load_dotenv()
        # Checks for openAI API key.
        if os.environ.get('OPENAI_API_KEY') is None:
            print("Please provide a valid OpenAI API key in the .env file. See .env.example for more information")
            exit(1)

        self.model = model
        self.pdf_file = f"PDFs/{model}.pdf"
        self.txt_file = f"txts/{model}.txt"
        self.db_name = f"DBs/{model}.pkl"
        self.chunks_path = f"chunks/{model}_chunks.index"

        if not os.path.isfile(self.txt_file):
            print(f"Creating {self.txt_file}...")
            self.parse_pdf()
        if not os.path.isfile(self.db_name):
            print(f"Splitting {self.txt_file}...")
            chunks = self.split_txt()
            print(f"Creating {self.db_name}...")
            self.create_faiss_db(chunks)

        self.load_faiss_db()
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        self.qa_chain = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.05), self.db.as_retriever(), memory=memory)


    def run(self):
        print("Starting chat. Type 'q' or 'exit' to quit.")
        while True:
            query = input(f"Model: {self.model}GPT. Ask your question: ")
            if not query:
                continue
            if query == "q" or query == "exit":
                return
            res = self.search(query)
            print(res)
            query = ""


    # Converts pdf file to txt file.
    def parse_pdf(self):
        text = extract_text(self.pdf_file)
        with open(self.txt_file, 'w+') as f:
            f.write(text)


    # Splits txt file into chunks.
    def split_txt(self):
        with open(self.txt_file, "r") as f:
            text = f.read()

        text_splitter = CharacterTextSplitter(chunk_size=1250, separator="\n\n")
        chunks = []
        splits = text_splitter.split_text(text)
        chunks.extend(splits)

        return chunks


    # Creates and saves Faiss db from the chunks generated by split_txt().
    def create_faiss_db(self, chunks):
        store = FAISS.from_texts(chunks, OpenAIEmbeddings())
        faiss.write_index(store.index, self.chunks_path)
        store.index = None
        with open(self.db_name, "wb") as f:
            pickle.dump(store, f)


    # Loads Faiss db into memory.
    def load_faiss_db(self):
        index = faiss.read_index(self.chunks_path)

        with open(self.db_name, "rb") as f:
            db = pickle.load(f)

        db.index = index
        self.db = db


    # Searches for the query in the Faiss db.
    def search(self, query):
        result = self.qa_chain({"question": query})
        return result["answer"]


ALL_MODELS = get_all_models()
ALL_MODELS.sort()
model = parse_args()
uva = UvaGPT(model=model)
uva.run()
